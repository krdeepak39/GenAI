{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62de2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec94c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(page_content=\"This is a pdf loader to get the information.\", metadata={\n",
    "    \"source\": \"test_source.txt\", \n",
    "    \"author\": \"Deepak\",\n",
    "    \"page\": 1,  \n",
    "    \"data\": \"2024-06-10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f87c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06a0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "#creating a simple text document\n",
    "\n",
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a966d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader =TextLoader(\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bacaeba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in directory: 2\n"
     ]
    }
   ],
   "source": [
    "dir_loader = DirectoryLoader(\"../data/text_files\", \n",
    "                             glob=\"*.txt\", \n",
    "                             loader_cls=TextLoader, \n",
    "                             loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "dir_documents = dir_loader.load()\n",
    "print(f\"Number of documents in directory: {len(dir_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3bfbeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 0}, page_content='DATA\\nSCIENCE\\nFREE\\n2024 EDITION\\nFULL ARCHIVE\\n Avi Chawla\\nDailyDoseofDS.com\\nDaily Dose of\\nData Science'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 1}, page_content='DailyDoseofDS.com\\nЕoٷ قo זaׁԷ قhԶ זoءق \\u05ebuف \\u05ebf قh֕آ ӹoתׂ\\nӞnԟ ځoٛ؟ قiוԷ?\\nReading time of this book is about 9-10 hours. But not all chapters will be of\\nrelevance to you. This 2-minute assessment will test your current expertise and\\nrecommend chapters that will be most useful to you.\\nScan the QR code below or open this link to start the assessment. It will only take\\n2 minutes to complete.\\nhttps://bit.ly/DS-assessment\\n1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 2}, page_content='DailyDoseofDS.com\\nҏaӸ\\u05cee \\u05ebf Ԏoכقeכكآ\\nѽeԍقiתל ש1\\u0602 όeԶ\\u05fb \\u05ceeӝ؟n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ت\\n\\u05f8.\\u05f7\\u0603 ѐeӝ؟n֕לg Ѯa؞Ӟd֕ճmء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ Ղ\\nҏrӝלsզԷr ѐeӝ؟n֕לgԝ ϾiכԷ-فٜn֕לgԝ іu\\u05cdقiفӞsׁ ѐeӝ؟n֕לg Ӟnԟ ϾeԟԷrӝقeԟ ѐeӝ؟n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊ Ղ\\nОnف؟oԟٜcف֖oכ قo ϾeԟԷrӝقeԟ ѐeӝ؟n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊\\u05f82\\nλu֕\\u05ced֕לg іu\\u05cdقi֓قaءׂ ѐeӝ؟n֕לg \\u0601MҎѐ) іoԟԷlء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊\\u05f85\\nίcف֖vԶ ѐeӝ؟n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉\\u05f89\\n\\u05f8.ٖ\\u0603 Ѻuכ֔t֕זe Ӟnԟ іeו\\u05ebrڀ ѡpف֖m֕ړaف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ٗ7\\nіoוԷnفٜm؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ7\\nіiٺԷd ѮrԶԎiء֖oכ ҏrӝ֖n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي3\\n ЄrӝԠiԶלt ρhԶԎk\\u05fa\\u05ebiכقiכճ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.հ1\\nЄrӝԠiԶלt ίcԍٜmٛ\\u05ceaف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ հ6\\nհ ѽt؞ӞtԶճiԶآ էo؞ іu\\u05cdقi֓ЄPҜ ҏrӝ֖n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉խ2\\n\\u05f8.ى\\u0603 іiءԎe\\u05cd\\u05ceaכԷoٛآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.խ6\\nѐaӸԷl ѽmת\\u05ebt֊֖nղ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉խ6\\nϾoԍӞl \\u05ceoءآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. խ9\\nЕoٷ όrת\\u05fboٛق ίcفٜa\\u05cd\\u05cey ҷo؞ׂsؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ظ4\\nОsءٜeء ٸiف\\u058b όrת\\u05fboٛق ֖n ρNјآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ت0\\nҷhӝق ЕiԟԠeכ ѐaڀԷrء Ӟnԟ ίcف֖vӝقiתל ϾuכԎt֕\\u05ebnء ίcفٜa\\u05cd\\u05cey όo؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ت5\\nѽhٛﬄԷ όaفӞ λeզ\\u05ebrԶ ҏrӝ֖n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ت9\\n\\u05f8.կ\\u0603 іoԟԷl Ԏoו\\u05fbrԶآs֕\\u05ebn؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ Ղ2\\nфnתٸlԶԠgԶ όiءقi\\u05cd\\u05ceaف֖oכ էo؞ іoԟԷl ρoו\\u05fbrԶآs֕\\u05ebn؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ Ղ2\\nίcف֖vӝقiתל Ѯrٛלiכճ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.Ղ9\\n\\u05f8.լ\\u0603 όe\\u05fa\\u05ceoڀזeכق؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ן1\\nόe\\u05fa\\u05ceoڀ іL іoԟԷlء էrתז оu\\u05faځtԶ؟ љoفԷbת\\u05ebk؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ן1\\nհ ҷaڀآ قo ҏeءق іL іoԟԷlء ֖n ѮrתԠuԍقiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ן6\\nұe؞آiתל ρoכقrת\\u05cel֕לg Ӟnԟ іoԟԷl Ѻeղ֖sف؟y؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ ן9\\n\\u05f8.ط\\u0603 ѐLѕآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.\\u05f80ٖ\\nҷhԶ؟e όiԟ قhԶ ЄPҜ іeו\\u05ebrڀ Єoؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f80ٖ\\nϾu\\u05cd\\u05ce-ו\\u05ebdԶ\\u05ce ϾiכԷ-فٜn֕לg ٲs؉ ѐoѹί ٲs؉ ѺAЃ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ \\u05f80Ձ\\nխ ѐLѕ ϾiכԷ-فٜn֕לg ҏeԍ\\u058bn֕؎uԶآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. \\u05f81ى\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 3}, page_content='DailyDoseofDS.com\\nѽeԍقiתל ש2\\u0602 ρlӝآs֕Ԏa\\u05cd іL؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f81ط\\nٗ؊1\\u0602 іL ϾuכԠaוԷnفӞlء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ \\u05f81ة\\nҏrӝ֖n֕לg Ӟnԟ ОnզԷrԶלcԶ ҏiוԷ ρoו\\u05fblԶٻiفځ \\u05ebf \\u05f80 іL ίlղ\\u05ebr֕قhוآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉\\u05f81ة\\nٗ5 іoءق Оm\\u05fa\\u05ebrفӞnف іaف\\u058beוӞt֕Ԏa\\u05cd όeﬁלiف֖oכآ ֖n όaفӞ ѽc֕ԷnԍԸ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. \\u05f81מ\\nЕoٷ قo Ѻe\\u05cd֖aӸ\\u05cey Оm\\u05fa؟oٱԷ ѮrתӹaӸ֖l֕آt֕Ԏ іu\\u05cdقiԍ\\u05ceaءآ-ԍ\\u05ceaءآiﬁԎaف֖oכ іoԟԷlء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊\\u05f82ى\\nӀoٛ؟ ϛnف֖rԶ іoԟԷl Оm\\u05fa؟oٱԷmԶלt ϛﬀ\\u05ebrفآ іiղ\\u058bt λe Єo֕לg ֖n ұa֕ל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉\\u05f82Ձ\\nѐoءآ ϾuכԎt֕\\u05ebn \\u05ebf \\u05f86 іL ίlղ\\u05ebs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ \\u05f83\\u05f7\\n\\u05f80 іoءق ρoוזoכ ѐoءآ ϾuכԎt֕\\u05ebn؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊\\u05f83ٖ\\nЕoٷ قo ίcفٜa\\u05cd\\u05cey ҝsԶ ҏrӝ֖nԝ ұa\\u05cd֖dӝقiתל Ӟnԟ ҏeءق ѽeف؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉\\u05f83ى\\nխ ρrתآs ұa\\u05cd֖dӝقiתל ҏeԍ\\u058bn֕؎uԶآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f83ة\\nҷhӝق ҏo όo ίfفԷr ρrתآs ұa\\u05cd֖dӝقiתל?؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f84ژ\\nόoٛӹlԶ όeءԎeכق ٲs؉ λiӝآ-ҰӞr֕ӟלcԶ ҏrӝԠe֓\\u05ebﬀ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f84լ\\nٗ؊2\\u0602 ѽtӝقiءك֖cӝ\\u05ce Ͼoٛלdӝقiתלs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ \\u05f84Ձ\\nіLϚ ٲs؉ ϛM — ҷhӝق’ء قhԶ όiﬀԷrԶלcԶؑ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ \\u05f84Ձ\\nρoכﬁԠeכԎe ОnفԷrٱӞl Ӟnԟ ѮrԶԠiԍقiתל ОnفԷrٱӞl؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f85ى\\nҷhڀ ֖s ѡLѼ ρa\\u05cd\\u05ceeԟ Ӟn ҝnӸ֖aءԷd ϛsف֖mӝقo؞ؑ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.\\u05f85מ\\nλhӝقtӝԎhӝ؟yڀӞ όiءقaכԎe؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ \\u05f86լ\\nҷhڀ ѮrԶէe؞ іa֊ӞlӝלoӸ֖s όiءقaכԎe ѡvԶ؟ ϛuԍ\\u05ceiԟԷaכ ԠiءقaכԎeؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. \\u05f86מ\\n\\u05f81 ҷaڀآ قo όeفԷrו֖nԶ όaفӞ љo؞זa\\u05cd֖tڀ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ \\u05f87ى\\nѮrתӹaӸ֖l֕قy ٲs؉ ѐiׁԷl֕\\u058boתԠ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.\\u05f87Ձ\\n\\u05f81 фeڀ ѮrתӹaӸ֖l֕قy όiءقr֕ӹuف֖oכآ ֖n όaفӞ ѽc֕ԷnԍԸ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.\\u05f88ى\\nί ρoוזoכ іiء֖nفԷr\\u05fa؟eفӞt֕\\u05ebn \\u05ebf ρoכقiכٜoٛآ ѮrתӹaӸ֖l֕قy όiءقr֕ӹuف֖oכآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. \\u05f88מ\\nٗ؊3\\u0602 Ͼeӝقu؞Է όeﬁלiف֖oכԞ ϛnղ֖nԶԷr֕לg Ӟnԟ ѽe\\u05cdԷcف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉\\u05f89լ\\n\\u05f81 ҏy\\u05faԷs \\u05ebf ұa؞֖aӸ\\u05ceeء ֖n Ӟ όaفӞsԶق؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. \\u05f89լ\\nρyԍ\\u05ceiԍӞl էeӝقu؞Է Էnԍ\\u05ebd֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ ٗ0կ\\nϾeӝقu؞Է όiءԎrԶقiڒӞt֕\\u05ebn؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ٗ0Ձ\\nت ρaفԷgת؟iԍӞl όaفӞ ϛnԍ\\u05ebd֕לg ҏeԍ\\u058bn֕؎uԶآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ ٗ1ٖ\\nѽhٛﬄԷ Ͼeӝقu؞Է Оm\\u05fa\\u05ebrفӞnԍԷ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ1լ\\nҏhԶ Ѯrתӹe іeف\\u058boԟ էo؞ Ͼeӝقu؞Է ѽe\\u05cdԷcف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ٗ1Ձ\\nٗ؊4\\u0602 Ѻeղ؟eءآiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ2\\u05f7\\nҷhڀ іeӝל ѽqٛӞrԶԠ ϛr؞\\u05ebr \\u0601MѼϛ)ؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ2\\u05f7\\nѽk\\u05cdԷa؞ל ѐiכԷa؞ Ѻeղ؟eءآiתל Еaء љo Еy\\u05faԷr\\u05faӞrӝזeفԷrء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ2լ\\nѮo֕آsתל Ѻeղ؟eءآiתל ٲs؉ ѐiכԷa؞ Ѻeղ؟eءآiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ٗ2מ\\nЕoٷ قo λu֕\\u05ced ѐiכԷa؞ іoԟԷlءؑ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ3ٖ\\n3'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 4}, page_content=\"DailyDoseofDS.com\\nόuוזy ұa؞֖aӸ\\u05cee ҏrӝ\\u05fb؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ3լ\\nұiءٜa\\u05cd\\u05cey ίsءԷsء ѐiכԷa؞ Ѻeղ؟eءآiתל Ѯe؞էo؞זaכԎe؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ ٗ3ط\\nѽtӝقsו\\u05ebdԶ\\u05ce Ѻeղ؟eءآiתל ѽuוזa؞ځ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ3Ձ\\n ЄeכԷrӝ\\u05ceiڒԷd ѐiכԷa؞ іoԟԷlء \\u0601Gяіs\\u0602؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ4լ\\nӒe؞\\u05eb-֕לﬂӞtԶԠ Ѻeղ؟eءآiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ4מ\\nЕuӸԷr Ѻeղ؟eءآiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ٗ5ٖ\\nٗ؊5\\u0602 όeԍ֖s֕\\u05ebn ҏrԶԷs Ӟnԟ ϛnءԷmӸ\\u05cee іeف\\u058boԟآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ5ط\\nρoכԠeכآe ѺaכԠoו Ͼo؞Էsف ֖nف\\u05eb Ӟ όeԍ֖s֕\\u05ebn ҏrԶԷ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ5ط\\nҏrӝלsզ\\u05ebrו όeԍ֖s֕\\u05ebn ҏrԶԷ ֖nف\\u05eb іaف؟iٺ ѡpԶ؟aف֖oכآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ٗ6ٖ\\nОnفԷrӝԎt֕ٲe\\u05cdځ Ѯrٛלe Ӟ όeԍ֖s֕\\u05ebn ҏrԶԷ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ٗ7ٖ\\nҷhڀ όeԍ֖s֕\\u05ebn ҏrԶԷs іuءق λe ҏhת؟oٛճh\\u05cdځ Оnء\\u05fbeԍقeԟ ίfفԷr ҏrӝ֖n֕לg؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ٗ7լ\\nόeԍ֖s֕\\u05ebn ҏrԶԷs ίLҶΰӀS ѡvԶ؟ﬁق!؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ7מ\\nѡOκ ұa\\u05cd֖dӝقiתל ֖n ѺaכԠoו Ͼo؞Էsف؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ٗ8ٖ\\nҏrӝ֖n ѺaכԠoו Ͼo؞Էsف \\u05ebn ѐa؞ճe όaفӞsԶقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ٗ8ط\\nί ұiءٜa\\u05cd Єu֕Ԡe قo ίdӝλoתآt؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ٗ8מ\\nٗ؊6\\u0602 όiוԷnء֖oכӞl֕قy Ѻeԟٜcف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ٗ9լ\\nҏhԶ ҝt֕\\u05ceiفځ \\u05ebf ؙVӝ؟iӝלcԶ؛ ֖n ѮCή؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ٗ9լ\\nфe؞לe\\u05cdѮCή ٲs؉ ѮCή؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ٗ9Ձ\\nѮCή ֖s לoف Ӟ ұiءٜa\\u05cd֖zӝقiתל ҏeԍ\\u058bn֕؎uԶ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي0ٖ\\nق-ѼљE ٲs؉ ѽNϚ — ҷhӝق'ء قhԶ ԠiﬀԷrԶלcԶؑ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي0ط\\nЕoٷ ҏo ίvת֖d Єeفقiכճ іiء\\u05ceeԟ ӹy ق-ѼљE Ѯrתֹeԍقiתלsؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي1ٖ\\nίcԍԷlԶ؟aفԷ قSјϛ ٸiف\\u058b ЄPҜ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي1լ\\nѽcӝ\\u05cee قSјϛ قo іi\\u05cd\\u05ceiתלs \\u05ebf όaفӞ Ѯo֕לtء ҷiف\\u058b \\u05ebpԶלTѼљE؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي1Ձ\\nѮCή ٲs؉ ق-ѼљE؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي2ژ\\nٗ؊7\\u0602 ρlٛآtԶ؟iכճ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي2ى\\nλeڀ\\u05ebnԟ фMԶӞnءԜ ظ іuءق-уלoٷ ҏy\\u05faԷs \\u05ebf ρlٛآtԶ؟iכճ ίlղ\\u05ebr֕قhוآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي2ى\\nОnف؟iכآiԍ іeӝآu؞Էs էo؞ ρlٛآtԶ؟iכճ ϛvӝ\\u05ceuӝقiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي2կ\\nλrԶӞt֊֖nղ фMԶӞnء ٲs фMԶӞnء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ي3ژ\\nЕoٷ όoԶآ іiכ֖Bӝقc֊фMԶӞnء ҷo؞ׂsؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ي3լ\\nίNј֔d؞֖vԶל фMԶӞnء ٸiف\\u058b Ͼa֕آs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ي4ژ\\nфMԶӞnء ٲs؉ Єaٛآs֕Ӟn іiٺقu؞Է іoԟԷlء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي4ٖ\\nόBѼρAј،+ԛ ί Ͼaءقe؞ Ӟnԟ ѽcӝ\\u05ceaӸ\\u05cfԷ ίlفԷrכӞt֕ٲe قo όBѼρAј؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي4լ\\nЕDκѽCήљ ٲs؉ όBѼρAј؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي4Ձ\\n4\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 5}, page_content='DailyDoseofDS.com\\nٗ؊8\\u0602 ρo؞؟e\\u05cdӞt֕\\u05ebn Ӟnӝ\\u05ceyء֖s؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. ي5ژ\\nρo؞؟e\\u05cdӞt֕\\u05ebn ե= ѮrԶԠiԍقiٱԷnԶآs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي5ژ\\nλeٷӞrԶ \\u05ebf ѽuוזa؞ځ ѽtӝقiءك֖cء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي5ى\\nѮeӝ؟sתל Ԏo؞؟e\\u05cdӞt֕\\u05ebn Ԏaכ \\u05ebn\\u05cdځ זeӝآu؞Է \\u05ceiכԷa؞ Ӟsء\\u05ebc֕Ӟt֕\\u05ebn؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي5լ\\nρo؞؟e\\u05cdӞt֕\\u05ebn ٸiف\\u058b ѡrԟ֖nӝ\\u05ce ρaفԷgת؟iԍӞl όaفӞ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي5Ձ\\nٗ؊9\\u0602 όr֕էt؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي6\\u05f7\\nіu\\u05cdقiٱӞr֕ӟقe ρoٱӞr֕ӟقe ѽh֕էt؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي6\\u05f7\\nҝs֕לg Ѯrתٻy֓ѐaӸԷl\\u05cd֖nղ قo ОdԶלt֕էy όr֕էt؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ ي7\\u05f7\\nٗ؊1ژ\\u0603 ׂNј؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ ي7լ\\nҝs֕לg ׂNјآ \\u05ebn ОmӸӞlӝלcԶԠ όaفӞsԶقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي7լ\\nίp\\u05fa؟oٺ֖mӝقe љeӝ؟eءق љe֕ճhӸ\\u05ebr ѽeӝ؟c֊ ҝs֕לg ОnٱԷrفԸԠ Ͼi\\u05cdԷ ОnԟԷx؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي8\\u05f7\\nٗ؊1\\u05f7\\u0603 фe؞לe\\u05cdآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي8լ\\nҷhڀ ֖s фe؞לe\\u05cd ҏr֕Ԏk ρa\\u05cd\\u05ceeԟ Ӟ ؓT؞֖cׁؓ?؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.ي8լ\\nҏhԶ іaف\\u058beוӞt֕Ԏs λe֊֖nԟ ѺBϽ фe؞לe\\u05cd؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي8Ձ\\nٗ؊1ٖ\\u0603 іiءآiכճ όaفӞ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي9\\u05f7\\nي ҏy\\u05faԷs \\u05ebf іiءآiכճ ұa\\u05cdٜeء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ي9\\u05f7\\nіiءآFת؟eءق Ӟnԟ ׂNј Оm\\u05faٜtӝقiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ي9ط\\nٗ؊1ى\\u0603 Ѯiفէa\\u05cd\\u05ces Ӟnԟ іiءԎoכԏԷpف֖oכآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ հ0\\u05f7\\nҷhԶל ֖s ѺaכԠoו ѽp\\u05cd֖tف֗לg ϾaفӞl էo؞ іL іoԟԷlءؑ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ0\\u05f7\\nϾeӝقu؞Է ѽcӝ\\u05ceiכճ ֖s љOҎ ίlٷӞyء љeԍԷsءӞrڀ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ0ط\\nί іiءԎoכԏԷpف֖oכ ίbתٜt ѐoղ ҏrӝלsզ\\u05ebrו؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉հ0Ձ\\nҏhԶ ҏrٛԷ Ѯu؞\\u05fboءԷ \\u05ebf Ͼeӝقu؞Է ѽcӝ\\u05ceiכճ Ӟnԟ ѽtӝלdӝ؟d֕ړaف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.հ1\\u05f7\\nѐ2 Ѻeղٜlӝ؟iڒӞt֕\\u05ebn ֖s љoف оuءق ҝsԶԠ էo؞ Ѻeղٜlӝ؟iڒӞt֕\\u05ebn؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.հ1կ\\nٗ؊1կ\\u0603 іiءԎe\\u05cd\\u05ceaכԷoٛآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ հ2ژ\\nОs Ӏoٛ؟ іoԟԷl όaفӞ όeﬁԎiԶלtؐ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ2ژ\\nλaڀԷs֕Ӟn ѡpف֖m֕ړaف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ հ2կ\\nҏrӝ֖n Ӟnԟ ҏeءق-ف֖mԶ όaفӞ ίuղזeכقaف֖oכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ հ2ط\\nٗ؊1լ\\u0603 όaفӞ ίnӝ\\u05ceyء֖s؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ3ژ\\n\\u05f85 ѮaכԠaء ↔Ѯo\\u05cdӞrء ↔ѽQя ↔ѮyѼ\\u05fba؞ׂ ҏrӝלs\\u05cdӞt֕\\u05ebnء؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.հ3ژ\\nٗ ίlفԷrכӞt֕ٲeء قo ѮaכԠaء؛ όeءԎr֕ӹe؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ3ى\\nίcԍԷlԶ؟aفԷ ѮaכԠaء ٸiف\\u058b ЄPҜ ҝs֕לg ѺAѭОDѼ ԎuϋϾ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉հ3ط\\nіiءآiכճ όaفӞ ίnӝ\\u05ceyء֖s ٸiف\\u058b Еeӝقmӝ\\u05fbs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ3Ձ\\nόaفӞF؞ӞmԶ ѽtڀ\\u05ceiכճ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ4ٖ\\n5'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 6}, page_content='DailyDoseofDS.com\\nՂ ίuف\\u05ebmӝقeԟ ϛDή ҏoת\\u05ces؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉հ4ى\\nٗ؊1ط\\u0603 όaفӞ ұiءٜa\\u05cd֖sӝقiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ հ4կ\\nіoءق Оm\\u05fa\\u05ebrفӞnف Ѯlתقs ֖n όaفӞ ѽc֕ԷnԍԸ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ4կ\\nЕoٷ ӞrԶ ѷQ Ѯlתقs ρrԶӞtԶԠ?؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ հ5\\u05f7\\nՂ ϛlԶճaכق ίlفԷrכӞt֕ٲeء قo ҏrӝԠiف֖oכӞl Ѯlתقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉հ5ط\\nОnفԷrӝԎt֕ٲe ρoכقrת\\u05ces؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.հ6կ\\nіoءӞiԍ Ѯlתقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ6լ\\nϛn؞֖c֊ іaف\\u05fblתقl֕ӹ Ѯlתقs ٸiف\\u058b ОnءԷt ίx֕آ Ӟnԟ ίnכ\\u05ebtӝقiתלs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ6ط\\nѮrתէeءآiתלa\\u05cd֖zԶ іaف\\u05fblתقl֕ӹ Ѯlתقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.հ6מ\\nѽaכׂeڀ όiӝճrӝזs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ հ7\\u05f7\\nѺiԟճe\\u05cd֖nԶ Ѯlתقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ7կ\\nѽpӝ؟k\\u05cd֖nԶ Ѯlתقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ7ة\\nٗ؊1ة\\u0603 ѽQя؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ7מ\\nЄrתٜp֕לg ѽeفآ, Ѻo\\u05cd\\u05ceu\\u05fa Ӟnԟ ρuӸԷ ֖n ѽQя؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉հ7מ\\nѽeו֖, ίnف֖, Ӟnԟ љaفٜrӝ\\u05ce оo֕לs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊հ8լ\\nҝsԶ ѽQя ؕNѠҏ ОNؖ ҷiف\\u058b ρaٛقiתל؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ9\\u05f7\\nٗ؊1Ձ\\u0603 Ѯyف\\u058boכ ѡOѭ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊ հ9ط\\nЄeفقe؞آ Ӟnԟ ѽeفقe؞آ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ9ط\\nόeءԎr֕\\u05fbtת؟s ֖n Ѯyف\\u058boכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊. հ9מ\\nٗ0 іoءق ρoוזoכ іaղ֖c іeف\\u058boԟآ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉խ0מ\\nіeו\\u05ebrڀ ϛﬃԎiԶלt ρlӝآs ѡbָԷcفآ ٜs֕לg ѽlתقs؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.խ1ژ\\nҷhڀ όoכ؝t ҷe Оnٱ\\u05ebkԶ זoԟԷl؉էo؞ٸa؞Ԡ(\\u0602 ֖n ѮyҎ\\u05ebrԍ\\u058b?؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊խ1լ\\nҏrٛԷ ѡOѭ ϛnԍӞpءٜlӝقiתל ֖s іiءآiכճ Ͼrתז Ѯyف\\u058boכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.խ2ژ\\nί ρoוזoכ іiءԎoכԏԷpف֖oכ ίbתٜt ٧_֕לiف٧_\\u0600\\u0603؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉ խ2կ\\nϾuכԎt֕\\u05ebn ѡvԶ؟lתӞd֕לg ֖n Ѯyف\\u058boכ؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉؊.؉խ2ة\\n6'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 7}, page_content='DailyDoseofDS.com\\nόeԶ\\u05fb\\n\\u05ceeӝ؟n֕לg\\n7'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 8}, page_content='DailyDoseofDS.com\\nѐeӝ؟n֕לg Ѯa؞Ӟd֕ճmء\\nҏrӝלsզԷr ѐeӝ؟n֕לgԝ ϾiכԷ-فٜn֕לgԝ іu\\u05cdقiفӞsׁ ѐeӝ؟n֕לg\\nӞnԟ ϾeԟԷrӝقeԟ ѐeӝ؟n֕לg\\nMost ML models are trained independently without any interaction with other\\nmodels. However, in the realm of real-world ML, there are many powerful\\nlearning techniques that rely on model interactions to improve performance.\\nThe following\\nimage\\nsummarizes\\nfour such\\nwell-adopted\\nand must-know\\ntraining\\nmethodologies:\\n8'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 9}, page_content='DailyDoseofDS.com\\nש1\\u0602 ҏrӝלsզԷr \\u05ceeӝ؟n֕לg\\nThis is extremely useful when:\\n●The task of interest has less data.\\n●But a related task has abundant data.\\nThis is how it works:\\n●Train a neural network model (base model) on the related task.\\n●Replace the last few layers on the base model with new layers.\\n●Train the network on the task of interest, but don’t update the weights of\\nthe unreplaced layers during backpropagation.\\nBy training a model on the related task ﬁrst, we can capture the core patterns of\\nthe task of interest. Later, we can adjust the last few layers to capture\\ntask-speciﬁc behavior.\\nAnother idea which is somewhat along these lines is knowledge distillation,\\nwhich involves the “transfer” of knowledge. We will discuss it in the upcoming\\nchapters.\\nTransfer learning is commonly used in many computer vision tasks.\\n9'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 10}, page_content='DailyDoseofDS.com\\nש2\\u0602 ϾiכԷ-فٜn֕לg\\nFine-tuning involves updating the weights of some or all layers of the pre-trained\\nmodel to adapt it to the new task.\\nThe idea may appear similar to transfer learning, but in ﬁne-tuning, we typically\\ndo not replace the last few layers of the pre-trained network.\\nInstead, the pretrained model itself is adjusted to the new data.\\nש3\\u0602 іu\\u05cdقi֓قaءׂ \\u05ceeӝ؟n֕לg\\nAs the name suggests, a model is trained to perform multiple tasks\\nsimultaneously.\\nThe model shares knowledge across tasks, aiming to improve generalization and\\nperformance on each task.\\n10'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 11}, page_content='DailyDoseofDS.com\\nIt can help in scenarios where tasks are related, or they can beneﬁt from shared\\nrepresentations.\\nIn fact, the motive for multi-task learning is not just to improve generalization.\\nWe can also save compute power during training by having a shared layer and\\ntask-speciﬁc segments.\\n●Imagine training two models independently on related tasks.\\n●Now compare it to having a network with shared layers and then\\ntask-speciﬁc branches.\\nOption 2 will typically result in:\\n●Better generalization across all tasks.\\n●Less memory utilization to store model weights.\\n●Less resource utilization during training.\\nש4\\u0602 ϾeԟԷrӝقeԟ \\u05ceeӝ؟n֕לg\\nLet’s discuss it in the next chapter.\\n11'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 12}, page_content='DailyDoseofDS.com\\nОnف؟oԟٜcف֖oכ قo ϾeԟԷrӝقeԟ ѐeӝ؟n֕לg\\nIn my opinion, federated learning is among those very powerful ML techniques\\nthat is not given the true attention it deserves.\\nHere’s a visual that depicts how it works:\\nLet’s understand this topic in this chapter and why I consider this to be an\\nimmensely valuable skill to have.\\nҏhԶ \\u05fbrתӹlԶז\\nModern devices (like smartphones) have access to a wealth of data that can be\\nsuitable for ML models.\\nTo get some perspective, consider the number of images you have on your phone\\nright now, the number of keystrokes you press daily, etc.\\nThat’s plenty of data, isn’t it?\\nAnd this is just about one user — you.\\nBut applications can have millions of users. The amount of data we can train ML\\nmodels on is unfathomable.\\nSo what is the problem here?\\nThe problem is that almost all data available on modern devices is private.\\n12'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 13}, page_content='DailyDoseofDS.com\\n●Images are private.\\n●Messages you send are private.\\n●Voice notes are private.\\nBeing private, it is likely that it cannot be aggregated in a central location, as\\ntraditionally, ML models are always trained on centrally located datasets.\\nBut this data is still valuable to us, isn’t it?\\nWe want to utilize it in some way.\\nҏhԶ آo\\u05cdٜt֕\\u05ebn\\nFederated learning smartly addresses this challenge of training ML models on\\nprivate data.\\nHere’s the core idea:\\n13'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 14}, page_content='DailyDoseofDS.com\\n●Instead of aggregating data on a central server, dispatch a model to an end\\ndevice.\\n●Train the model on the user’s private data on their device.\\n●Fetch the trained model back to the central server.\\n●Aggregate all models obtained from all end devices to form a complete\\nmodel.\\nThat’s an innovative solution because each client possesses a local training\\ndataset that remains exclusively on their device and is never uploaded to the\\nserver.\\nYet, we still get to train a model on this private data.\\nSend a global model to the user’s device, train a model on private data, and\\nretrieve it back.\\nFurthermore, federated learning distributes most computation to a user’s device.\\nAs a result, the central server does not need the enormous computing that it\\nwould have demanded otherwise.\\nThis is the core idea behind federated learning.\\n14'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 15}, page_content='DailyDoseofDS.com\\nλu֕\\u05ced֕לg іu\\u05cdقi֓قaءׂ ѐeӝ؟n֕לg \\u0601MҎѐ) іoԟԷlء\\nMost ML models are trained on one task. As a result, many struggle to intuitively\\nunderstand how a model can be trained on multiple tasks simultaneously.\\nSo let’s discuss it in this chapter.\\nTo reiterate, in MTL, the network has a few shared layers and task-speciﬁc\\nsegments. During backpropagation, gradients are accumulated from all branches,\\nas depicted in the animation below:\\nLet’s take a simple example to understand its implementation.\\nConsider we want our model to take a real value (x) as input and generate two\\noutputs:\\n●sin(x)\\n●cos(x)\\nThis can be formulated as an MTL problem.\\n15'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 16}, page_content='DailyDoseofDS.com\\nFirst, we deﬁne our model class using PyTorch.\\n●We have some fully connected layers in self.model →These are the shared\\nlayers.\\n●Furthermore, we have the output-speciﬁc layers to predict sin(x) and cos(x).\\nNext, let’s deﬁne the forward\\npass in the class above:\\n●First, we pass the\\ninput through the\\nshared layers\\n(self.model).\\n●The output of the\\nshared layers is passed\\nthrough the sin and\\ncos branches.\\n●We return the output\\nfrom both branches.\\n16'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 17}, page_content='DailyDoseofDS.com\\nWe are almost done. The ﬁnal part of this implementation is to train the model.\\nLet’s use mean squared error as the loss function. The training loop is\\nimplemented below:\\n●We pass the input data through the model.\\n●It returns two outputs, one from each segment of the network.\\n●We compute the branch-speciﬁc loss values (loss1 and loss2) using true\\npredictions.\\n●We add the two loss values to get the total loss for the network.\\n●Finally, we run the backward pass.\\nWith this, we have trained our MTL model. Also, we get a decreasing loss, which\\ndepicts that the model is being trained.\\nAnd that’s how we train an MTL model. You can extend the same idea to build\\nany MTL model of your choice.\\n17'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 18}, page_content='DailyDoseofDS.com\\nDo remember that building an MTL model on unrelated tasks will not produce\\ngood results.\\nThus, “task-relatedness” is a critical component of all MTL models because of\\nthe shared layers. Also, it is NOT necessary that every task must equally\\ncontribute to the entire network’s loss.\\nWe may assign weights to each task as well, as depicted below:\\nThe weights could be based on task importance.\\nOr…\\nAt times, I also use dynamic task weights, which could be inversely proportional\\nto the validation accuracy achieved on that task.\\nMy rationale behind this technique is that in an MTL setting, some tasks can be\\neasy while others can be diﬃcult.\\nIf the model achieves high accuracy on one task during training, we can safely\\nreduce its loss contribution so that the model focuses more on the second task.\\nYou can download the notebook for this chapter here: https://bit.ly/3ztY5hy.\\n18'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 19}, page_content='DailyDoseofDS.com\\nίcف֖vԶ ѐeӝ؟n֕לg\\nThere’s not much we can do to build a supervised system when the data we begin\\nwith is unlabeled.\\nUsing unsupervised techniques (if they ﬁt the task) can be a solution, but\\nsupervised systems are typically on par with unsupervised ones.\\nAnother way, if feasible, is to rely on self-supervised learning.\\nSelf-supervised learning is when we have an unlabeled dataset (say text data), but\\nwe somehow ﬁgure out a way to build a supervised learning model out of it.\\nThis becomes possible due to the inherent nature of the task.\\nConsider an LLM, for instance.\\nIn a nutshell, its core objective is to predict the next token based on previously\\npredicted tokens (or the given context).\\n19'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 20}, page_content='DailyDoseofDS.com\\nThis is a classiﬁcation task, and the labels are tokens.\\nBut text data is raw. It has no labels.\\nThen how did we train this classiﬁcation task?\\nSelf-supervised techniques solve this problem.\\nDue to the inherent nature of the task (next-token prediction, to be speciﬁc),\\nevery piece of raw text data is already self-labeled.\\n20'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 21}, page_content='DailyDoseofDS.com\\nThe model is only supposed to learn the mapping from previous tokens to the\\nnext token.\\nThis is called self-supervised learning, which is quite promising, but it has\\nlimited applicability, largely depending on the task.\\nAt this stage, the only possibility one notices is annotating the dataset. However,\\ndata annotation is diﬃcult, expensive, time-consuming, and tedious.\\nActive learning is a relatively easy, inexpensive, quick, and interesting way to\\naddress this.\\nAs the name suggests, the idea is to build the model with active human feedback\\non examples it is struggling with. The visual below summarizes this:\\n21'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 22}, page_content='DailyDoseofDS.com\\nLet’s get into the details.\\nWe begin by manually labeling a tiny percentage of the dataset.\\nWhile there’s no rule on how much data should be labeled, I have used active\\nlearning (successfully) while labeling as low as ~1% of the dataset, so try\\nsomething in that range.\\nNext, build a model on this small labeled dataset.\\nOf course, this won’t be a perfect model, but that’s okay. Next, generate\\npredictions on the dataset we did not label:\\n22'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 23}, page_content='DailyDoseofDS.com\\nIt’s obvious that we cannot determine if these predictions are correct as we do\\nnot have any labels.\\nThat’s why we need to be a bit selective with the type of model we choose.\\nMore speciﬁcally, we need a model that, either implicitly or explicitly, can also\\nprovide a conﬁdence level with its predictions.\\nAs the name suggests, a conﬁdence level reﬂects the model’s conﬁdence in\\ngenerating a prediction.\\nIf a model could speak, it would be like:\\n●I am predicting a “cat” and am 95% conﬁdent about my prediction.\\n●I am predicting a “cat” and am 5% conﬁdent about my prediction.\\n●And so on…\\nProbabilistic models (ones that provide a probabilistic estimate for each class) are\\ntypically a good ﬁt here.\\n23'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 24}, page_content='DailyDoseofDS.com\\nThis is because one can determine a proxy for conﬁdence level from probabilistic\\noutputs.\\nIn the above two examples, consider the gap between 1st and 2nd highest\\nprobabilities:\\n●In example #1, the gap is large. This can indicate that the model is quite\\nconﬁdent in its prediction.\\n●In example #2, the gap is small. This can indicate that the model is NOT\\nquite conﬁdent in its prediction.\\nNow, go back to the predictions generated above and rank them in order of\\nconﬁdence:\\n24'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 25}, page_content='DailyDoseofDS.com\\nIn the above image:\\n●The model is already quite conﬁdent with the ﬁrst two instances. There’s\\nno point checking those.\\n●Instead, it would be best if we (the human) annotate the instances with\\nwhich it is least conﬁdent.\\nTo get some more perspective, consider the image below. Logically speaking,\\nwhich data point’s human label will provide more information to the model? I\\nknow you already know the answer.\\nThus, in the next step, we provide our human label to the low-conﬁdence\\npredictions and feed it back to the model with the previously labeled dataset:\\nRepeat this a few times and stop when you are satisﬁed with the performance.\\nIn my experience, active learning has always been an immensely time-saving\\napproach to building supervised models on unlabeled datasets.\\nThe only thing that you have to be careful about is generating conﬁdence\\nmeasures.\\n25'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 26}, page_content='DailyDoseofDS.com\\nIf you mess this up, it will aﬀect every subsequent training step.\\nThere’s one more thing I like to do when using active learning.\\nWhile combining the low-conﬁdence data with the seed data, we can also use the\\nhigh-conﬁdence data. The labels would be the model’s predictions.\\nThis variant of active learning is called cooperative learning.\\n26'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 27}, page_content='DailyDoseofDS.com\\nѺuכ֔t֕זe Ӟnԟ іeו\\u05ebrڀ\\nѡpف֖m֕ړaف֖oכ\\nіoוԷnفٜm\\nAs we progress towards building larger\\nand larger models, every bit of possible\\noptimization becomes crucial.\\nAnd, of course, there are various ways to speed up model training, like:\\n●Batch processing\\n●Leverage distributed training using frameworks like PySpark MLLib.\\n●Use better Hyperparameter Optimization, like Bayesian Optimization,\\nwhich we will discuss in this chapter.\\n●and many other techniques.\\nMomentum is another reliable and eﬀective technique to speed up model\\ntraining. While Momentum is pretty popular, many people struggle to intuitively\\nunderstand how it works and why it is eﬀective. Let’s understand in this chapter.\\nОsءٜeء ٸiف\\u058b ЄrӝԠiԶלt όeءԎeכق\\nIn gradient descent, every parameter update solely depends on the current\\ngradient. This is clear from the gradient weight update rule shown below:\\n27'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 28}, page_content='DailyDoseofDS.com\\nAs a result, we end up having many unwanted oscillations during the\\noptimization process.\\nLet’s understand this more visually.\\nImagine this is the loss function contour plot, and the optimal location\\n(parameter conﬁguration where the loss function is minimum) is marked here:\\nSimply put, this plot illustrates how gradient descent moves towards the optimal\\nsolution. At each iteration, the algorithm calculates the gradient of the loss\\nfunction at the current parameter values and updates the weights.\\nThis is depicted below:\\nNotice two things here:\\n●It unnecessarily oscillates vertically.\\n●It ends up at the non-optimal solution aϔer some epochs.\\n28'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 29}, page_content='DailyDoseofDS.com\\nIdeally, we would have expected our weight updates to look this:\\n●It must have taken longer steps in the horizontal direction…\\n●…and smaller vertical steps because a movement in this direction is\\nunnecessary.\\nThis idea is also depicted below:\\nЕoٷ іoוԷnفٜm آo\\u05cdٲeء قh֕آ \\u05fbrתӹlԶז?\\nMomentum-based optimization slightly modiﬁes the update rule of gradient\\ndescent. More speciﬁcally, it also considers a moving average of past gradients:\\n29'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 30}, page_content='DailyDoseofDS.com\\nThis helps us handle the unnecessary vertical oscillations we saw earlier.\\nHow?\\nAs Momentum considers a moving average of past gradients, so if the recent\\ngradient update trajectory looks as shown in the following image, then it is clear\\nthat its average in the vertical direction will be very low while that in the\\nhorizontal direction will be large (which is precisely what we want):\\nAs this moving average gets added to the gradient updates, it helps the\\noptimization algorithm take larger steps in the desired direction.\\nThis way, we can:\\n●Smoothen the optimization trajectory.\\n30'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 31}, page_content='DailyDoseofDS.com\\n●Reduce unnecessary oscillations in parameter updates, which also speeds\\nup training.\\nThis is also evident from the image below:\\nThis time, the gradient update trajectory shows much smaller oscillations in the\\nvertical direction, and it also manages to reach an optimum under the same\\nnumber of epochs as earlier.\\nThis is the core idea behind Momentum and how it works.\\nOf course, Momentum does introduce another hyperparameter (Momentum rate)\\nin the model, which should be tuned appropriately like any other\\nhyperparameter:\\nFor instance, considering the 2D contours we discussed above:\\n31'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 32}, page_content='DailyDoseofDS.com\\n●Setting an extremely large value of Momentum rate will signiﬁcantly\\nexpedite gradient update in the horizontal direction. This may lead to\\novershooting the minima, as depicted below:\\n●What’s more, setting an extremely small value of Momentum will slow\\ndown the optimal gradient update, defeating the whole purpose of\\nMomentum.\\nIf you want to have a more hands-on experience, check out this tool:\\nhttps://bit.ly/4cOrJN1.\\n32'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 33}, page_content='DailyDoseofDS.com\\nіiٺԷd ѮrԶԎiء֖oכ ҏrӝ֖n֕לg\\nρoכقeٺك\\nTypical deep learning libraries are really conservative when it comes to assigning\\ndata types.\\nThe data type assigned by default is usually 64-bit or 32-bit, when there is also\\nscope for 16-bit, for instance. This is also evident from the code below:\\nAs a result, we are not entirely optimal at eﬃciently allocating memory. Of\\ncourse, this is done to ensure better precision in representing information.\\n33'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 34}, page_content='DailyDoseofDS.com\\nHowever, this precision always comes at the cost of additional memory\\nutilization, which may not be desired in all situations.\\nIn fact, it is also observed that many tensor operations, especially matrix\\nmultiplication, are much faster when we operate under smaller precision data\\ntypes than larger ones, as demonstrated below:\\n34'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 35}, page_content='DailyDoseofDS.com\\nMoreover, since ﬂ\\u05ebaف\\u05f86 is only half the size of ﬂ\\u05ebaفي2, its usage reduces the\\nmemory required to train the network. This also allows us to train larger models,\\ntrain on larger mini-batches (resulting in even more speedup), etc.\\nMixed precision training is a pretty reliable and widely adopted technique in the\\nindustry to achieve this.\\nAs the name suggests, the idea is to employ lower precision ﬂ\\u05ebaف\\u05f86 (wherever\\nfeasible, like in convolutions and matrix multiplications) along with ﬂ\\u05ebaفي2 —\\nthat is why the name “mixed precision.”\\nThis is a list of some models I found that were trained using mixed precision:\\nIt’s pretty clear that mixed precision training is much more popularly used, but\\nwe don’t get to hear about it oϔen.\\nλeզ\\u05ebrԶ ٸe ճeف ֖nف\\u05eb قhԶ قeԍ\\u058bn֕Ԏa\\u05cd ԠeفӞi\\u05cdآ…\\nFrom the above discussion, it must be clear that as we use a low-precision data\\ntype (ﬂ\\u05ebaف\\u05f86), we might unknowingly introduce some numerical inconsistencies\\nand inaccuracies.\\n35'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 36}, page_content='DailyDoseofDS.com\\nTo avoid them, there are some best practices for mixed precision training that I\\nwant to talk about next, along with the code.\\nіiٺԷd \\u05fbrԶԎiء֖oכ قrӝ֖n֕לg ֖n ѮyҎ\\u05ebrԍ\\u058b Ӟnԟ λeءق ѮrӝԎt֕ԏԷs\\nLeveraging mixed precision training in PyTorch requires a few modiﬁcations in\\nthe existing network training implementation. Consider this is our current\\nPyTorch model training implementation:\\nThe ﬁrst thing we introduce here is a scaler object that will scale the loss value:\\n36'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 37}, page_content='DailyDoseofDS.com\\nWe do this because, at times, the original loss value can be so low, that we might\\nnot be able to compute gradients in ﬂ\\u05ebaف\\u05f86 with full precision. Such situations\\nmay not produce any update to the model’s weights.\\nScaling the loss to a higher numerical range ensures that even small gradients\\ncan contribute to the weight updates.\\nBut these minute gradients can only be accommodated into the weight matrix\\nwhen the weight matrix itself is represented in high precision, i.e., ﬂoat32. Thus,\\nas a conservative measure, we tend to keep the weights in ﬂ\\u05ebaفي2.\\nThat said, the loss scaling step is not entirely necessary because, in my\\nexperience, these little updates typically appear towards the end stages of the\\nmodel training. Thus, it can be fair to assume that small updates may not\\ndrastically impact the model performance. But don’t take this as a deﬁnite\\nconclusion, so it’s something that I want you to validate when you use mixed\\nprecision training.\\nMoving on, as the weights\\n(which are matrices) are\\nrepresented in ﬂ\\u05ebaفي2,\\nwe can not expect the\\nspeedup from\\nrepresenting them in\\nﬂ\\u05ebaف\\u05f86, if they remain\\nthis way:\\n37'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 38}, page_content='DailyDoseofDS.com\\nTo leverage these ﬂaot16-based speedups, here are the steps we follow:\\n1. We make a ﬂ\\u05ebaف\\u05f86 copy of weights during the forward pass.\\n2. Next, we compute the loss value in ﬂ\\u05ebaفي2 and scale it to have more\\nprecision in gradients, which works in ﬂ\\u05ebaف\\u05f86.\\na. The reason we compute gradients in ﬂ\\u05ebaف\\u05f86 is because, like forward\\npass, gradient computations also involve matrix multiplications.\\nb. Thus, keeping them in ﬂ\\u05ebaف\\u05f86 can provide additional speedup.\\n3. Once we have computed the gradients in ﬂ\\u05ebaف\\u05f86, the heavy matrix\\nmultiplication operations have been completed. Now, all we need to do is\\nupdate the original weight matrix, which is in ﬂ\\u05ebaفي2.\\n4. Thus, we make a ﬂ\\u05ebaفي2 copy of the above gradients, remove the scale we\\napplied in Step 2, and update the ﬂ\\u05ebaفي2 weights.\\n5. Done!\\n38'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 39}, page_content='DailyDoseofDS.com\\nThe mixed-precision settings in the forward pass are carried out by the\\nقo؞Ԏh؉Ӟuف\\u05ebcӝآt\\u0600\\u0603 context manager:\\nNow, it’s time to handle the backward pass.\\n39'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 40}, page_content='DailyDoseofDS.com\\n●Line 13 →آcӝ\\u05cee؞؊sԍӞlԶ\\u0601lתآs\\u0602؊bӝԎkٷӞrԟ\\u0601): The scaler object scales the loss\\nvalue and ӹaԍׂwӝ؟d\\u0600\\u0603 is called to compute the gradients.\\n●Line 14 →آcӝ\\u05cee؞؊sفԷp\\u0600\\u05ebpف\\u0603: Unscale gradients and update weights.\\n●Line 15 →آcӝ\\u05cee؞؊u\\u05faԠaفԷ(\\u0602: Update the scale for the next iteration.\\n●Line 16 →\\u05ebpف؊zԶ؟o٦ճrӝԠ(\\u0602: Zero gradients.\\nDone!\\nThe eﬃcacy of mixed precision scaling over traditional training is evident from\\nthe image below:\\nMixed precision training is over ٗ.լٻ faster than conventional training.\\n40'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 41}, page_content='DailyDoseofDS.com\\n ЄrӝԠiԶלt ρhԶԎk\\u05fa\\u05ebiכقiכճ\\nNeural networks primarily utilize memory in two ways:\\n1. When they store model weights (this is ﬁxed memory utilization).\\n2. When they are trained (this is dynamic). It happens in two ways:\\na. During forward pass while computing and storing activations of all\\nlayers.\\nb. During backward pass while computing gradients at each layer.\\nThe latter, i.e., dynamic memory utilization, oϔen restricts us from training\\nlarger models with bigger batch sizes.\\nThis is because memory utilization scales proportionately with the batch size.\\nThat said, there’s a pretty incredible technique that lets us increase the batch size\\nwhile maintaining the overall memory utilization.\\nIt is called Gradient checkpointing, and in my experience, it’s a highly underrated\\ntechnique to reduce the memory overheads of neural networks.\\nLet’s understand this in more detail.\\n41'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 42}, page_content='DailyDoseofDS.com\\nЕoٷ ճrӝԠiԶלt ԎhԶԏׂpת֖nف֗לg ٸo؞ׂsؐ\\nGradient checkpointing is based on two key observations on how neural\\nnetworks typically work:\\n1) The activations of a speciﬁc layer\\ncan be solely computed using the\\nactivations of the previous layer. For\\ninstance, in the image below, “Layer\\nB” activations can be computed from\\n“Layer A” activations only.\\n2) Updating the weights of a layer only\\ndepends on two things:\\na. The activations of that layer.\\nb. The gradients computed in the\\nnext (right) layer (or rather, the\\nrunning gradients).\\nGradient checkpointing exploits these two observations to optimize memory\\nutilization. Here’s how it works:\\n●Step 1) Divide the network into segments before the forward pass:\\n42'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 43}, page_content='DailyDoseofDS.com\\n●Step 2) During the forward pass, only store the activations of the ﬁrst layer\\nin each segment. Discard the rest when they have been used to compute\\nthe activations of the next layer.\\n●Step 3) Now comes backpropagation. To update the weights of a layer, we\\nneed its activations. Thus, we recompute those activations using the ﬁrst\\nlayer in that segment. For instance, as shown in the image below, to update\\nthe weights of the red layers, we recompute their activations using the\\nactivations of the cyan layer, which are already available in memory.\\nDone!\\nThis is how gradient checkpointing works.\\n43'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 44}, page_content='DailyDoseofDS.com\\nTo summarize, the idea is that we don’t need to store all the intermediate\\nactivations in memory. Instead, storing a few of them and recomputing the rest\\nonly when they are needed can signiﬁcantly reduce the memory requirement. The\\nwhole idea makes intuitive sense as well.\\nThis also allows us to train the\\nnetwork on larger batches of data.\\nTypically, my observation has been\\nthat gradient checkpointing (GCP) can\\nreduce memory usage by at least\\n50-60%, which is massive.\\nOf course, as we compute some activations twice, this does come at the cost of\\nincreased run-time, which can typically range between 15-25%. So there’s always\\na tradeoﬀbetween memory and run-time.\\nThat said, another advantage is that it allows us to use a larger batch size, which\\ncan slightly (not entirely though) counter the increased run-time.\\nNonetheless, gradient checkpointing is an extremely powerful technique to train\\nlarger models, which I have found to be pretty helpful at times, without resorting\\nto more intensive techniques like distributed training, for instance.\\nThankfully, gradient checkpointing is also implemented by many open-source\\ndeep learning frameworks like Pytorch, etc.\\nHere’s a demo.\\n44'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 45}, page_content='DailyDoseofDS.com\\nЄrӝԠiԶלt ԎhԶԏׂpת֖nف֗לg ֖n ѮyҎ\\u05ebrԍ\\u058b\\nTo utilize this, we begin by importing the necessary libraries and functions:\\nNext, we deﬁne our neural network:\\nAs demonstrated above, in the forward method, we use the\\ncheckpoint_sequential method to use gradient checkpointing and divide the\\nnetwork into two segments.\\nNext, we can proceed with network training as we usually would.\\n45'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 46}, page_content='DailyDoseofDS.com\\nЄrӝԠiԶלt ίcԍٜmٛ\\u05ceaف֖oכ\\nUnder memory\\nconstraints, it is\\nalways\\nrecommended to\\ntrain the neural\\nnetwork with a\\nsmall batch size.\\nDespite that, there’s a technique called gradient accumulation, which lets us\\n(logically) increase batch size without explicitly increasing the batch size.\\nConfused?\\nLet’s understand in this chapter. But before that, we must understand…\\nҷhڀ Ԡo לeٛ؟a\\u05cd לeفٸo؞ׂs قy\\u05fa֖cӝ\\u05celڀ Էx\\u05fa\\u05ceoԟԷ Ԡu؞֖nղ قrӝ֖n֕לgؐ\\nThe primary memory\\noverhead in a neural\\nnetwork comes from\\nbackpropagation. This is\\nbecause, during\\nbackpropagation, we must\\nstore the layer activations\\nin memory. Aϔer all, they\\nare used to compute the\\ngradients.\\nThe bigger the network, the more activations a network must store in memory.\\nAlso, under memory constraints, having a large batch size will result in:\\n●storing many activations\\n●using those many activations to compute the gradients\\n46'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 47}, page_content='DailyDoseofDS.com\\nThis may lead to more resource consumption than available — resulting in\\ntraining failure. But by reducing the batch size, we can limit the memory usage\\nand train the network.\\nҷhӝق ֖s ЄrӝԠiԶלt ίcԍٜmٛ\\u05ceaف֖oכ Ӟnԟ \\u058boٷ ԠoԶآ ֖t \\u058be\\u05cd\\u05fb ֖n\\n֖nԍ؟eӝآiכճ ӹaفԎh آiڒԷ ֖n זeו\\u05ebrڀ Ԏoכآt؞Ӟiכقsؐ\\nConsider we are training a neural network on mini-batches.\\nWe train the network as follows:\\n●On every mini-batch:\\n○Run the forward pass while storing the activations.\\n○During backward pass:\\n■Compute the loss\\n■Compute the gradients\\n■Update the weights\\nGradient accumulation modiﬁes the last step of the backward pass, i.e., weight\\nupdates. More speciﬁcally, instead of updating the weights on every mini-batch,\\nwe can do this:\\n47'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 48}, page_content='DailyDoseofDS.com\\n1. Run the forward pass on a mini-batch.\\n2. Compute the gradient values for weights in the network.\\n3. Don’t update the weights yet.\\n4. Run the forward pass on the next mini-batch.\\n5. Compute the gradient values for weights and add them to the gradients\\nobtained in step 2.\\n6. Repeat steps 3-5 for a few more mini-batches.\\n7. Update the weights only aϔer processing a few mini-batches.\\nThis technique works\\nbecause accumulating the\\ngradients across multiple\\nmini-batches results in the\\nsame sum of gradients as if\\nwe were processing them\\ntogether. Thus, logically\\nspeaking, using gradient\\naccumulation, we can\\nmimic a larger batch size\\nwithout having to explicitly\\nincrease the batch size.\\nFor instance, say we want to use a batch size of 64. However, current memory can\\nonly support a batch size of 16.\\nNo worries!\\n●We can use a batch size of size 16.\\n●We can accumulate the gradients from every mini-batch.\\n●We can update the weights only once every 8 mini-batches.\\nThus, eﬀectively, we used a batch size of 16*8 (=128) instead of what we originally\\nintended — 64.\\n48'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 49}, page_content='DailyDoseofDS.com\\nОm\\u05fa\\u05ceeוԷnفӞt֕\\u05ebn\\nLet’s look at how we can implement this. In PyTorch, a typical training loop is\\nimplemented as follows:\\n●We clear the gradients\\n●Run the forward pass\\n●Compute the loss\\n●Compute the gradients\\n●Update the weights\\nHowever, as discussed earlier, if needed, we can only update the weights aϔer a\\nfew iterations. Thus, we must continue to accumulate the gradients, which is\\nprecisely what \\u05ceoءآ.ӸӞcׁٸa؞Ԡ(\\u0602 does.\\n49'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 50}, page_content='DailyDoseofDS.com\\nAlso, as \\u05ebpف֖m֕ړe؞؊zԶ؟o٦ճrӝԠ(\\u0602 clears the gradients, we must only execute it\\naϔer updating the weights. This idea is implemented below:\\n●First, we deﬁne acc_steps — the number of mini-batches aϔer which we\\nwant to update the weights.\\n●Next, we run the forward pass.\\n●Moving on, we compute the loss and the gradients.\\n●As discussed earlier, we will not update the weights yet and instead let the\\ngradients accumulate for a few more mini-batches.\\n●We only update the weights when the if condition is true.\\n●Aϔer updating, we clear the accumulated gradients.\\nThis way, we can optimize neural network training in memory-constrained\\nsettings.\\n50'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 51}, page_content='DailyDoseofDS.com\\nόe\\u05faӞrف֖nղ לoفԷ\\nBefore we end, it is essential to note that gradient accumulation is NOT a remedy\\nto improve run-time in memory-constrained situations. In fact, we can also verify\\nthis from my experiment:\\nInstead, its objective is to reduce overall memory usage.\\nOf course, it’s true that we are updating the weights only aϔer a few iterations.\\nSo, it will be a bit faster than updating on every iteration. Yet, we are still\\nprocessing and computing gradients on small mini-batches, which is the core\\noperation here.\\nNonetheless, the good thing is that even if you are not under memory constraints,\\nyou can still use gradient accumulation.\\n●Specify your typical batch size.\\n●Run forward pass.\\n●Compute loss and gradients.\\n●Update only aϔer a few iterations.\\nYou can download the notebook here: https://bit.ly/3xNCfFt.\\n51'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 52}, page_content='DailyDoseofDS.com\\nհ ѽt؞ӞtԶճiԶآ էo؞ іu\\u05cdقi֓ЄPҜ ҏrӝ֖n֕לg\\nBy default, deep learning models only utilize a single GPU for training, even if\\nmultiple GPUs are available.\\nAn ideal way to proceed (especially in big-data settings) is to distribute the\\ntraining workload across multiple GPUs. The graphic below depicts four\\ncommon strategies for multi-GPU training:\\n52'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 53}, page_content='DailyDoseofDS.com\\nש1\\u0602 іoԟԷl \\u05fba؞Ӟl\\u05cdԷl֕آm\\n●Diﬀerent parts (or layers) of the model are placed on diﬀerent GPUs.\\n●Useful for huge models that do not ﬁt on a single GPU.\\n●However, model parallelism also introduces severe bottlenecks as it\\nrequires data ﬂow between GPUs when activations from one GPU are\\ntransferred to another GPU.\\nש2\\u0602 ҏeכآo؞ \\u05fba؞Ӟl\\u05cdԷl֕آm\\n●Distributes and processes individual tensor operations across multiple\\ndevices or processors.\\n●It is based on the idea that a large tensor operation, such as matrix\\nmultiplication, can be divided into smaller tensor operations, and each\\nsmaller operation can be executed on a separate device or processor.\\n53'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 54}, page_content='DailyDoseofDS.com\\n●Such parallelization strategies are inherently built into standard\\nimplementations of PyTorch and other deep learning frameworks, but they\\nbecome much more pronounced in a distributed setting.\\nש3\\u0602 όaفӞ \\u05fba؞Ӟl\\u05cdԷl֕آm\\n●Replicate the model across all GPUs.\\n●Divide the available data into smaller batches, and each batch is processed\\nby a separate GPU.\\n●The updates (or gradients) from each GPU are then aggregated and used to\\nupdate the model parameters on every GPU.\\nש4\\u0602 Ѯi\\u05faԷl֕לe \\u05fba؞Ӟl\\u05cdԷl֕آm\\n●This is oϔen considered a combination of data parallelism and model\\nparallelism.\\n54'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 55}, page_content='DailyDoseofDS.com\\n●So the issue with standard model parallelism is that 1st GPU remains idle\\nwhen data is being propagated through layers available in 2nd GPU:\\n●Pipeline parallelism addresses this by loading the next micro-batch of data\\nonce the 1st GPU has ﬁnished the computations on the 1st micro-batch\\nand transferred activations to layers available in the 2nd GPU. The process\\nlooks like this:\\n○1st micro-batch passes through the layers on 1st GPU.\\n○2nd GPU receives activations on 1st micro-batch from 1st GPU.\\n○While the 2nd GPU passes the data through the layers, another\\nmicro-batch is loaded on the 1st GPU.\\n○And the process continues.\\n●GPU utilization drastically improves this way. This is evident from the\\nanimation below where multi-GPUs are being utilized at the same\\ntimestamp (look at ق=\\u05f7Ԟ ق=ٖԞ ق=լԞ and t=6):\\nThose were four common strategies for multi-GPU training.\\n55'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 56}, page_content='DailyDoseofDS.com\\nіiءԎe\\u05cd\\u05ceaכԷoٛآ\\nѐaӸԷl ѽmת\\u05ebt֊֖nղ\\nFor every instance in single-label classiﬁcation datasets, the entire probability\\nmass belongs to a single class, and the rest are zero. This is depicted below:\\nThe issue is that, at times, such label distributions excessively motivate the model\\nto learn the true class for every sample with pretty high conﬁdence. This can\\nimpact its generalization capabilities.\\nLabel smoothing is a lesser-talked regularisation technique that elegantly\\naddresses this issue.\\n56'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 57}, page_content='DailyDoseofDS.com\\nAs depicted above, with label smoothing:\\n●We intentionally reduce the probability mass of the true class slightly.\\n●The reduced probability mass is uniformly distributed to all other classes.\\nSimply put, this can be thought of as asking the model to be “less overconﬁdent”\\nduring training and prediction while still attempting to make accurate\\npredictions.\\nThe eﬃcacy of this technique is evident from the image below:\\nIn this experiment, I trained two neural networks on the Fashion MNIST dataset\\nwith the exact same weight initialization.\\n●One without label smoothing.\\n●Another with label smoothing.\\nThe model with label smoothing resulted in a better test accuracy, i.e., better\\ngeneralization.\\n57'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 58}, page_content='DailyDoseofDS.com\\nҷhԶל לoف قo ٜsԶ \\u05ceaӸԷl آmת\\u05ebt֊֖nղؑ\\nAϔer using label smoothing for many of my projects, I have also realized that it is\\nnot well suited for all use cases. So it’s important to know when you should not\\nuse it.\\nSee, if you only care about getting the ﬁnal prediction correct and improving\\ngeneralization, label smoothing will be a pretty handy technique. However, I\\nwouldn’t recommend utilizing it if you care about:\\n●Getting the prediction correct.\\n●And understanding the model’s conﬁdence in generating a prediction.\\nThis is because as we discussed above, label smoothing guides the model to\\nbecome “less overconﬁdent” about its predictions. Thus, we typically notice a\\ndrop in the conﬁdence values for every prediction, as depicted below:\\nOn a speciﬁc test instance:\\n●The model without label smoothing outputs 99% probability for class 3.\\n●With label smoothing, although the prediction is still correct, the\\nconﬁdence drops to 74%.\\nThis is something to keep in mind when using label smoothing. Nonetheless, the\\ntechnique is indeed pretty promising for regularizing deep learning models. You\\ncan download the code notebook for this chapter here: https://bit.ly/4ePt08d.\\n58'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 59}, page_content='DailyDoseofDS.com\\nϾoԍӞl \\u05ceoءآ\\nBinary classiﬁcation\\ntasks are typically\\ntrained using the\\nbinary cross entropy\\n(BCE) loss function:\\nFor notational convenience, if we deﬁne p\\x00 as the following:\\n…then we can also write the cross-entropy loss function as:\\nThat said, one limitation of BCE loss is that it weighs probability predictions for\\nboth classes equally, which is evident from its symmetry:\\nFor more clarity, consider the table below, which depicts two instances, one from\\nthe minority class and another from the majority class, both with the same loss:\\n59'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 60}, page_content='DailyDoseofDS.com\\nThis causes problems when we use BCE for imbalanced datasets, wherein most\\ninstances from the dominating class are “easily classiﬁable.” Thus, a loss value of,\\nsay, ֔lתճ(ژ؊3\\u0602 from the majority class instance should (ideally) be weighed LESS\\nthan the same loss value from the minority class.\\nFocal loss is a pretty handy and useful alternative to address this issue. It is\\ndeﬁned as follows:\\nAs depicted above, it introduces an additional multiplicative factor called\\ndownweighing, and the parameter γ \\u0601Gӝזmӝ\\u0603 is a hyperparameter.\\nPlotting BCE (class y=1) and Focal loss (for class y=1 and γ=3), we get the\\nfollowing curve:\\n60'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 61}, page_content='DailyDoseofDS.com\\nAs shown in the ﬁgure above, focal loss reduces the contribution of the\\npredictions the model is pretty conﬁdent about. Also, the higher the value of γ\\n(Gamma), the more downweighing takes place, as shown in this plot below:\\nMoving on, while the Focal loss function reduces the contribution of conﬁdent\\npredictions, we aren’t done yet.\\nThe focal loss function now is still symmetric like BCE:\\n61'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 62}, page_content='DailyDoseofDS.com\\nTo address this, we must add another weighing parameter (α), which is the\\ninverse of the class frequency, as depicted below:\\nThe α parameter is the inverse of the class frequency Thus, the ﬁnal loss function\\ncomes out to be the following:\\nBy using both downweighing and inverse weighing, the model gradually learns\\npatterns speciﬁc to the hard examples instead of always being overly conﬁdent in\\npredicting easy instances.\\n62'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 63}, page_content='DailyDoseofDS.com\\nTo test the eﬃcacy of focal loss\\nin a class imbalance setting, I\\ncreated a dummy classiﬁcation\\ndataset with a 90:10 imbalance\\nratio:\\nNext, I trained two neural network models (with the same architecture of 2\\nhidden layers):\\n●One with BCE loss\\n●Another with Focal loss\\nThe decision region plot and test accuracy for these two models is depicted\\nbelow:\\nIt is clear that:\\n●The model trained with BCE loss (leϔ) always predicts the majority class.\\n●The model trained with focal loss (right) focuses relatively more on\\nminority class patterns. As a result, it performs better.\\nDownload this Jupyter notebook to get started with Focal loss:\\nhttps://bit.ly/45XzNZC.\\n63'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 64}, page_content='DailyDoseofDS.com\\nЕoٷ όrת\\u05fboٛق ίcفٜa\\u05cd\\u05cey ҷo؞ׂsؐ\\nSome time back, I was invited by a tech startup to conduct their ML interviews. I\\ninterviewed 12 candidates and mostly asked practical ML questions.\\nHowever, there were some conceptual questions as well, like the one below,\\nwhich I intentionally asked every candidate:\\nЕoٷ ԠoԶآ όrת\\u05fboٛق ٸo؞ׂ?\\nPretty simple, right? Apparently, every candidate gave me an incomplete answer,\\nwhich I have mentioned below:\\nρaכԠiԟӞtԶآ’ ίnءٸe؞\\nIn a gist, the idea is to zero out neurons randomly in a neural network. This is\\ndone to regularize the network.\\nDropout is only applied during training, and which neuron activations to zero out\\n(or drop) is decided using a Bernoulli distribution:\\n“p” is the dropout probability speciﬁed in, say, PyTorch →nn.Dropout(p).\\n64'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 65}, page_content='DailyDoseofDS.com\\nіy էo\\u05cd\\u05ceoٷ֔u\\u05fa ؎uԶآt֕\\u05ebnԛ Оs قhԶ؟e Ӟnڀقh֕לg ԷlءԸ قhӝك ٸe Ԡo ֖n όrת\\u05fboٛق?\\nρaכԠiԟӞtԶآ: љoԝ قhӝك؛s ֖t؉ ҷe \\u05ebn\\u05cdځ ړe؞\\u05eb \\u05ebuف לeٛ؟oכآ Ӟnԟ قrӝ֖n قhԶ\\nלeفٸo؞ׂ Ӟs ٸe ٜsٛӞl\\u05cdځ ٸoٛ\\u05ced؉\\nљoٷԞ Ԏoו֖nղ ӹaԍׂ قo قhԶ قo\\u05fa֖c…\\nOf course, I am not saying that the above details are incorrect. They are correct.\\nHowever, this is just 50% of how Dropout works, and disappointingly, most\\nresources don’t cover the remaining 50%. If you too are only aware of the 50%\\ndetails I mentioned above, continue reading.\\nЕoٷ όrת\\u05fboٛق Ӟcفٜa\\u05cd\\u05cey ٸo؞ׂsؐ\\nTo begin, we must note that Dropout is only applied during training, but not\\nduring the inference/evaluation stage:\\nNow, consider that a neuron’s input is computed using 100 neurons in the\\nprevious hidden layer:\\n65'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 66}, page_content='DailyDoseofDS.com\\nFor simplicity, let’s assume a couple of things here:\\n●The activation of every yellow neuron is 1.\\n●The edge weight from the yellow neurons to the blue neuron is also 1.\\nAs a result, the input received by the blue neuron will be 100, as depicted below:\\nNow, during training, if we were using Dropout with, say, a 40% dropout rate,\\nthen roughly 40% of the yellow neuron activations would have been zeroed out.\\nAs a result, the input received by the blue neuron would have been around 60:\\n66'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 67}, page_content='DailyDoseofDS.com\\nHowever, the above point is only valid for the training stage.\\nIf the same scenario had existed during the inference stage instead, then the\\ninput received by the blue neuron would have been 100.\\nThus, under similar conditions:\\n●The input received during training →60.\\n●The input received during inference →100.\\nDo you see any problem here?\\nDuring training, the average neuron inputs are signiﬁcantly lower than those\\nreceived during inference.\\nMore formally, using Dropout signiﬁcantly aﬀects the scale of the activations.\\nHowever, it is desired that the neurons throughout the model must receive the\\nroughly same mean (or expected value) of activations during training and\\ninference. To address this, Dropout performs one additional step.\\nThis idea is to scale the remaining active inputs during training. The simplest\\nway to do this is by scaling all activations during training by a factor of \\u05f8/\\u0600\\u05f8-\\u05fa\\u0603,\\nwhere p is the dropout rate. For instance, using this technique on the neuron\\ninput of 60, we get the following (recall that we set p=40%):\\n67'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 68}, page_content='DailyDoseofDS.com\\nAs depicted above, scaling the neuron input brings it to the desired range, which\\nmakes training and inference stages coherent for the network.\\nұe؞֖fڀ֗לg Էx\\u05faԸ؟iוԷnفӞl\\u05cdځ\\nIn fact, we can verify that typical implementations of Dropout, from PyTorch, for\\ninstance, do carry out this step. Let’s deﬁne a dropout layer as follows:\\nNow, let’s consider a random tensor and apply this dropout layer to it:\\nAs depicted above, the retained values have increased.\\n●The second value goes from 0.13 →0.16.\\n●The third value goes from 0.93 →1.16.\\n●and so on…\\nWhat’s more, the retained values are precisely the same as we would have\\nobtained by explicitly scaling the input tensor with 1/(1-p):\\n68'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 69}, page_content='DailyDoseofDS.com\\nIf we were to do the same thing in evaluation mode instead, we notice that no\\nvalue is dropped and no scaling takes place either, which makes sense as Dropout\\nis only used during training:\\nThis is the remaining 50% details, which, in my experience, most resources do\\nnot cover, and as a result, most people aren’t aware of.\\nBut it is a highly important step in Dropout, which maintains numerical\\ncoherence between training and inference stages.\\nWith that, now you know 100% of how Dropout works.\\nNext, let’s discuss an issue with Dropout in case of CNNs.\\n69'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 70}, page_content='DailyDoseofDS.com\\nОsءٜeء ٸiف\\u058b όrת\\u05fboٛق ֖n ρNјآ\\nWhen it comes to training neural networks, it is always recommended to use\\nDropout to improve its generalization power.\\nThis applies not just to CNNs but to all other neural networks. And I am sure you\\nalready know the above details, so let’s get into the interesting part.\\nҏhԶ \\u05fbrתӹlԶז \\u05ebf ٜs֕לg όrת\\u05fboٛق ֖n ρNјآ\\nThe core operation that makes CNNs so powerful is convolution, which allows\\nthem to capture local patterns, such as edges and textures, and helps extract\\nrelevant information from the input.\\nFrom a purely mathematical perspective, we slide a ﬁlter (shown in yellow below)\\nover the input (shown in green below) and take the element-wise sum between\\nthe ﬁlter and the overlapped input to get the convolution output:\\nHere, if were to apply the traditional Dropout, the input features would look\\nsomething like this:\\n70'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 71}, page_content='DailyDoseofDS.com\\nIn fully connected layers, we zero out neurons. In CNNs, however, we randomly\\nzero out the pixel values before convolution, as depicted above.\\nBut this isn’t found to be that eﬀective speciﬁcally for convolution layers. To\\nunderstand this, consider we have some image data. In every image, we would\\nﬁnd that nearby features (or pixels) are highly correlated spatially.\\nFor instance, imagine zooming in on the pixel level of the digit ‘9’. Here, we\\nwould notice that the red pixel (or feature) is highly correlated with other features\\nin its vicinity:\\nThus, dropping the red feature using Dropout will likely have no eﬀect and its\\ninformation can still be sent to the next layer.\\nSimply put, the nature of the convolution operation defeats the entire purpose of\\nthe traditional Dropout procedure.\\n71'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 72}, page_content='DailyDoseofDS.com\\nҏhԶ آo\\u05cdٜt֕\\u05ebn\\nDropBlock is a much better, eﬀective, and intuitive way to regularize CNNs. The\\ncore idea in DropBlock is to drop a contiguous region of features (or pixels)\\nrather than individual pixels. This is depicted below:\\nSimilar to Dropout in fully connected layers, wherein the network tries to\\ngenerate more robust ways to ﬁt the data in the absence of some activations, in\\nthe case of DropBlock, the convolution layers get more robust to ﬁt the data\\ndespite the absence of a block of features.\\nMoreover, the idea of DropBlock also makes intuitive sense — if a contiguous\\nregion of a feature is dropped, the problem of using Dropout with convolution\\noperation can be avoided.\\nόrת\\u05fbB\\u05cd\\u05ebcׁ \\u05fba؞ӞmԶقe؞آ\\nDropBlock has two main parameters:\\n●λlתԎk٦آiڒԷ: The size of the box to be dropped.\\n●όrת\\u05fb_؞ӞtԶ: The drop probability of the central pixel.\\n72'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 73}, page_content='DailyDoseofDS.com\\nTo apply DropBlock, ﬁrst, we create a binary mask on the input sampled from the\\nBernoulli distribution:\\nNext, we create a block of size ӹlתԎk٦آiڒԷ*Ӹ\\u05ceoԍׂ_ء֖zԶ which has the sampled\\npixels at the center:\\nThe eﬃcacy of DropBlock over Dropout is evident from the results table below:\\nOn the ImageNet classiﬁcation dataset:\\n●DropBlock provides a 1.33% gain over Dropout.\\n●DropBlock with Label smoothing (discussed in the last chapter) provides a\\n1.55% gain over Dropout.\\n73'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 74}, page_content='DailyDoseofDS.com\\nThankfully, DropBlock is also integrated with PyTorch.\\nThere’s also a library for DropBlock, called “Ԡrת\\u05fbb\\u05cd\\u05ebcׁ,” which also provides the\\nlinear scheduler for drop_rate.\\nSo the thing is that the researchers who proposed DropBlock found the\\ntechnique to be more eﬀective when the drop_rate was increased gradually.\\nThe DropBlock library implements the scheduler. But of course, there are ways to\\ndo this in PyTorch as well. So it’s entirely up to you which implementation you\\nwant to use:\\n●DropBlock PyTorch: https://bit.ly/3xZfT3E.\\n●DropBlock library: https://github.com/miguelvr/dropblock.\\n74'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 75}, page_content='DailyDoseofDS.com\\nҷhӝق\\nЕiԟԠeכ\\nѐaڀԷrء\\nӞnԟ\\nίcف֖vӝقiתל\\nϾuכԎt֕\\u05ebnء\\nίcفٜa\\u05cd\\u05cey όo\\nEveryone knows the objective of an activation function in a neural network. They\\nlet the network learn non-linear patterns. There is nothing new here, and I am\\nsure you are aware of that too.\\nHowever, one thing I have oϔen\\nrealized is that most people struggle to\\nbuild an intuitive understanding of\\nwhat exactly a neural network\\nconsistently tries to achieve during its\\nlayer-aϔer-layer transformations.\\nIn this chapter, let me share a unique perspective on this, which would really help\\nyou understand the internal workings of a neural network.\\nI have supported this chapter with plenty of visuals for better understanding.\\nAlso, for simplicity, we shall consider a binary classiﬁcation use case.\\nλaԍׂg؞\\u05ebuכԠ\\nThe data undergoes a series of transformations at each hidden layer:\\n●Linear transformation of the data obtained from the previous layer\\n●…followed by a non-linearity using an activation function — ReLU,\\nSigmoid, Tanh, etc.\\n75'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 76}, page_content='DailyDoseofDS.com\\nThe above transformations are performed on every hidden layer of a neural\\nnetwork. Now, notice something here.\\nAssume that we just applied the\\nabove data transformation on the\\nvery last hidden layer of the neural\\nnetwork. Once we do that, the\\nactivations progress toward the\\noutput layer of the network for one\\nﬁnal transformation, which is\\nentirely linear.\\nThe above transformation is entirely linear because all sources of non-linearity\\n(activations functions) exist on or before the last hidden layer. And during the\\nforward pass, once the data leaves the last hidden layer, there is no further scope\\nfor non-linearity.\\nThus, to make accurate predictions, the data received by the output layer from\\nthe last hidden layer MUST BE linearly separable.\\n76'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 77}, page_content='DailyDoseofDS.com\\nTo summarize….\\nWhile transforming the data through all its hidden layers and just before\\nreaching the output layer, a neural network is constantly hustling to project the\\ndata to a space where it somehow becomes linearly separable. If it does, the\\noutput layer becomes analogous to a logistic regression model, which can easily\\nhandle this linearly separable data.\\nIn fact, we can also verify this experimentally.\\nTo visualize the input transformation, we can add a dummy hidden layer with\\njust two neurons right before the output layer and train the neural network again.\\nWhy do we add a layer with just two neurons?\\nThis way, we can easily visualize the transformation. We expect that if we plot\\nthe activations of this 2D dummy hidden layer, they must be linearly separable.\\nThe below visual precisely depicts this.\\n77'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 78}, page_content='DailyDoseofDS.com\\nAs we notice above, while the input data was linearly inseparable, the input\\nreceived by the output layer is indeed linearly separable.\\nThis transformed data can be easily handled by the output classiﬁcation layer.\\nAnd this shows that all a neural network is trying to do is transform the data into\\na linearly separable form before reaching the output layer.\\n78'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 79}, page_content='DailyDoseofDS.com\\nѽhٛﬄԷ όaفӞ λeզ\\u05ebrԶ ҏrӝ֖n֕לg\\nDeep learning models may fail\\nto converge due to various\\nreasons. Some causes are\\nobvious and common, and\\ntherefore, quickly rectiﬁable,\\nlike too high/low learning\\nrate, no data normalization,\\nno batch normalization, etc.\\nBut the problem arises when the cause isn’t that apparent. Therefore, it may take\\nsome serious time to debug if you are unaware of them. In this chapter, I want to\\ntalk about one such data-related mistake, which I once committed during my\\nearly days in machine learning. Admittedly, it took me quite some time to ﬁgure\\nit out back then because I had no idea about the issue.\\nϛx\\u05faԷr֕זeכق\\nConsider a classiﬁcation neural network trained using mini-batch gradient\\ndescent.\\nіiכ֖-ӸӞtԍ\\u058b ճrӝԠiԶלt ԠeءԎeכق: ҝpԟӞtԶ לeفٸo؞ׂ ٸe֕ճhفآ ٜs֕לg Ӟ էeٷ ԠaفӞ\\n\\u05fbo֕לtء Ӟt Ӟ قiוԷ.\\n79'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 80}, page_content='DailyDoseofDS.com\\nHere, we train two diﬀerent neural\\nnetworks:\\n●Version 1: The dataset is\\nordered by labels.\\n●Version 2: The dataset is\\nproperly shuﬄed by labels.\\nAnd, of course, before training, we ensure that both networks had the same\\ninitial weights, learning rate, and other settings.\\nThe image depicts the\\nepoch-by-epoch\\nperformance of the two\\nmodels. On the leϔ, we\\nhave the model trained\\non label-ordered data,\\nand the one on the right\\nwas trained on the\\nshuﬄed dataset.\\nIt is clear that the model receiving a label-ordered dataset miserably fails to\\nconverge while the other model, although overﬁts, shows that model has been\\nlearn eﬀectively.\\nҷhڀ ԠoԶآ قhӝك \\u058ba\\u05fa\\u05fbeכؑ\\nNow, if you think about it for a second, overall, both models received the same\\ndata, didn’t they? Yet, the order in which the data was fed to these models totally\\ndetermined their performance. I vividly remember that when I faced this issue, I\\nknew that my data was ordered by labels.\\n80'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 81}, page_content='DailyDoseofDS.com\\nYet, it never occurred to me that ordering may inﬂuence the model performance\\nbecause the data will always be the same regardless of the ordering.\\nBut later, I realized that this point\\nwill only be valid when the model\\nsees the entire data and updates\\nthe model weights in one go, i.e.,\\nin batch gradient descent, as\\ndepicted in this image.\\nBut in the case of mini-batch gradient descent, the weights are updated aϔer\\nevery mini-batch. Thus, the prediction and weight update on a subsequent\\nmini-batch is inﬂuenced by the previous mini-batches.\\nIn the context of label-ordered data, where samples of the same class are grouped\\ntogether, mini-batch gradient descent will lead the model to learn patterns\\nspeciﬁc to the class it excessively saw early on in training. In contrast, randomly\\nordered data ensures that each mini-batch contains a balanced representation of\\nclasses. This allows the model to learn a more comprehensive set of features\\nthroughout the training process.\\nOf course, the idea of shuﬄing is not valid for time-series datasets as their\\ntemporal structure is important. The good thing is that if you happen to use, say,\\nPyTorch DataLoader, you are safe. This is because it already implements\\nshuﬄing. But if you have a custom implementation, ensure that you are not\\nmaking any such error.\\nBefore I end, one thing that you must ALWAYS remember when training neural\\nnetworks is that these models can proﬁciently learn entirely non-existing\\npatterns about your dataset. So never give them any chance to do so.\\n81'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 82}, page_content='DailyDoseofDS.com\\nіoԟԷl Ԏoו\\u05fbrԶآs֕\\u05ebn\\nфnתٸlԶԠgԶ όiءقi\\u05cd\\u05ceaف֖oכ էo؞ іoԟԷl ρoו\\u05fbrԶآs֕\\u05ebn\\nModel accuracy alone (or an equivalent performance metric) rarely determines\\nwhich model will be deployed.\\nThis is because we also consider several operational metrics, such as:\\n●Inference Latency: Time taken by the model to return a prediction.\\n●Model size: The memory occupied by the model.\\n●Ease of scalability, etc.\\nIn this chapter, let me share a technique (with a demo) called knowledge\\ndistillation, which is commonly used to compress ML models and contribute to\\nthe above operational metrics.\\nҷhӝق ֖s ׂnתٸlԶԠgԶ Ԡiءقi\\u05cd\\u05ceaف֖oכؑ\\nIn a gist, the idea is to train a smaller/simpler model (called the “student” model)\\nthat mimics the behavior of a larger/complex model (called the “teacher” model).\\n82'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 83}, page_content='DailyDoseofDS.com\\nThis involves two steps:\\n●Train the teacher model as we typically would.\\n●Train a student model that matches the output of the teacher model.\\nIf we compare it to an academic teacher-student scenario, the student may not be\\nas performant as the teacher.\\nBut with consistent training, a smaller model may get (almost) as good as the\\nlarger one.\\nA classic example of a model developed in this way is DistillBERT. It is a student\\nmodel of BERT.\\n●DistilBERT is approximately 40% smaller than BERT, which is a massive\\ndiﬀerence in size.\\n●Still, it retains approximately 97% of the BERT’s capabilities.\\nNext, let’s look at a demo.\\n83'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 84}, page_content='DailyDoseofDS.com\\nфnתٸlԶԠgԶ Ԡiءقi\\u05cd\\u05ceaف֖oכ Ԡeו\\u05eb\\nIn the interest of time, let’s say we have already trained the following CNN model\\non the MNIST dataset (I have provided the full Jupyter notebook towards the end,\\ndon’t worry):\\nThe epoch-by-epoch training loss and validation accuracy is depicted below:\\nNext, let’s deﬁne a simpler model without any convolutional layers:\\n84'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 85}, page_content='DailyDoseofDS.com\\nBeing a classiﬁcation model, the output will be a probability distribution over the\\n<N> classes:\\nThus, we can train the student model such that its probability distribution\\nmatches that of the teacher model.\\nOne way to do this is to use KL divergence as a loss function.\\n85'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 86}, page_content='DailyDoseofDS.com\\nIt measures how much information is lost when we use distribution Q to\\napproximate distribution P.\\nί ؎uԶآt֕\\u05ebn էo؞ ځoٛԜ ҷhӝق ٸi\\u05cd\\u05ce ӹe قhԶ фL ԠiٱԷrղԸלcԶ ֖f Ѯ=Ѷؑ\\nThus, in our case:\\n●P will be the probability distribution from the teacher model.\\n●Q will be the probability distribution from the student model.\\nThe loss function is implemented below:\\nFinally, we train the student model as follows:\\n86'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 87}, page_content='DailyDoseofDS.com\\nDone!\\nThe following image compares the training loss and validation accuracy of the\\ntwo models:\\nOf course, as shown in the highlighted lines above, the performance of the\\nstudent model is not as good as the teacher model, which is expected.\\nHowever, it is still pretty promising, given that it was only composed of simple\\nfeed-forward layers.\\nAlso, as depicted below, the student model is approximately 35% faster than the\\nteacher model, which is a signiﬁcant increase in the inference run-time of the\\nmodel for about a \\u05f8-ٖ؈ drop in the test accuracy.\\n87'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 88}, page_content='DailyDoseofDS.com\\nThat said, one of the biggest downsides of knowledge distillation is that one must\\nstill train a larger teacher model ﬁrst to train the student model.\\nBut in a resource-constrained environment, it may not be feasible to train a large\\nteacher model.\\nSo this technique assumes that we are not resource-constrained at least in the\\ndevelopment environment.\\nIn the next chapter, let’s discuss one more technique to compress ML models and\\nreduce their memory footprint.\\n88'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 89}, page_content='DailyDoseofDS.com\\nίcف֖vӝقiתל Ѯrٛלiכճ\\nOnce we complete network training, we are almost always leϔ with plenty of\\nuseless neurons — ones that make nearly zero contribution to the network’s\\nperformance, but they still consume memory.\\nIn other words, there is a high percentage of neurons, which, if removed from the\\ntrained network, will not aﬀect the performance remarkably:\\nAnd, of course, I am not saying this as a random and uninformed thought. I have\\nexperimentally veriﬁed this over and over across my projects.\\nHere’s the core idea.\\nAϔer training is complete, we run the\\ndataset through the model (no\\nbackpropagation this time) and\\nanalyze the average activation of\\nindividual neurons. Here, we oϔen\\nobserve that many neuron activations\\nare always close to near-zero values.\\nThus, they can be pruned from the network, as they will have very little impact on\\nthe model’s output.\\nFor pruning, we can decide on a pruning threshold (λ) and prune all neurons\\nwhose activations are less than this threshold.\\n89'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 90}, page_content='DailyDoseofDS.com\\nThis makes intuitive sense as well.\\nMore speciﬁcally, if a neuron rarely possesses a high activation value, then it is\\nfair to assume that it isn’t contributing to the model’s output, and we can safely\\nprune it.\\nThe following table\\ncompares the\\naccuracy of the\\npruned model with\\nthe original (full)\\nmodel across a\\nrange of pruning\\nthresholds (λ):\\nAt a pruning threshold λ=0.4, the validation accuracy of the model drops by just\\n0.62%, but the number of parameters drops by 72%.\\nThat is a huge reduction, while both models being almost equally good! Of\\ncourse, there is a trade-oﬀbecause we are not doing as well as the original model.\\nBut in many cases, especially when deploying ML models, accuracy is not the\\nonly primary metric that decides these.\\nInstead, several operational\\nmetrics like eﬃciency, speed,\\nmemory consumption, etc., are\\nalso a key deciding factor.\\nThat is why model\\ncompression techniques are so\\ncrucial in such cases.\\n90'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 91}, page_content='DailyDoseofDS.com\\nόe\\u05fa\\u05ceoڀזeכق\\nόe\\u05fa\\u05ceoڀ іL іoԟԷlء էrתז оu\\u05faځtԶ؟ љoفԷbת\\u05ebk\\nThe core objective of model deployment is to obtain an API endpoint that can be\\nused for inference purposes:\\nWhile this sounds simple, deployment is typically quite a tedious and\\ntime-consuming process. One must maintain environment ﬁles, conﬁgure\\nvarious settings, ensure all dependencies are correctly installed, and many more.\\nSo, in this chapter, I want to help you simplify this process. More speciﬁcally, we\\nshall learn how to deploy any ML model right from a Jupyter Notebook in just\\nthree simple steps using the Modelbit API.\\nModelbit lets us seamlessly deploy ML models directly from our Python\\nnotebooks (or git) to Snowﬂake, Redshiϔ, and REST.\\n91'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 92}, page_content='DailyDoseofDS.com\\nόe\\u05fa\\u05ceoڀזeכق ٸiف\\u058b іoԟԷlӸ֖t\\nAssume we have already trained our model.\\nFor simplicity, let’s assume it to be a linear regression model trained using\\nsklearn, but it can be any other model as well:\\nLet’s see how we can deploy this model with Modelbit!\\n●First, we install the Modelbit package via pip:\\n●Next, we log in to Modelbit from our Jupyter Notebook (make sure you\\nhave created an account here: Modelbit)\\n92'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 93}, page_content='DailyDoseofDS.com\\n●Finally, we deploy it, but here’s an important point to note:\\nTo deploy a model using Modelbit, we must deﬁne an inference function.\\nSimply put, this function contains the code that will be executed at inference.\\nThus, it will be responsible for returning the prediction.\\nWe must specify the input parameters required by the model in this method.\\nAlso, we can name it anything we want.\\nFor our linear regression case, the inference function can be as follows:\\n●We deﬁne a function זy٦\\u05cer٦Ԡe\\u05fa\\u05ceoڀזeכق(\\u0602.\\n●Next, we specify the input of the model as a parameter of this method.\\n●We validate the input for its data type.\\n●Finally, we return the prediction.\\nOne good thing about Modelbit is that every dependency of the function (the\\nmodel object in this case) is pickled and sent to production automatically along\\n93'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 94}, page_content='DailyDoseofDS.com\\nwith the function. Thus, we can reference any object in this method. Once we\\nhave deﬁned the function, we can proceed with deployment as follows:\\nWe have successfully\\ndeployed the model in\\nthree simple steps,\\nthat too, right from the\\nJupyter Notebook!\\nOnce our model has\\nbeen successfully\\ndeployed, it will\\nappear in our Modelbit\\ndashboard.\\nAs shown above, Modelbit provides an API endpoint. We can use it for inference\\npurposes as follows:\\n94'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 95}, page_content='DailyDoseofDS.com\\nIn the above request, data passed to the endpoint is a list of lists.\\nThe ﬁrst number in the list is the input ID. All entries following the ID in a list\\nare the function parameters.\\nLastly, we can also specify speciﬁc versions of the libraries or Python used while\\ndeploying our model. This is depicted below:\\nIsn’t that cool, simple, and elegant over traditional deployment approaches?\\n95'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 96}, page_content='DailyDoseofDS.com\\nհ ҷaڀآ قo ҏeءق іL іoԟԷlء ֖n ѮrתԠuԍقiתל\\nDespite rigorously testing an ML model locally (on validation and test sets), it\\ncould be a terrible idea to instantly replace the previous model with a new model.\\nA more reliable strategy is to test the model in production (yes, on real-world\\nincoming data). While this might sound risky, ML teams do it all the time, and it\\nisn’t that complicated. The following visual depicts 4 common strategies to do so:\\n96'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 97}, page_content='DailyDoseofDS.com\\n●The current model is called the legacy model.\\n●The new model is called the candidate model.\\nש1\\u0602 ί/κ قeءك֖nղ\\n●Distribute the incoming requests non-uniformly between the legacy model\\nand the candidate model.\\n●Intentionally limit the exposure of the candidate model to avoid any\\npotential risks. Thus, the number of requests sent to the candidate model\\nmust be low.\\nש2\\u0602 ρaכӞrڀ قeءك֖nղ\\n●In A/B testing, since traﬃc is randomly redirected to either model\\nirrespective of the user, it can potentially aﬀect all users.\\n●In canary testing, the candidate model is released to a small subset of users\\nin production and gradually rolled out to more users.\\n97'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 98}, page_content='DailyDoseofDS.com\\nש3\\u0602 ОnفԷr\\u05cdԸӞvԶԠ قeءك֖nղ\\n●This involves mixing the predictions of multiple models in the response.\\n●Consider Amazon’s recommendation engine. In interleaved deployments,\\nsome product recommendations displayed on the homepage can come\\nfrom the legacy model, while some can come from the candidate model.\\nש4\\u0602 ѽhӝԠoٷ قeءك֖nղ\\n●All of the above techniques aﬀect some (or all) users.\\n●Shadow testing (or dark launches) lets us test a new model in a production\\nenvironment without aﬀecting the user experience.\\n●The candidate model is deployed alongside the existing legacy model and\\nserves requests like the legacy model. However, the output is not sent back\\nto the user. Instead, the output is logged for later use to benchmark its\\nperformance against the legacy model.\\n●We explicitly deploy the candidate model instead of testing oﬄine because\\nthe production environment is diﬃcult to replicate oﬄine.\\nShadow testing oﬀers risk-free testing of the candidate model in a production\\nenvironment.\\n98'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 99}, page_content='DailyDoseofDS.com\\nұe؞آiתל ρoכقrת\\u05cel֕לg Ӟnԟ іoԟԷl Ѻeղ֖sف؟y\\nReal-world ML deployment is never just about “deployment” — host the model\\nsomewhere, obtain an API endpoint, integrate it into the application, and you are\\ndone!\\nThis is because, in reality, plenty of things must be done post-deployment to\\nensure the model’s reliability and performance.\\nש1\\u0602 ұe؞آiתל Ԏoכقrת\\u05ce\\nTo begin, it is immensely crucial to\\nversion control ML deployments. You\\nmay have noticed this while using\\nChatGPT, for instance.\\nBut updating does not simply mean overwriting the previous version.\\nInstead, ML models are always version-controlled (using git tools).\\nThe advantages of version-controlling ML deployments are pretty obvious:\\n●In case of sudden mishaps post-deployment, we can instantly roll back to\\nan older version.\\n●We can facilitate parallel development with branching, and many more.\\n99'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 100}, page_content='DailyDoseofDS.com\\nש2\\u0602 іoԟԷl ؟eղ֖sف؟y\\nAnother practical idea is to maintain a model registry for deployments. Let’s\\nunderstand what it is.\\nSimply put, a model registry can be considered\\nrepository of models. See, typically, we might be\\ninclined to version our code and the ML model\\ntogether:\\nHowever, when we use a model registry, we version models separately from the\\ncode. Let me give you an intuitive example to understand this better. Imagine our\\ndeployed model takes three inputs to generate a prediction:\\nWhile writing the inference code, we overlooked that, at times, one of the inputs\\nmight be missing. We realized this by analyzing the model’s logs.\\n100'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 101}, page_content='DailyDoseofDS.com\\nWe may want to ﬁx this quickly (at least for a while) before we decide on the next\\nsteps more concretely. Thus, we may decide to update the inference code by\\nassigning a dummy value for the missing input.\\nThis will allow the model to still process the incoming request.\\nLet me ask you a question: “Did we update the model?”\\nNo, right?\\nHere, we only need to update the inference code. The model will remain the\\nsame.\\nBut if we were to version the model and code together, it would lead to a\\nredundant model and take up extra space.\\nHowever, by maintaining a model registry:\\n●We can only update the inference code.\\n●Avoid pushing a new (yet unwanted) model to deployment.\\nThis makes intuitive sense as well, doesn’t it?\\n101'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 102}, page_content=\"DailyDoseofDS.com\\nѐLѕآ\\nҷhԶ؟e όiԟ قhԶ ЄPҜ іeו\\u05ebrڀ Єoؐ\\nGPT-2 (XL) has 1.5 Billion parameters, and its parameters consume ~3GB of\\nmemory in 16-bit precision.\\nUnder 16-bit precision, one parameter takes up 2 bytes of memory, so 1.5B\\nparameters will consume 3GB of memory.\\nWhat’s your estimate for the minimum memory needed to train GPT-2 on a\\nsingle GPU?\\n●Optimizer →Adam\\n●Batch size →32\\n●Number of transformer layers →48\\n●Sequence length →1000\\nҷhӝق'ء ځoٛ؟ Էsف֖mӝقe էo؞ قhԶ זiכ֖mٛז זeו\\u05ebrڀ לeԶԠeԟ قo قrӝ֖n Ӟ ЄPҎ֔2\\nזoԟԷl \\u0601s֕ړe يGκآ) \\u05ebn Ӟ آiכճlԶ ЄPҜؑ\\n-\\nհ-ط ЄB\\n-\\nՂ-\\u05f7ڙ ЄB\\n-\\n\\u05f82֓\\u05f85 ЄB\\n-\\nي2֓ي5 ЄB\\n-\\nխ0؋ ЄB\\nThe answer might surprise you.\\n102\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 103}, page_content='DailyDoseofDS.com\\nOne can barely train a 3GB GPT-2 model on a single GPU with 32GB of\\nmemory.\\nBut how could that be even possible? Where does all the memory go?\\nLet’s understand.\\nThere are so many fronts on which the model consistently takes up memory\\nduring training.\\nש1\\u0602 ѡpف֖m֕ړe؞ آtӝقeءԞ ճrӝԠiԶלtءԞ Ӟnԟ \\u05fba؞ӞmԶقe؞ זeו\\u05ebrڀ\\nMixed precision training is widely used to speed up model training.\\nAs the name suggests, the idea is to utilize lower-precision ﬂ\\u05ebaف\\u05f86 (wherever\\nfeasible, like in convolutions and matrix multiplications) along with ﬂ\\u05ebaفي2 —\\nthat is why the name “mixed precision.”\\nBoth the forward and backward propagation are performed using the 16-bit\\nrepresentations of weights and gradients.\\nThus, if the model has Φ parameters, then:\\n●Weights will consume 2*Φ bytes.\\n●Gradients will consume 2*Φ bytes.\\n103'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 104}, page_content='DailyDoseofDS.com\\nHere, the ﬁgure “2” represents a memory consumption of 2 bytes/paramter\\n(16-bit).\\nMoreover, the updates at the end of the backward propagation are still performed\\nunder 32-bit for eﬀective computation. I am talking about the circled step in the\\nimage below:\\nAdam is one of the most popular optimizers for model training.\\nWhile many practitioners use it just because it is popular, they don’t realize that\\nduring training, Adam stores two optimizer states to compute the updates —\\nmomentum and variance of the gradients:\\nThus, if the model has Φ parameters, then these two optimizer states will\\nconsume:\\n●4*Φ bytes for momentum.\\n104'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 105}, page_content='DailyDoseofDS.com\\n●Another 4*Φ bytes for variance.\\nHere, the ﬁgure “4” represents a memory consumption of 4 bytes/paramter\\n(32-bit).\\nLastly, as shown in the ﬁgure above, the ﬁnal updates are always adjusted in the\\n32-bit representation of the model weights. This leads to:\\n●Another 4*Φ bytes for model parameters.\\nLet’s sum them up:\\nThat’s 16*Φ, or 24GB of memory, which is ridiculously higher than the 3GB\\nmemory utilized by 16-bit parameters.\\nAnd we haven’t considered everything yet.\\n105'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 106}, page_content='DailyDoseofDS.com\\nש2\\u0602 ίcف֖vӝقiתלs\\nFor big deep learning models, like LLMs, Activations take up signiﬁcant memory\\nduring training.\\nMore formally, the total number of activations computed in one transform block\\nof GPT-2 are:\\nThus, across all transformer blocks, this comes out to be:\\nThis is the conﬁguration for GPT2-XL:\\nThis comes out to be ~30B activations. As each activation is represented in 16-bit,\\nall activations collectively consume 60GB of memory.\\n106'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 107}, page_content='DailyDoseofDS.com\\nWith techniques like gradient checkpointing (discussed in the previous chapter),\\nthis could be brought down to about 8-9GB at the expense of 25-30% more\\nrun-time.\\nThis technique takes complete memory consumption to about 32-35GB range,\\nwhich I mentioned earlier, for a meager 3GB model, and that too with a pretty\\nsmall batch size of just 32. On top of this, there are also some more overheads\\ninvolved, like memory fragmentation.\\nIt occurs when there are small, unused gaps between allocated memory blocks,\\nleading to ineﬃcient use of the available memory.\\nMemory allocation requests fail because of the unavailability of contiguous\\nmemory blocks.\\nρoכԎlٛآiתל\\nIn the above discussion, we considered a relatively small model — GPT-2 (XL)\\nwith 1.5 Billion parameters, which is tiny compared to the scale of models being\\ntrained these days.\\nHowever, the discussion may have helped you reﬂect on the inherent challenges\\nof building LLMs. Many people oϔen say that GPTs are only about stacking more\\nand more layers in the model and making the network bigger.\\nIf it was that easy, everybody would have been doing it. From this discussion, you\\nmay have understood that it’s not as simple as appending more layers.\\nEven one additional layer can lead to multiple GBs of additional memory\\nrequirement. Multi-GPU training is at the forefront of these models, which we\\ncovered in an earlier chapter in this book.\\n107'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 108}, page_content='DailyDoseofDS.com\\nϾu\\u05cd\\u05ce-ו\\u05ebdԶ\\u05ce ϾiכԷ-فٜn֕לg ٲs؉ ѐoѹί ٲs؉ ѺAЃ\\nHere’s a visual which illustrates “full-model ﬁne-tuning,” “ﬁne-tuning with\\nLoRA,” and “retrieval augmented generation (RAG).”\\nAll three techniques are used to augment the knowledge of an existing model\\nwith additional data.\\n108'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 109}, page_content='DailyDoseofDS.com\\nש1\\u0602 Ͼu\\u05cd\\u05ce ﬁלe֓قuכ֖nղ\\nFine-tuning means adjusting the weights of a pre-trained model on a new dataset\\nfor better performance.\\nWhile this ﬁne-tuning technique has been successfully used for a long time,\\nproblems arise when we use it on much larger models — LLMs, for instance,\\nprimarily because of:\\n●Their size.\\n●The cost involved in ﬁne-tuning all weights.\\n●The cost involved in maintaining all large ﬁne-tuned models.\\nש2\\u0602 ѐoѹί ﬁלe֓قuכ֖nղ\\nLoRA ﬁne-tuning addresses the limitations of traditional ﬁne-tuning. The core\\nidea is to decompose the weight matrices (some or all) of the original model into\\nlow-rank matrices and train them instead. For instance, in the graphic below, the\\nbottom network represents the large pre-trained model, and the top network\\nrepresents the model with LoRA layers.\\n109'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 110}, page_content=\"DailyDoseofDS.com\\nThe idea is to train only the LoRA network and freeze the large model.\\nLooking at the above visual, you might think:\\nBut the LoRA model has more neurons than the original model. How does that\\nhelp? To understand this, you must make it clear that neurons don't have\\nanything to do with the memory of the network.\\nThey are just used to illustrate the dimensionality transformation from one layer\\nto another.\\nIt is the weight matrices (or the connections between two layers) that take up\\nmemory. Thus, we must be comparing these connections instead:\\nLooking at the above visual, it is pretty clear that the LoRA network has\\nrelatively very few connections.\\nש3\\u0602 ѺAЃ\\nRetrieval augmented generation (RAG) is another pretty cool way to augment\\nneural networks with additional information, without having to ﬁne-tune the\\nmodel.\\nThis is illustrated below:\\n110\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 111}, page_content='DailyDoseofDS.com\\nThere are 7 steps, which are also marked in the above visual:\\n●Step 1-2: Take additional data, and dump it in a vector database aϔer\\nembedding. (This is only done once. If the data is evolving, just keep\\ndumping the embeddings into the vector database. There’s no need to\\nrepeat this again for the entire data)\\n●Step 3: Use the same embedding model to embed the user query.\\n●Step 4-5: Find the nearest neighbors in the vector database to the\\nembedded query.\\n●Step 6-7: Provide the original query and the retrieved documents (for more\\ncontext) to the LLM to get a response.\\nIn fact, even its name entirely justiﬁes what we do with this technique:\\n111'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 112}, page_content='DailyDoseofDS.com\\n●Retrieval: Accessing and retrieving information from a knowledge source,\\nsuch as a database or memory.\\n●Augmented: Enhancing or enriching something, in this case, the text\\ngeneration process, with additional information or context.\\n●Generation: The process of creating or producing something, in this\\ncontext, generating text or language.\\nOf course, there are many problems with RAG too, such as:\\n●RAGs involve similarity matching between the query vector and the vectors\\nof the additional documents. However, questions are structurally very\\ndiﬀerent from answers.\\n●Typical RAG systems are well-suited only for lookup-based\\nquestion-answering systems. For instance, we cannot build a RAG pipeline\\nto summarize the additional data. The LLM never gets info about all the\\ndocuments in its prompt because the similarity matching step only\\nretrieves top matches.\\nSo, it’s pretty clear that RAG has both pros and cons.\\n●We never have to ﬁne-tune the model, which saves a lot of computing\\npower.\\n●But this also limits the applicability to speciﬁc types of systems.\\nLet’s continue the discussion on LLM ﬁne-tuning in the next chapter.\\n112'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 113}, page_content='DailyDoseofDS.com\\nխ ѐLѕ ϾiכԷ-فٜn֕לg ҏeԍ\\u058bn֕؎uԶآ\\nTraditional ﬁne-tuning (depicted below) is infeasible with LLMs because these\\nmodels have billions of parameters and are hundreds of GBs in size, and not\\neveryone has access to such computing infrastructure.\\nBut today, we have many optimal ways to ﬁne-tune LLMs, and ﬁve popular\\ntechniques are depicted below:\\n113'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 114}, page_content='DailyDoseofDS.com\\n-\\nLoRA: Add two low-rank matrices A and B alongside weight matrices,\\nwhich contain the trainable parameters. Instead of ﬁne-tuning W, adjust\\nthe updates in these low-rank matrices.\\n-\\nLoRA-FA: While LoRA considerably decreases the total trainable\\nparameters, it still requires substantial activation memory to update the\\nlow-rank weights. LoRA-FA (FA stands for Frozen-A) freezes the matrix A\\nand only updates matrix B.\\n-\\nVeRA: In LoRA, every layer has a diﬀerent pair of low-rank matrices A and\\nB, and both matrices are trained. In VeRA, however, matrices A and B are\\nfrozen, random, and shared across all model layers. VeRA focuses on\\nlearning small, layer-speciﬁc scaling vectors, denoted as b and d, which are\\nthe only trainable parameters in this setup.\\n114'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 115}, page_content='DailyDoseofDS.com\\n-\\nDelta-LoRA: Here, in addition to training low-rank matrices, the matrix W\\nis also adjusted but not in the traditional way. Instead, the diﬀerence (or\\ndelta) between the product of the low-rank matrices A and B in two\\nconsecutive training steps is added to W:\\n-\\nLoRA+: In LoRA, both matrices A and B are updated with the same\\nlearning rate. Authors found that setting a higher learning rate for matrix\\nB results in more optimal convergence.\\n115'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 116}, page_content='DailyDoseofDS.com\\nρlӝآs֕Ԏa\\u05cd іL\\n116'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 117}, page_content='DailyDoseofDS.com\\nіL ϾuכԠaוԷnفӞlء\\nҏrӝ֖n֕לg Ӟnԟ ОnզԷrԶלcԶ ҏiוԷ ρoו\\u05fblԶٻiفځ \\u05ebf \\u05f80 іL\\nίlղ\\u05ebr֕قhוآ\\nHere’s the run-time complexity of the 10 most popular ML algorithms.\\nBut why even care about run time? There are multiple reasons why I always care\\nabout run time and why you should too.\\n117'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 118}, page_content='DailyDoseofDS.com\\nTo begin, we know that everyone is a big fan of sklearn implementations. It\\nliterally takes just two (max three) lines of code to run any ML algorithm with\\nsklearn. Yet, in my experience, due to this simplicity, most users oϔen overlook:\\n●The core understanding of an algorithm.\\n●The data-speciﬁc conditions that allow us to use an algorithm.\\nFor instance, you’ll be up for a big surprise if you use SVM or t-SNE on a dataset\\nwith plenty of samples.\\n●SVM’s run-time grows cubically with the total number of samples.\\n●t-SNE’s run-time grows quadratically with the total number of samples.\\nAnother advantage of ﬁguring out the run-time is that it helps us understand\\nhow an algorithm works end-to-end. Of course, in the above table, I have made\\nsome assumptions here and there. For instance:\\n●In a random forest, all decision trees may have diﬀerent depths. But here, I\\nhave assumed that they are equal.\\n●During inference in kNN, we ﬁrst ﬁnd the distance to all data points. This\\ngives a list of distances of size ל (total samples).\\n○Then, we ﬁnd the k-smallest distances from this list.\\n○The run-time to determine the k-smallest values may depend on the\\nimplementation.\\n■Sorting and selecting the k-smallest values will be ѡ(כ\\u05ceoղל).\\n■But if we use a priority queue, it will take ѡ(כ\\u05ceoղ\\u0601k\\u0602\\u0603.\\n●In t-SNE, there’s a learning step. Since the major run-time comes from\\ncomputing the pairwise similarities in the high-dimensional space, we\\nhave ignored that step.\\nNonetheless, the table still accurately reﬂects the general run-time of each of\\nthese algorithms.\\nAs an exercise, I would encourage you to derive these run-time complexities\\nyourself. This activity will provide you so much conﬁdence in algorithmic\\nunderstanding.\\n118'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 119}, page_content='DailyDoseofDS.com\\nٗ5\\nіoءق\\nОm\\u05fa\\u05ebrفӞnف\\nіaف\\u058beוӞt֕Ԏa\\u05cd όeﬁלiف֖oכآ ֖n\\nόaفӞ ѽc֕ԷnԍԸ\\nؕIء זaف\\u058beוӞt֕Ԏa\\u05cd ׂnתٸlԶԠgԶ ֖m\\u05fa\\u05ebrفӞnف ֖n ԠaفӞ آc֕ԷnԍԸ Ӟnԟ זaԍ\\u058biכԷ\\n\\u05ceeӝ؟n֕לgؐؗ\\nThis is a question that so many people have, especially those who are just getting\\nstarted.\\nShort answer: Yes, it’s important, and here’s why I say so.\\nSee…these days, one can do “ML” without understanding any mathematical\\ndetails of an algorithm. For instance (and thanks to sklearn, by the way):\\n●One can use any clustering algorithm in 2-3 lines of code.\\n●One can train classiﬁcation models in 2-3 lines of code.\\n●And more.\\nThis is both good and bad:\\n●It’s good because it saves us time.\\n●It’s bad because this tempts us to ignore the underlying details.\\nIn fact, I know many data scientists (mainly on the applied side) who do not\\nentirely understand the mathematical details but can still build and deploy\\nmodels.\\nNothing wrong.\\n119'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 120}, page_content='DailyDoseofDS.com\\nHowever, when I talk to them, I also see some disconnect between “What they\\nare using” and “Why they are using it.”\\nDue to a lack of understanding of the underlying details:\\n●They ﬁnd it quite diﬃcult to optimize their models.\\n●They struggle to identify potential areas of improvement.\\n●They take a longer time to debug when things don’t work well.\\n●They do not fully understand the role of speciﬁc hyperparameters.\\n●They use any algorithm without estimating their time complexity ﬁrst.\\nIf it feels like you are one of them, it’s okay. This problem can be solved.\\nThat said, if you genuinely aspire to excel in this ﬁeld, building a curiosity for the\\nunderlying mathematical details holds exponential returns.\\n●Algorithmic awareness will give you conﬁdence.\\n●It will decrease your time to build and iterate.\\nGradually, you will go from a hit-and-trial approach to “I know what should\\nwork.”\\nTo help you take that ﬁrst step, I prepared the following visual, which lists some\\nof the most important mathematical formulations used in Data Science and\\nStatistics (in no speciﬁc order).\\nBefore reading ahead, look at them one by one and calculate how many of them\\ndo you already know:\\n120'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 121}, page_content='DailyDoseofDS.com\\nSome of the terms are pretty self-explanatory, so I won’t go through each of them,\\nlike:\\n●Gradient Descent, Normal Distribution, Sigmoid, Correlation, Cosine\\nsimilarity, Naive Bayes, F1 score, ReLU, Soϔmax, MSE, MSE + L2\\nregularization, KMeans, Linear regression, SVM, Log loss.\\nHere are the remaining terms:\\n●MLE (Maximum Likelihood Estimation): A method for estimating the\\nparameters of a statistical model by maximizing the likelihood of the\\nobserved data. We covered it in the previous chapter.\\n121'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 122}, page_content='DailyDoseofDS.com\\n●Z-score: A standardized value that indicates how many standard deviations\\na data point is from the mean.\\n●Ordinary Least Squares: A closed-form solution for linear regression\\nobtained using the MLE step mentioned above.\\n●Entropy: A measure of the uncertainty of a random variable.\\n●Eigen Vectors: The non-zero vectors that do not change their direction\\nwhen a linear transformation is applied. It is widely used in dimensionality\\nreduction techniques like PCA.\\n●R2 (R-squared): A statistical measure that represents the proportion of\\nvariance explained by a regression model.\\n●KL divergence: Assess how much information is lost when one distribution\\nis used to approximate another distribution. It is used as a loss function in\\nthe t-SNE algorithm.\\n●SVD: A factorization technique that decomposes a matrix into three other\\nmatrices, oϔen noted as U, Σ, and V. It is fundamental in linear algebra for\\napplications like dimensionality reduction, noise reduction, and data\\ncompression.\\n●Lagrange multipliers: They are commonly used mathematical techniques\\nto solve constrained optimization problems. For instance, consider an\\noptimization problem with an objective function է(ٺ\\u0603 and assume that the\\nconstraints are ճ(ٺ\\u0603=ژ and \\u058b(ٺ\\u0603=ژ. Lagrange multipliers solve this.\\n122'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 123}, page_content='DailyDoseofDS.com\\nЕoٷ\\nقo\\nѺe\\u05cd֖aӸ\\u05cey\\nОm\\u05fa؟oٱԷ\\nѮrתӹaӸ֖l֕آt֕Ԏ\\nіu\\u05cdقiԍ\\u05ceaءآ-ԍ\\u05ceaءآiﬁԎaف֖oכ іoԟԷlء\\nML model building is typically an iterative process. Given some dataset:\\n●We train a model.\\n●We evaluate it.\\n●And we continue to improve it until we are satisﬁed with the performance.\\nHere, the eﬃcacy of any model improvement strategy (say, introducing a new\\nfeature) is determined using some sort of performance metric.\\nHowever, I have oϔen observed that when improving probabilistic\\nmulticlass-classiﬁcation models, this technique can be a bit deceptive when the\\neﬃcacy is determined using “Accuracy.”\\n123'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 124}, page_content='DailyDoseofDS.com\\nѮrתӹaӸ֖l֕آt֕Ԏ זu\\u05cdقiԍ\\u05ceaءآ-ԍ\\u05ceaءآiﬁԎaف֖oכ זoԟԷlء ӞrԶ قhתآe זoԟԷlء قhӝك\\n\\u05ebuف\\u05fbuف \\u05fbrתӹaӸ֖l֕قiԶآ Ԏo؞؟eء\\u05fboכԠiכճ قo Էaԍ\\u058b Ԏlӝآsԝ \\u05ceiׁԷ לeٛ؟a\\u05cd לeفٸo؞ׂs؉\\nIn other words, it is possible that we are actually making good progress in\\nimproving the model, but “Accuracy” is not reﬂecting that (yet).\\nLet’s understand.\\nѮiفէa\\u05cd\\u05ce \\u05ebf ίcԍٜrӝԎy\\nIn probabilistic multiclass-classiﬁcation models, Accuracy is determined using\\nthe output label that has the highest probability:\\nNow, it’s possible that the actual label is not predicted with the highest\\nprobability by the model, but it’s in the top “k” output labels.\\n124'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 125}, page_content='DailyDoseofDS.com\\nFor instance, in the image below, the actual label (Class C) is not the highest\\nprobability label, but it’s at least in the top 2 predicted probabilities (Class B and\\nClass C):\\nAnd what if in an earlier version of our model, the output probability of Class C\\nwas the lowest, as depicted below:\\nNow, of course, in both cases, the ﬁnal prediction is incorrect.\\nHowever, while iterating from “Version 1” to “Version 2” using some model\\nimprovement techniques, we genuinely made good progress.\\n125'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 126}, page_content='DailyDoseofDS.com\\nNonetheless, Accuracy entirely discards this as it only cares about the highest\\nprobability label.\\nI hope you understand the problem here.\\nѽo\\u05cdٜt֕\\u05ebn\\nWhenever I am building and iteratively improving any probabilistic multiclass\\nclassiﬁcation model, I always use the top-k accuracy score. As the name suggests,\\nit computes whether the correct label is among the top k labels predicted\\nprobabilities or not.\\nAs you may have already guessed, top-1 accuracy score is the traditional\\nAccuracy score. This is a much better indicator to assess whether my model\\nimprovement eﬀorts are translating into meaningful enhancements in predictive\\nperformance or not.\\nFor instance, if the top-3 accuracy score goes from 75% to 90%, this totally\\nsuggests that whatever we did to improve the model was eﬀective:\\n●Earlier, the correct prediction was in the top 3 labels only 75% of the time.\\n●But now, the correct prediction is in the top 3 labels 90% of the time.\\n126'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 127}, page_content='DailyDoseofDS.com\\nAs a result, one can eﬀectively redirect their engineering eﬀorts in the right\\ndirection. Of course, what I am saying should only be used to assess the model\\nimprovement eﬀorts.\\nThis is because true predictive power will inevitably be determined using\\ntraditional model accuracy. So make sure you are gradually progressing on the\\nAccuracy front too.\\nIdeally, it is expected that “Top-k Accuracy” may continue to increase during\\nmodel iterations, which reﬂects improvement in performance. Accuracy,\\nhowever, may stay the same for a while, as depicted below:\\nTop-k accuracy score is also available in Sklearn:\\n127'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 128}, page_content='DailyDoseofDS.com\\nӀoٛ؟ ϛnف֖rԶ іoԟԷl Оm\\u05fa؟oٱԷmԶלt ϛﬀ\\u05ebrفآ іiղ\\u058bt λe\\nЄo֕לg ֖n ұa֕ל\\nBack in 2019, I was working with an ML research group in Germany.\\nOne day, a Ph.D. student\\ncame up to me (and\\nothers in the lab), handed\\nover a small sample of\\nthe dataset he was\\nworking with, and\\nrequested us to label it,\\ndespite having true\\nlabels.\\nThis made me curious about why gathering human labels was necessary for him\\nwhen he already had ground truth labels available. So I asked.\\nWhat I learned that day changed my approach to incremental model\\nimprovement, and I am sure you will ﬁnd this idea fascinating too.\\nLet me explain what I learned.\\nConsider we are building a multiclass classiﬁcation model. Say it’s a model that\\nclassiﬁes an input image as a rock, paper, or scissors:\\n128'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 129}, page_content='DailyDoseofDS.com\\nFor simplicity, let’s assume there’s no class imbalance. Calculating the class-wise\\nvalidation accuracies gives us the following results:\\nѷuԶآt֕\\u05ebnԛ ҷh֕Ԏh Ԏlӝآs ٸoٛ\\u05ced ځoٛ זoءق ֖nفٜiف֖vԶ\\u05cey \\u05fbrתԎeԶԠ قo ֖nء\\u05fbeԍق\\nէu؞قhԶ؟ Ӟnԟ ֖m\\u05fa؟oٱԷ قhԶ זoԟԷl \\u05ebnؐ\\nAϔer looking at these results,\\nmost people believe that\\n“Scissor” is the worst-performing\\nclass and should be inspected\\nfurther.\\nBut this might not be true. And this is precisely what that Ph.D. student wanted\\nto verify by collecting human labels. Let’s say that the human labels give us the\\nfollowing results:\\nBased on this, do you still think the model performs the worst on the “Scissor”\\nclass?\\nNo, right?\\n129'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 130}, page_content='DailyDoseofDS.com\\nI mean, of course, the model has the least accuracy on the “Scissor” class, and I\\nam not denying it. However, with more context, we notice that the model is doing\\na pretty good job classifying the “Scissor” class. This is because an average\\nhuman is achieving just 2% higher accuracy in comparison to what our model is\\nable to achieve.\\nHowever, the above results astonishingly reveal that it is the “Rock” class instead\\nthat demands more attention. The accuracy diﬀerence between an average\\nhuman and the model is way too high (13%). Had we not known this, we would\\nhave continued to improve the “Scissor” class, when in reality, “Rock” requires\\nmore improvement.\\nEver since I learned this technique, I have found it super helpful to determine my\\nnext steps for model improvement, if possible. I say “if possible” because I\\nunderstand that many datasets are hard for humans to interpret and label.\\nNonetheless, if it is feasible to set up such a “human baseline,” one can get so\\nmuch clarity into how the model is performing.\\nAs a result, one can eﬀectively redirect their engineering eﬀorts in the right\\ndirection.\\nOf course, I am not claiming that this will be universally useful in all use cases.\\nFor instance, if the model is already performing better than the baseline, the\\nmodel improvements from there on will have to be guided based on past results.\\nYet, in such cases, surpassing a human baseline at least helps us validate that the\\nmodel is doing better than what a human can do.\\n130'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 131}, page_content='DailyDoseofDS.com\\nѐoءآ ϾuכԎt֕\\u05ebn \\u05ebf \\u05f86 іL ίlղ\\u05ebs\\nThe below visual depicts the most commonly used loss functions by various ML\\nalgorithms.\\n131'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 132}, page_content='DailyDoseofDS.com\\n\\u05f80 іoءق ρoוזoכ ѐoءآ ϾuכԎt֕\\u05ebn\\nThe below visual depicts some commonly used loss functions in regression and\\nclassiﬁcation tasks.\\n132'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 133}, page_content='DailyDoseofDS.com\\nЕoٷ قo ίcفٜa\\u05cd\\u05cey ҝsԶ ҏrӝ֖nԝ ұa\\u05cd֖dӝقiתל Ӟnԟ ҏeءق ѽeف\\nIt is pretty conventional to split the given data into train, test, and validation sets.\\nHowever, there are quite a few misconceptions about how they are meant to be\\nused, especially the validation and test sets.\\nIn this chapter, let’s clear them up and see how to truly use train, validation, and\\ntest sets.\\nWe begin by splitting the data into:\\n●Train\\n●Validation\\n●Test\\nAt this point, just assume that the test data does not even exist. Forget about it\\ninstantly.\\n133'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 134}, page_content='DailyDoseofDS.com\\nBegin with the train set. This is your whole world now.\\n●You analyze it\\n●You transform it\\n●You use it to determine features\\n●You ﬁt a model on it\\nAϔer modeling, you would want to measure the model’s performance on unseen\\ndata, wouldn’t you?\\nBring in the validation set now.\\nBased on validation performance, improve the model. Here’s how you iteratively\\nbuild your model:\\n134'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 135}, page_content='DailyDoseofDS.com\\n●Train using a train set\\n●Evaluate it using the validation set\\n●Improve the model\\n●Evaluate again using the validation set\\n●Improve the model again\\n●and so on.\\nUntil...\\nYou reach a point where you start overﬁtting the validation set.\\nThis indicates that you have exploited (or polluted) the validation set.\\nNo worries.\\nMerge it with the train set and generate a new split of train and validation.\\nNote: Rely on cross-validation if needed, especially when you don’t have much\\ndata. You may still use cross-validation if you have enough data. But it can be\\ncomputationally intensive.\\nNow, if you are happy with the model’s performance, evaluate it on test data.\\nWhat you use a test set for:\\n●Get a ﬁnal and unbiased review of the model.\\nWhat you DON’T use a test set for:\\n●Analysis, decision-making, etc.\\n135'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 136}, page_content='DailyDoseofDS.com\\nIf the model is underperforming on the test set, no problem. Go back to the\\nmodeling stage and improve it.\\nBUT (and here’s what most people do wrong)!\\nThey use the same test set again. This is not allowed!\\nThink of it this way.\\nYour professor taught you in the classroom. All in-class lessons and examples are\\nthe train set.\\nThe professor gave you take-home assignments, which acted like validation sets.\\nYou got some wrong and some right. Based on this, you adjusted your topic\\nfundamentals, i.e., improved the model.\\nNow, if you keep solving the same take-home assignment repeatedly, you will\\neventually overﬁt it, won’t you?\\nThat is why we bring in a new validation set aϔer some iterations.\\nThe ﬁnal exam day paper is your test set. If you do well, awesome!\\nBut if you fail, the professor cannot give you the exact exam paper next time, can\\nthey? This is because you know what’s inside.\\nOf course, by evaluating a model on the test set, the model never gets to “know”\\nthe precise examples inside that set. But the issue is that the test set has been\\nexposed now.\\nYour previous evaluation will inevitably inﬂuence any further evaluations on that\\nspeciﬁc test set. That is why you must always use a speciﬁc test set only ONCE.\\nOnce you do, merge it with the train and validation set and generate an entirely\\nnew split.\\nRepeat.\\nAnd that is how you use train, validation, and test sets in machine learning.\\n136'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 137}, page_content='DailyDoseofDS.com\\nխ ρrתآs ұa\\u05cd֖dӝقiתל ҏeԍ\\u058bn֕؎uԶآ\\nTuning and validating machine learning models on a single validation set can be\\nmisleading and sometimes yield overly optimistic results.\\nThis can occur due to a lucky random split of data, which results in a model that\\nperforms exceptionally well on the validation set but poorly on new, unseen data.\\nThat is why we oϔen use cross validation instead of simple single-set validation.\\nCross validation involves repeatedly partitioning the available data into subsets,\\ntraining the model on a few subsets, and validating on the remaining subsets.\\nThe main advantage of cross validation is that it provides a more robust and\\nunbiased estimate of model performance compared to the traditional validation\\nmethod.\\nBelow are ﬁve of the most commonly used and must-know cross validation\\ntechniques.\\nѐeӝٲe֓ѡnԶ֔Oٛق ρrתآs ұa\\u05cd֖dӝقiתל\\n●Leave one data point for validation.\\n●Train the model on the remaining data points.\\n●Repeat for all points.\\n●Of course, as you may have guessed, this is practically infeasible when you\\nhave many data points. This is because number of models is equal to\\nnumber of data points.\\n●We can extend this to Leave-p-Out Cross Validation, where, in each\\niteration, p observations are reserved for validation, and the rest are used\\nfor training.\\n137'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 138}, page_content='DailyDoseofDS.com\\nф-Ͻ\\u05eblԟ ρrתآs ұa\\u05cd֖dӝقiתל\\n●Split data into k equally-sized subsets.\\n●Select one subset for validation.\\n●Train the model on the remaining subsets.\\n●Repeat for all subsets.\\nѺo\\u05cd\\u05ceiכճ ρrתآs ұa\\u05cd֖dӝقiתל\\n●Mostly used for data with temporal structure.\\n●Data splitting respects the temporal order, using a ﬁxed-size training\\nwindow.\\n●The model is evaluated on the subsequent window.\\n138'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 139}, page_content='DailyDoseofDS.com\\nλlתԎkԶԠ ρrתآs ұa\\u05cd֖dӝقiתל\\n●Another common technique for time-series data.\\n●In contrast to rolling cross validation, the slice of data is intentionally kept\\nshort if the variance does not change appreciably from one window to the\\nnext.\\n●This also saves computation over rolling cross validation.\\nѽt؞Ӟt֕ﬁԷd ρrתآs ұa\\u05cd֖dӝقiתל\\n●The above-discussed techniques may not work for imbalanced datasets.\\nStratiﬁed cross validation is mainly used for preserving the class\\ndistribution.\\n●The partitioning ensures that the class distribution is preserved.\\nLet’s continue our discussion on cross validation in the next chapter.\\n139'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 140}, page_content='DailyDoseofDS.com\\nҷhӝق ҏo όo ίfفԷr ρrתآs ұa\\u05cd֖dӝقiתל?\\nLet me ask you a question.\\nBut before I do that, I need to borrow your imagination for just a moment.\\nОmӝճiכԷ ځoٛ ӞrԶ ӹu֕\\u05ced֕לg آoוԷ آu\\u05faԷrٱ֖sԶԠ זaԍ\\u058biכԷ \\u05ceeӝ؟n֕לg זoԟԷl؉ Ӏoٛ\\nӞrԶ ٜs֕לg Ԏrתآs֓ٲa\\u05cd֖dӝقiתל قo ԠeفԷrו֖nԶ Ӟn \\u05ebpف֖mӝ\\u05ce آeف \\u05ebf\\n\\u058by\\u05faԷr\\u05faӞrӝזeفԷrء؊\\nEssentially, every hyperparameter conﬁguration corresponds to a cross-validation\\nperformance:\\nAϔer obtaining the best hyperparameters, we need to ﬁnalize a model (say for\\nproduction); otherwise, what is the point of all this hassle? Now, here’s the\\nquestion:\\nAϔer obtaining the optimal hyperparameters, what would you be more inclined\\nto do:\\n1) Retrain the model again on the\\nentire data (train + validation + test)\\nwith the optimal hyperparameters?\\nIf we do this, remember that we can’t\\nreliably validate this new model as\\nthere is no unseen data leϔ.\\n140'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 141}, page_content='DailyDoseofDS.com\\n2) Just proceed with the\\nbest-performing model based on\\ncross-validation performance itself. If\\nwe do this, remember that we are\\nleaving out important data, which we\\ncould have trained our model with.\\nWhat would you do?\\nѺeԍ\\u05ebmוԷnԟԸԠ \\u05fbaف\\u058b\\nMy strong preference has almost always been “retraining a new model with\\nentire data.”\\nThere are, of course, some considerations to keep in mind, which I have learned\\nthrough the models I have built and deployed. That said, in most cases, retraining\\nis the ideal way to proceed.\\nLet me explain.\\nҷhڀ ؟eفؠӞiכ قhԶ זoԟԷlؐ\\nWe would want to retrain a new model because, in a way, we are already satisﬁed\\nwith the cross-validation performance, which, by its very nature, is an out-of-fold\\nmetric.\\nAn out-of-fold data is data that has not been seen by the model during the\\ntraining. An out-of-fold metric is the performance on that data.\\n141'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 142}, page_content='DailyDoseofDS.com\\nIn other words, we already believe that the model aligns with how we expect it to\\nperform on unseen data.\\nThus, incorporating this unseen validation set in the training data and retraining\\nthe model will MOST LIKELY have NO eﬀect on its performance on unseen data\\naϔer deployment (assuming a sudden covariate shiϔ hasn’t kicked in, which is a\\ndiﬀerent issue altogether).\\nIf, however, we were not satisﬁed with the cross-validation performance itself, we\\nwouldn’t even be thinking about ﬁnalizing a model in the ﬁrst place.\\nInstead, we would be thinking about ways to improve the model by working on\\nfeature engineering, trying new hyperparameters, experimenting with diﬀerent\\nmodels, and more.\\nThe reasoning makes intuitive sense as well.\\nIt’s hard for me to recall any instance where retraining did something\\ndisastrously bad to the overall model.\\n142'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 143}, page_content='DailyDoseofDS.com\\nIn fact, I vividly remember one instance wherein, while I was productionizing the\\nmodel (it took me a couple of days aϔer retraining), the team had gathered some\\nmore labeled data.\\nThe model didn’t show any performance degradation when I evaluated it (just to\\ndouble-check). As an added beneﬁt, this also helped ensure that I had made no\\nerrors while productionizing my model.\\nѽoוԷ ԎoכآiԟԷrӝقiתלs\\nHere, please note that it’s not a rule that you must always retrain a new model.\\nThe ﬁeld itself and the tasks one can solve are pretty diverse, so one must be\\nopen-minded while solving the problem at hand. One of the reasons I wouldn’t\\nwant to retrain a new model is that it takes days or weeks to train the model.\\nIn fact, even if we retrain a new model, there are MANY business situations in\\nwhich stakes are just too high.\\n143'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 144}, page_content='DailyDoseofDS.com\\nThus, one can never aﬀord to be negligent about deploying a model without\\nre-evaluating it — transactional fraud, for instance.\\nIn such cases, I have seen that while a team works on productionizing the model,\\ndata engineers gather some more data in the meantime.\\nBefore deploying, the team would do some ﬁnal checks on that dataset.\\nThe newly gathered data is then considered in the subsequent iterations of model\\nimprovements.\\n144'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 145}, page_content='DailyDoseofDS.com\\nόoٛӹlԶ όeءԎeכق ٲs؉ λiӝآ-ҰӞr֕ӟלcԶ ҏrӝԠe֓\\u05ebﬀ\\nIt is well-known that as the number of model parameters increases, we typically\\noverﬁt the data more and more. For instance, consider ﬁtting a polynomial\\nregression model trained on this dummy dataset below:\\nОn ԎaءԷ ځoٛ Ԡoכ؛t ׂnתٸ, قh֕آ ֖s Ԏa\\u05cd\\u05ceeԟ Ӟ \\u05fbo\\u05cdځnתזiӝ\\u05ce ؟eղؠԷsء֖oכ זoԟԷlԛ\\nIt is expected that as we’ll increase the degree (m) and train the polynomial\\nregression model:\\n●The training loss will get closer and closer to zero.\\n●The test (or validation) loss will ﬁrst reduce and then get bigger and bigger.\\nThis is because, with a higher degree, the model will ﬁnd it easier to contort its\\nregression ﬁt through each training data point, which makes sense.\\n145'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 146}, page_content='DailyDoseofDS.com\\nIn fact, this is also evident from the following loss plot:\\nBut notice what happens when we continue to increase the degree (ז):\\nThat’s strange, right?\\nWhy does the test loss increase to a certain point but then decrease?\\nThis was not expected, was it?\\n146'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 147}, page_content='DailyDoseofDS.com\\nWell…what you are seeing is called the “double descent phenomenon,” which is\\nquite commonly observed in many ML models, especially deep learning models.\\nIt shows that, counterintuitively, increasing the model complexity beyond the\\npoint of interpolation can improve generalization performance.\\nIn fact, this whole idea is deeply rooted to why LLMs, although massively big\\n(billions or even trillions of parameters), can still generalize pretty well.\\nAnd it’s hard to accept it because this phenomenon directly challenges the\\ntraditional bias-variance trade-oﬀwe learn in any introductory ML class:\\nPutting it another way, training very large models, even with more parameters\\nthan training data points, can still generalize well.\\nTo the best of my knowledge, this is still an open question, and it isn’t entirely\\nclear why neural networks exhibit this behavior.\\nThere are some theories around regularization, however, such as this one:\\nIt could be that the model applies some sort of implicit regularization, with\\nwhich, it can precisely focus on an apt number of parameters for generalization.\\nBut to be honest, nothing is clear yet.\\n147'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 148}, page_content='DailyDoseofDS.com\\nѽtӝقiءك֖cӝ\\u05ce Ͼoٛלdӝقiתלs\\nіLϚ ٲs؉ ϛM — ҷhӝق’ء قhԶ όiﬀԷrԶלcԶؑ\\nMaximum likelihood estimation (MLE) and expectation maximization (EM) are\\ntwo popular techniques to determine the parameters of statistical models.\\nDue to its applicability in MANY statistical models, I have seen it being asked in\\nplenty of data science interviews as well, especially the distinction between the\\ntwo. The following visual summarizes how they work:\\n148'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 149}, page_content='DailyDoseofDS.com\\nіaٺ֖mٛז \\u05ceiׁԷl֕\\u058boתԠ Էsف֖mӝقiתל \\u0601Mяϛ)\\nMLE starts with a labeled dataset and aims to determine the parameters of the\\nstatistical model we are trying to ﬁt.\\nThe process is pretty simple and straightforward. In MLE, we:\\n●Start by assuming a data generation process. Simply put, this data\\ngeneration process reﬂects our belief about the distribution of the output\\nlabel (ځ), given the input (Һ).\\n●Next, we deﬁne the likelihood of observing the data. As each observation is\\nindependent, the likelihood of observing the entire data is the same as the\\nproduct of observing individual observations:\\n149'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 150}, page_content='DailyDoseofDS.com\\n●The likelihood function above depends on parameter values (θ). Our\\nobjective is to determine those speciﬁc parameter values that maximize the\\nlikelihood function. We do this as follows:\\nThis gives our parameter estimates that would have most likely generated the\\ngiven data.\\nThat was pretty simple, wasn’t it?\\nBut what do we do if we\\ndon’t have true labels?\\nWe still want to\\nestimate the parameters,\\ndon’t we?\\nMLE, as you may have guessed, will not be applicable. The true label (y), being\\nunobserved, makes it impossible to deﬁne a likelihood function like we did\\nearlier.\\nIn such cases, advanced techniques like expectation maximization are pretty\\nhelpful.\\n150'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 151}, page_content='DailyDoseofDS.com\\nϛx\\u05faԷcفӞt֕\\u05ebn זaٺ֖m֕ړaف֖oכ \\u0601Eѕ\\u0603\\nEM is an iterative optimization technique to estimate the parameters of\\nstatistical models. It is particularly useful when we have an unobserved (or\\nhidden) label. One example situation could be as follows:\\nAs depicted above, we assume that the data was generated from multiple\\ndistributions (a mixture). However, the observed/complete data does not contain\\nthat information. In other words, the observed dataset does not have information\\nabout whether a speciﬁc row was generated from distribution 1 or distribution 2.\\nHad it contained the label (ځ) information, we would have already used MLE.\\nEM helps us with parameter estimates of such datasets. The core idea behind EM\\nis as follows:\\n●Make a guess about the initial parameters (θ).\\n●Expectation (E) step: Compute the posterior probabilities of the\\nunobserved label (let’s call it ‘z’) using the above parameters.\\n151'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 152}, page_content='DailyDoseofDS.com\\n○Here, ‘z’ is also called a latent variable, which means hidden or\\nunobserved.\\n○Relating it to our case, we know that the true label exists in nature.\\nBut we don’t know what it is.\\n○Thus, we replace it with a latent variable ‘z’ and estimate its\\nposterior probabilities using the guessed parameters.\\n●Given that we now have a proxy (not precise, though) for the true label, we\\ncan deﬁne an “expected likelihood” function. Thus, we use the above\\nposterior probabilities to do so:\\n●Maximization (M) step: So now we have a likelihood function to work with.\\nMaximizing it with respect to the parameters will give us a new estimate\\nfor the parameters (θ`).\\n●Next, we use the updated parameters (θ`) to recompute the posterior\\nprobabilities we deﬁned in the expectation step.\\n●We will update the likelihood function (L) using the new posterior\\nprobabilities.\\n●Again, maximizing it will give us a new estimate for the parameters (θ).\\n●And this process goes on and on until convergence.\\nThe point is that in expectation maximization, we repeatedly iterate between the\\nE and the M steps until the parameters converge. A good thing about EM is that\\nit always converges. Yet, at times, it might converge to a local extrema.\\nMLE vs. EM is a popular question asked in many data science interviews.\\n152'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 153}, page_content='DailyDoseofDS.com\\nρoכﬁԠeכԎe ОnفԷrٱӞl Ӟnԟ ѮrԶԠiԍقiתל ОnفԷrٱӞl\\nStatistical estimates always have some uncertainty.\\nFor instance, a linear regression model never predicts an actual value.\\nConsider a simple example of modeling house prices just based on its area. A\\nprediction wouldn’t tell the true value of a house based on its area. This is\\nbecause diﬀerent houses of the same size can have diﬀerent prices.\\nInstead, what it predicts is the mean value related to the outcome at a particular\\ninput.\\nThe point is…\\nThere’s always some uncertainty involved in statistical estimates, and it is\\nimportant to communicate it.\\nIn this speciﬁc case, there are two types of uncertainties:\\n●The uncertainty in estimating the true mean value.\\n●The uncertainty in estimating the true value.\\nConﬁdence interval and prediction interval help us capture these uncertainties.\\nLet’s understand.\\n153'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 154}, page_content='DailyDoseofDS.com\\nConsider the following dummy dataset:\\nLet’s ﬁt a linear regression model using statsmodel and print a part of the\\nregression summary:\\nNotice that the coeﬃcient of the predictor “x1” is ٗ.ژխ0ى with a ן5؇ interval\\nof ԇ1؉Ղ7ٖԞ ٗ.ٖٗ8Ԉ.\\nОt ֖s Ӟ ן5؇ ֖nفԷrٱӞl ӹeԍӞuءԷ ڙ.מت5֓ڙ.ژٗ5 Օ ڙ.מխ.\\nThis is known as the conﬁdence interval, which comes from sampling\\nuncertainty.\\n154'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 155}, page_content='DailyDoseofDS.com\\nMore speciﬁcally, this\\nuncertainty arises because\\nthe data we used above for\\nmodeling is just a sample of\\nthe population.\\nSo, if we gathered more such samples and ﬁt an OLS to each sample, the true\\ncoeﬃcient (which we can only know if we had the data for the entire population)\\nwould lie 95% of the time in this conﬁdence interval.\\nNext, we use this model to make a prediction as follows:\\n●The predicted value is ت.طٗ \\u0601mԶӞn\\u0602.\\n●The ן5؇ conﬁdence interval is ԇ6؉Ղ3ٖԞ Ղ.կ\\u05f89Ԉ.\\n●The ן5؇ prediction interval is ԇ1؉ڙ6ԝ \\u05f84؉\\u05f89Ԉ.\\nThe conﬁdence interval we saw above was for the coeﬃcient, so what does the\\nconﬁdence interval represent in this case?\\nSimilar to what we discussed above, the data is just a sample of the population.\\nThe regression ﬁt obtained by this sample produced a prediction (some mean\\nvalue) for the input ٻ=ى.\\nHowever, if we gathered more such samples and ﬁt an OLS to each dataset, the\\ntrue mean value (which we can only know if we had the data for the entire\\n155'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 156}, page_content='DailyDoseofDS.com\\npopulation) for this speciﬁc input (ٻ=ى) would lie ן5؇ of the time in this\\nconﬁdence interval.\\nComing to the prediction interval…\\n…we notice that it is wider than the conﬁdence interval. Why is it, and what does\\nthis interval tell?\\nWhat we saw above with conﬁdence interval was about estimating the true\\npopulation mean at a speciﬁc input.\\nWhat we are talking about now is obtaining an interval where the true value for\\nan input can lie.\\nThus, this additional uncertainty appears because in our dataset, for the same\\nvalue of input x, there can be multiple diﬀerent values of the outcome. This is\\ndepicted below:\\n156'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 157}, page_content='DailyDoseofDS.com\\nThus, it is wider than the conﬁdence interval. Plotting it across the entire input\\nrange, we get the following plot:\\nGiven that the model is predicting a mean value (as depicted below), we have to\\nrepresent the prediction uncertainty that the actual value can lie anywhere in the\\nprediction interval:\\nA ן5؇ prediction interval tells us that we can be ן5؇ sure that the actual value\\nof this observation will fall within this interval.\\n157'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 158}, page_content='DailyDoseofDS.com\\nSo to summarize:\\n●A conﬁdence interval captures the sampling uncertainty. More data means\\nless sampling uncertainty, which in turn leads to a smaller interval.\\n●In addition to the sampling uncertainty, the prediction interval also\\nrepresents the uncertainty in estimating the true value of a particular data\\npoint. Thus, it is wider than the conﬁdence interval.\\nCommunicating these uncertainties is quite crucial in decision-making because\\nit provides a clearer understanding of the reliability and precision of predictions.\\nThis transparency allows stakeholders to make more informed decisions by\\nconsidering the range of possible outcomes and the associated risks.\\n158'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 159}, page_content='DailyDoseofDS.com\\nҷhڀ ֖s ѡLѼ ρa\\u05cd\\u05ceeԟ Ӟn ҝnӸ֖aءԷd ϛsف֖mӝقo؞ؑ\\nThe OLS estimator for linear regression (shown below) is known as an unbiased\\nestimator.\\n●What do we mean by that?\\n●Why is OLS called such?\\nλaԍׂg؞\\u05ebuכԠ\\nThe goal of statistical modeling is to make conclusions about the whole\\npopulation.\\nHowever, it is pretty obvious that observing the entire population is impractical.\\nIn other words, given that we cannot observe (or collect data of) the entire\\npopulation, we cannot obtain the true parameter (β) for the population:\\n159'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 160}, page_content='DailyDoseofDS.com\\nThus, we must obtain parameter estimates (B̂) on samples and infer the true\\nparameter (β) for the population from those estimates:\\nAnd, of course, we want these sample estimates (B̂) to be reliable to determine the\\nactual parameter (β).\\nThe OLS estimator ensures that.\\nLet’s understand how!\\nҏrٛԷ \\u05fbo\\u05faٜlӝقiתל זoԟԷl\\nWhen using a linear regression model, we assume that the response variable (Y)\\nand features (X) for the entire population are related as follows:\\n●β is the true parameter that we are not aware of.\\n●ε is the error term.\\nϛx\\u05faԷcفԸԠ ұa\\u05cdٜe \\u05ebf ѡLѼ ϛsف֖mӝقeء\\nThe closed-form solution of OLS is given by:\\n160'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 161}, page_content='DailyDoseofDS.com\\nWhat’s more, as discussed above, using OLS on diﬀerent samples will result in\\ndiﬀerent parameter estimates:\\nLet’s ﬁnd the expected value of OLS estimates ϛ[κ̂ ԉ.\\nSimply put, the expected value is the average value of the parameters if we run\\nOLS on many samples.\\nThis is given by:\\nSubstitute λ̂ as the OLS solution\\nHere, substitute Ӏ Օ βҺ ، ε:\\nIf you are wondering how we can substitute Ӏ Օ βҺ ، ε when we don’t know\\nwhat β is, then here’s the explanation:\\nSee, we can do that substitution because even if we don’t know the parameter β\\nfor the whole population, we know that the sample was drawn from the\\npopulation.\\n161'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 162}, page_content='DailyDoseofDS.com\\nThus, the equation in terms of the true parameters (Ӏ Օ βҺ ، ε) still holds for\\nthe sample.\\nLet me give you an example.\\nSay the population data was deﬁned by ځ Օ آiכ\\u0601x\\u0602 ، ε. Of course, we wouldn’t\\nknow this, but just keep that aside for a second.\\nNow, even if we were to draw samples from this population data, the true\\nequation ځ Օ آiכ\\u0601x\\u0602 ، ε would still be valid on the sampled data points,\\nwouldn’t it?\\nThe same idea has been extended for expected value.\\nComing back to the following:\\nLet’s open the inner parenthesis:\\n162'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 163}, page_content='DailyDoseofDS.com\\nSimplifying, we get:\\nAnd ﬁnally, what do we get?\\nThe expected value of parameter estimates on the samples equals the true\\nparameter value β.\\nAnd this is precisely what the deﬁnition of an unbiased estimator is.\\nMore formally, an estimator is called unbiased if the expected value of the\\nparameters is equal to the actual parameter value.\\nAnd that is why we call OLS an unbiased estimator.\\n163'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 164}, page_content='DailyDoseofDS.com\\nίn ֖m\\u05fa\\u05ebrفӞnف قaׁԷaٷӞy\\nMany people misinterpret unbiasedness with the idea that the parameter\\nestimates from a single run of OLS on a sample are equal to the true parameter\\nvalues.\\nDon’t make that mistake.\\nInstead, unbiasedness implies that if we were to generate OLS estimates on many\\ndiﬀerent samples (drawn from the same population), then the expected value of\\nobtained estimates will be equal to the true population parameter.\\nAnd, of course, all this is based on the assumption that we have good\\nrepresentative samples and that the assumptions of linear regression are not\\nviolated.\\n164'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 165}, page_content='DailyDoseofDS.com\\nλhӝقtӝԎhӝ؟yڀӞ όiءقaכԎe\\nAssessing the similarity between two probability distributions is quite helpful at\\ntimes. For instance, imagine we have a labeled dataset \\u0601Xԝ ځ).\\nBy analyzing the label (ځ) distribution, we may hypothesize its distribution before\\nbuilding a statistical model.\\nWe also looked at this in an earlier chapter on generalized linear models (GLMs).\\nWhile visual inspection is oϔen\\nhelpful, this approach is quite\\nsubjective and may lead to\\nmisleading conclusions.\\nThus, it is essential to be aware of quantitative measures as well. Bhattacharyya\\ndistance is one such reliable measure.\\nIt quantiﬁes the similarity between two probability distributions.\\nThe core idea is to approximate the overlap between two distributions, which\\nmeasures the “closeness” between the two distributions under consideration.\\n165'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 166}, page_content='DailyDoseofDS.com\\nBhattacharyya distance is measured as follows:\\nFor two discrete probability\\ndistributions\\nFor two continuous probability\\ndistributions (replace\\nsummation with an integral):\\nIts eﬀectiveness is evident from the image below.\\nHere, we have an\\nobserved distribution\\n(Blue). Next, we\\nmeasure its distance\\nfrom:\\n●Gaussian →0.19.\\n●Gamma →0.03.\\nA high Bhattacharyya\\ndistance indicates less\\noverlap or more\\ndissimilarity. This lets\\nus conclude that the\\nobserved distribution\\nresembles a Gamma\\ndistribution.\\nThe results also resonate with visual inspection.\\n166'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 167}, page_content='DailyDoseofDS.com\\nфL όiٱԷrղԸלcԶ ٲs؉ λhӝقtӝԎhӝ؟yڀӞ ԠiءقaכԎe\\nNow, many oϔen get confused between KL Divergence and Bhattacharyya\\ndistance. Eﬀectively, both are quantitative measures to determine the “similarity”\\nbetween two distributions.\\nHowever, their notion of “similarity” is entirely diﬀerent.\\nThe core idea behind KL Divergence is to assess how much information is lost\\nwhen one distribution is used to approximate another distribution.\\nThe more information is lost, the more the KL Divergence and, consequently, the\\nless the “similarity”. Also, approximating a distribution Q using P may not be the\\nsame as doing the reverse — P using Q. This makes KL Divergence asymmetric\\nin nature.\\nMoving on,\\nBhattacharyya distance\\nmeasures the overlap\\nbetween two\\ndistributions.\\nThis “overlap” is oϔen interpreted as a measure of closeness (or distance)\\nbetween the two distributions under consideration. Thus, Bhattacharyya distance\\nprimarily serves a distance metric, like Euclidean, for instance.\\n167'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 168}, page_content='DailyDoseofDS.com\\nBeing a distance metric, it is symmetric\\nin nature. We can also verify this from\\nthe distance formula:\\nJust like we use Euclidean distance to\\nﬁnd the distance between two points, we\\ncan use Bhattacharyya distance to ﬁnd\\nthe distance between two distributions.\\nBut if we intend to measure the amount of information lost when we approximate\\none distribution using another, KL divergence is more apt. In fact, KL divergence\\nalso serves as a loss function in machine learning algorithms at times (in t-SNE).\\n●Say we have an observed distribution (P) and want to approximate it with\\nanother simpler distribution Q.\\n●So, we can deﬁne a simpler parametric distribution Q.\\n●Next, we can measure the information lost by approximating P using Q\\nwith KL divergence.\\n●As we want to minimize the information lost, we can use KL divergence as\\nour objective function.\\n●Finally, we can use gradient descent to determine the parameters of Q such\\nthat we minimize the KL divergence.\\nBhattacharyya distance has many applications, not just in machine learning but\\nin many other domains. For instance:\\n●Using this distance, we can simplify complex distributions to simple ones\\nif the distance is low.\\n●In image processing, Bhattacharyya distance is oϔen used for image\\nmatching. By comparing the color or texture distributions of images, it\\nhelps identify similar objects or scenes, etc.\\nThe only small caveat is that Bhattacharyya distance does not satisfy the triangle\\ninequality, so that’s something to keep in mind.\\n168'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 169}, page_content='DailyDoseofDS.com\\nҷhڀ ѮrԶէe؞ іa֊ӞlӝלoӸ֖s όiءقaכԎe ѡvԶ؟ ϛuԍ\\u05ceiԟԷaכ\\nԠiءقaכԎeؐ\\nDuring distance calculation, Euclidean distance assumes independent axes.\\nThus, Euclidean distance will produce misleading results if your features are\\ncorrelated. For instance, consider this dummy dataset below:\\nClearly, the features are correlated. Here, consider three points marked P1, P2,\\nand P3 in this dataset.\\n169'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 170}, page_content='DailyDoseofDS.com\\nConsidering the data distribution, something tells us that P2 is closer to P1 than\\nP3. This is because P2 lies more within the data distribution than P3.\\nYet, Euclidean distance ignores this, and P2 and P3 come out to be equidistant to\\nP1, as depicted below:\\nMahalanobis distance addresses this limitation. It is a distance metric that takes\\ninto account the data distribution.\\nAs a result, it can measure how far away a data point is from the distribution,\\nwhich Euclidean can not.\\nReferring to the earlier dataset again, with Mahalanobis distance, P2 comes out\\nto be closer to P1 than P3.\\n170'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 171}, page_content='DailyDoseofDS.com\\nЕoٷ ԠoԶآ ֖t ٸo؞ׂ?\\nIn a gist, the objective is to construct a new coordinate system with independent\\nand orthogonal axes. The steps are:\\n●Step 1: Transform the columns into uncorrelated variables.\\n●Step 2: Scale the new variables to make their variance equal to 1.\\n●Step 3: Find the Euclidean distance in this new coordinate system.\\nSo, eventually, we do use Euclidean distance. However, we ﬁrst transform the\\ndata to ensure that it obeys the assumptions of Euclidean distance.\\nҝsԶآ\\nOne of the most common\\nuse cases of Mahalanobis\\ndistance is outlier\\ndetection. Reconsidering\\nthe dataset we discussed\\nearlier where P3 is clearly\\nan outlier.\\n171'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 172}, page_content='DailyDoseofDS.com\\nIf we consider P1 as the distribution’s centroid and use Euclidean distance, we\\nwill infer that P3 is not an outlier as both P2 and P3 are equidistant to P1.\\nUsing Mahalanobis distance, however, provides a clearer picture:\\nThis becomes more useful in a high-dimensional setting where visualization is\\ninfeasible.\\nAnother use case we typically do not hear of oϔen, but that exists is a variant of\\nkNN that is implemented with Mahalanobis distance instead.\\nScipy implements the Mahalanobis distance, which you can check here:\\nhttps://bit.ly/3LjAymm.\\n172'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 173}, page_content='DailyDoseofDS.com\\n\\u05f81 ҷaڀآ قo όeفԷrו֖nԶ όaفӞ љo؞זa\\u05cd֖tڀ\\nMany ML models assume (or work better) under the presence of normal\\ndistribution.\\nFor instance:\\n●Linear regression assumes residuals are normally distributed.\\n●At times, transforming the data to normal distribution can be beneﬁcial.\\n●Linear discriminant analysis (LDA) is derived under the assumption of\\nnormal distribution, etc.\\nThus, being aware of the ways to test normality is extremely crucial for data\\nscientists. The visual below depicts the 11 essential ways to test normality.\\nLet’s understand these in this chapter.\\n173'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 174}, page_content='DailyDoseofDS.com\\nש1\\u0602 Ѯlתقt֕לg іeف\\u058boԟآ \\u0601sԶ\\u05cef֓Էx\\u05fa\\u05ceaכӞtת؟y\\u0602\\n●Histogram\\n●QQ Plot (We shall cover it in the plotting section of this book).\\n●KDE Plot\\n●Violin Plot\\nWhile plotting is oϔen reliable, it is a subjective approach and prone to errors.\\nThus, we must know reliable quantitative measures as well.\\nש2\\u0602 ѽtӝقiءك֖cӝ\\u05ce іeف\\u058boԟآ:\\n1) Shapiro-Wilk test:\\n●Finds a statistic using the correlation between the observed data and\\nthe expected values under a normal distribution.\\n●The p-value indicates the likelihood of observing such a correlation\\nif the data were normally distributed.\\n●A high p-value indicates a normal distribution.\\n2) KS test:\\n●Measures the max diﬀerence between the cumulative distribution\\nfunctions (CDF) of observed and normal distribution.\\n●The output statistic is based on the max diﬀerence between the two\\nCDFs.\\n●A high p-value indicates a normal distribution.\\n3) Anderson-Darling test:\\n●Measures the diﬀerences between the observed data and the\\nexpected values under a normal distribution.\\n●Emphasizes the diﬀerences in the tail of the distribution.\\n●This makes it particularly eﬀective at detecting deviations in the\\nextreme values.\\n174'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 175}, page_content='DailyDoseofDS.com\\n4) Lilliefors test:\\n●It is a modiﬁcation of the KS test.\\n●The KS test is appropriate in situations where the parameters of the\\nreference distribution are known.\\n●If the parameters are unknown, Lilliefors is recommended.\\n●Get started: Statsmodel Docs.\\nש3\\u0602 όiءقaכԎe іeӝآu؞Էs\\nDistance measures are another reliable and more intuitive way to test normality.\\nBut they can be a bit tricky to use.\\nSee, the problem is that a single distance value needs more context for\\ninterpretability.\\nFor instance, if the distance between two distributions is 5, is this large or small?\\nWe need more context.\\nI prefer using these measures as follows:\\n●Find the distance between the observed distribution and multiple reference\\ndistributions.\\n●Select the reference distribution with the minimum distance to the\\nobserved distribution.\\n175'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 176}, page_content='DailyDoseofDS.com\\nHere are a few distance common and useful measures:\\n1) Bhattacharyya distance:\\n●Measure the overlap between two distributions.\\n●This “overlap” is oϔen interpreted as closeness between two\\ndistributions.\\n●Choose the distribution that has the least Bhattacharyya distance to\\nthe observed distribution.\\n2) Hellinger distance:\\n●It is used quite similar to how we use the Bhattacharyya distance\\n●The diﬀerence is that Bhattacharyya distance does not satisfy\\ntriangular inequality.\\n●But Hellinger distance does.\\n176'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 177}, page_content='DailyDoseofDS.com\\n3) KL Divergence:\\n●It is not entirely a \"distance metric\" per se, but can be used in this\\ncase.\\n●Measure information lost when one distribution is approximated\\nusing another distribution.\\n●The more information is lost, the more the KL Divergence.\\n●Choose the distribution that has the least KL divergence from the\\nobserved distribution.\\nKL divergence is used as a loss function in the t-SNE algorithm.\\n177'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 178}, page_content='DailyDoseofDS.com\\nѮrתӹaӸ֖l֕قy ٲs؉ ѐiׁԷl֕\\u058boתԠ\\nIn data science and statistics, many folks oϔen use “probability” and “likelihood”\\ninterchangeably.\\nHowever, likelihood and probability DO NOT convey the same meaning.\\nAnd the misunderstanding is somewhat understandable, given that they carry\\nsimilar meanings in our regular language.\\nWhile writing this chapter, I searched for their meaning in the Cambridge\\nDictionary. Here’s what it says:\\n●Probability: the level of possibility of something happening or being true.\\n●Likelihood: the chance that something will happen.\\nIf you notice closely, “likelihood” is the only synonym of “probability”.\\nAnyway.\\nIn my opinion, it is crucial to understand that probability and likelihood convey\\nvery diﬀerent meanings in data science and statistics.\\nLet’s understand!\\n178'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 179}, page_content='DailyDoseofDS.com\\nProbability is used in contexts where you wish to know the possibility/odds of an\\nevent.\\nFor instance, what is the:\\n●Probability of obtaining an even number in a die roll?\\n●Probability of drawing an ace of diamonds from a deck?\\n●and so on…\\nWhen translated to ML, probability can be thought of as:\\n●What is the probability that a transaction is fraud?\\n●What is the probability that an image depicts a cat?\\n●and so on…\\nEssentially, many classiﬁcation models, like logistic regression or a classiﬁcation\\nneural network, etc., assign the probability of a speciﬁc label to an input.\\nWhen calculating probability, the model’s parameters are known. Also, we\\nassume that they are trustworthy.\\nFor instance, to determine the probability of a head in a coin toss, we mostly\\nassume and trust that it is a fair coin.\\nLikelihood, on the other hand, is about explaining events that have already\\noccurred.\\nUnlike probability (where parameters are known and assumed to be\\ntrustworthy)...\\n179'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 180}, page_content='DailyDoseofDS.com\\n…likelihood helps us determine if we can trust the parameters in a model based\\non the observed data.\\nLet me elaborate more on that.\\nAssume you have collected some 2D data and wish to ﬁt a straight line with two\\nparameters — slope (m) and intercept (c).\\nHere, likelihood is deﬁned as the support provided by a data point for some\\nparticular parameter values in your model.\\nHere, you will ask questions like:\\n●If I model this data with the parameters:\\n○ז=ٖ and Ԏ=\\u05f7, what is the likelihood of observing the data?\\n○ז=ى and Ԏ=ٖ, what is the likelihood of observing the data?\\n○and so on…\\nThe above formulation popularly translates into the maximum likelihood\\nestimation (MLE), which we discussed here: (іLϚ ٲs؉ ϛM — ҷhӝق’ء قhԶ όiﬀԷrԶלcԶؑ)\\nIn maximum likelihood estimation, you have some observed data and you are\\ntrying to determine the speciﬁc set of parameters (θ) that maximize the likelihood\\nof observing the data.\\n180'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 181}, page_content='DailyDoseofDS.com\\nUsing the term “likelihood” is like:\\n●I have a possible explanation for my data. (In the above illustration,\\n“explanation” can be thought of as the parameters you are trying to\\ndetermine)\\n●How well does my explanation explain what I’ve already observed? This is\\nprecisely quantiﬁed with likelihood.\\nFor instance:\\n●Observation: The outcomes of \\u05f80 coin tosses are “ЕHДЖЕHДҏHД”.\\n●Explanation: I think it is a fair coin (\\u05fb=ژ؊5).\\n●What is the likelihood that my above explanation is true based on the\\nobserved data?\\nTo summarize…\\nIt is immensely important to understand that in data science and statistics,\\nlikelihood and probability DO NOT convey the same meaning.\\nAs explained above, they are pretty diﬀerent.\\n181'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 182}, page_content='DailyDoseofDS.com\\nIn probability:\\n●We determine the possibility of an event.\\n●We know the parameters associated with the event and assume them to be\\ntrustworthy.\\nIn likelihood:\\n●We have some observations.\\n●We have an explanation (or parameters).\\n●Likelihood helps us quantify whether the explanation is trustworthy.\\nHope that helped!\\n182'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 183}, page_content='DailyDoseofDS.com\\n\\u05f81 фeڀ ѮrתӹaӸ֖l֕قy όiءقr֕ӹuف֖oכآ ֖n όaفӞ ѽc֕ԷnԍԸ\\nStatistical models assume an underlying data generation process.\\nThis is exactly what lets us formulate the generation process, using which we\\ndeﬁne the maximum likelihood estimation (MLE) step.\\nThus, when dealing with statistical models, the model performance becomes\\nentirely dependent on:\\n●Your understanding of the data generation process.\\n●The distribution you chose to model data with, which, in turn, depends on\\nhow well you understand various distributions.\\nҷe آhӝ\\u05cel Ӟlء\\u05eb \\u05ceoתׂ Ӟt قh֕آ ֖n قhԶ Ԏhӝ\\u05fbtԶ؟ \\u05ebn ЄLѕآ \\u0601قh֕آ Ԏhӝ\\u05fbtԶ؟\\u0603.\\nThus, it is crucial to be aware of some of the most important distributions and\\nthe type of data they can model.\\nThe visual below depicts the 11 most important distributions in data science:\\n183'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 184}, page_content='DailyDoseofDS.com\\nLet’s understand them brieﬂy and how they are used.\\n184'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 185}, page_content='DailyDoseofDS.com\\nљo؞זa\\u05cd όiءقr֕ӹuف֖oכ\\n●The most widely used distribution in data science.\\n●Characterized by a symmetric bell-shaped curve\\n●It is parameterized by two parameters—mean and standard deviation.\\n●Example: Height of individuals.\\nλe؞לoٛ\\u05cel֕ όiءقr֕ӹuف֖oכ\\n●A discrete probability distribution that models the outcome of a binary\\nevent.\\n●It is parameterized by one parameter—the probability of success.\\n●Example: Modeling the outcome of a single coin ﬂip.\\nλiכ\\u05ebm֕Ӟl όiءقr֕ӹuف֖oכ\\n●It is Bernoulli distribution repeated multiple times.\\n●A discrete probability distribution that represents the number of successes\\nin a ﬁxed number of independent Bernoulli trials.\\n●It is parameterized by two parameters—the number of trials and the\\nprobability of success.\\n185'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 186}, page_content='DailyDoseofDS.com\\nѮo֕آsתל όiءقr֕ӹuف֖oכ\\n●A discrete probability distribution that models the number of events\\noccurring in a ﬁxed interval of time or space.\\n●It is parameterized by one parameter—lambda, the rate of occurrence.\\n●Example: Analyzing the number of goals a team will score during a speciﬁc\\ntime period.\\nϛx\\u05fa\\u05ebnԶלt֕Ӟl όiءقr֕ӹuف֖oכ\\n●A continuous probability distribution that models the time between events\\noccurring in a Poisson process.\\n●It is parameterized by one parameter—lambda, the average rate of events.\\n●Example: Analyzing the time between goals scored by a team.\\nЄaוזa όiءقr֕ӹuف֖oכ\\n●It is a variation of the exponential distribution.\\n●A continuous probability distribution that models the waiting time for a\\nspeciﬁed number of events in a Poisson process.\\n●It is parameterized by two parameters—alpha (shape) and beta (rate).\\n●Example: Analysing the time it would take for a team to score three goals.\\n186'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 187}, page_content='DailyDoseofDS.com\\nλeفӞ όiءقr֕ӹuف֖oכ\\n●It is used to model probabilities, thus, it is bounded between [0,1].\\n●Diﬀers from Binomial in this respect that in Binomial, probability is a\\nparameter.\\n●But in Beta, the probability is a random variable.\\nҝn֕էo؞ז όiءقr֕ӹuف֖oכ\\n●All outcomes within a given range are equally likely.\\n●It can be continuous or discrete.\\n●It is parameterized by two parameters: a (min value) and b (max value).\\n●Example: Simulating the roll of a fair six-sided die, where each outcome (1,\\n2, 3, 4, 5, 6) has an equal probability.\\n187'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 188}, page_content='DailyDoseofDS.com\\nѐoղ֔Nת؟mӝ\\u05ce όiءقr֕ӹuف֖oכ\\n●A continuous probability distribution where the logarithm of the variable\\nfollows a normal distribution.\\n●It is parameterized by two parameters—mean and standard deviation.\\n●Example: Typically, in stock returns, the natural logarithm follows a\\nnormal distribution.\\nѽtٛԠeכق ق-ԟ֖sف؟iӸٜt֕\\u05ebn\\n●It is similar to normal distribution but with longer tails (shown above).\\n●It is used in t-SNE to model low-dimensional pairwise similarities.\\nҷe֕ӹu\\u05cd\\u05ce\\n●Models the waiting time for an event.\\n●Oϔen employed to analyze time-to-failure data.\\n188'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 189}, page_content='DailyDoseofDS.com\\nί\\nρoוזoכ\\nіiء֖nفԷr\\u05fa؟eفӞt֕\\u05ebn\\n\\u05ebf\\nρoכقiכٜoٛآ\\nѮrתӹaӸ֖l֕قy όiءقr֕ӹuف֖oכآ\\nConsider the\\nfollowing\\nprobability density\\nfunction of a\\ncontinuous\\nprobability\\ndistribution. Say it\\nrepresents the time\\none may take to\\ntravel from point A\\nto B.\\n●For simplicity, we are assuming a uniform distribution in the interval [1,5].\\n●Essentially, it says that it will take somewhere between 1 and 5 minutes to\\ngo from A to B. Never more, never less.\\nThus, the probability density function (PDF) can be written as follows:\\n189'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 190}, page_content='DailyDoseofDS.com\\nAnswer the following question for me:\\nѷ) ҷhӝق ֖s قhԶ \\u05fbrתӹaӸ֖l֕قy قhӝك \\u05ebnԶ ٸi\\u05cd\\u05ce قaׁԷ \\u05fbrԶԎiءԷlڀ قh؞Էe זiכٜtԶآ\\nѮ(ҎՕ3\\u0602 قo ؟eӝԎh \\u05fbo֕לt λ?\\n●ί) \\u05f8/կ \\u0601o؞ ڙ.ٖխ)\\n●λ) ίrԶӞ ٜnԟԷr قhԶ Ԏu؞ٲe էrתז ق=Ԇ\\u05f8,ىԉ.\\n●ρ) ίrԶӞ ٜnԟԷr قhԶ Ԏu؞ٲe էrתז ق=Ԇي,լԉ.\\nDecide on an answer before you read further.\\nWell, all of the above answers are wrong.\\nThe correct answer is ZERO.\\nAnd I intentionally kept only wrong answers here so that you never forget\\nsomething fundamentally important about continuous probability distributions.\\nLet’s dive in!\\nThe probability density function of a continuous probability distribution may\\nlook as follows:\\nSome conditions for this probability density function are:\\n190'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 191}, page_content='DailyDoseofDS.com\\n●It should be deﬁned for all real numbers (can be zero for some values).\\nThis is in contrast to a discrete probability distribution which is only deﬁned for\\na list of values.\\n●The area should be 1.\\n●The function should be non-negative for all real values.\\nHere, many folks oϔen misinterpret that the probability density function\\nrepresents the probability of obtaining a speciﬁc value.\\n191'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 192}, page_content='DailyDoseofDS.com\\nFor instance, by looking at the above probability density function, many\\nincorrectly conclude that the probability of the random variable X being ٗ is\\nclose to ڙ.ٖت.\\nBut contrary to this common belief, a probability density function:\\n●DOES NOT depict the probabilities of a speciﬁc value.\\n●is not meant to depict a discrete random variable.\\nInstead, a probability density function:\\n●depicts the rate at which probabilities accumulate around each point.\\n●is only meant to depict a continuous random variable.\\nNow, there are inﬁnitely possible values that a continuous random variable may\\ntake.\\nSo the probability of obtaining a speciﬁc value is always zero (or inﬁnitesimally\\nsmall).\\nThus, answering our original question, the probability that one will take three\\nminutes to reach point B is ZERO.\\nSo what is the purpose of using a probability density function?\\nIn statistics, a PDF is used to calculate the probability over an interval of values.\\n192'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 193}, page_content='DailyDoseofDS.com\\nThus, we can use it to answer questions such as…\\n●What is the probability that it will take between:\\n○3 to 4 minutes to reach point B from point A, or,\\n○2 to 4 minutes to reach point B from point A, and so on…\\nAnd we do this using integrals.\\nMore formally, the probability that a random variable Һ will take values in the\\ninterval ԇaԝӹ] is:\\nSimply put, it’s the area under the curve from ԇaԝӹ].\\nFrom the above probability estimation over an interval, we can also verify that the\\nprobability of obtaining a speciﬁc value is indeed zero.\\n193'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 194}, page_content='DailyDoseofDS.com\\nBy substituting ӹ=ӝ, we get:\\nTo summarize, always remember that in a continuous probability distribution:\\n●The probability density function does not depict the exact probability of\\nobtaining a speciﬁc value.\\n●Estimating the probability for a precise value of the random value makes\\nno sense because it is inﬁnitesimally small.\\nInstead, we use the probability density function to calculate the probability over\\nan interval of values.\\n194'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 195}, page_content='DailyDoseofDS.com\\nϾeӝقu؞Է όeﬁלiف֖oכԞ ϛnղ֖nԶԷr֕לg\\nӞnԟ ѽe\\u05cdԷcف֖oכ\\n\\u05f81 ҏy\\u05faԷs \\u05ebf ұa؞֖aӸ\\u05ceeء ֖n Ӟ όaفӞsԶق\\nIn any tabular dataset, we typically categorize the columns as either a feature or a\\ntarget.\\nHowever, there are so many variables that one may ﬁnd/deﬁne in their dataset,\\nwhich I want to discuss in this chapter. These are depicted in the image below:\\n195'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 196}, page_content='DailyDoseofDS.com\\nש1֓ٗ) ОnԟԷpԶלdԶםق Ӟnԟ Ԡe\\u05faԷnԟԸלt ٲa؞֖aӸ\\u05ceeء\\nThese are the most common and fundamental to ML.\\nIndependent variables are the features that are used as input to predict the\\noutcome. They are also referred to as predictors/features/explanatory variables.\\nThe dependent variable is the outcome that is being predicted. It is also called\\nthe target, response, or output variable.\\nש3֓հ) ρoכէoٛלd֕םճ Ӟnԟ Ԏo؞؟e\\u05cdӞtԶԠ ٲa؞֖aӸ\\u05ceeء\\nConfounding variables are typically found in a cause-and-eﬀect study (causal\\ninference).\\nThese variables are not of primary interest in the cause-and-eﬀect equation but\\ncan potentially lead to spurious associations.\\n196'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 197}, page_content='DailyDoseofDS.com\\nTo exemplify, say we want to measure the eﬀect of ice cream sales on the sales of\\nair conditioners.\\nAs you may have guessed, these two measurements are highly correlated.\\nHowever, there’s a confounding variable — temperature, which inﬂuences both\\nice cream sales and the sales of air conditioners.\\nTo study the true casual impact, it is essential to consider the confounder\\n(temperature). Otherwise, the study will produce misleading results.\\nIn fact, it is due to the confounding variables that we hear the statement:\\n“Correlation does not imply causation.”\\nIn the above example:\\n●There is a high correlation between ice cream sales and sales of air\\nconditioners.\\n●But the sales of air conditioners (eﬀect) are NOT caused by ice cream sales.\\n197'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 198}, page_content='DailyDoseofDS.com\\nAlso, in this case, the air conditioner and ice cream sales are correlated variables.\\nMore formally, a change in one variable is associated with a change in another.\\nש5\\u0602 ρoכقrת\\u05ce ٲa؞֖aӸ\\u05ceeء\\nIn the above example, to measure the true eﬀect of ice cream sales on air\\nconditioner sales, we must ensure that the temperature remains unchanged\\nthroughout the study.\\nOnce controlled, temperature becomes a control variable.\\nMore formally, these are variables that are not the primary focus of the study but\\nare crucial to account for to ensure that the eﬀect we intend to measure is not\\nbiased or confounded by other factors.\\nש6\\u0602 ѐaفԷnف ٲa؞֖aӸ\\u05ceeء\\nA variable that is not directly observed but is inferred from other observed\\nvariables.\\nFor instance, we use clustering algorithms because the true labels do not exist,\\nand we want to infer them somehow.\\n198'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 199}, page_content='DailyDoseofDS.com\\nThe true label is a latent variable in this case.\\nAnother common example of a latent variable is “intelligence.”\\nIntelligence itself cannot be directly measured; it is a latent variable.\\nHowever, we can infer intelligence through various observable indicators such as\\ntest scores, problem-solving abilities, and memory retention.\\nש7\\u0602 ОnفԷrӝԎt֕\\u05ebn ٲa؞֖aӸ\\u05ceeء\\nAs the name suggests, these variables represent the interaction eﬀect between\\ntwo or more variables, and are oϔen used in regression analysis.\\nHere’s an instance I remember using them in.\\nIn a project, I studied the impact of population density and income levels on\\nspending behavior.\\n●I created three groups for population density — HIGH, MEDIUM, and\\nLOW (one-hot encoded).\\n●Likewise, I created three groups for income levels — HIGH, MEDIUM,\\nand LOW (one-hot encoded).\\nTo do regression analysis, I created interaction variables by cross-multiplying\\nboth one-hot columns.\\n199'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 200}, page_content='DailyDoseofDS.com\\nThis produced 9 interaction variables:\\n●Population-High and Income-High\\n●Population-High and Income-Med\\n●Population-High and Income-Low\\n●Population-Med and Income-High\\n●and so on…\\nConducting the regression analysis on interaction variables revealed more useful\\ninsights than what I observed without them.\\nTo summarize, the core idea is to study two or more variables together rather\\nthan independently.\\nש8֓ן) ѽtӝقiתלa؞ځ Ӟnԟ љoכ֔SفӞt֕\\u05ebnӝ؟y ٲa؞֖aӸ\\u05ceeءԜ\\nThe concept of stationarity oϔen appears in time-series analysis.\\nStationary variables are those whose statistical properties (mean, variance) DO\\nNOT change over time.\\nOn the ﬂip side, if a variable’s statistical properties change over time, they are\\ncalled non-stationary variables.\\nPreserving stationarity in statistical learning is critical because these models are\\nfundamentally reliant on the assumption that samples are identically distributed.\\nBut if the probability distribution of variables is evolving over time,\\n(non-stationary), the above assumption gets violated.\\n200'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 201}, page_content='DailyDoseofDS.com\\nThat is why, typically, using direct values of the non-stationary feature (like the\\nabsolute value of the stock price) is not recommended.\\nInstead, I have always found it better to deﬁne features in terms of relative\\nchanges:\\nש1ژ\\u0603 ѐaղճeԟ ٲa؞֖aӸ\\u05ceeء\\nTalking of time series, lagged variables are pretty commonly used in feature\\nengineering and data analytics.\\nAs the name suggests, a lagged variable represents previous time points’ values of\\na given variable, essentially shiϔing the data series by a speciﬁed number of\\nperiods/rows.\\nFor instance, when predicting next month’s sales ﬁgures, we might include the\\nsales ﬁgures from the previous month as a lagged variable.\\nLagged features may include:\\n●7-day lag on website traﬃc to predict current website traﬃc.\\n●30-day lag on stock prices to predict the next month’s closing prices.\\n●And so on…\\n201'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 202}, page_content='DailyDoseofDS.com\\nש1\\u05f7\\u0603 ѐeӝׂy ٲa؞֖aӸ\\u05ceeء\\nYet again, as the name suggests, these variables (unintentionally) provide\\ninformation about the target variable that would not be available at the time of\\nprediction.\\nThis leads to overly optimistic model performance during training but fails to\\ngeneralize to new data.\\nConsider a dataset containing medical imaging data.\\nEach sample consists of multiple images (e.g., diﬀerent views of the same\\npatient’s body part), and the model is intended to detect the severity of a disease.\\nIn this case, randomly splitting the images into train and test sets will result in\\ndata leakage.\\nThis is because images of the same patient will end up in both the training and\\ntest sets, allowing the model to “see” information from the same patient during\\ntraining and testing.\\n202'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 203}, page_content='DailyDoseofDS.com\\nHere’s a paper that committed this mistake (and later corrected it):\\nTo avoid this, a patient must only belong to the test or train/val set, not both.\\nThis is called group splitting:\\nCreating forward-lag features is another way leaky variables get created\\nunintentionally at times:\\nLet’s get into more detail about the issue with random splitting below.\\n203'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 204}, page_content='DailyDoseofDS.com\\nρyԍ\\u05ceiԍӞl էeӝقu؞Է Էnԍ\\u05ebd֕לg\\nIn typical machine learning datasets, we mostly ﬁnd features that progress from\\none value to another: For instance:\\n●Numerical features like age, income, transaction amount, etc.\\n●Categorical features like t-shirt size, income groups, age groups, etc.\\nHowever, there is one more type of feature, which, in most cases, deserves special\\nfeature engineering eﬀort but is oϔen overlooked. These are cyclical features, i.e.,\\nfeatures with a recurring pattern (or cycle).\\nUnlike other features that progress continuously (or have no inherent order),\\ncyclical features exhibit periodic behavior and repeat aϔer a speciﬁc interval. For\\ninstance, the hour-of-the-day, the day-of-the-week, and the month-of-an-year are\\nall common examples of cyclical features. Talking speciﬁcally about, say, the\\nhour-of-the-day, its value can range between 0 to 23:\\nIf we DON’T consider this as a cyclical feature and don’t utilize appropriate\\nfeature engineering techniques, we will lose some really critical information.\\nTo understand better, consider this:\\n204'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 205}, page_content='DailyDoseofDS.com\\nRealistically speaking, the values “23”\\nand “0” must be close to each other in\\nour “ideal” feature representation of\\nthe hour-of-the-day.\\nMoreover, the distance between “0” and “1” must be the same as the distance\\nbetween “23” and “0”.\\nHowever, standard representation does not fulﬁll these properties. Thus, the\\nvalue “23” is far from “0”. In fact, the distance property isn’t satisﬁed either.\\nNow, think about it for a second. Intuitively speaking, don’t you think this feature\\ndeserves special feature engineering, i.e., one that preserves the inherent natural\\nproperty?\\nI am sure you do!\\nLet’s understand how we typically do it.\\nρyԍ\\u05ceiԍӞl էeӝقu؞Է Էnԍ\\u05ebd֕לg\\nOne of the most common techniques to encode such a feature is using\\ntrigonometric functions, speciﬁcally, sine and cosine. These are helpful because\\nsine and cosine are periodic, bounded, and deﬁned for all real values.\\n205'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 206}, page_content='DailyDoseofDS.com\\nѡf Ԏoٛ؟sԶԞ ԷvԶל \\u05ebt֊Էr قr֕ճoכ\\u05ebmԶقr֕Ԏ էuכԎt֕\\u05ebnء ӞrԶ Ӟlء\\u05eb \\u05fbe؞֖oԟ֗Ԏ, ӹuف قhԶځ\\nӞrԶ Ӟlء\\u05eb ٜnԟԷﬁלeԟ էo؞ آoוԷ ٲa\\u05cdٜeءԞ \\u05ceiׁԷ, قaכ\\u0601p֕ؽ2\\u0602؊\\nFor instance, consider\\nrepresenting the linear\\nhour-of-the-day feature as a\\ncyclical feature:\\nThe central angle (2π) represents 24 hours. Thus, the linear feature values can be\\neasily converted into cyclical features as follows:\\nThe beneﬁt of doing this is how neatly the engineered feature satisﬁes the\\nproperties we discussed earlier:\\nAs depicted above, the distance between the cyclical feature representation of\\n“23” and “0” is the same as the distance between “0” and “1”. The standard linear\\nrepresentation of the hour-of-the-day feature, however, violates this property,\\nwhich results in loss of information…\\n206'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 207}, page_content='DailyDoseofDS.com\\n…or rather, I should say that the standard linear representation of the\\nhour-of-the-day feature results in an underutilization of information, which the\\nmodel can beneﬁt from. Had it been the day-of-the-week instead, the central\\nangle (2π) must have represented 7 days.\\nThe same idea can be extended to all sorts of cyclical features you may ﬁnd in\\nyour dataset:\\n●Wind direction, if represented categorically, will go in this order: N, NE, E,\\nSE, S, SW, W, NW, and then back to N.\\n●Phases of the moon, like new moon, ﬁrst quarter, full moon, and last\\nquarter, can be represented as categories with a cyclical order.\\n●Seasons, such as spring, summer, fall, and winter, are categorical features\\nwith a cyclical pattern as they repeat annually.\\nThe point is that as you will inspect the dataset features, you will intuitively\\nknow which features are cyclical and which are not.\\nTypically, the model will ﬁnd it easier to interpret the engineered features and\\nutilize them in modeling the dataset accurately.\\n207'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 208}, page_content='DailyDoseofDS.com\\nϾeӝقu؞Է όiءԎrԶقiڒӞt֕\\u05ebn\\nDuring model development, one of the techniques that many don’t experiment\\nwith is feature discretization. As the name suggests, the idea behind\\ndiscretization is to transform a continuous feature into discrete features.\\nWhy, when, and how would you do that? Let’s understand in this chapter.\\nіoف֖vӝقiתל\\nMy rationale for using feature discretization has almost always been simple: “It\\njust makes sense to discretize a feature.”\\nFor instance, consider your dataset has an age feature:\\nIn many use cases, like understanding spending behavior based on transaction\\nhistory, such continuous variables are better understood when they are\\ndiscretized into meaningful groups →youngsters, adults, and seniors.\\nFor instance, say we model this transaction dataset without discretization. This\\nwould result in some coeﬃcients for each feature, which would tell us the\\ninﬂuence of each feature on the ﬁnal prediction.\\n208'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 209}, page_content='DailyDoseofDS.com\\nBut if you think again, in our goal of understanding spending behavior, are we\\nreally interested in learning the correlation between exact age and spending\\nbehavior?\\nIt makes very little sense to do that. Instead, it makes more sense to learn the\\ncorrelation between diﬀerent age groups and spending behavior.\\nAs a result, discretizing the age feature can potentially unveil much more\\nvaluable insights than using it as a raw feature.\\nٗ Ԏoוזoכ قeԍ\\u058bn֕؎uԶآ\\nNow that we understand the rationale, there are 2 techniques that are widely\\npreferred.\\nOne way of discretizing features involves decomposing a feature into equally\\nsized bins.\\nAnother technique involves decomposing a feature into equal frequency bins:\\n209'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 210}, page_content='DailyDoseofDS.com\\nAϔer that, the discrete values are one-hot encoded.\\nOne advantage of feature discretization is that it enables non-linear behavior\\neven though the model is linear. This can potentially lead to better accuracy,\\nwhich is also evident from the image below:\\nA linear model with feature discretization results in a:\\n●non-linear decision boundary.\\n●better test accuracy.\\nSo, in a way, we get to use a simple linear model but still get to learn non-linear\\npatterns.\\nAnother advantage of discretizing continuous features is that it helps us improve\\nthe signal-to-noise ratio.\\n210'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 211}, page_content='DailyDoseofDS.com\\nSimply put, “signal” refers to the meaningful or valuable information in the data.\\nBinnng a feature helps us mitigate the inﬂuence of minor ﬂuctuations, which are\\noϔen mere noise.\\nEach bin acts as a means of “smoothing” out the noise within speciﬁc data\\nsegments.\\nBefore I conclude, do remember that feature discretization with one-hot\\nencoding increases the number of features →thereby increasing the data\\ndimensionality.\\nAnd typically, as we progress towards higher dimensions, data become more\\neasily linearly separable. Thus, feature discretization can lead to overﬁtting.\\nTo avoid this, don’t overly discretize all features. Instead, use it when it makes\\nintuitive sense, as we saw earlier.\\nOf course, its utility can vastly vary from one application to another, but at times,\\nI have found that:\\n●Discretizing geospatial data like latitude and longitude can be useful.\\n●Discretizing age/weight-related data can be useful.\\n●Features that are typically constrained between a range makes sense, like\\nsavings/income (practically speaking), etc.\\n211'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 212}, page_content='DailyDoseofDS.com\\nت ρaفԷgת؟iԍӞl όaفӞ ϛnԍ\\u05ebd֕לg ҏeԍ\\u058bn֕؎uԶآ\\nHere are 7 ways to encode categorical features:\\nWe covered them in detail here: https://bit.ly/3LkfVq5.\\n212'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 213}, page_content='DailyDoseofDS.com\\nѡnԶ֔hתق Էnԍ\\u05ebd֕לgԛ\\n●Each category is represented by a binary vector of 0s and 1s.\\n●Each category gets its own binary feature, and only one of them is \"hot\"\\n(set to 1) at a time, indicating the presence of that category.\\n●Number of features = Number of unique categorical labels\\nόuוזy Էnԍ\\u05ebd֕לgԛ\\n●Same as one-hot encoding but with one additional step.\\n●Aϔer one-hot encoding, we drop a feature randomly.\\n●We do this to avoid the dummy variable trap (discussed in this chapter).\\n●Number of features = Number of unique categorical labels - 1\\nϛﬀԷcف Էnԍ\\u05ebd֕לgԛ\\n●Similar to dummy encoding but with one additional step.\\n●Alter the row with all zeros to -1.\\n●This ensures that the resulting binary features represent not only the\\npresence or absence of speciﬁc categories but also the contrast between\\nthe reference category and the absence of any category.\\n●Number of features = Number of unique categorical labels - 1.\\nѐaӸԷl Էnԍ\\u05ebd֕לgԛ\\n●Assign each category a unique label.\\n●Label encoding introduces an inherent ordering between categories, which\\nmay not be the case.\\n●Number of features = 1.\\nѡrԟ֖nӝ\\u05ce Էnԍ\\u05ebd֕לgԛ\\n●Similar to label encoding — assign a unique integer value to each category.\\n●The assigned values have an inherent order, meaning that one category is\\nconsidered greater or smaller than another.\\n●Number of features = 1.\\n213'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 214}, page_content='DailyDoseofDS.com\\nρoٛלt Էnԍ\\u05ebd֕לgԛ\\n●Also known as frequency encoding.\\n●Encodes categorical features based on the frequency of each category.\\n●Thus, instead of replacing the categories with numerical values or binary\\nrepresentations, count encoding directly assigns each category with its\\ncorresponding count.\\n●Number of features = 1.\\nλiכӞrڀ Էnԍ\\u05ebd֕לgԛ\\n●Combination of one-hot encoding and ordinal encoding.\\n●It represents categories as binary code.\\n●Each category is ﬁrst assigned an ordinal value, and then that value is\\nconverted to binary code.\\n●The binary code is then split into separate binary features.\\n●Useful when dealing with high-cardinality categorical features (or a high\\nnumber of features) as it reduces the dimensionality compared to one-hot\\nencoding.\\n●Number of features = log(n) (in base 2).\\nWhile these are some of the most popular techniques, do note that these are not\\nthe only techniques for encoding categorical data.\\nYou can try plenty of techniques with the category-encoders library:\\nhttps://pypi.org/project/category-encoders.\\n214'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 215}, page_content='DailyDoseofDS.com\\nѽhٛﬄԷ Ͼeӝقu؞Է Оm\\u05fa\\u05ebrفӞnԍԷ\\nI oϔen ﬁnd “Shuﬄe Feature Importance” to be a handy and intuitive technique\\nto measure feature importance.\\nAs the name suggests, it observes how shuﬄing a feature inﬂuences the model\\nperformance. The visual below illustrates this technique in four simple steps:\\n215'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 216}, page_content='DailyDoseofDS.com\\nHere’s how it works:\\n●Train the model and measure its performance →Ѯ1.\\n●Shuﬄe one feature randomly and measure performance again →Ѯ2 (model\\nis NOT trained again).\\n●Measure feature importance using performance drop = (Ѯ1-Ѯ2).\\n●Repeat for all features.\\nThis makes intuitive sense as well, doesn’t it?\\nSimply put, if we randomly shuﬄe just one feature and everything else stays the\\nsame, then the performance drop will indicate how important that feature is.\\n●If the performance drop is low →This means the feature has a very low\\ninﬂuence on the model’s predictions.\\n●If the performance drop is high →This means that the feature has a very\\nhigh inﬂuence on the model’s predictions.\\nDo note that to eliminate any potential eﬀects of randomness during feature\\nshuﬄing, it is recommended to:\\n●Shuﬄe the same feature multiple times\\n●Measure average performance drop.\\nA few things that I love about this technique are:\\n216'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 217}, page_content='DailyDoseofDS.com\\n●It requires no repetitive model training. Just train the model once and\\nmeasure the feature importance.\\n●It is pretty simple to use and quite intuitive to interpret.\\n●This technique can be used for all ML models that can be evaluated.\\nOf course, there is one caveat as well.\\nSay two features are highly correlated, and one of them is permuted/shuﬄed.\\nIn this case, the model will still have access to the feature through its correlated\\nfeature.\\nThis will result in a lower importance value for both features.\\nOne way to handle this is to cluster highly correlated features and only keep one\\nfeature from each cluster.\\n217'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 218}, page_content='DailyDoseofDS.com\\nҏhԶ Ѯrתӹe іeف\\u058boԟ էo؞ Ͼeӝقu؞Է ѽe\\u05cdԷcف֖oכ\\nReal-world ML development is all about achieving a sweet balance between\\nspeed, model size, and performance.\\nOne common way to:\\n●Improve speed,\\n●Reduce size, and\\n●Maintain (or minimally degrade) performance…\\n…is by using featuring selection. The idea is to select the most useful subset of\\nfeatures from the dataset.\\nWhile there are many many methods for feature selection, I have oϔen found the\\n“Probe Method” to be pretty reliable, practical and intuitive to use.\\n218'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 219}, page_content='DailyDoseofDS.com\\nThe image below depicts how it works:\\n●Step 1) Add a random feature (noise).\\n●Step 2) Train a model on the new dataset.\\n●Step 3) Measure feature importance (can use shuﬄe feature importance)\\n●Step 4) Discard original features that rank below the random feature.\\n●Step 5) Repeat until convergence.\\nThis whole idea makes intuitive sense as well.\\n219'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 220}, page_content='DailyDoseofDS.com\\nMore speciﬁcally, if a feature’s importance is ranked below a random feature, it is\\nprobably a useless feature for the model.\\nThis can be especially useful in cases where we have plenty of features, and we\\nwish to discard those that don’t contribute to the model.\\nOf course, one shortcoming is that when using the Probe Method, we must train\\nmultiple models:\\n1. Train the ﬁrst model with the random feature and discard useless features.\\n2. Keep training new models until the random feature is ranked as the least\\nimportant feature (although typically, convergence does not result in plenty\\nof models).\\n3. Train the ﬁnal model without the random feature.\\nNonetheless, the approach can be quite useful to reduce model complexity.\\n220'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 221}, page_content='DailyDoseofDS.com\\nѺeղ؟eءآiתל\\nҷhڀ іeӝל ѽqٛӞrԶԠ ϛr؞\\u05ebr \\u0601MѼϛ)ؐ\\nSay you wish to train a linear regression\\nmodel. We know that we train it by\\nminimizing the squared error:\\nBut have you ever wondered why we speciﬁcally use the squared error?\\nSee, many functions can potentially minimize the diﬀerence between observed\\nand predicted values. But of all the possible choices, what is so special about the\\nsquared error?\\nIn my experience, people oϔen say:\\n●Squared error is diﬀerentiable. That is why we use it as a loss function.\\nWRONG.\\n●It is better than using absolute error as squared error penalizes large errors\\nmore. WRONG.\\nSadly, each of these explanations are incorrect.\\nBut approaching it from a probabilistic perspective helps us understand why the\\nsquared error is the ideal choice.\\nLet’s understand in this chapter.\\n221'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 222}, page_content='DailyDoseofDS.com\\nIn linear regression, we predict our target variable y using the inputs X as\\nfollows:\\nHere, epsilon is an error term that captures the random noise for a speciﬁc data\\npoint (i).\\nWe assume the noise is drawn from a Gaussian distribution with zero mean\\nbased on the central limit theorem:\\nThus, the probability of observing the error term can be written as:\\nSubstituting the error term from the linear regression equation, we get:\\nFor a speciﬁc set of parameters θ, the above tells us the probability of observing a\\ndata point (i).\\nNext, we can deﬁne the likelihood function as follows:\\n222'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 223}, page_content='DailyDoseofDS.com\\nIt means that by varying θ, we can ﬁt a distribution to the observed data and\\nquantify the likelihood of observing it.\\nWe further write it as a product for individual data points because we assume all\\nobservations are independent.\\nThus, we get:\\nSince the log function is monotonic, we use the log-likelihood and maximize it.\\nThis is called maximum likelihood estimation (MLE).\\n223'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 224}, page_content='DailyDoseofDS.com\\nSimplifying, we get:\\nTo reiterate, the objective is to ﬁnd the θ that maximizes the above expression.\\nBut the ﬁrst term is independent of θ. Thus, maximizing the above expression is\\nequivalent to minimizing the second term. And if you notice closely, it’s precisely\\nthe squared error.\\nThus, you can maximize the log-likelihood by minimizing the squared error. And\\nthis is the origin of least-squares in linear regression. See, there’s clear proof and\\nreasoning behind using squared error as a loss function in linear regression.\\nNothing comes from thin air in machine learning.\\n224'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 225}, page_content='DailyDoseofDS.com\\nѽk\\u05cdԷa؞ל ѐiכԷa؞ Ѻeղ؟eءآiתל Еaء љo Еy\\u05faԷr\\u05faӞrӝזeفԷrء\\nAlmost all ML models we work with have some hyperparameters, such as:\\n●Learning rate\\n●Regularization\\n●Layer size (for neural network), etc.\\nBut as shown in the image below, why don’t we see any hyperparameter in\\nSklearn’s Linear Regression implementation?\\nIt must have learning rate as a hyperparameter, right?\\nTo understand the reason why it has no hyperparameters, we ﬁrst need to learn\\nthat the Linear Regression can model data in two diﬀerent ways:\\n225'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 226}, page_content='DailyDoseofDS.com\\n1. Gradient Descent (which many other ML algorithms use for optimization):\\n○It is a stochastic algorithm, i.e., involves some randomness.\\n○It ﬁnds an approximate solution using optimization.\\n○It has hyperparameters.\\n2. Ordinary Least Square (OLS):\\n○It is a deterministic algorithm. Thus, if run multiple times, it will\\nalways converge to the same weights.\\n○It always ﬁnds the optimal solution.\\n○It has no hyperparameters.\\nNow, instead of the typical gradient descent approach, Sklearn’s Linear\\nRegression class implements the OLS method.\\nThat is why it has no hyperparameters.\\nЕoٷ ԠoԶآ ѡLѼ ٸo؞ׂ?\\nWith OLS, the idea is to ﬁnd the set of\\nparameters (Θ) such that:\\n●X: input data with dimensions (n,m).\\n●Θ: parameters with dimensions (m,1).\\n●y: output data with dimensions (n,1).\\n●n: number of samples.\\n●m: number of features.\\n226'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 227}, page_content='DailyDoseofDS.com\\nOne way to determine the parameter\\nmatrix Θ is by multiplying both sides of\\nthe equation with the inverse of X, as\\nshown below:\\nBut because X might be a non-square matrix, its inverse may not be deﬁned.\\nTo resolve this, ﬁrst, we multiply with the transpose of X on both sides, as shown\\nbelow:\\nThis makes the product of X with its transpose a square matrix.\\nThe obtained matrix, being square, can be inverted (provided it is non-singular).\\nNext, we take the collective inverse of the product to get the following:\\n227'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 228}, page_content='DailyDoseofDS.com\\nIt’s clear that the above deﬁnition has:\\n●No hyperparameters.\\n●No randomness. Thus, it will always return the same solution, which is also\\noptimal.\\nThis is precisely what the Linear Regression class of Sklearn implements.\\nTo summarize, it uses the OLS method instead of gradient descent.\\nThat is why it has no hyperparameters.\\nOf course, do note that there is a signiﬁcant tradeoﬀbetween run time and\\nconvenience when using OLS vs. gradient descent.\\nThis is also clear from the time-complexity table we discussed in an earlier\\nchapter.\\nAs depicted above, the run-time of OLS is cubically related to the number of\\nfeatures (m).\\nThus, when we have many features, it may not be a good idea to use the\\nѐiכԷa؞Ѻeղ؟eءآiתל(\\u0602 class. Instead, use the ѽGϋѺeղ؟eءآo؞\\u0601) class from Sklearn.\\nOf course, the good thing about LinearRegression() class is that it involves no\\nhyperparameter tuning.\\nThus, when we use OLS, we trade run-time for ﬁnding an optimal solution\\nwithout hyperparameter tuning.\\n228'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 229}, page_content='DailyDoseofDS.com\\nѮo֕آsתל Ѻeղ؟eءآiתל ٲs؉ ѐiכԷa؞ Ѻeղ؟eءآiתל\\nLinear regression comes with its own set of challenges/assumptions. For\\ninstance, aϔer modeling, the output can be negative for some inputs.\\nBut this may not make sense at times — predicting the number of goals scored,\\nnumber of calls received, etc. Thus, it is clear that it cannot model count (or\\ndiscrete) data.\\nFurthermore, in linear regression, residuals are expected to be normally\\ndistributed around the mean. So the outcomes on either side of the mean (ז-ٺԞ\\nז+ٺ) are equally likely.\\nFor instance:\\n●if the expected number (mean) of calls received is 1...\\n●...then, according to linear regression, receiving 3 calls (1+2) is just as likely\\nas receiving -1 (1-2) calls. (This relates to the concept of prediction\\nintervals, which we covered in the an earlier chapter.)\\n●But in this case, a negative prediction does not make any sense.\\nThus, if the above assumptions do not hold, linear regression won’t help.\\nInstead, in this speciﬁc case, what you may need is Poisson regression.\\n229'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 230}, page_content='DailyDoseofDS.com\\nPoisson regression is more suitable if your response (or outcome) is count-based.\\nIt assumes that the response comes from a Poisson distribution.\\nIt is a type of generalized linear model (GLM) that is used to model count data. It\\nworks by estimating a Poisson distribution parameter (λ), which is directly linked\\nto the expected number of events in a given interval.\\nContrary to linear regression, in Poisson regression, residuals may follow an\\nasymmetric distribution around the mean (λ). Hence, outcomes on either side of\\nthe mean (λ-x, λ+x) are NOT equally likely.\\nFor instance:\\n●if the expected number (mean) of calls received is 1...\\n●...then, according to Poisson regression, it is possible to receive 3 (1+2)\\ncalls, but it is impossible to receive -1 (1-2) calls.\\n●This is because its outcome is also non-negative.\\nThe regression ﬁt is mathematically deﬁned as follows:\\n230'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 231}, page_content='DailyDoseofDS.com\\nThe following visual neatly summarizes this post:\\nWe shall continue this discussion in the next chapter.\\n231'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 232}, page_content='DailyDoseofDS.com\\nЕoٷ قo λu֕\\u05ced ѐiכԷa؞ іoԟԷlءؑ\\nIn this chapter, I will help you cultivate what I think is one of the MOST\\noverlooked and underappreciated skills in developing linear models.\\nI can guarantee that harnessing this skill will give you a lot of clarity and\\nintuition in the modeling stages.\\nTo begin, understand that the Poisson regression we disussed above is no magic.\\nIt’s just that, in our speciﬁc use case, the data generation process didn’t perfectly\\nalign with what linear regression is designed to handle. In other words, earlier\\nwhen we trained a linear regression model, we inherently assumed that the data\\nwas sampled from a normal distribution. But that was not true in this Poisson\\nregression case.\\nInstead, it came from a Poisson distribution, which is why Poisson regression\\nworked better. Thus, the takeaway is that whenever you train linear models,\\nalways always and always think about the data generation process. It goes like\\nthis:\\n●Okay, I have this data.\\n●I want to ﬁt a linear model through it.\\n●What information do I get about the data generation process that can help\\nme select an appropriate linear model?\\n232'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 233}, page_content='DailyDoseofDS.com\\nYou’d start appreciating the importance of data generation when you realize that\\nliterally every member of the generalized linear model family stems from altering\\nthe data generation process.\\nFor instance:\\n●If the data generation process involves a Normal distribution →you get\\nlinear regression.\\n●If the data has only positive integers in the response variable, maybe it\\ncame from a Poisson distribution →and this gives us Poisson regression.\\nThis is precisely what we discussed yesterday.\\n●If the data has only two targets — 0 and 1, maybe it was generated using\\nBernoulli distribution →and this gives rise to logistic regression.\\n●If the data has ﬁnite and ﬁxed categories (0, 1, 2,…n), then this hints\\ntowards Binomial distribution →and we get Binomial regression.\\n233'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 234}, page_content='DailyDoseofDS.com\\nSee…\\nEvery linear model makes an assumption and is then derived from an underlying\\ndata generation process.\\nThus, developing a habit of holding for a second and thinking about the data\\ngeneration process will give you so much clarity in the modeling stages.\\nI am conﬁdent this will help you get rid of that annoying and helpless habit of\\nrelentlessly using a speciﬁc sklearn algorithm without truly knowing why you are\\nusing it.\\nConsequently, you’d know which algorithm to use and, most importantly, why.\\nThis improves your credibility as a data scientist and allows you to approach data\\nscience problems with intuition and clarity rather than hit-and-trial.\\nIn fact, once you understand the data generation process, you will automatically\\nget to know about most of the assumptions of that speciﬁc linear model.\\n234'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 235}, page_content='DailyDoseofDS.com\\nόuוזy ұa؞֖aӸ\\u05cee ҏrӝ\\u05fb\\nWith one-hot encoding, we\\nintroduce a big problem in the data.\\nWhen we one-hot encode\\ncategorical data, we unknowingly\\nintroduce perfect multicollinearity.\\nMulticollinearity arises when two or\\nmore features can predict another\\nfeature. In this case, as the sum of\\none-hot encoded features is always\\n1, it leads to perfect\\nmulticollinearity.\\nThis is oϔen called the Dummy Variable Trap. It is bad because the model has\\nredundant features. Morevero, the regressions coeﬃcients aren’t reliable in the\\npresence of multicollinearity.\\nSo how to resolve this?\\nThe solution is simple. Drop any arbitrary\\nfeature from the one-hot encoded\\nfeatures.\\nThis instantly mitigates multicollinearity\\nand breaks the linear relationship which\\nexisted before.\\n235'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 236}, page_content='DailyDoseofDS.com\\nұiءٜa\\u05cd\\u05cey ίsءԷsء ѐiכԷa؞ Ѻeղ؟eءآiתל Ѯe؞էo؞זaכԎe\\nLinear regression assumes that the model residuals (Օaԍقuӝ\\u05ce-\\u05fa؟eԟ֖cفԷd) are\\nnormally distributed. If the model is underperforming, it may be due to a\\nviolation of this assumption.\\nHere, I oϔen use a residual distribution plot to verify this and determine the\\nmodel’s performance. As the name suggests, this plot depicts the distribution of\\nresiduals (Օaԍقuӝ\\u05ce-\\u05fa؟eԟ֖cفԷd), as shown below:\\nA good residual plot will:\\n●Follow a normal distribution\\n●NOT reveal trends in residuals\\nA bad residual plot will:\\n●Show skewness\\n●Reveal patterns in residuals\\n236'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 237}, page_content='DailyDoseofDS.com\\nThus, the more normally distributed the residual plot looks, the more conﬁdent\\nwe can be about our model. This is especially useful when the regression line is\\ndiﬃcult to visualize, i.e., in a high-dimensional dataset.\\nWhy?\\nBecause a residual distribution plot depicts the distribution of residuals, which is\\nalways one-dimensional.\\nThus, it can be plotted and visualized easily.\\nOf course, this was just about validating one assumption — the normality of\\nresiduals.\\nHowever, linear regression relies on many other assumptions, which must be\\ntested as well.\\nStatsmodel provides a pretty comprehensive report for this, which we shall\\ndiscuss in the next chapter.\\n237'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 238}, page_content='DailyDoseofDS.com\\nѽtӝقsו\\u05ebdԶ\\u05ce Ѻeղ؟eءآiתל ѽuוזa؞ځ\\nStatsmodel provides one of the most comprehensive summaries for regression\\nanalysis.\\nYet, I have seen so many people struggling to interpret the critical model details\\nmentioned in this report. In this chapter, let’s understand the entire summary\\nsupport provided by statsmodel and why it is so important.\\nѽeԍقiתל ש1\\nThe ﬁrst column of the ﬁrst section lists the model’s settings (or conﬁg). This\\npart has nothing to do with the model’s performance.\\n238'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 239}, page_content='DailyDoseofDS.com\\n●Dependent variable: The variable we are predicting.\\n●Model and Method: We are using OLS to ﬁt a linear model.\\n●Date and time: You know it.\\n●No. of observations: The dataset’s size.\\n●Df residuals: The degrees of freedom associated with the residuals. It is\\nessentially the number of data points minus the number of parameters\\nestimated in the model (including intercept term).\\n●Df Model: This represents the degrees of freedom associated with the\\nmodel. It is the number of predictors, 2 in our case — Һ and آiכ٧X.\\nIf your data has categorical features, statsmodel will one-hot encode them. But in\\nthat process, it will drop one of the one-hot encoded features.\\nThis is done to avoid the dummy variable trap, which we discussed in an earlier\\nchapter (this chapter).\\n●Covariance type: This is related to the assumptions about the distribution\\nof the residual.\\n○In linear regression, we assume that the residuals have a constant\\nvariance (homoscedasticity).\\n○We use “nonrobust” to train a model under that assumption.\\nIn the second column, statsmodel provides the overall performance-related\\ndetails:\\n239'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 240}, page_content='DailyDoseofDS.com\\n●R-squared: The fraction of original data variability captured by the model.\\n○For instance, in this case, 0.927 means that the current model\\ncaptures 92.7% of the original variability in the training data.\\n○Statsmodel reports R2 on the input data, so you must not overly\\noptimize for it. If you do, it will lead to overﬁtting.\\n●Adj. R-squared:\\n○It is somewhat similar to R-squared, but it also accounts for the\\nnumber of predictors (features) in the model.\\n○The problem is that R-squared always increases as we add more\\nfeatures.\\n○So even if we add totally irrelevant features, R-squared will never\\ndecrease. Adj. R-squared penalizes this behavior of R-squared.\\n●F-statistic and Prob (F-statistic):\\n○These assess the overall signiﬁcance of a regression model.\\n○They compare the estimated coeﬃcients by OLS with a model whose\\nall coeﬃcients (except for the intercept) are zero.\\n○F-statistic tests whether the independent variables collectively have\\nany eﬀect on the dependent variable or not.\\n○Prob (F-statistic) is the associated p-value with the F-statistic.\\n○A small p-value (typically less than 0.05) indicates that the model as a\\nwhole is statistically signiﬁcant.\\n○This means that at least one independent variable has a signiﬁcant\\neﬀect on the dependent variable.\\n●Log-Likelihood:\\n240'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 241}, page_content='DailyDoseofDS.com\\n○This tells us the log-likelihood that the given data was generated by\\nthe estimated model.\\n○The higher the value, the more likely the data was generated by this\\nmodel.\\n●AIC and BIC:\\n○Like adjusted R-squared, these are performance metrics to\\ndetermine goodness of ﬁt while penalizing complexity.\\n○Lower AIC and BIC values indicate a better ﬁt.\\nѽeԍقiתל ש2\\nThe second section provides details related to the features:\\n●coef: The estimated coeﬃcient for a feature.\\n●t and P>|t|:\\n○Earlier, we used F-statistic to determine the statistical signiﬁcance\\nof the model as a whole.\\n○t-statistic is more granular on that front as it determines the\\nsigniﬁcance of every individual feature.\\n○P>|t| is the associated p-value with the t-statistic.\\n○A small p-value (typically less than 0.05) indicates that the feature is\\nstatistically signiﬁcant.\\n○For instance, the feature “X” has a p-value of ~0.6. This suggests that\\nthere is a 60% chance that the feature “X” has no eﬀect on “Y”.\\n●[0.025, 0.975] and std err:\\n241'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 242}, page_content='DailyDoseofDS.com\\n○See, the coeﬃcients we have obtained from the model are just\\nestimates. They may not be absolute true coeﬃcients of the process\\nthat generated the data.\\n○Thus, the estimated parameters are subject to uncertainty, aren’t\\nthey?\\n○Note: The width of the interval [0.025, 0.975] is 0.95 →or 95%. This\\nconstitutes the area between 2 standard deviations from the mean in\\na normal distribution.\\n○A 95% conﬁdence interval provides a range of values within which\\nyou can be 95% conﬁdent that the true value of the parameter lies.\\n○For instance, the interval for sin_X is (0.092, 6.104). So although the\\nestimated coeﬃcient is 3.09, we can be 95% conﬁdent that the true\\ncoeﬃcient lies in the range (0.092, 6.104).\\n242'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 243}, page_content='DailyDoseofDS.com\\nѽeԍقiתל ש3\\nDetails in the last section of the report test the assumptions of linear regression.\\n●Omnibus and Prob(Omnibus):\\n○They test the normality of the residuals.\\n○Omnibus value of zero means residuals are perfectly normal.\\n○Prob(Omnibus) is the corresponding p-value.\\n■In this case, Prob(Omnibus) is 0.001. This means there is a\\n0.1% chance that the residuals are normally distributed.\\n●Skew and Kurtosis:\\n○They also provide information about the distribution of the\\nresiduals.\\n○Skewness measures the asymmetry of the distribution of residuals.\\n■Zero skewness means perfect symmetry.\\n■Positive skewness indicates a distribution with a long right\\ntail. This indicates a concentration of residuals on lower\\nvalues. Good to check for outliers in this case.\\nNegative skewness indicates a\\ndistribution with a long leϔ tail. This is\\nmostly indicative of poor features. For\\ninstance, consider ﬁtting a sin curve with\\na linear feature (X). Most residuals will be\\nhigh, resulting in negative skewness.\\n243'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 244}, page_content='DailyDoseofDS.com\\n●Durbin-Watson:\\n○This measures autocorrelation between residuals.\\n○Autocorrelation occurs when the residuals are correlated, indicating\\nthat the error terms are not independent.\\n○But linear regression assumes that residuals are not correlated.\\n○The Durbin-Watson statistic ranges between 0 and 4.\\n■A value close to 2 indicates no autocorrelation.\\n■Values closer to 0 indicate positive autocorrelation.\\n■Values closer to 4 indicate negative autocorrelation.\\n●Jarque-Bera (JB) and Prob(JB):\\n○They solve the same purpose as Omnibus and Prob(Omnibus) —\\nmeasuring the normality of residuals.\\n●Condition Number:\\n○This tests multicollinearity.\\n○Multicollinearity occurs when two features are correlated, or two or\\nmore features determine the value of another feature.\\n○A standalone value for Condition Number can be diﬃcult to\\ninterpret so here’s how I use it:\\n■Add features one by one to the regression model and notice\\nany spikes in the Condition Number.\\nAs discussed above, every section of this report has its importance:\\n●The ﬁrst section tells us about the model’s conﬁg, the overall performance\\nof the model, and its statistical signiﬁcance.\\n●The second section tells us about the statistical signiﬁcance of individual\\nfeatures, the model’s conﬁdence in ﬁnding the true coeﬃcient, etc.\\n●The last section lets us validate the model’s assumptions, which are\\nimmensely critical to linear regression’s performance.\\nNow you know how to interpret the entire regression summary from statsmodel.\\n244'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 245}, page_content='DailyDoseofDS.com\\n ЄeכԷrӝ\\u05ceiڒԷd ѐiכԷa؞ іoԟԷlء \\u0601Gяіs\\u0602\\nA linear regression model is undeniably an extremely powerful model, in my\\nopinion. However, it makes some strict assumptions about the type of data it can\\nmodel, as depicted below.\\nThese conditions oϔen restrict its applicability to data situations that do not\\nobey the above assumptions. That is why being aware of its extensions is\\nimmensely important.\\nGeneralized linear models (GLMs) precisely do that. They relax the assumptions\\nof linear regression to make linear models more adaptable to real-world datasets.\\nҷhڀ ЄLѕآ?\\nLinear regression is pretty restricted in terms of the kind of data it can model.\\nFor instance, its assumed data generation process looks like this:\\n245'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 246}, page_content='DailyDoseofDS.com\\nThe assumed data generation process of linear regression\\n●Firstly, it assumes that the conditional distribution of Y given X is a\\nGaussian.\\n●Next, it assumes a very speciﬁc form for the mean of the above Gaussian. It\\nsays that the mean should always be a linear combination of the features\\n(or predictors).\\n●Lastly, it assumes a constant variance for the conditional distribution\\nP(Y|X) across all levels of X. A graphical way of illustrating this is as\\nfollows:\\nThese conditions oϔen restrict its applicability to data situations that do not\\nobey the above assumptions.\\nIn other words, nothing stops real-world datasets from violating these\\nassumptions.\\nIn fact, in many scenarios, the data might exhibit complex relationships,\\nheteroscedasticity (varying variance), or even follow entirely diﬀerent\\ndistributions altogether.\\n246'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 247}, page_content='DailyDoseofDS.com\\nYet, if we intend to build linear models, we should formulate better algorithms\\nthat can handle these peculiarities.\\nGeneralized linear models (GLMs) precisely do that.\\nThey relax the assumptions of linear regression to make linear models more\\nadaptable to real-world datasets.\\nMore speciﬁcally, they consider the following:\\n●What if the distribution isn’t normal but some other distribution?\\n●What if X has a more sophisticated relationship with the mean?\\n●What if the variance varies with X?\\n247'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 248}, page_content='DailyDoseofDS.com\\nThe eﬀectiveness of a speciﬁc GLM — Poisson regression (which we discussed in\\nan earlier chapter) over linear regression is evident from the image below:\\n●Linear regression assumes the data is drawn from a Gaussian, when in\\nreality, it isn’t. Hence, it underperforms.\\n●Poisson regression adapts its regression ﬁt to a non-Gaussian distribution.\\nHence, it performs signiﬁcantly better.\\n248'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 249}, page_content='DailyDoseofDS.com\\nӒe؞\\u05eb-֕לﬂӞtԶԠ Ѻeղ؟eءآiתל\\nThe target variable of typical regression datasets is somewhat evenly distributed.\\nBut, at times, the target variable may have plenty of zeros. Such datasets are\\ncalled zero-inﬂated datasets.\\nThey may raise many problems during regression modeling. This is because a\\nregression model can not always predict exact “zero” values when, ideally, it\\nshould. For instance, consider simple linear regression. The regression line will\\noutput exactly “zero” only once (if it has a non-zero slope).\\nThis issue persists not only in higher dimensions but also in complex models like\\nneural nets for regression.\\nOne great way to solve this is by training a combination of a classiﬁcation and a\\nregression model.\\n249'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 250}, page_content='DailyDoseofDS.com\\nThis goes as follows:\\n●Mark all non-zero targets as “1” and the rest as “0”.\\n●Train a binary classiﬁer on this dataset.\\n●Next, train a regression model only on those data points with a non-zero\\ntrue target.\\nDuring prediction:\\n250'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 251}, page_content=\"DailyDoseofDS.com\\n●If the classiﬁer's output is “0”, the ﬁnal output is also zero.\\n●If the classiﬁer's output is “1”, use the regression model to predict the ﬁnal\\noutput.\\nIts eﬀectiveness over the regular regression model is evident from the image\\nbelow:\\nRegression vs. Regression + Classiﬁcation results\\n●Linear regression alone underﬁts the data.\\n●Linear regression with a classiﬁer performs as expected.\\n251\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 252}, page_content='DailyDoseofDS.com\\nЕuӸԷr Ѻeղ؟eءآiתל\\nOne big problem with regression models is that they are sensitive to outliers.\\nConsider linear regression. Even a few outliers can signiﬁcantly impact Linear\\nRegression performance, as shown below:\\nAnd it isn’t hard to identify the cause of this problem. Essentially, the loss\\nfunction (MSE) scales quickly with the residual term (true-predicted). \\nThus, even a few data points with a large residual can impact parameter\\nestimation.\\n252'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 253}, page_content='DailyDoseofDS.com\\nHuber loss (or Huber Regression) precisely addresses this problem. In a gist, it\\nattempts to reduce the error contribution of data points with large residuals.\\nOne simple, intuitive, and obvious way to do this is by applying a threshold (δ) on\\nthe residual term:\\n●If the residual is smaller than the threshold, use MSE (no change here).\\n●Otherwise, use a loss function which has a smaller output than MSE —\\nlinear, for instance.\\nThis is depicted below:\\n●For residuals\\nsmaller than the\\nthreshold (δ) →we\\nuse MSE.\\n●Otherwise, we use\\na linear loss\\nfunction which has\\na smaller output\\nthan MSE.\\nMathematically, Huber loss is deﬁned as follows:\\n253'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 254}, page_content='DailyDoseofDS.com\\nIts eﬀectiveness is\\nevident from the\\nfollowing image:\\n●Linear Regression\\nis aﬀected by\\noutliers\\n●Huber Regression\\nis more robust.\\nЕoٷ Ԡo ٸe ԠeفԷrו֖nԶ قhԶ قh؞Էs֊\\u05eblԟ \\u0601δ\\u0603?\\nWhile trial and error is one way, I oϔen like to create a residual plot. This is\\ndepicted below: The below plot is generally called a lollipop plot because of its\\nappearance.\\n●Train a linear regression model as you usually would.\\n●Compute the residuals (=true-predicted) on the training data.\\n●Plot the absolute residuals for every data point.\\nOne good thing is that we can create this plot for any dimensional dataset. The\\nobjective is just to plot (true-predicted) values, which will always be 1D.\\n254'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 255}, page_content='DailyDoseofDS.com\\nNext, you can subjectively decide a reasonable threshold value δ.\\nЕe؞Է’ء ӞnתقhԶ؟ ֖nفԷrԶآt֕לg ֖dԶӞ.\\nBy using a linear loss function in Huber regressor, we intended to reduce the\\nlarge error contributions that would have happened otherwise by using MSE.\\nThus, we can further reduce that error contribution by using, say, a square root\\nloss function, as shown below:\\nI am unsure if this has been proposed before, so I decided to call it the\\nόa֕\\u05ceyϋ\\u05ebsԶ\\u05ecէDӝقaѼԎiԶלcԶ Ѻeղ؟eءآo؞\\n.\\nIt is clear that the error contribution of the square root loss function is the lowest\\nfor all residuals above the threshold δ.\\n255'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 256}, page_content='DailyDoseofDS.com\\nόeԍ֖s֕\\u05ebn ҏrԶԷs Ӟnԟ ϛnءԷmӸ\\u05cee\\nіeف\\u058boԟآ\\nρoכԠeכآe ѺaכԠoו Ͼo؞Էsف ֖nف\\u05eb Ӟ όeԍ֖s֕\\u05ebn ҏrԶԷ\\nThere’s an interesting technique, using which, we can condense an entire random\\nforest model into a single decision tree.\\nThe beneﬁts?\\nThis technique can:\\n●Decrease the prediction run-time.\\n●Improve interpretability.\\n●Reduce the memory footprint.\\n●Simplify the model.\\n●Preserve the generalization power of the random forest model.\\nLet’s understand in this chapter.\\n256'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 257}, page_content='DailyDoseofDS.com\\nҏeԍ\\u058bn֕؎uԶ ҷa\\u05cdׂt֊؟oٛճh\\nLet’s ﬁt a decision tree model on the following dummy dataset. It produces a\\ndecision region plot shown on the right.\\nIt’s clear that there is high overﬁtting.\\nIn fact, we must note that, by default, a decision tree can always 100% overﬁt any\\ndataset (we will use this information shortly). This is because it is always allowed\\nto grow until all samples have been classiﬁed correctly.\\nThis overﬁtting problem is resolved by a random forest model, as depicted below:\\n257'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 258}, page_content=\"DailyDoseofDS.com\\nThis time, the decision region plot suggests that we don’t have a complex\\ndecision boundary. The test accuracy has also improved (69.5% to 74%).\\nNow, here’s an interesting thing we can do.\\nWe know that the random forest model has learned some rules that generalize on\\nunseen data.\\nSo, how about we train a decision tree on the predictions generated by the\\nrandom forest model on the training set?\\nMore speciﬁcally, given a dataset (X, y):\\n●Train a random forest model. This will learn some rules from the training\\nset which are expected to generalize on unseen data (due to Bagging).\\n●Generate predictions on X, which produces the output y'. These\\npredictions will capture the essence of the rules learned by the random\\nforest model.\\n●Finally, train a decision tree model on (X, y'). Here, we want to\\nintentionally overﬁt this mapping as this mapping from (X) to (y') is a proxy\\nfor the rules learned by the random forest model.\\n258\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 259}, page_content='DailyDoseofDS.com\\nThis idea is implemented below:\\nThe decision region plot we get with the new decision tree is pretty similar to\\nwhat we saw with the random forest earlier:\\nMeasuring the test accuracy of the decision tree and random forest model, we\\nnotice them to be similar too:\\n259'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 260}, page_content='DailyDoseofDS.com\\nIn fact, this approach also signiﬁcantly reduces the run-time, as depicted below:\\nIsn’t that cool?\\nAnother rationale for considering doing this is that it adds interpretability.\\nThis is because if we have 100 trees in a random forest, there’s no way we can\\ninterpret them.\\nHowever, if we have condensed it to a decision tree, now we can inspect it.\\n260'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 261}, page_content='DailyDoseofDS.com\\nί Ԡe\\u05faӞrف֖nղ לoفԷ\\nI devised this very recently. I also tested this approach on a couple of datasets,\\nand they produced promising results.\\nBut it won’t be fair to make any conclusions based on just two instances.\\nWhile the idea makes intuitive sense, I understand there could be some potential\\nﬂaws that are not evident right now.\\nSo, I not saying that you should adopt this technique right away.\\nInstead, it is advised to test this approach on your random forest use cases.\\nConsidering reverting back to me with what you discovered.\\nThe code for this chapter is available here: https://bit.ly/3XSPejD.\\nIn this next chapter, let’s understand a technique to transform a decision tree into\\nmatrix operations which can run on GPUs.\\n261'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 262}, page_content='DailyDoseofDS.com\\nҏrӝלsզ\\u05ebrו όeԍ֖s֕\\u05ebn ҏrԶԷ ֖nف\\u05eb іaف؟iٺ ѡpԶ؟aف֖oכآ\\nInference using a decision tree is an iterative process. We traverse a decision tree\\nby evaluating the condition at a speciﬁc node in a layer until we reach a leaf\\nnode.\\nIn this chapter, let’s learn a superb technique that to represent inferences from a\\ndecision tree in the form of matrix operations.\\nAs a result:\\n1. It makes inference much faster as matrix operations can be radically\\nparallelized.\\n2. These operations can be loaded on a GPU for even faster inference, making\\nthem more deployment-friendly.\\nѽeفٜp\\nConsider a binary classiﬁcation dataset with 5 features.\\n262'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 263}, page_content='DailyDoseofDS.com\\nLet’s say we get the following tree structure aϔer ﬁtting a decision tree on the\\nabove dataset:\\nљoفӞt֕\\u05ebnء\\nBefore proceeding ahead, let’s assume that:\\n●ז →Total features in the dataset (5 in the dataset above).\\n●Է →Total evaluation nodes in the tree (4 blue nodes in the tree above).\\n●\\u05ce →Total leaf nodes in the tree (5 green nodes in the tree).\\n●Ԏ →Total classes in the dataset (2 in the dataset above).\\nҏrԶԷ قo іaف؟iԍԷs\\nThe core idea in\\nthis conversion is\\nto derive ﬁve\\nmatrices (ί, λ, ρ,\\nό, ϛ) that capture\\nthe structure of the\\ndecision tree.\\n263'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 264}, page_content='DailyDoseofDS.com\\nLet’s understand them one by one!\\nљoفԷ: ҏhԶ آtԶ\\u05fbs ٸe آhӝ\\u05cel ӹe \\u05fbe؞էo؞זiכճ ӹe\\u05cd\\u05ebw זiղ\\u058bt לoف זaׁԷ ֖mוԷd֕ӞtԶ\\nآeכأԷ آo \\u05fblԶӞsԶ ׂeԶ\\u05fb ؟eӝԠiכճ. Ӏoٛ؛l\\u05cd ٜnԟԷrءقaכԠ ԷvԶ؟yف\\u058biכճ \\u05ebnԍԷ ٸe \\u058baٱԷ\\nԠe؞֖vԶԠ Ӟl\\u05cd խ זaف؟iԍԷs؉\\nש1\\u0602 іaف؟iٺ ί\\nThis matrix captures\\nthe relationship\\nbetween input features\\nand evaluation nodes\\n(blue nodes above).\\nSo it’s an (ז×Է) shaped\\nmatrix.\\nA speciﬁc entry is set to “1” if the corresponding node in the column evaluates\\nthe corresponding feature in the row. For instance, in our decision tree, “Node 0”\\nevaluates “Feature 2”.\\nThus, the corresponding entry will be “1” and all other entries will be “0.”\\n264'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 265}, page_content='DailyDoseofDS.com\\nFilling out the entire matrix this way, we get:\\nש2\\u0602 іaف؟iٺ λ\\nThe entries of matrix B are the threshold value at each node. Thus, its shape is\\n1×e.\\nThis is vector though, but the terminology is not important here.\\nש3\\u0602 іaف؟iٺ ρ\\nThis is a matrix between every pair of leaf nodes and evaluation nodes. Thus, its\\ndimensions are Է×\\u05ce.\\n265'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 266}, page_content='DailyDoseofDS.com\\nA speciﬁc entry is set to:\\n●“1” if the corresponding leaf node in the column lies in the leϔ sub-tree of\\nthe corresponding evaluation node in the row.\\n●“-1” if the corresponding leaf node in the column lies in the right sub-tree\\nof the corresponding evaluation node in the row.\\n●“0” if the corresponding leaf node and evaluation node have no link.\\nFor instance, in our decision tree, the “leaf node 4” lies on the leϔ sub-tree of\\nboth “evaluation node 0” and “evaluation node 1”. Thus, the corresponding values\\nwill be 1.\\n266'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 267}, page_content='DailyDoseofDS.com\\nFilling out the entire matrix this way, we get:\\nש4\\u0602 іaف؟iٺ \\u0601o؞ ұeԍقo؞\\u0603 ό\\nThe entries of vector D are the sum of non-negative entries in every column of\\nMatrix C:\\nש5\\u0602 іaف؟iٺ ϛ\\nFinally, this matrix\\nholds the mapping\\nbetween leaf nodes and\\ntheir corresponding\\noutput labels. Thus, its\\ndimensions are \\u05ce×Ԏ.\\n267'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 268}, page_content='DailyDoseofDS.com\\nIf a leaf node classiﬁes a sample to “Class 1”, the corresponding entry will be 1,\\nand the other cell entry will be 0.\\nFor instance, “lead node 4” outputs “Class 1”, thus the corresponding entries for\\nthe ﬁrst row will be (1,0):\\nWe repeat this for all other leaf nodes to get the following matrix as Matrix E:\\nWith this, we have compiled our decision tree into matrices. To recall, these are\\nthe ﬁve matrices we have created so far:\\n268'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 269}, page_content='DailyDoseofDS.com\\n●Matrix A captures which input feature was used at each evaluation node.\\n●Matrix B stores the threshold of each evaluation node.\\n●Matrix C captures whether a leaf node lies on the leϔ or right sub-tree of a\\nspeciﬁc evaluation node or has no relation to it.\\n●Matrix D stores the sum of non-negative entries in every column of Matrix\\nC.\\n●Finally, Matrix E maps from leaf nodes to their class labels.\\nОnզԷrԶלcԶ ٜs֕לg זaف؟iԍԷs\\nSay this is our input feature vector X (5 dimensions):\\nThe whole inference can now be done using just these matrix operations:\\n●XA < B gives:\\n269'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 270}, page_content='DailyDoseofDS.com\\n●The above result multiplied by C gives:\\n●The above result, when matched to D, gives:\\n●Finally, multiplying with E, we get:\\nThe ﬁnal prediction comes out to be “Class 1,” which is indeed correct! Notice\\nthat we carried out the entire inference process using only matrix operations:\\n270'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 271}, page_content='DailyDoseofDS.com\\nAs a result, the inference operation can largely beneﬁt from parallelization and\\nGPU capabilities.\\nThe run-time eﬃcacy of this technique is evident from the image below:\\n●Here, we have trained a random forest model.\\n●The compiled model runs:\\n○Over twice as fast on a CPU (Tensor CPU Model).\\n○~40 times faster on a GPU, which is huge (Tensor GPU Model).\\n●All models have the same accuracy — indicating no loss of information\\nduring compilation.\\n271'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 272}, page_content='DailyDoseofDS.com\\nОnفԷrӝԎt֕ٲe\\u05cdځ Ѯrٛלe Ӟ όeԍ֖s֕\\u05ebn ҏrԶԷ\\nOne thing I always appreciate\\nabout decision trees is their ease\\nof visual interpretability. No\\nmatter how many features our\\ndataset has, we can ALWAYS\\nvisualize and interpret a decision\\ntree.\\nThis is not always possible with other intuitive and simple models like linear\\nregression. But decision trees stand out in this respect. Nonetheless, one thing I\\noϔen ﬁnd a bit time-consuming and somewhat hit-and-trial-driven is pruning a\\ndecision tree.\\nҷhڀ \\u05fbrٛלeؐ\\nThe problem is\\nthat under default\\nconditions,\\ndecision trees\\nALWAYS 100%\\noverﬁt the dataset,\\nas depicted in this\\nimage:\\n272'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 273}, page_content='DailyDoseofDS.com\\nThus, pruning is ALWAYS necessary to reduce model variance. Scikit-learn\\nalready provides a method to visualize them as shown below:\\nBut the above visualisation is pretty non-elegant, tedious, messy, and static (or\\nnon-interactive). I recommend using an interactive Sankey diagram to prune\\ndecision trees. This is depicted below:\\n273'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 274}, page_content='DailyDoseofDS.com\\nAs shown above, the Sankey diagram allows us to interactively visualize and\\nprune a decision tree by collapsing its nodes.\\nAlso, the number of data points\\nfrom each class is size and\\ncolor-encoded in each node, as\\nshown below.\\nThis instantly gives an estimate of the node’s impurity, based on which, we can\\nvisually and interactively prune the tree in seconds. For instance, in the full\\ndecision tree shown below, pruning the tree at a depth of two appears reasonable:\\nNext, we can train a new decision tree aϔer obtaining an estimate for\\nhyperparameter values. This will help us reduce the variance of the decision tree.\\nYou can download the code notebook for the interactive decision tree here:\\nhttps://bit.ly/4bBwY1p. Instructions are available in the notebook.\\nNext, let’s understand a point of caution when using decision trees.\\n274'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 275}, page_content='DailyDoseofDS.com\\nҷhڀ όeԍ֖s֕\\u05ebn ҏrԶԷs іuءق λe ҏhת؟oٛճh\\u05cdځ Оnء\\u05fbeԍقeԟ\\nίfفԷr ҏrӝ֖n֕לg\\nIf we were to visualize the\\ndecision rules (the conditions\\nevaluated at every node) of ANY\\ndecision tree, we would\\nALWAYS ﬁnd them to be\\nperpendicular to the feature\\naxes, as depicted in the image.\\nIn other words, every decision tree progressively segregates feature space based\\non such perpendicular boundaries to split the data.\\nOf course, this is not a “problem” per se.\\nIn fact, this perpendicular splitting is what makes it so powerful to perfectly\\noverﬁt any dataset. However, this also brings up a pretty interesting point that is\\noϔen overlooked when ﬁtting decision trees. More speciﬁcally, what would\\nhappen if our dataset had a diagonal decision boundary, as depicted below:\\n275'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 276}, page_content='DailyDoseofDS.com\\nIt is easy to guess that in such a case, the\\ndecision boundary learned by a decision tree is\\nexpected to appear as follows:\\nIn fact, if we plot this decision tree, we notice that it creates so many splits just to\\nﬁt this easily separable dataset, which a model like logistic regression, support\\nvector machine (SVM), or even a small neural network can easily handle:\\nIt becomes more evident if we zoom into this decision tree and notice how close\\nthe thresholds of its split conditions are:\\n276'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 277}, page_content='DailyDoseofDS.com\\nThis is a bit concerning because it clearly shows that the decision tree is\\nmeticulously trying to mimic a diagonal decision boundary, which hints that it\\nmight not be the best model to proceed with. To double-check this, I oϔen do the\\nfollowing:\\n●Take the training data \\u0601Xԝ ځ);\\n○Shape of Һ: \\u0601nԝ ז).\\n○Shape of ځ: \\u0601nԝ \\u05f8).\\n●Run PCA on Һ to project data into an orthogonal space of m dimensions.\\nThis will give Һ_\\u05faԎa, whose shape will also be \\u0601nԝ ז).\\n●Fit a decision tree on Һ_\\u05faԎa and visualize it (thankfully, decision trees are\\nalways visualizable).\\n●If the decision tree depth is signiﬁcantly smaller in this case, it validates\\nthat there is a diagonal separation.\\nFor instance, the PCA projections on the above dataset are shown below:\\nIt is clear that the decision boundary on PCA projections is almost perpendicular\\nto the Һ2փ feature (the 2nd principal component).\\n277'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 278}, page_content='DailyDoseofDS.com\\nFitting a decision tree on this Һ_\\u05faԎa drastically reduces its depth, as depicted\\nbelow:\\nThis lets us determine that we might be better oﬀusing some other algorithm\\ninstead.\\nOr, we can spend some time engineering better features that the decision tree\\nmodel can easily work with using its perpendicular data splits.\\nAt this point, if you are thinking, why can’t we use the decision tree trained on\\nҺ_\\u05faԎa?\\nWhile nothing stops us from doing that, do note that PCA components are not\\ninterpretable, and maintaining feature interpretability can be important at times.\\nThus, whenever you train your next decision tree model, consider spending some\\ntime inspecting what it’s doing.\\nλeզ\\u05ebrԶ Էnԟ֖nղ قh֕آ Ԏhӝ\\u05fbtԶ؟\\nО Ԡoכ؛t ֖nفԷnԟ قo ԠiءԎoٛ؟aղԷ قhԶ ٜsԶ \\u05ebf Ԡeԍ֖s֕\\u05ebn قrԶԷs؉ ҏhԶځ ӞrԶ قhԶ\\nӹu֕\\u05ced֕לg ӹlתԎkء \\u05ebf آoוԷ \\u05ebf قhԶ זoءق \\u05fboٷԷrզٜl ԷnءԸזb\\u05cdԷ זoԟԷlء ٸe ٜsԶ\\nقoԟӞy؉\\nіy \\u05fbo֕לt ֖s قo ӹr֕לg էo؞ٸa؞Ԡ قhԶ آt؞ٜcفٝ؟a\\u05cd էo؞זu\\u05cdӞt֕\\u05ebn \\u05ebf Ԡeԍ֖s֕\\u05ebn قrԶԷs\\nӞnԟ ٸhڀؽw֊Էn قhԶځ זiղ\\u058bt לoف ӹe Ӟn ֖dԶӞl Ӟlղ\\u05ebr֕قhו قo ٸo؞ׂ ٸiف\\u058b.\\n278'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 279}, page_content='DailyDoseofDS.com\\nόeԍ֖s֕\\u05ebn ҏrԶԷs ίLҶΰӀS ѡvԶ؟ﬁق!\\nIn addition to the above inspection, there’s one more thing you need to be careful\\nof when using decision trees. This is about overﬁtting.\\nThe thing is that, by default, a decision tree (in sklearn’s implementation, for\\ninstance), is allowed to grow until all leaves are pure. This happens because a\\nstandard decision tree algorithm greedily selects the best split at each node.\\nThis makes its nodes more and more pure as we traverse down the tree. As the\\nmodel correctly classiﬁes ALL training instances, it leads to 100% overﬁtting, and\\npoor generalization.\\nFor instance, consider this dummy dataset:\\n279'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 280}, page_content='DailyDoseofDS.com\\nFitting a decision tree on this dataset gives us the following decision region plot:\\nIt is pretty evident from the decision region plot, the training and test accuracy\\nthat the model has entirely overﬁtted our dataset.\\nρoءق-ԍ\\u05ebm\\u05fa\\u05ceeٺ֖tڀ֔p؞ٜn֕לg \\u0601CπѮ) is an eﬀective technique to prevent this.\\nCCP considers a combination of two factors for pruning a decision tree:\\n●Cost (C): Number of misclassiﬁcations\\n●Complexity (C): Number of nodes\\nThe core idea is to iteratively drop sub-trees, which, aϔer removal, lead to a\\nminimal increase in classiﬁcation cost AND a maximum reduction of complexity\\n(or nodes). In other words, if two sub-trees lead to a similar increase in\\nclassiﬁcation cost, then it is wise to remove the sub-tree with more nodes.\\n280'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 281}, page_content='DailyDoseofDS.com\\nIn sklearn, you can control cost-complexity-pruning using the ccp_alpha\\nparameter:\\n●large value of Ԏc\\u05fa٧a\\u05cd\\u05fbhӝ →results in underﬁtting\\n●small value of Ԏc\\u05fa٧a\\u05cd\\u05fbhӝ →results in overﬁtting\\nThe objective is to determine the optimal value of Ԏc\\u05fa٧a\\u05cd\\u05fbhӝ, which gives a\\nbetter model. The eﬀectiveness of cost-complexity-pruning is evident from the\\nimage below:\\nAs depicted above, CCP results in a much simpler and acceptable decision region\\nplot.\\n281'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 282}, page_content='DailyDoseofDS.com\\nѡOκ ұa\\u05cd֖dӝقiתל ֖n ѺaכԠoו Ͼo؞Էsف\\nAϔer training an ML model on a training set, we always keep a held-out\\nvalidation/test set for evaluation. I am sure you already know the purpose, so we\\nwon’t discuss that.\\nBut do you know that random forests are an exception to that? In other words,\\none can somewhat “evaluate” a random forest using the training set itself.\\nLet’s understand how.\\nTo recap, a random forest is trained as follows:\\n●First, we create diﬀerent subsets of data with replacement (this process is\\ncalled bootstrapping).\\n●Next, we train one decision tree per subset.\\n●Finally, we aggregate all predictions to get the ﬁnal prediction.\\nThis process is depicted below:\\n282'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 283}, page_content='DailyDoseofDS.com\\nIf we look closely above, every subset has some missing data points from the\\noriginal training set.\\nWe can use these observations to validate the model. This is also called\\nout-of-bag validation. Calculating the out-of-bag score for the whole random\\nforest is simple too.\\nBut one thing to remember is that we CAN NOT evaluate individual decision\\ntrees on their speciﬁc out-of-bag sample and generate some sort of “aggregated\\nscore” for the entire random forest model.\\n283'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 284}, page_content='DailyDoseofDS.com\\nThis is because a random forest is not about what a decision tree says\\nindividually. Instead, it’s about what all decision trees say collectively.\\nSo here’s how we can generate the out-of-bag score for the random forest model.\\nFor every data point in the training set:\\n●Gather predictions from all decision trees that did not use it as a training\\ndata point.\\n●Aggregate predictions to get the ﬁnal prediction.\\nFor instance, consider a RF model with 5 decision trees →\\u0601Pԝ ѷ, Ѻ, ѽ, ҏ). Say a\\nspeciﬁc data point Һ was used as a training data in decision trees Ѯ and Ѻ.\\nSo we shall gather the out-of-bag prediction for data point X from decision trees\\nQ, S and T.\\nAϔer obtaining out-of-bag predictions for all samples, we score them to get the\\nout-of-bag score.\\nSee…this technique allowed us to evaluate a random forest model on the training\\nset. Of course, I don’t want you to blindly adopt out-of-bag validation without\\nunderstanding some of its advantages and considerations.\\nI have found out-of-bag validation to be particularly useful in the following\\nsituations:\\n284'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 285}, page_content='DailyDoseofDS.com\\n●In low-data situations, out-of-bag validation prevents data splitting whilst\\nobtaining a good proxy for model validation.\\n●In large-data situations, traditional cross-validation techniques are\\ncomputationally expensive. Here, out-of-bag validation provides an\\neﬃcient alternative. This is because, by its very nature, even\\ncross-validation provides an out-of-fold metric. Out-of-bag validation is\\nalso based on a similar principle.\\nAnd, of course, an inherent advantage of out-of-bag validation is that it\\nguarantees no data leakage. Luckily, out-of-bag validation is also neatly tied in\\nsklearn’s random forest implementation.\\nThe most signiﬁcant consideration about out-of-bag score is to use it with\\ncaution for model selection, model improvement, etc.\\nThis is because if we do, we typically tend to overﬁt the out-of-bag score as the\\nmodel is essentially being tuned to perform well on the data points that were leϔ\\nout during its training. And if we consistently improve the model based on the\\nout-of-bag score, we obtain an overly optimistic evaluation of its generalization\\nperformance.\\nIf I were to share just one lesson here based on my experience, it would be that if\\nwe don’t have a true (and entirely diﬀerent) held-out set for validation, we will\\noverﬁt to some extent.\\nThe decisions made may be too speciﬁc to the out-of-bag sample and may not\\ngeneralize well to new data.\\n285'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 286}, page_content='DailyDoseofDS.com\\nҏrӝ֖n ѺaכԠoו Ͼo؞Էsف \\u05ebn ѐa؞ճe όaفӞsԶقs\\nMost classical ML algorithms cannot be trained with a batch implementation.\\nThis limits their usage to only small/intermediate datasets. For instance, this is\\nthe list of sklearn implementations that support a batch API:\\nIt’s pretty small, isn’t it?\\nThis is concerning because, in the enterprise space, the data is primarily tabular.\\nClassical ML techniques, such as tree-based ensemble methods, are frequently\\nused for modeling.\\nHowever, typical implementations of these models are not “big-data-friendly”\\nbecause they require the entire dataset to be present in memory.\\n286'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 287}, page_content='DailyDoseofDS.com\\nThere are two ways to approach this:\\n1. Use big-data frameworks like Spark MLlib to train them.\\n2. There’s one more way, which Dr. Gilles Louppe discussed in his PhD thesis\\n— Understanding Random Forests.\\nHere’s what he proposed.\\nѺaכԠoו ѮaفԎhԶآ\\nBefore explaining, note that this approach will only work in an ensemble setting.\\nSo, you would have to train multiple models. The idea is to sample random data\\npatches (rows and columns) and train a tree model on them.\\nRepeat this step multiple times by generating diﬀerent patches of data randomly\\nto obtain the entire random forest model.\\nThe eﬃcacy?\\nThe thesis presents many benchmarks (check pages 174 and 178 if you need more\\ndetails) on 13 datasets, and the results are shown below:\\n287'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 288}, page_content='DailyDoseofDS.com\\nFrom leϔ to right →ρiզӞr\\u05f7ڙ, זn֕آtىٲ8ԝ זn֕آtկٲ9ԝ זn֕آtԝ ֖sת\\u05ceeفԞ ӞrԍԷnԶԞ\\nӹrԶӞsفٗ, זaԟԷlתל, זa؞قiԝ ؟eղԷdԝ آeԍ\\u05ebnԟԞ قh֕آ, and آiԟ\\u05eb.\\nFrom the above image, it is clear that in most cases, the random patches\\napproach performs better than the traditional random forest.\\nIn other cases, there wasn’t a signiﬁcant diﬀerence in performance.\\nAnd this is how we can train a random forest model on large datasets that do not\\nﬁt into memory.\\nҷhڀ ԠoԶآ ֖t ٸo؞ׂ?\\nWhile the thesis did not provide a clear intuition behind this, I can understand\\nwhy such an approach would still be as eﬀective as random forest.\\nIn a gist, building trees that are as diﬀerent as possible guarantees a greater\\nreduction in variance.\\nIn this case, the dataset overlap between any two trees is NOT expected to be\\nhuge compared to the typical random forest. This aids in the Bagging objective.\\n288'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 289}, page_content='DailyDoseofDS.com\\nί ұiءٜa\\u05cd Єu֕Ԡe قo ίdӝλoתآt\\nThe following visual summarizes how Boosting models work:\\n●Boosting is an iterative training process.\\n●The subsequent model puts more focus on misclassiﬁed samples from the\\nprevious model.\\n●The ﬁnal prediction is a weighted combination of all predictions\\nHowever, many ﬁnd it diﬃcult to understand how this model is precisely trained\\nand how instances are reweighed for subsequent models. AdaBoost is a common\\nBoosting model, so in this chapter, let’s understand how it works.\\nίdӝλoתآt ֖nفԷrכӞl ٸo؞ׂiכճ\\nThe core idea behind Adaboost is to train many weak learners to build a more\\npowerful model. This technique is also called ensembling.\\nSpeciﬁcally talking about Adaboost, the weak classiﬁers progressively learn from\\nthe previous model’s mistakes, creating a powerful model when considered as a\\nwhole.\\n289'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 290}, page_content='DailyDoseofDS.com\\nThese weak learners are usually decision trees.\\nLet me make it more clear by implementing AdaBoost using the\\nόeԍ֖s֕\\u05ebnҎ؟eԶρlӝآs֕ﬁԷr class from sklearn.\\nConsider we have the following classiﬁcation dataset:\\nTo begin, every row has an equal weight, and it is equal to (1/n), where n is the\\nnumber of training instances.\\nѽtԶ\\u05fb \\u05f8: ҏrӝ֖n Ӟ ٸeӝׂ \\u05ceeӝ؟nԶؠ\\nIn Adaboost, every decision tree\\nhas a unit depth, and they are also\\ncalled stumps.\\n290'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 291}, page_content='DailyDoseofDS.com\\nThus, we deﬁne DecisionTreeClassiﬁer with a max_depth of 1, and train it on the\\nabove dataset.\\nѽtԶ\\u05fb ٗ: ρa\\u05cdԎu\\u05cdӞtԶ قhԶ \\u05ceeӝ؟nԶؠ؛s Ԏoءق\\nOf course, there will be some correct and incorrect predictions.\\nThe total cost (or error/loss) of this speciﬁc weak learner is the sum of the\\nweights of the incorrect predictions. In our case, we have two errors, so the total\\nerror is:\\n291'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 292}, page_content='DailyDoseofDS.com\\nNow, as discussed above, the idea is to let the weak learn progressively learn from\\nprevious learner’s mistakes. So, going ahead, we want the subsequent model to\\nfocus more on the incorrect predictions produced earlier.\\nHere’s how we do this:\\nѽtԶ\\u05fb ي: ρa\\u05cdԎu\\u05cdӞtԶ قhԶ \\u05ceeӝ؟nԶؠ؛s ֖m\\u05fa\\u05ebrفӞnԍԷ\\nFirst, we determine the importance of the weak learner. Quite intuitively, we\\nwant the importance to be inversely related to the above error.\\n●If the weak learner has a high error, it must have a lower importance.\\n●If the weak learner has a low error, it must have a higher importance.\\nOne choice is the following function:\\n●This function is only deﬁned between [0,1].\\n●When the error is high (~1), this means there were no correct predictions →\\nThis gives a negative importance to the weak learner.\\n●When the error is low (~0), this means there were no incorrect predictions\\n→This gives a positive importance to the weak learner.\\nОf ځoٛ էeԶ\\u05ce قhԶ؟e Ԏoٛ\\u05ced ӹe Ӟ ӹeفقe؞ էuכԎt֕\\u05ebn قo ٜsԶ \\u058be؞Է, ځoٛ ӞrԶ էrԶԷ قo\\nٜsԶ قhӝك Ӟnԟ Ԏa\\u05cd\\u05ce ֖t ځoٛ؟ \\u05ebwכ λoתآt֕לg Ӟlղ\\u05ebr֕قhו؊\\n292'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 293}, page_content='DailyDoseofDS.com\\nNow, we have the importance of the learner.\\nThe importance value is used during model inference to weigh the predictions\\nfrom the weak learners. So the next step is to…\\nѽtԶ\\u05fb հ: ѺeٷԷiղ\\u058b قhԶ قrӝ֖n֕לg ֖nءقaכԎeء\\nAll the correct predictions are weighed down as follows:\\nAnd all the incorrect predictions are weighed up as follows:\\nOnce done, we normalize the new weights to add up to one.\\nThat’s it!\\nѽtԶ\\u05fb խ: ѽaו\\u05fblԶ էrתז ؟eٷԷiղ\\u058beԟ ԠaفӞsԶق\\nFrom step 4, we have the reweighed dataset.\\n293'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 294}, page_content='DailyDoseofDS.com\\nWe sample instances\\nfrom this dataset in\\nproportion to the new\\nweights to create a new\\ndataset.\\nNext, go back to step 1 — Train the next weak learner.\\nAnd repeat the above process over and over for some pre-deﬁned max iterations.\\nThat’s how we build the AdaBoost model.\\nAll we have to do is consider the errors from the previous model, reweigh and\\nsample the training instances for the next model, and repeat.\\n294'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 295}, page_content='DailyDoseofDS.com\\nόiוԷnء֖oכӞl֕قy Ѻeԟٜcف֖oכ\\nҏhԶ ҝt֕\\u05ceiفځ \\u05ebf ؙVӝ؟iӝלcԶ؛ ֖n ѮCή\\nThe core objective of PCA is to retain the maximum variance of the original data\\nwhile reducing the dimensionality. The rationale is that if we retain variance, we\\nwill retain maximum information.\\nBut why?\\nMany people struggle to intuitively understand the motivation for using\\n“variance” here. In other words:\\nҷhڀ ؟eفӞiכ֖nղ זaٺ֖mٛז ٲa؞֖aכԎe ֖s Ӟn ֖nԟ֗Ԏaف\\u05ebr էo؞ ؟eفӞiכ֖nղ זaٺ֖mٛז\\n֖nզ\\u05ebrוӞt֕\\u05ebnؐ\\nThis chapter provides an intuitive explanation of this.\\nImagine someone gave us the\\nfollowing weight and height\\ninformation about three\\nindividuals:\\nIt’s clear that the height column has more variation than weight.\\n295'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 296}, page_content='DailyDoseofDS.com\\nThus, even if we discard the weight column, we can still identify these people\\nsolely based on their heights.\\nDropping the weight column\\n●The one in the middle is Nick.\\n●The leϔmost person is Jonas.\\n●The rightmost one is Andrew.\\nThat was super simple. But what if we discarded the height column instead?\\nCan you identify them now?\\nNo, right?\\nAnd why?\\nThis is because their heights have more variation than their weights.\\n296'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 297}, page_content='DailyDoseofDS.com\\nAnd it’s clear from the above example that, typically, if a column has more\\nvariation, it holds more information.\\nThat is the core idea PCA is built around, and that is why it tries to retain\\nmaximum data variance. Simply put, PCA is devised on the premise that more\\nvariance means more information.\\nMore variance means more information and less variance means less information\\nThus, during dimensionality reduction, we can (somewhat) say that we are\\nretaining maximum information if we retain maximum variance.\\nOf course, as we are using variance, this also means that it can be easily\\ninﬂuenced by outliers. That is why we say that PCA is inﬂuenced by outliers.\\nAs a concluding note, always remember that when using PCA, we don’t just\\nmeasure column-wise variance and drop the columns with the least variance.\\nInstead, we must ﬁrst transform the data to create uncorrelated features. Aϔer\\nthat, we drop the new features based on their variance.\\n297'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 298}, page_content='DailyDoseofDS.com\\nфe؞לe\\u05cdѮCή ٲs؉ ѮCή\\nDuring dimensionality reduction, principal component analysis (PCA) tries to\\nﬁnd a low-dimensional linear subspace that the given data conforms to.\\nFor instance, consider the following dummy dataset:\\nIt’s pretty clear from the above visual that there is a linear subspace along which\\nthe data could be represented while retaining maximum data variance. This is\\nshown below:\\nBut what if our data conforms to a low-dimensional yet non-linear subspace.\\n298'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 299}, page_content='DailyDoseofDS.com\\nFor instance, consider the following dataset:\\nDo you see a low-dimensional non-linear subspace along which our data could be\\nrepresented?\\nNo?\\nDon’t worry. Let me show you!\\nThe above curve is a continuous non-linear and low-dimensional subspace that\\nwe could represent our data given along.\\nOkay…so why don’t we do it then?\\nThe problem is that PCA cannot determine this subspace because the data points\\nare non-aligned along a straight line.\\n299'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 300}, page_content='DailyDoseofDS.com\\nIn other words, PCA is a linear dimensionality reduction technique.\\nThus, it falls short in such situations.\\nNonetheless, if we consider the above non-linear data, don’t you think there’s still\\nsome intuition telling us that this dataset can be reduced to one dimension if we\\ncan capture this non-linear curve.\\nKernelPCA (which uses the kernel trick) precisely addresses this limitation of\\nPCA.\\nThe idea is pretty simple:\\n●Project the data to another high-dimensional space using a kernel\\nfunction, where the data becomes linearly representable. Sklearn provides\\na KernelPCA wrapper, supporting many popularly used kernel functions.\\n●Apply the standard PCA algorithm to the transformed data.\\nThe eﬃcacy of KernelPCA over PCA is evident from the demo below.\\nAs shown below, even though the data is non-linear, PCA still produces a linear\\nsubspace for projection:\\n300'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 301}, page_content='DailyDoseofDS.com\\nHowever, KernelPCA produces a non-linear subspace:\\nWhat’s the catch, you might be wondering?\\nThe catch is the run time.\\nPlease note that the run time of PCA is already cubically related to the number of\\ndimensions.\\nKernelPCA involves the kernel trick, which is quadratically related to the number\\nof data points (n).\\nThus, it increases the overall run time.\\nThis is something to be aware of when using KernelPCA.\\n301'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 302}, page_content='DailyDoseofDS.com\\nѮCή ֖s לoف Ӟ ұiءٜa\\u05cd֖zӝقiתל ҏeԍ\\u058bn֕؎uԶ\\nPCA, by its very nature, is a dimensionality reduction technique. Yet, at times,\\nmany use PCA for visualizing high-dimensional datasets. This is done by\\nprojecting the given data into two dimensions and visualizing it.\\nWhile this may appear like a fair thing to do, there’s a big problem here that oϔen\\ngets overlooked.\\nTo understand this problem, we ﬁrst need to understand a bit about how PCA\\nworks.\\nЕoٷ ѮCή ٸo؞ׂsؐ\\nThe core idea in PCA is to linearly project the data to another space using the\\neigenvectors of the covariance matrix.\\nWhy eigenvectors?\\n1. It creates uncorrelated features, which is useful because if features are\\nindependent, the features with the least variance can be dropped for\\ndimensionality reduction.\\n2. It ensures that new features collectively preserve the original data variance.\\n302'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 303}, page_content='DailyDoseofDS.com\\nComing back to the visualization topic…\\nAs discussed above, aϔer applying PCA, each new feature captures a fraction of\\nthe original data variance. Thus, if we intend to use PCA for visualization by\\nprojecting the data to 2-dimensions…\\n…then this visualization will only be useful if the ﬁrst two principal components\\ncollectively capture most of the original data variance.\\nIf they don’t, then the two-dimensional visualization will be highly misleading\\nand incorrect.\\nЕoٷ قo ԠeفԷrו֖nԶ قhԶ ٲa؞֖aכԎe Ԏoכقr֕ӹuف֖oכ \\u05ebf قhԶ ﬁ؟sف\\nقwת \\u05fbr֕לc֕\\u05fba\\u05cd Ԏoו\\u05fboכԷnفآ?\\nWe can avoid this mistake by plotting a cumulative explained variance (CEV)\\nplot. As the name suggests, it plots the cumulative variance explained by\\nprincipal components.\\nIn sklearn, for instance, the explained variance fraction is available in the\\nԷx\\u05fa\\u05cea֕לeԟ٧vӝ؟iӝלcԶ٧rӝقiת٧ attribute:\\n303'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 304}, page_content='DailyDoseofDS.com\\nWe can create a cumulative plot of explained variance and check whether the ﬁrst\\ntwo components explain the majority of variance.\\nFor instance, in the plot below, the ﬁrst two components only explain 55% of the\\noriginal data variance.\\nThus, visualizing this dataset in 2D using PCA may not be a good choice because\\nplenty of data variance is missing.\\nHowever, in the below plot, the ﬁrst two components explain 90% of the original\\ndata variance.\\n304'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 305}, page_content='DailyDoseofDS.com\\nThus, using PCA for visualization looks like a fair thing to do. As a takeaway, use\\nPCA for 2D visualization only when the above plot suggests so.\\nIf it does not, then refrain from using PCA for 2D visualization and use other\\ntechniques speciﬁcally meant for visualization, like t-SNE, UMAP, etc. The\\neﬃcacy of t-SNE over PCA is depicted below:\\n2D projections created by\\nPCA do not consider local\\nstructure. Instead, its\\nprincipal components\\nprimarily focus on preserving\\nthe maximum variance. But\\nt-SNE projections provide\\nmuch more clarity into data\\nclusters. The clusters\\nproduced by t-SNE are well\\nseparated. But those in PCA\\nhave a signiﬁcant overlap, and\\nthey hardly convey anything.\\n305'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 306}, page_content=\"DailyDoseofDS.com\\nق-ѼљE ٲs؉ ѽNϚ — ҷhӝق'ء قhԶ ԠiﬀԷrԶלcԶؑ\\nIn this chapter, let’s continue discussing the t-SNE algorithm.\\nThe t-SNE algorithm is an improved version of the SNE algorithm, both used for\\ndimensionality reduction.\\nThe core idea of SNE (not t-SNE) is the following:\\n●Step 1) For every point (֖) in the given high-dimensional data, convert the\\nhigh-dimensional Euclidean distances to all other points (ֹ) into\\nconditional Gaussian probabilities.\\n○For instance, consider the marked red point in the dataset on the leϔ\\nbelow.\\n○Converting Euclidean distance to all other points into Gaussian\\nprobabilities (the distribution on the right above) shows that other\\nred points have a higher probability of being its neighbor than other\\npoints.\\n306\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 307}, page_content='DailyDoseofDS.com\\n●Step 2) For every data point xᵢ, randomly initialize its counterpart yᵢin\\n2-dimensional space. These will be our projections.\\n●Step 3) Just like we deﬁned conditional probabilities in the\\nhigh-dimensional space in Step 1, we deﬁne the conditional probabilities\\nin the low-dimensional space, using Gaussian distribution again.\\n●Step 4) Now, every data point (i) has a high-dimensional probability\\ndistribution and a corresponding low-dimensional distribution:\\n○The objective is to match these two probability distributions. Thus,\\nwe can make the positions of counterpart yᵢ’s learnable such that this\\ndiﬀerence is minimized.\\n○Using KL divergence as a loss function helps us achieve this. It\\nmeasures how much information is lost when we use distribution Q\\nto approximate distribution P.\\n307'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 308}, page_content='DailyDoseofDS.com\\n○Ideally, we want to have the minimum loss value (which is zero), and\\nthis will be achieved when P=Q.\\nThe model can be trained using gradient descent, and it works pretty well.\\nFor instance, the following image depicts a 2-dimensional visualization produced\\nby the SNE algorithm on 256-dimensional handwritten digits:\\nSNE produces good clusters.\\nWhat’s even more astonishing is that properties like orientation, skew, and\\nstrokethickness vary smoothly across the space within each cluster. This is\\ndepicted below:\\n308'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 309}, page_content='DailyDoseofDS.com\\nNonetheless, it has some limitations, which the t-SNE algorithm addresses.\\nNotice that the clusters produced by SNE are not well separated.\\nHere, it could be fair to assume that the original data clusters, the one in the\\n256-dimensional space, most likely would have been well separated. Thus:\\n●All zeros must have been together but well separated from other digits.\\n●All ones must have been together but well separated from other digits.\\n●And so on.\\n309'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 310}, page_content='DailyDoseofDS.com\\nYet, SNE still produces tightly packed clusters. This is also called the “crowding\\nproblem.”\\nTo eliminate this problem, t-SNE was proposed, standing for t-distributed\\nStochastic Neighbor Embedding (t-SNE).\\nHere’s the diﬀerence.\\nRecall that in SNE, we used a Gaussian distribution to deﬁne the\\nlow-dimensional conditional probabilities. But it is not producing well-separated\\nclusters.\\nOne solution is to use some other probability distribution, such that for distant\\npoints, we get the same value of the conditional probability as we would have\\nobtained from a Gaussian distribution but at a larger Euclidean distance.\\nLet me simplify that a bit. Compare the following two distributions:\\nNotice that Gaussian achieves a speciﬁc value of low probability density at a\\nsmaller distance. But t-distribution achieves it at a larger distance.\\nThis is precisely what we intend to achieve.\\nWe need a heavier tail distribution so that we can still minimize the diﬀerence\\nbetween the two probability distributions but at a larger distance in the\\nlow-dimensional space.\\n310'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 311}, page_content='DailyDoseofDS.com\\nThe Student t-distribution is a perfect ﬁt for it. The following image depicts the\\ndiﬀerence this change brings:\\nAs shown above:\\n●SNE produces closely packed clusters.\\n●t-SNE produces well-separated clusters.\\nAnd that’s why t-distribution is used in t-SNE.\\nThat said, besides producing well-separated clusters, using the Student\\nt-distribution has many more advantages.\\nFor instance, it is computationally much faster to evaluate the density of a point\\nunder a Student t-distribution than under a Gaussian.\\n311'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 312}, page_content='DailyDoseofDS.com\\nЕoٷ ҏo ίvת֖d Єeفقiכճ іiء\\u05ceeԟ ӹy ق-ѼљE Ѯrתֹeԍقiתלsؐ\\nFrom the above discussion, t-SNE sounds promising, doesn’t it? While the\\nalgorithm is quite powerful, many consistently make misleading conclusions\\nfrom the t-SNE projections of their high-dimensional data.\\nIn this chapter, I want to point out a few of these mistakes so that you don’t make\\nthose mistakes ever.\\nTo begin, the performance of the t-SNE algorithm is primarily reliant on\\n\\u05fbe؞\\u05fc\\u05ceeٺ֖tڀ — a hyperparameter of t-SNE. That is why it is considered the most\\nimportant hyperparameter in the t-SNE algorithm.\\nSimply put, the perplexity value provides a rough estimate for the number of\\nneighbors a point may have in a cluster. And diﬀerent values of perplexity create\\nvery diﬀerent low-dimensional cluster spaces, as depicted below:\\n312'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 313}, page_content='DailyDoseofDS.com\\nAs shown above, most projections do depict the original clusters. However, they\\nvary signiﬁcantly in shape.\\nThere are ﬁve takeaways from the above image:\\n●NEVER make any conclusions about the original cluster shape by looking\\nat these projections.\\n○Diﬀerent projections have diﬀerent low-dimensional cluster shapes,\\nand they do not resemble the original cluster shape.\\n○For low perplexity values (5 and 10), cluster shapes signiﬁcantly\\ndiﬀer from the original ones.\\n○Although, in this case, the clusters were color-coded, which provided\\nmore clarity. But it may not always be the case, as tSNE is an\\nunsupervised algorithm.\\n●Cluster sizes in a t-SNE plot do not convey anything either.\\n●The dimensions (or coordinates of data points) created by t-SNE in low\\ndimensions have no inherent meaning.\\n○The axes tick labels of the low-dimensional plots are diﬀerent and\\nsomewhat random.\\n○Similar to PCA’s principal components, they oﬀer no\\ninterpretability.\\n●The distances between clusters in a projection do not mean anything.\\n○In the original dataset, the blue and red clusters are close.\\n○Yet, most projections do not preserve the global structure of the\\noriginal dataset.\\n●Strange things happen at \\u05fbe؞\\u05fc\\u05ceeٺ֖tڀՕ2 and \\u05fbe؞\\u05fc\\u05ceeٺ֖tڀՕ1ژڙ.\\n○At \\u05fbe؞\\u05fc\\u05ceeٺ֖tڀՕ2, the low-dimensional mapping conveys nothing.\\n■As discussed earlier, the perplexity value provides a rough\\nestimate of the number of neighbors a point may have in a\\ncluster.\\n■t-SNE tries to maintain approx. 2 points per cluster. That is\\nwhy the distortion.\\n313'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 314}, page_content='DailyDoseofDS.com\\n○At \\u05fbe؞\\u05fc\\u05ceeٺ֖tڀՕ1ژڙ, the global structure is preserved, but the local\\nstructure gets distorted.\\n○Thus, tweaking the perplexity hyperparameter is extremely critical\\nhere.\\n○That is why I mentioned above that it is the most important\\nhyperparameter of this algorithm.\\nAs a concluding note, it is found that the ideal perplexity values typically lie in\\nthe range ԇ5ԝխ0Ԉ. So try experimenting in that range and see what looks\\npromising.\\nNext time you use t-SNE, consider the above points, as these plots can get tricky\\nto interpret. This is especially true if you don’t understand the internal workings\\nof this algorithm.\\nNonetheless, understanding the algorithm will massively help you develop an\\nintuition on its interpretability.\\n314'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 315}, page_content='DailyDoseofDS.com\\nίcԍԷlԶ؟aفԷ قSјϛ ٸiف\\u058b ЄPҜ\\nSklearn implementations are\\ndriven by NumPy, which typically\\nrun on a single core of a CPU.\\nThus, the ability to run\\nparallelized operations is quite\\nlimited at times.\\nAnother major limitation is that scikit-learn models cannot run on GPUs.\\nThis bottleneck provides massive room for run-time improvement. The same\\napplies to the tSNE algorithm, which is among the most powerful dimensionality\\nreduction techniques to visualize high-dimensional datasets.\\n315'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 316}, page_content='DailyDoseofDS.com\\nBecause the biggest issue with tSNE is\\nthat its run-time is quadratically\\nrelated to the number of data points.\\nThus, beyond, say, 20k-25k data points, it becomes pretty diﬃcult to use tSNE\\nfrom Sklearn implementations. There are two ways to handle this:\\n●Either keep waiting.\\n●Or use optimized implementations that could be possibly accelerated with\\nGPU.\\nI was stuck due to the same issue in one of my projects, and I found a pretty\\nhandy solution.\\ntSNE-CUDA is an optimized CUDA version of the tSNE algorithm, which, as the\\nname suggests, can leverage hardware accelerators. As a result, it provides\\nimmense speedups over the standard Sklearn implementation:\\n316'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 317}, page_content='DailyDoseofDS.com\\nAs depicted above, the GPU-accelerated implementation:\\n●Is 33 times faster than the Sklearn implementation.\\n●Produces similar quality clustering as the Sklearn implementation.\\nDo note that this implementation only supports ל_ԍ\\u05ebm\\u05fa\\u05ecלeכقsՔٗ, i.e., you can\\nonly project to two dimensions. As per the docs, the authors have no plans to\\nsupport more dimensions, as this will require signiﬁcant changes to the code.\\nBut in my opinion, that doesn’t matter because, for more than 99% of cases, tSNE\\nis used to obtain 2D projections. So we are good here.\\nBefore I conclude this chapter, I also found the following benchmarking results\\nby the authors:\\nIt depicts that on the CIFAR-10 training set (50k images), tSNE-CUDA is 700x\\nFaster than Sklearn, which is an insane speedup.\\nHere’s my Colab notebook for you to get started: https://bit.ly/3WdEone.\\n317'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 318}, page_content='DailyDoseofDS.com\\nѽcӝ\\u05cee قSјϛ قo іi\\u05cd\\u05ceiתלs \\u05ebf όaفӞ Ѯo֕לtء ҷiف\\u058b \\u05ebpԶלTѼљE\\ntSNE-CUDA discussed above provides immense speedups over the standard\\nSklearn implementation using a GPU. But what if you don’t have access to a\\nGPU?\\nopenTSNE is another optimized Python implementation of t-SNE, which\\nprovides massive speed improvements and enables us to scale t-SNE to millions\\nof data points — a place where Sklearn implementation may never reach.\\nAs depicted above, the openTSNE implementation:\\n●is 20 times faster than the Sklearn implementation.\\n●produces similar quality clustering as the Sklearn implementation.\\n318'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 319}, page_content='DailyDoseofDS.com\\nThe authors have also provided the following benchmarking results:\\nAs depicted above, openTSNE can produce low dimensional visualization of a\\nmillion data points in just ~15 minutes.\\nHowever, it is clear from their benchmarks that the run-time of the Sklearn\\nimplementation has already reached a couple of hours with just ~250k data\\npoints.\\nDownload the notebook here to try out openTSNE: https://bit.ly/3zAHCZ8.\\n319'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 320}, page_content='DailyDoseofDS.com\\nѮCή ٲs؉ ق-ѼљE\\nFinally, it would be good to look at the core diﬀerences between PCA and t-SNE\\nalgorithms. The following table for you which neatly summarizes the major\\ndiﬀerences between the two algorithms:\\nש1\\u0602 Ѯu؞\\u05fboءԷ\\n●While many interpret PCA\\nas a data visualization\\nalgorithm, it is primarily a\\ndimensionality reduction\\nalgorithm.\\n●t-SNE, however, is a data\\nvisualization algorithm. We\\nuse it to project\\nhigh-dimensional data to\\nlow dimensions (primarily\\n2D).\\n320'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 321}, page_content='DailyDoseofDS.com\\nש2\\u0602 ҏy\\u05faԷ \\u05ebf Ӟlղ\\u05ebr֕قhו\\n●PCA is a deterministic algorithm. Thus, if we run the algorithm twice on\\nthe same dataset, we will ALWAYS get the same result.\\n●t-SNE, however, is a stochastic algorithm. Thus, rerunning the algorithm\\ncan lead to entirely diﬀerent results.\\nש3\\u0602 ҝn֕؎uԶלeءآ \\u05ebf آo\\u05cdٜt֕\\u05ebn\\nAs far as uniqueness and interpretation of results is concerned…\\n●PCA always has a unique solution for the projection of data points. Simply\\nput, PCA is just a rotation of axes such that the new features we get are\\nuncorrelated.\\n●t-SNE, as discussed above, can provide entirely diﬀerent results, and its\\ninterpretation is subjective in nature.\\nש4\\u0602 Ѯrתֹeԍقiתל قy\\u05faԷ\\n●PCA is a linear dimensionality reduction approach. It can only ﬁnd a linear\\nsubspace to project the given dataset. KernelPCA addresses this, which we\\ncovere earlier in this section.\\n●t-SNE is a non-linear approach. It can handle non-linear datasets.\\n321'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 322}, page_content='DailyDoseofDS.com\\nש5\\u0602 Ѯrתֹeԍقiתל Ӟp\\u05fa؟oӝԎh\\n●PCA only aims to retain the global variance of the data. Thus, local\\nrelationships (such as clusters) are oϔen lost aϔer projection, as shown\\nbelow:\\n●t-SNE preserves local relationships. Thus, data points in a cluster in the\\nhigh-dimensional space are much more likely to lie together in the\\nlow-dimensional space.\\n○In t-SNE, we do not explicitly specify global structure preservation.\\nBut it typically does create well-separated clusters.\\n○Nonetheless, as discussed in the previous chapter, the distance\\nbetween two clusters in low-dimensional space is NEVER an\\nindicator of cluster separation in high-dimensional space.\\n322'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 323}, page_content='DailyDoseofDS.com\\nρlٛآtԶ؟iכճ\\nλeڀ\\u05ebnԟ фMԶӞnءԜ ظ іuءق-уלoٷ ҏy\\u05faԷs \\u05ebf ρlٛآtԶ؟iכճ\\nίlղ\\u05ebr֕قhוآ\\nClustering is one of the core branches of unsupervised learning in ML. The ﬁrst\\n(and sometimes the only) clustering algorithm folks learn is KMeans. Yet, it is\\nimportant to note that KMeans is not a universal solution to all clustering\\nproblems. In fact, there’s a whole world of clustering algorithms beyond KMeans,\\nwhich we must be familiar with. The visual below summarizes 6 diﬀerent types of\\nclustering algorithms:\\nCentroid-based: Cluster data\\nbased on proximity to centroids.\\nConnectivity-based: Cluster\\npoints based on proximity\\nbetween clusters.\\nDensity-based: Cluster points\\nbased on their density. It is more\\nrobust to clusters with varying\\ndensities and shapes than\\ncentroid-based clustering.\\nGraph-based: Cluster points\\nbased on graph distance.\\nDistribution-based: Cluster points based on their likelihood of belonging to the\\nsame distribution. Gaussian Mixture Model in one example. We will discuss\\nGaussian mixture models (GMMs) in an upcoming chapter.\\nCompression-based: Transform data to a lower dimensional space and then\\nperform clustering\\n323'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 324}, page_content='DailyDoseofDS.com\\nОnف؟iכآiԍ іeӝآu؞Էs էo؞ ρlٛآtԶ؟iכճ ϛvӝ\\u05ceuӝقiתל\\nWithout labeled data, it is not possible to objectively measure how well the\\nclustering algorithm has grouped similar data points together and separated\\ndissimilar ones.\\nAlso, most clustering datasets are multi-dimensional, so directly visualizing the\\nresults is not possible.\\nThus, we must rely on intrinsic measures to determine clustering quality in such\\ncases.\\nThe way I like to use them is as follows:\\n●Say I am using KMeans:\\n○Run KMeans with a range of k values.\\n○Evaluate the performance.\\n○Select the value of k based on the most promising cluster quality\\nmetric.\\nLet’s understand a few of these metrics.\\n324'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 325}, page_content='DailyDoseofDS.com\\nש1\\u0602 ѽi\\u05cd\\u058boٛԷtفԸ ρoԶﬃԎiԶלtԛ\\nThe Silhouette Coeﬃcient indicates how well each data point ﬁts into its\\nassigned cluster.\\nThe idea is that if the average distance to all data points in the same cluster is\\nsmall but that to another cluster is large, this intuitively indicates that the\\nclusters are well separated and somewhat reliable.\\n325'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 326}, page_content='DailyDoseofDS.com\\nIt is measured as follows:\\n●For every data point:\\n○ﬁnd the average distance to all other points within its cluster →A\\n○ﬁnd the average distance to all points in the nearest cluster →B\\n○point’s score = (B-A)/max(B, A)\\n●Compute the average of all individual scores to get the overall clustering\\nscore.\\nNow consider this:\\n●If, for all data points, B is much greater than A, this would mean that:\\n○The average distance to points in the closest cluster is large.\\n○The average distance to points in the same cluster is small.\\n○Thus, the overall score will be close to 1, indicating that the clusters\\nare well separated.\\nSo, a higher Silhouette Coeﬃcient would mean that, generally speaking, all data\\npoints ﬁt into their ideal clusters.\\nMeasuring it across a range of centroids (k) can reveal which clustering results\\nare most promising:\\n326'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 327}, page_content='DailyDoseofDS.com\\nש2\\u0602 ρa\\u05cd֖nءׂi֓Еa؞Ӟbӝآz ОnԟԷx\\nThe issue with the Silhouette score is that its run-time grows quadratically with\\nthe number of data points.\\nCalinski-Harabasz Index is another metric that is measured quite similarly to the\\nSilhouette Coeﬃcient.\\nHere’s what it computes:\\n●A →sum of squared distance between all centroids and overall dataset\\ncenter.\\n●B →sum of squared distance between all points and their speciﬁc centroid.\\n●metric is computed as A/B (with an additional scaling factor ).\\nYet again, is A is much greater than B:\\n●The distance of centroids to the dataset center is large.\\n●The distance of data points to their speciﬁc centroid is small\\n●Thus, it will result in a higher score, indicating that the clusters are well\\nseparated.\\n327'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 328}, page_content='DailyDoseofDS.com\\nThe reason I prefer the Calinski-Harabasz Index over the Silhouette Coeﬃcient\\nis that:\\n●It is relatively much faster to compute than the Silhouette Coeﬃcient.\\n●It makes the same intuitive sense for interpretation as the Silhouette\\nCoeﬃcient.\\nש3\\u0602 όBπұ\\nThe issue with the Silhouette score and Calinski-Harabasz index is that they are\\ntypically higher for convex (or somewhat spherical) clusters.\\nHowever, using them to evaluate arbitrary-shaped clustering, like those obtained\\nwith density-based clustering, can produce misleading results. This is evident\\nfrom the image below:\\n328'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 329}, page_content='DailyDoseofDS.com\\nAs depicted above, while the clustering output of KMeans is worse, the\\nSilhouette score is still higher than Density-based clustering.\\nDBCV — density-based clustering validation is a better metric in such cases.\\nAs the name suggests, it is speciﬁcally meant to evaluate density-based\\nclustering. Simply put, DBCV computes two values:\\n●The density within a cluster\\n●The density overlap between clusters\\nA high density within a cluster and a low density overlap between clusters\\nindicate good clustering results. The eﬀectiveness of DBCV is also evident from\\nthe image below:\\nWith DBCV, the score for the clustering output of KMeans is worse, and that of\\ndensity-based clustering is higher.\\n329'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 330}, page_content='DailyDoseofDS.com\\nλrԶӞt֊֖nղ фMԶӞnء ٲs фMԶӞnء\\nKMeans is widely used for its simplicity and eﬀectiveness as a clustering\\nalgorithm. Yet, we all know that its performance is entirely dependent on the\\ncentroid initialization step. Thus, it is likely that we may obtain inaccurate\\nclusters, as depicted below:\\nOf course, rerunning with diﬀerent initialization does help at times. But I have\\nnever liked the unnecessary run-time overhead it introduces.\\nIn this chapter, let’s learn a supercharged upgrade to KMeans, which addresses\\nthis issue while also producing better clustering results. It’s called the Breathing\\nKMeans algorithm.\\nѽtԶ\\u05fb \\u05f8: Ѻuכ фmԶӞnء\\nFirst, we run the usual KMeans clustering only once, i.e., without rerunning the\\nalgorithm with a diﬀerent initialization.\\n330'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 331}, page_content='DailyDoseofDS.com\\nClustering results of KMeans aϔer running the algorithm without repetition and\\nuntil convergence\\nThis gives us the location of “k” centroids, which may or may not be accurate.\\nѽtԶ\\u05fb ٗ: λrԶӞt֊Է ֖n آtԶ\\u05fb\\nTo the “k” centroids obtained from Step 1, we add “m” new centroids.\\nAs per the research paper of Breathing Kmeans, m=5 was found to be good value.\\nNow, you might be thinking, where do we add these “m” centroids?\\nThe addition of new centroids is decided based on the error associated with a\\ncentroid. Simply put, a centroid’s error is the sum of the squared distance to the\\npoints associated with that centroid.\\nThus, we add “m” centroids in the vicinity of centroids with high error. Let’s\\nunderstand more intuitively why this makes sense.\\n331'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 332}, page_content='DailyDoseofDS.com\\nIn the above KMeans clustering results:\\n●The centroid at the top has a high error.\\n●All other centroids have relatively low error.\\nIntuitively speaking, if a centroid has a very high error, it is possible that multiple\\nclusters are associated with it. Thus, we would want to split this cluster. Adding\\nnew centroids near clusters with high error will precisely fulﬁll this objective.\\nAϔer adding “m” new centroids, we get a total of “k+m” centroids.\\nFinally, we run KMeans again with “k+m” centroids only once. This gives us the\\nlocation of “k+m” centroids.\\nѽtԶ\\u05fb ي: λrԶӞt֊Է \\u05ebuف آtԶ\\u05fb\\nNext, we want to remove “m” centroids from the “k+m” centroids obtained above.\\nHere, you might be thinking, which “m” centroids should we remove? The\\nremoval of centroids is decided based on the “utility” of a centroid.\\nSimply put, a centroid’s utility is proportional to its distance from all other\\ncentroids. The greater the distance, the more isolated it will be. Hence, the more\\nthe utility.\\nThis makes intuitive sense as well. If two centroids are pretty close, they will\\nhave low utility.\\nThus, they are likely in the same cluster, and we must remove one of them.\\n332'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 333}, page_content='DailyDoseofDS.com\\nThis is demonstrated below:\\nAϔer removing one of the low-utility centroids, the other centroid becomes very\\nuseful. So, in practice, aϔer removing one centroid, we update the utility values of\\nall other centroids.\\nWe repeat the process until all “m” low-utility centroids have been removed. This\\ngives back “k” centroids.\\nFinally, we run KMeans with these “k” centroids only once.\\nѽtԶ\\u05fb հ: όeԍ؟eӝآe ז ӹy \\u05f8.\\nѽtԶ\\u05fb խ: Ѻe\\u05faԷaف ѽtԶ\\u05fbs ٗ قo հ ٜnف֖l ז=ژ؊\\nDone!\\nThese repeated breathing cycles (breathe-in and breathe-out steps) almost always\\nprovide a faster and better solution than standard KMeans with repetitions.\\nIn each cycle:\\n●New centroids are added at “good” locations. This helps in splitting\\nclusters occupied by a single centroid.\\n●Low-utility centroids are removed. This helps in eliminating centroids that\\nare likely in the same cluster.\\n333'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 334}, page_content='Python\\nPython\\nDailyDoseofDS.com\\nAs a result, it is expected to converge to the optimal solution faster. The\\neﬀectiveness of Breathing KMeans over KMeans is evident from the image\\nbelow:\\n●KMeans produced two misplaced centroids\\n●Breathing KMeans accurately clustered the data with a 50% run-time\\nimprovement.\\nThere is also an open-source implementation of Breathing KMeans, with a\\nsklearn-like API. To get started, install the bkmeans library:\\npip install bkmeans\\nNext, run the algorithm as follows:\\nfrom bkmeans import BKMeans\\nbkm = BKMeans(n_clusters = 100)\\nbkm.fit(X)\\n334'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 335}, page_content='DailyDoseofDS.com\\nЕoٷ όoԶآ іiכ֖Bӝقc֊фMԶӞnء ҷo؞ׂsؐ\\nMany ML algorithms implemented by Sklearn support incremental learning, i.e.,\\nmini-batch-based training. This makes them quite useful when the entire data\\ndoes not ﬁt into memory.\\nThese are the supported algorithms for your reference:\\nIf you look closely, there’s KMeans in that list too. Being a popular clustering, it’s\\ngood that we can run it on large datasets if needed.\\nBut when I ﬁrst learned that KMeans can be implemented in a mini-batch\\nfashion, I wondered how exactly a batch implementation of KMeans might work?\\nGiving it a thought for a minute or so gave me an idea, which I wish to share with\\nyou in this chapter.\\n335'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 336}, page_content='DailyDoseofDS.com\\nҏhԶ آoٛ؟cԶ \\u05ebf \\u05ebvԶ؟hԶӞd ֖n фMԶӞnء\\nBefore understanding the mini-batch implementation, we must know the issue\\nwith the usual KMeans implementation.\\nTo recall, KMeans works as follows:\\n●Step 1) Initialize centroids.\\n●Step 2) Find the nearest centroid for each point.\\n●Step 3) Reassign centroids as the average of points assigned to them.\\n●Step 4) Repeat until convergence.\\nIn this implementation, the primary memory bottleneck originates from Step 3.\\nThis is because, in this step, we compute the average of all points assigned to a\\ncentroid to shiϔ the centroid.\\nThus, traditionally, all data points assigned to a centroid must be available in\\nmemory to take an average over.\\n336'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 337}, page_content='DailyDoseofDS.com\\nBut in our case, all data points DO NOT ﬁt into memory at once. Thus, direct\\naveraging isn’t feasible. Nonetheless, here’s how we can smartly address this\\nissue.\\nλeզ\\u05ebrԶ О \\u05fbrתԎeԶԠ էu؞قhԶ؟:\\nЕoכԷsف\\u05cey آpԶӞk֕לgԝ О \\u058baٱԷnؚق ԎhԶԏׂeԟ قhԶ \\u05fbrԶԎiءԷ ֖m\\u05fa\\u05ceeוԷnفӞt֕\\u05ebn \\u05ebf\\nіiכ֖Bӝقc֊фMԶӞnء ֖n ѽk\\u05cdԷa؞ל. ҏhԶ Ԡoԍآ Ԡoכ؛t قe\\u05cd\\u05ce זuԍ\\u058b Էiف\\u058be؞؊\\nҏhԶ آo\\u05cdٜt֕\\u05ebn О Ӟm Ӟbתٜt قo آhӝ؟e ֖s قhԶ \\u05ebnԶ О ԎaוԷ ٜp ٸiف\\u058b ٸhԶל О ٸaء\\nقrڀ֖nղ قo ﬁճu؞Է \\u05ebuف \\u058boٷ Ӟ זiכ֖-ӸӞtԍ\\u058b фMԶӞnء Ԏaכ ӹe \\u05fboءآiӸ\\u05cey\\n֖m\\u05fa\\u05ceeוԷnفԸԠ.\\nѽo ֖f قhԶ؟eؚآ Ӟ \\u05fbe؞էeԍق \\u05ebvԶ؟lӝ\\u05fb ٸiف\\u058b ѽk\\u05cdԷa؞ל, ճrԶӞtդ\\nОf לoفԞ ֖tؚآ ﬁלe ӹeԍӞuءԷ قhԶ آo\\u05cdٜt֕\\u05ebn О Ӟm Ӟbתٜt قo آhӝ؟e ֖s \\u05fbe؞էeԍقlڀ\\nٲiӝӹlԶ قoת؊\\nіiכ֖Bӝقc֊фMԶӞnء\\nWe begin the algorithm by selecting a value of k — the number of centroids.\\nNext, we need an initial location for the centroids so we can sample m (m << n)\\nrandom data points to initialize the centroids.\\nNow, recall the bottleneck we discussed above — All data points must be\\navailable in memory to average over.\\nThus, to address this, we need a way to keep track of the data points assigned to\\neach centroid as we load and process each mini-batch.\\n337'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 338}, page_content='DailyDoseofDS.com\\nWe cannot maintain a growing list of data points assigned to each centroid\\nbecause that would eventually take up the memory of the entire dataset.\\nSo here’s how we can smartly fulﬁll this objective.\\nWe can utilize the following mathematical property that relates sum and average:\\nHow?\\nSee, in a typical KMeans setting, we average over all data points assigned to a\\ncentroid to determine its new location. But as discussed above, we can’t keep all\\ndata points in memory at once.\\nSo how about we do this in the mini-batch setting of KMeans:\\nFor every centroid:\\n●We maintain a “sum-vector”, which stores the sum of the vectors of all data\\npoints assigned to that centroid. To begin, this will be a zero vector.\\n●A “count” variable, which tracks the number of data points assigned to a\\ncentroid. To begin, the count will be zero.\\n338'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 339}, page_content='DailyDoseofDS.com\\nWhile iterating through mini-batches:\\n●Every “sum-vector” of a centroid will continue to accumulate the sum of all\\ndata points that have been assigned to it so far.\\n●Every “count” variable of a centroid will continue to track the number of\\ndata points that have been assigned to it so far.\\nAϔer iterating through all mini-batches, we can perform a scalar division\\nbetween sum-vector and count to get the average vector:\\nThis average will precisely tell us the new location of the centroid as if all data\\npoints were present in memory. We can repeat this until convergence.\\nAlso, before starting the next iteration of MiniBatchKMeans, we must reset the\\nsum-vector to a zero vector and count to zero. And that’s how a mini-batch\\nversion of KMeans can be implemented.\\n339'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 340}, page_content='DailyDoseofDS.com\\nίNј֔d؞֖vԶל фMԶӞnء ٸiف\\u058b Ͼa֕آs\\nAs discussed in the previous chapter, KMeans is trained as follows:\\n●Step 1) Initialize centroids\\n●Step 2) Find the nearest centroid for each point\\n●Step 3) Reassign centroids\\n●Step 4) Repeat until convergence\\nBut in this implementation, “Step 2” has a run-time bottleneck, as this step\\ninvolves a brute-force and exhaustive search. In other words, this ﬁnds the\\ndistance of every data point from every centroid.\\nAs a result, this step isn’t optimized, and it takes plenty of time to train and\\npredict. This is especially challenging with large datasets.\\nTo speed up KMeans, one of the implementations I usually prefer, especially on\\nlarge datasets, is Faiss by Facebook AI Research.\\nTo elaborate further, Faiss provides a much faster nearest-neighbor search using\\napproximate nearest-neighbor search algorithms.\\nIt uses an “Inverted Index,” which is an optimized data structure to store and\\nindex the data point.\\n340'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 341}, page_content='DailyDoseofDS.com\\nThis makes performing clustering extremely eﬃcient, especially on large\\ndatasets, which is also evident from the image below:\\nAs shown above, on a dataset of 500k data points (1024 dimensions), Faiss is\\nroughly 20x faster than KMeans from Sklearn, which is an insane speedup.\\nWhat’s more, Faiss can also run on a GPU, which can further speed up your\\nclustering run-time performance.\\nGet started with Faiss here: https://github.com/facebookresearch/faiss.\\n341'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 342}, page_content='DailyDoseofDS.com\\nфMԶӞnء ٲs؉ Єaٛآs֕Ӟn іiٺقu؞Է іoԟԷlء\\nI like to think of Gaussian Mixture Models as a more generalized version of\\nKMeans, which addresses some of the widely known limitations of KMeans.\\nѐiו֖tӝقiתלs \\u05ebf фMԶӞnء\\nTo begin, KMeans\\ncan only produce\\nglobular clusters.\\nFor instance, as\\nshown below, even\\nif the data has\\nnon-circular\\nclusters, it still\\nproduces round\\nclusters.\\nMoreover, it performs a hard assignment. There are no probabilistic estimates of\\neach data point belonging to each cluster.\\nLastly, it only relies on distance-based measures to assign data points to clusters.\\n342'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 343}, page_content='DailyDoseofDS.com\\nTo understand this point better, consider two clusters in 2D — A and B. Cluster\\nA has a higher spread than B.\\nNow consider a line that is mid-way between centroids of A and B.\\nAlthough A has a higher spread, even if a point is slightly right to the midline, it\\nwill get assigned to cluster B.\\nIdeally, however, cluster A should have had a larger area of inﬂuence.\\n343'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 344}, page_content='DailyDoseofDS.com\\nЄaٛآs֕Ӟn іiٺقu؞Է іoԟԷlء\\nThese limitations oϔen make KMeans a non-ideal choice for clustering. Gaussian\\nMixture Models are oϔen a superior algorithm in this respect. As the name\\nsuggests, they can cluster a dataset that has a mixture of many Gaussian\\ndistributions. They can be thought of as a more ﬂexible twin of KMeans.\\nThe primary diﬀerence is that,\\nKMeans learns centroids but\\nGaussian mixture models learn a\\ndistribution. For instance, in 2\\ndimensions, KMeans can only\\ncreate circular clusters but GMM\\ncan create oval-shaped clusters.\\nThe eﬀectiveness of GMMs over KMeans is evident from the image below.\\n●KMeans just relies on distance and ignores the distribution of each cluster.\\n●GMM learns the distribution and produces better clustering.\\n344'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 345}, page_content='DailyDoseofDS.com\\nόBѼρAј،+ԛ ί Ͼaءقe؞ Ӟnԟ ѽcӝ\\u05ceaӸ\\u05cfԷ ίlفԷrכӞt֕ٲe قo\\nόBѼρAј\\nNon-parametric unsupervised algorithms are widely used in industry to analyze\\nlarge datasets. This is because in many practical applications, gathering true\\nlabels is pretty diﬃcult or infeasible.\\nIn such cases:\\n●Either data teams annotate the data, which can be practically impossible at\\ntimes,\\n●Or use unsupervised methods to identify patterns.\\nWhile KMeans is widely used here due to its simplicity and eﬀectiveness as a\\nclustering algorithm, it has many limitations as we also discussed in the previous\\nchapter:\\n●It does not account for cluster covariance.\\n●It can only produce spherical clusters. As shown below, even if the data has\\nnon-circular clusters, it still produces round clusters.\\nDensity-based algorithms, like DBSCAN, quite eﬀectively address these\\nlimitations.\\nThe core idea behind DBSCAN is to group together data points based on\\n“density”, i.e., points that are close to each other in a high-density region and are\\nseparated by lower-density regions.\\n345'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 346}, page_content='DailyDoseofDS.com\\nBut unfortunately, with the use of DBSCAN, we again run into an overlooked\\nproblem. One of the things that makes DBSCAN infeasible to use at times is its\\nrun-time.\\nUntil recently, it was believed that DBSCAN had a run-time of O(nlogn), but it\\nwas proven to be O(n²) in the worst case.\\nIn fact, this can also be veriﬁed from the ﬁgure below. It depicts the run-time of\\nDBSCAN with the number of samples:\\n346'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 347}, page_content='DailyDoseofDS.com\\nIt is clear that DBSCAN has a quadratic relation with the dataset’s size.\\nThus, there is an increasing need to establish more eﬃcient versions of\\nDBSCAN.\\nDBSCAN++ is a major step towards a fast and scalable DBSCAN.\\nSimply put,\\nDBSCAN++ is\\nbased on the\\nobservation that\\nwe only need to\\ncompute the\\ndensity estimates\\nfor a subset “m” of\\nthe “n” data points\\nin the given\\ndataset, where “m”\\ncan be much\\nsmaller than “n” to\\ncluster properly.\\nThe eﬀectiveness\\nof DBSCAN++ is\\nevident from this\\nimage.\\nAs depicted above, on a dataset of 60k data points:\\n●DBSCAN++ is 20x faster than DBSCAN.\\n●DBSCAN++ produces better clustering scores than DBSCAN.\\n347'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 348}, page_content='DailyDoseofDS.com\\nЕDκѽCήљ ٲs؉ όBѼρAј\\nIn this chapter, let’s dive into HDBSCAN and understand how it diﬀers from\\nDBSCAN.\\nLike any other algorithm, DBSCAN also has some limitations. To begin,\\nDBSCAN assumes that the local density of data points is (somewhat) globally\\nuniform. This is governed by its eps parameter.\\nThus, it may struggle to identify clusters with varying densities. This may need\\nseveral hyperparameter tuning attempts to get promising results.\\nHDBSCAN can be a better choice for density-based clustering. It relaxes the\\nassumption of local uniform density, which makes it more robust to clusters of\\nvarying densities by exploring many diﬀerent density scales.\\nFor instance, consider the clustering results obtained with DBSCAN on the\\ndummy dataset below, where each cluster has diﬀerent densities:\\nIt is clear that\\nDBSCAN\\nproduces bad\\nclustering\\nresults.\\n348'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 349}, page_content='DailyDoseofDS.com\\nNow compare it with HDBSCAN results depicted below:\\nOn a dataset with\\nthree clusters, each\\nwith varying\\ndensities,\\nHDBSCAN is found\\nto be more robust.\\nThere’s one more thing I love about HDBSCAN:\\n●DBSCAN is a scale variant algorithm. Thus, clustering results for data X,\\n2X, 3X, etc., can be entirely diﬀerent.\\n●On the other hand, HDBSCAN is scale-invariant. So, clustering results\\nremain the same across diﬀerent scales of data.\\nWe have DBSCAN\\non the leϔ, and we\\ncan see that the\\nresults vary with\\nthe scale of the\\ndata.\\nHowever,\\nclustering from\\nHDBSCAN (on the\\nright) remains\\nunaltered with the\\nscale for\\nHDBSCAN.\\n349'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 350}, page_content='DailyDoseofDS.com\\nρo؞؟e\\u05cdӞt֕\\u05ebn Ӟnӝ\\u05ceyء֖s\\nρo؞؟e\\u05cdӞt֕\\u05ebn ե= ѮrԶԠiԍقiٱԷnԶآs\\nCorrelation measures how two features vary with one another linearly (or\\nmonotonically).\\nThis makes correlation symmetric: corr(A, B) = corr(B, A).\\nYet, associations are oϔen asymmetric. For instance, given a date, it is easy to tell\\nthe month. But given a month, you can never tell the date.\\nCorrelation, being symmetric, entirely ignores this notion. What’s more, it is not\\nmeant to quantify how well a feature can predict the outcome, as demonstrated\\nbelow:\\n350'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 351}, page_content='DailyDoseofDS.com\\nYet, at times, it is misinterpreted as a measure of “predictiveness”. Lastly,\\ncorrelation is mostly limited to numerical data. But categorical data is equally\\nimportant for predictive models.\\nThe Predictive Power Score (PPS) addresses each of these limitations. As the\\nname suggests, it measures the predictive power of a feature.\\nѮPѼ\\u0601a →ӹ) is calculated as follows:\\n●If the target (b) is numeric:\\n○Train a Decision Tree Regressor that predicts ӹ using Ӟ.\\n○Find PPS by comparing its MAE to the MAE of a baseline model\\n(median prediction).\\n●If the target (b) is categorical:\\n○Train a Decision Tree Classiﬁer that predicts b using a.\\n○Find PPS by comparing its F1 to the F1 of a baseline model (random\\nor most frequent prediction).\\nThus, PPS:\\n●is asymmetric, meaning PPS(a, b) != PPS(b, a).\\n●can be used on categorical targets (b).\\n351'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 352}, page_content='DailyDoseofDS.com\\n●can be used to measure the predictive power of categorical features (a).\\n●works well for linear and non-linear relationships.\\n●works well for monotonic and non-monotonic relationships.\\nIts eﬀectiveness is evident from the image below. For all three datasets:\\n●Correlation is low.\\n●PPS (x →y) is high.\\n●PPS (y →x) is zero.\\nThat being said, it is important to note that correlation has its place.\\nWhen selecting between PPS and correlation, ﬁrst set a clear objective about\\nwhat you wish to learn about the data:\\n●Do you want to know the general monotonic trend between two variables?\\nCorrelation will help.\\n●Do you want to know the predictiveness of a feature? PPS will help.\\nGet started with PPS: https://github.com/8080labs/ppscore.\\n352'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 353}, page_content='DailyDoseofDS.com\\nλeٷӞrԶ \\u05ebf ѽuוזa؞ځ ѽtӝقiءك֖cء\\nMany data scientists solely rely on the correlation matrix to study the association\\nbetween variables. But unknown to them, the obtained statistic can be heavily\\ndriven by outliers. This is evident from the image below.\\nAdding just two outliers drastically changed the correlation coeﬃcient and the\\nregression ﬁt. Thus, plotting the data is highly important.\\nThis can save you from drawing wrong conclusions, which you may have drawn\\notherwise by solely looking at the summary statistics.\\nOne thing that I oϔen do when using a correlation matrix is creating a PairPlot\\nas well (shown below).\\n353'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 354}, page_content='DailyDoseofDS.com\\nThis lets me infer if the scatter plot of two variables and their corresponding\\ncorrelation measure resonate with each other or not. We shall continue this\\ndiscussion in the next chapter.\\n354'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 355}, page_content='DailyDoseofDS.com\\nѮeӝ؟sתל Ԏo؞؟e\\u05cdӞt֕\\u05ebn Ԏaכ \\u05ebn\\u05cdځ זeӝآu؞Է \\u05ceiכԷa؞\\nӞsء\\u05ebc֕Ӟt֕\\u05ebn\\nPearson correlation is commonly used to determine the association between two\\ncontinuous variables. Many frameworks (like Pandas, for instance) have it as their\\ndefault correlation metric.\\nYet, unknown to many, Pearson correlation:\\n●Only measures the linear relationship.\\n●Penalizes a non-linear yet monotonic association.\\nInstead, Spearman correlation is a better alternative. It assesses monotonicity,\\nwhich can be linear as well as non-linear.\\n355'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 356}, page_content='DailyDoseofDS.com\\nThis is evident from the illustration below:\\nPearson vs. Spearman on linear and non-linear data\\n●Pearson and Spearman correlation is the same on linear data.\\n●But Pearson correlation underestimates a non-linear association.\\nSpearman correlation is also useful when data is ranked or ordinal, which will\\nshall discuss in the next chapter.\\nAlso, before we end this chapter, remember to always be cautious before drawing\\nany conclusions using summary statistics. We also saw this in the last chapter.\\nWhile analyzing data, so many people get tempted to draw conclusions solely\\nbased on its statistics. Yet, the actual data might be conveying a totally diﬀerent\\nstory.\\n356'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 357}, page_content='DailyDoseofDS.com\\nThis is also evident from the image below:\\nAll nine datasets have approx. zero correlation between the two variables.\\nHowever, the summary statistic, Pearson correlation in this case, gives no clue\\nabout what’s inside the data because it is always zero.\\nIn fact, this is not just about Pearson correlation but applies to all summary\\nstatistics. The idea is that whenever you generate any summary statistic, you lose\\nessential information.\\nThus, the importance of looking at the data cannot be stressed enough. It saves\\nus from drawing wrong conclusions, which we could have made otherwise by\\nlooking at the statistics alone.\\n357'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 358}, page_content='DailyDoseofDS.com\\nρo؞؟e\\u05cdӞt֕\\u05ebn ٸiف\\u058b ѡrԟ֖nӝ\\u05ce ρaفԷgת؟iԍӞl όaفӞ\\nImagine you have an ordinal categorical feature. You want to measure its\\ncorrelation with other continuous features.\\nѡrԟ֖nӝ\\u05ce էeӝقu؞Է ֖s ԎaفԷgת؟iԍӞl ԠaفӞ ٸiف\\u058b Ӟ לaفٜrӝ\\u05ce \\u05ebrԟԷr֕לg ֖n ԎaفԷgת؟iԶآ.\\nBefore proceeding\\nwith the correlation\\nanalysis, you will\\nencode the feature,\\nwhich is a fair thing\\nto do.\\nYet, unknown to many, the choice of encoding can largely aﬀect the correlation\\nresults. For instance, consider the dataset below:\\n358'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 359}, page_content='DailyDoseofDS.com\\nHere, we have:\\n●An ordinal categorical feature: t-shirt size (S, M, L, XL).\\n●A continuous feature: weight.\\nIf we look at it graphically, there does exist a monotonic relationship between the\\ntwo features. However, as depicted below, altering the categorical encoding\\naﬀects the Pearson correlation.\\n●In the leϔ plot, we have the following encoding →S(1), M(2), L(3) and XL(4).\\n●In the right plot, we have the following encoding →S(1), M(2), L(4) and\\nXL(8).\\nIt is clear that the Spearman correlation is a better alternative to assess the\\nmonotonicity between ordinal and continuous features.\\n359'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 360}, page_content='DailyDoseofDS.com\\nIt always remains the same, irrespective of the choice of categorical encoding.\\nThis is because the Spearman correlation is rank-based.\\nIt operates on the ranks of the data, which makes it more suitable for such cases\\nof correlation analysis.\\nUsing Spearman correlation is pretty simple as well. For instance, if you are\\nusing Pandas, just specify the desired correlation measure as follows:\\n360'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 361}, page_content='DailyDoseofDS.com\\nόr֕էt\\nіu\\u05cdقiٱӞr֕ӟقe ρoٱӞr֕ӟقe ѽh֕էt\\nAlmost all real-world\\nML models gradually\\ndegrade in\\nperformance due to\\ncovariate shiϔ.\\nFor starters, covariate shiϔ happens when the distribution of features changes\\nover time, but the true (natural) relationship between input and output remains\\nthe same.\\nIt is a serious problem because we trained the model on one distribution, but it is\\nbeing used to predict on another distribution in production. Thus, it is critical to\\ndetect covariate shiϔ early so that models continue to work well.\\nОsءٜeء ٸiف\\u058b ԠeفԷcف֖oכ זeف\\u058boԟآ\\nOne of the most common and intuitive\\nways to detect covariate shiϔ is by\\nsimply comparing the feature\\ndistribution in training data to that in\\nproduction.\\n361'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 362}, page_content='DailyDoseofDS.com\\nThis could be done in many ways, such as:\\n●Compare their summary statistics — mean, median, etc.\\n●Inspect visually using distribution plots.\\n●Perform hypothesis testing.\\n●Measure distances between training/production distributions using\\nBhattacharyya distance, KS test, etc.\\nWhile these approaches are oϔen eﬀective, the biggest problem is that they work\\non a single feature at a time. But, in real life, we may observe multivariate\\ncovariate shiϔ (MCS) as well. It happens when:\\n●The distribution of individual distributions remains the same:\\n○Ѯ(ҹ\\u05f8) and Ѯ(ҹٗ) individually remain the same.\\n●But their joint distribution changes:\\n○Ѯ(ҹ\\u05f8, Һ2\\u0602 changes.\\n●From the KDE plots on the top and the right, it is clear that the\\ndistribution of both features (covariates) is almost the same.\\n362'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 363}, page_content='DailyDoseofDS.com\\n●But, the scatter plot reveals that their joint distribution in training (Blue)\\ndiﬀers from that in production (Red).\\nAnd it is easy to guess that the univariate covariate shiϔ detection methods\\ndiscussed above will produce misleading results. For instance, as demonstrated\\nbelow, measuring the Bhattacharyya distance between a training and production\\nfeature gives a very low distance value, indicating high similarity:\\nIn fact, even though the individual feature distribution is the same, we can\\nconﬁrm experimentally that this will result in a drop in model performance:\\nLet’s say the true output ځ Օ ٗ*ϽԷaفٜrԶ٧A ، ي*ϽԷaفٜrԶ٧B ، לo֕آe.\\n363'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 364}, page_content='DailyDoseofDS.com\\nIn most cases, the true output predictions on production data are not\\nimmediately available. In the above demonstration, just assume that we have\\nalready gathered the true predictions somehow.\\nAs depicted above, the model performance drops by 20% in production, which is\\nhuge. Here, we may completely rule out the possibility of covariate shiϔ if we\\nthink that covariate shiϔ can never be multivariate in nature.\\nOf course, in the ﬁgure below, it was easy to identify multivariate covariate shiϔ\\nbecause we are only looking at two features.\\nBut multivariate covariate shiϔ can happen with more than two features as well.\\nUnlike the bivariate case above, visual inspection will not be possible for higher\\ndimensions.\\nόeفԷcف֖nղ זu\\u05cdقiٱӞr֕ӟقe ԎoٱӞr֕ӟقe آh֕էt\\nTo begin, it is important to understand that multivariate covariate shiϔ is a big\\nproblem, and there is no direct (or single) approach to handle this. Below, I will\\nshare a couple of ideas that I oϔen use myself and have seen others use as well to\\nhandle multivariate covariate shiϔ.\\n364'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 365}, page_content='DailyDoseofDS.com\\nОdԶӞ ש1ԛ іCѼ ֖s ؟a؞Է\\nAs long as we are checking two (or even three) features at a time, visual\\ninspection can be used to detect covariate shiϔ. So here, at times, many simply\\nignore the possibility of any covariate shiϔ beyond three features.\\nThe rationale is that beyond three features, it is pretty unlikely that:\\n●Ѯ(ҹ\\u05f8)ԝ Ѯ(ҹٗ)ԝ Ѯ(ҹي)ԝ Ѯ(ҹհ)ԝ… →all of them individually will almost remain\\nthe same.\\n●But their joint distribution Ѯ(ҹ\\u05f8, Һ2ԝ Һ3ԝ Һ4ԝ…\\u0603 will change.\\nThus, it might be fair to limit multivariate covariate shiϔ analysis to just one,\\ntwo, and three features at a time. But of course, this may not be always true,\\nwhich brings us to another idea:\\nОdԶӞ ש2ԛ όaفӞ ؟eԍ\\u05ebnءقrٛԎt֕\\u05ebn\\nThis is another cool and practical idea that I used in one of my projects. Data\\nreconstruction, as the name suggests, revolves around learning a mapping that\\nprojects the data to low dimensions and then reconstructs the original data from\\nthe low dimensions.\\n365'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 366}, page_content='DailyDoseofDS.com\\nThis is precisely what Autoencoders do. They are a class of neural networks that\\nlearn to encode data in a lower-dimensional space, and then decode it back to the\\noriginal data space.\\nThe objective is to minimize the data reconstruction error. It’s like asking the\\nmodel to learn a mapping that:\\n●Takes some data.\\n●Projects the data to low dimensions.\\n●And then gives us the exact input data back.\\nSo here’s what we can do:\\n●Train an Autoencoder on the original training dataset. This will give us the\\nweights for the neural network model that reconstructs the dataset.\\n●Use this model on new data to check multivariate covariate shiϔ:\\n○If the reconstruction loss is high, the distribution has changed.\\n○If the reconstruction loss is low, the distribution is almost the same.\\n366'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 367}, page_content='DailyDoseofDS.com\\nOne of the best things about this approach is that it does not need a labeled\\ndataset. Essentially, Autoencoders aim to reconstruct the dataset without labels.\\nThis is quite useful in real-world models because, as we discussed yesterday, in\\nmost cases, the true output predictions on production data are never immediately\\navailable. Instead, they always take some time.\\nFor instance, I remember when I was working on a transactional fraud detection\\nmodel at Mastercard, a card holder’s issuer bank may take upto 45-50 days to\\nsend the fraud label for the transactions that went through Mastercard’s network.\\nThis is a lot of time, isn’t it?\\nBut using Autoencoders, we can still check data reconstruction errors on the\\nunlabeled data. In fact, we don’t necessarily have to use Autoencoders. It was just\\nan example here.\\nOther data reconstruction techniques can also be used, such as PCA is one of\\nthem. Nonetheless, when using any data reconstruction approach, it is important\\nto take care of one thing. Earlier, we discussed that:\\n●If the reconstruction loss is high, the distribution has changed.\\n●If the reconstruction loss is low, the distribution is almost the same.\\nBut interpreting reconstruction loss can be pretty subjective, and it also needs\\nsome context.\\n367'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 368}, page_content='DailyDoseofDS.com\\nFor instance, if the reconstruction loss is 0.4 (say), how do we determine whether\\nthis is signiﬁcant? Contrary to what many think, covariate shiϔ happens\\ngradually in ML models. In other words, it is unlikely that our model is\\nperforming pretty well one day, and the next day, it starts underperforming DUE\\nTO COVARIATE SHIFT.\\nOf course, abrupt degradation is possible but it is rarely due to covariate shiϔ.\\nMost of the time, it happens due to other issues like - May be due to some\\ncompliance issue, the team has stopped collecting a feature. As a result, your\\nmodel no longer has access to a feature you trained it with.Or may be there were\\nsome updates to the inference logic which was no thoroughly tested.and more.\\nBut performance degradation happens gradually due to covariate shiϔ. And in\\nthat phase of performance degradation, we must decide to update the model at\\nsome point to adjust for distributional changes.\\n368'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 369}, page_content='DailyDoseofDS.com\\nWhile data reconstruction loss is helpful in deciding model updates, it conveys\\nvery little meaning alone. Simply put, we need a baseline reconstruction loss\\nvalue to compare our future reconstruction losses.\\nSo here’s what we can do.\\nLet’s say we decided to review our model every week. Aϔer training the model on\\nthe gathered data:\\n●We determine our baseline reconstruction score using the new data\\ngathered over the ﬁrst week in the post-training phase.\\n●We use this baseline score to compare future losses of every subsequent\\nweek.\\n369'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 370}, page_content='DailyDoseofDS.com\\nA high diﬀerence between reconstruction loss and the baseline score will\\nindicate a covariate shiϔ.\\nSimple and intuitive, right? The above reconstruction metric, coupled with the\\nmodel performance, becomes a reliable way to determine if covariate shiϔ has\\nstepped in. Of course, as discussed above, true labels might not be immediately\\navailable at times.\\nThus, determining performance in production can be diﬃcult. To counter this,\\nall ML teams consistently try to gather feedback from the end user to learn if the\\nmodel did well or not.\\nThis is especially seen in recommendation systems:\\n●Was this advertisement relevant to you?\\n●Was this new video relevant to you?\\n●And more.\\nThese things are a part of the model logging phase, where we track model\\nperformance in production. This helps us decide whether we should update our\\nmodel or not.\\n370'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 371}, page_content='DailyDoseofDS.com\\nҝs֕לg Ѯrתٻy֓ѐaӸԷl\\u05cd֖nղ قo ОdԶלt֕էy όr֕էt\\nAlmost all real-world ML models gradually degrade in performance due to a driϔ\\nin feature distribution:\\nIt is a serious\\nproblem because we\\ntrained the model\\non one distribution,\\nbut it is being used\\nto generate\\npredictions on\\nanother distribution\\nin production.\\nThus, it is critical to\\ndetect driϔ early so\\nthat models\\ncontinue to work\\nwell. The image\\ndepicts an intuitive\\ntechnique I oϔen\\nuse to determine\\nwhich features are\\ndriϔing in my\\ndataset.\\n371'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 372}, page_content='DailyDoseofDS.com\\nConsider we have two versions of the dataset — the old version (one on which the\\nmodel was trained) and the current version (one on which the model is generating\\npredictions):\\nThe core idea is to assess if there are any distributional dissimilarities between\\nthese two versions.\\nSo here’s what we do:\\n●Append a \\u05ceaӸԷlՔ\\u05f8 column to the old dataset.\\n●Append a \\u05ceaӸԷlՔڙ column to the current dataset.\\nNow, merge these two datasets and train a supervised learning classiﬁcation\\nmodel on the combined dataset:\\n372'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 373}, page_content='DailyDoseofDS.com\\nThe choice of the classiﬁcation model could be a bit arbitrary, but it should be\\nensured that it is possible to reliably determine feature importance.\\nThus, I personally prefer a random forest classiﬁer because it has an inherent\\nmechanism to determine feature importance:\\nThat said, it is not necessary to use a random forest.\\nTechniques like shuﬄe feature importance (which we discussed above) on top of\\na classiﬁcation model can be used as well.\\nIf the feature importance values suggest that there are features with high feature\\nimportance, this means that those features are driϔing.\\nWhy?\\n373'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 374}, page_content='DailyDoseofDS.com\\nThis is because if some features can reliably distinguish between the two\\nversions of the dataset, then it is pretty likely that their distribution\\ncorresponding to \\u05ceaӸԷlՔ\\u05f8 and \\u05ceaӸԷlՔڙ (conditional distribution) are varying.\\n●If there are distributional diﬀerences, the model will capture them.\\n●If there are no distributional diﬀerences, the model will struggle to\\ndistinguish between the classes.\\nThis idea makes intuitive sense as well.\\nAt this point, one question that many have is that…\\nWhy can’t we just monitor the model accuracy to determine driϔ?\\nOf course, we can do that as long as we have the true labels for the current\\nversion or have a way to compare the model performance on the old version and\\nthe new version.\\nBut in many cases, the true output predictions on production data are never\\nimmediately available.\\nInstead, they always take some time.\\nFor instance, when I was working on a transactional fraud detection model at\\nMastercard, a cardholder’s issuer bank may take up to 45-50 days to send the\\nfraud label for the transactions that went through Mastercard’s network.\\nThis is a lot of time, isn’t it?\\nThus, one must rely on some relevant feedback from the system to determine\\nwhether the model’s performance is dropping or not.\\nUsing this “proxy-labeling technique” discussed above is something I have found\\nto be immensely useful.\\n374'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 375}, page_content='DailyDoseofDS.com\\nׂNј\\nҝs֕לg ׂNјآ \\u05ebn ОmӸӞlӝלcԶԠ όaفӞsԶقs\\nOne of the things that always makes me a bit cautious and skeptical when using\\nkNN is its HIGH sensitivity to the parameter ׂ. To understand better, consider\\nthis dummy 2D dataset below. The red data point is a test instance we intend to\\ngenerate a prediction for using kNN.\\nSay we set the value of ׂ=ة. The prediction for the red instance is generated in\\ntwo steps:\\n●First, we count the 7 nearest neighbors of the red data point.\\n●Next, we assign it to the class with the highest count among those 7\\nnearest neighbors.\\nThis is depicted below:\\n375'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 376}, page_content='DailyDoseofDS.com\\nThe problem is that step 2 is entirely based on the notion of class contribution —\\nthe class that maximally contributes to the k nearest neighbors is assigned to the\\ndata point. But this can miserably fail at times, especially when we have a class\\nwith few samples.\\nFor instance, as shown in\\nthis image, with k=7, the red\\ndata point can NEVER be\\nassigned to the yellow class,\\nno matter how close it is to\\nthat cluster.\\nWhile it is easy to tweak the hyperparameter k visually in the above demo, this\\napproach is infeasible in high-dimensional datasets. There are two ways to\\naddress this.\\n376'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 377}, page_content='DailyDoseofDS.com\\nѽo\\u05cdٜt֕\\u05ebn ש1ԛ ҝsԶԠ ԠiءقaכԎe֓ٸe֕ճhԶԠ ׂNј\\nDistance-weighted\\nkNNs are a much more\\nrobust alternative to\\ntraditional kNNs. As\\nthe name suggests, in\\nstep 2, they consider the\\ndistance to the nearest\\nneighbor.\\nAs a result, the closer a\\nspeciﬁc neighbor is, the\\nmore impact it will have\\non the ﬁnal prediction.\\nIts eﬀectiveness is\\nevident from this image.\\n●In the ﬁrst plot, traditional kNN (with k=7) can never predict the blue class.\\n●In the second plot, distance-weighted kNN is found to be more robust in\\nits prediction.\\nAs per my observation, a distance-weighted kNN typically works better than a\\ntraditional kNN. Yet, this may go unnoticed because, by default, the kNN\\nimplementation of sklearn considers “uniform” weighting.\\n377'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 378}, page_content='DailyDoseofDS.com\\nѽo\\u05cdٜt֕\\u05ebn ש2ԛ όyכӞm֕Ԏa\\u05cd\\u05cey ٜpԟӞtԶ قhԶ \\u058by\\u05faԷr\\u05faӞrӝזeفԷr ׂ\\nRecall the above demo again:\\nHere, one may argue that we must refrain from setting the hyperparameter k to\\nany value greater than the minimum number of samples that belong to a class in\\nthe dataset.\\nOf course, I agree with this to an extent.\\nBut let me tell you the downside of doing that. Setting a very low value of k can\\nbe highly problematic in the case of extremely imbalanced datasets.\\nTo give you more perspective, I have personally used kNN on datasets that had\\nmerely one or two instances for a particular class in the training set. And I\\n378'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 379}, page_content='DailyDoseofDS.com\\ndiscovered that setting a low of k (say, 1 or 2) led to suboptimal performance\\nbecause the model was not as holistically evaluating the nearest neighbor\\npatterns as it was when a large value of k was used.\\nIn other words, setting a relatively larger value of k typically gives more informed\\npredictions than using lower values. But we just discussed above that if we set a\\nlarge value of k, the majority class can dominate the classiﬁcation result:\\nTo address this, I found dynamically updating the hyperparameter k to be much\\nmore eﬀective. More speciﬁcally, there are three steps in this approach. For every\\ntest instance:\\n1. Begin with a standard value of k as we usually would and ﬁnd the k nearest\\nneighbors.\\n2. Next, update the value of the k as follows:\\na. For all unique classes that appear in the k nearest neighbor, ﬁnd the\\ntotal number of training instances they have.\\n379'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 380}, page_content=\"DailyDoseofDS.com\\nb. Update the value of k to:\\n3. Now perform majority voting only on the ﬁrst k' neighbors only.\\nThis makes an intuitive sense as well:\\n●If a minority class appears in the top k nearest neighbor, we must reduce\\nthe value of k so that the majority class does not dominate.\\n●If a minority class DOES NOT appears in the top k nearest neighbor, we\\nwill likely not update the value of k and proceed with a holistic\\nclassiﬁcation.\\nThe only shortcoming is that you wouldn’t ﬁnd this approach in any open-source\\nimplementations. In fact, in my projects as well, I had to write a custom\\nimplementation, so take that into account.\\n380\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 381}, page_content='DailyDoseofDS.com\\nίp\\u05fa؟oٺ֖mӝقe\\nљeӝ؟eءق\\nљe֕ճhӸ\\u05ebr\\nѽeӝ؟c֊\\nҝs֕לg\\nОnٱԷrفԸԠ Ͼi\\u05cdԷ ОnԟԷx\\nOne of the biggest issues with nearest neighbor search using kNN is that it\\nperforms an exhaustive search.\\nIn other words, the query data point must be matched across all data points to\\nﬁnd the nearest neighbor(s). This is highly ineﬃcient, especially when we have\\nmany data points and a near-real-time response is necessary.\\nThat is why approximate nearest neighbor search algorithms are becoming\\nincreasingly popular. The core idea is to narrow down the search space using\\nindexing techniques, thereby improving the overall run-time performance.\\n381'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 382}, page_content='DailyDoseofDS.com\\nInverted File Index (IVF) is possibly one of the simplest and most intuitive\\ntechniques here, which you can immediately start using.\\nЕe؞Է’ء \\u058boٷ ֖t ٸo؞ׂs\\nGiven a set of data points in a high-dimensional space, the idea is to organize\\nthem into diﬀerent partitions, typically using clustering algorithms like k-means.\\nAs a result, each partition has a corresponding centroid, and every data point\\ngets associated with only one partition corresponding to its nearest centroid.\\nAlso, every centroid maintains information about all the data points that belong\\nto its partition.\\n382'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 383}, page_content='DailyDoseofDS.com\\nIndexing done!\\nЕe؞Է’ء \\u058boٷ ٸe آeӝ؟c֊؊\\nWhen searching for the nearest neighbor(s) to the query data point, instead of\\nsearching across the entire dataset, we ﬁrst ﬁnd the closest centroid to the query:\\nOnce we ﬁnd the nearest centroid, the nearest neighbor is searched in only those\\ndata points that belong to the closest partition found:\\nLet’s see how the run-time complexity stands in comparison to traditional kNN.\\nConsider the following:\\n●There are N data points\\n●Each data point is D dimensional\\n●We create K partitions.\\n●Lastly, for simplicity, let’s assume that each partition gets equal data\\npoints.\\n383'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 384}, page_content='DailyDoseofDS.com\\nIn kNN, the query data point is matched to all N data points, which makes the\\ntime complexity →ѡ(јό).\\nIn IVF, however, there are two steps:\\n1. Match to all centroids →ѡ(уό).\\n2. Find the nearest neighbor in the nearest partition →ѡ(јό/у\\u0603.\\nThe ﬁnal time complexity comes out to be the following:\\n…which is signiﬁcantly lower than that of kNN. To get some perspective, assume\\nwe have 10M data points. The search complexity of kNN will be proportional to\\n10M. But with IVF, say we divide the data into 100 centroids, and each partition\\ngets roughly 100k data points.\\nThus, the time complexity comes out to be proportional to 100 + 100k = 100100,\\nwhich is nearly 100 times faster.\\nOf course, it is essential to note that if\\nsome data points are actually close to\\nthe input data point but still happen to\\nbe in the neighboring partition, we will\\nmiss them during the nearest neighbor\\nsearch, as shown in this image.\\nBut this accuracy tradeoﬀis something we willingly accept for better run-time\\nperformance, which is precisely why these techniques are called “approximate\\nnearest neighbors search.”\\n384'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 385}, page_content='DailyDoseofDS.com\\nфe؞לe\\u05cdآ\\nҷhڀ ֖s фe؞לe\\u05cd ҏr֕Ԏk ρa\\u05cd\\u05ceeԟ Ӟ ؓT؞֖cׁؓ?\\nSo many ML algorithms use kernels for robust modeling, like SVM, KernelPCA,\\netc.\\nIn a gist, a kernel function lets us compute dot products in some other feature\\nspace (mostly high-dimensional) without even knowing the mapping from the\\ncurrent space to the other space.\\nBut how does that even happen?\\nLet’s understand!\\nҏhԶ \\u05ebbָԷcف֖vԶ\\nFirstly, it is important to note that the kernel provides a way to compute the dot\\nproduct between two vectors, Һ and Ӏ, in some high-dimensional space without\\nprojecting the vectors to that space.\\nThis is depicted below, where the output of the kernel function is expected to be\\nthe same as the dot product between projected vectors:\\n385'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 386}, page_content='DailyDoseofDS.com\\nThe key advantage is that the kernel function is applied to the vectors in the\\noriginal feature space.\\nHowever, that equals the dot product between the two vectors when projected\\ninto a higher-dimensional (yet unknown) space.\\nIf that is a bit confusing, let me give an example.\\nί זoف֖vӝقiכճ Էxӝזp\\u05cdԷ\\nLet’s assume the following polynomial kernel function:\\nFor simplicity, let’s say both X and Y are two-dimensional vectors:\\nSimplifying the kernel expression above, we get the following:\\nExpanding the square term, we get:\\n386'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 387}, page_content='DailyDoseofDS.com\\nNow notice the ﬁnal expression:\\nThe above expression is the dot product between the following 6-dimensional\\nvectors:\\nThus, our projection function comes out to be:\\nThis shows that the kernel function we chose earlier computes the dot product in\\na 6-dimensional space without explicitly visiting that space.\\nAnd that is the primary reason why we also call it the “kernel trick.”\\nMore speciﬁcally, it’s framed as a “trick” since it allows us to operate in\\nhigh-dimensional spaces without explicitly computing the coordinates of the\\ndata in that space.\\n387'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 388}, page_content='DailyDoseofDS.com\\nҏhԶ іaف\\u058beוӞt֕Ԏs λe֊֖nԟ ѺBϽ фe؞לe\\u05cd\\nAs we saw above, a kernel provides a way to compute the dot product between\\ntwo vectors, X and Y, in some high-dimensional space without projecting the\\nvectors to that space.\\nIn that post, we looked at the polynomial kernel and saw that it computes the dot\\nproduct of a 2-dimensional vector in a 6-dimensional space without explicitly\\nvisiting that space.\\nNext, let’s talk about the RBF kernel, another insanely powerful kernel, which is\\nalso the default kernel in a support vector classiﬁer class implemented by\\nsklearn:\\n388'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 389}, page_content='DailyDoseofDS.com\\nTo begin, the mathematical expression of the RBF kernel is depicted below (and\\nconsider that we have just a 1-dimensional feature vector):\\nYou may remember from high school mathematics that the exponential function\\nis deﬁned as follows:\\nExpanding the square term in the RBF kernel expression, we get:\\nDistributing the gamma term and expanding the exponential term using the\\nexponent rule, we get:\\nNext, we apply the exponential expansion to the last term and get the following:\\n389'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 390}, page_content='DailyDoseofDS.com\\nNotice closely that the exponential expansion above can be rewritten as the dot\\nproduct between the following two vectors:\\nAnd there we get our projection function:\\nIt is evident that this function maps the 1-dimensional input to an\\ninﬁnite-dimensional feature space.\\nThis shows that the RBF kernel\\nfunction we chose earlier\\ncomputes the dot product in an\\ninﬁnite-dimensional space\\nwithout explicitly visiting that\\nspace.\\nThis is why the RBF kernel is considered so powerful, allowing it to easily model\\nhighly complex decision boundaries.\\nHere, I want to remind you that even though the kernel is equivalent to the dot\\nproduct between two inﬁnite-dimensional vectors, we NEVER compute that dot\\nproduct, so the computation complexity is never compromised.\\nThat is why the kernel trick is called a “trick.” In other words, it allows us to\\noperate in high-dimensional spaces without explicitly computing the coordinates\\nof the data in that space.\\n390'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 391}, page_content='DailyDoseofDS.com\\nіiءآiכճ όaفӞ\\nي ҏy\\u05faԷs \\u05ebf іiءآiכճ ұa\\u05cdٜeء\\nAϔer seeing missing values in a dataset, most people jump directly into imputing\\nthem. But as counterintuitive as it may sound, the ﬁrst step towards imputing\\nmissing data should NEVER be imputation.\\nInstead, the ﬁrst step must be to understand the reason behind data missingness.\\nThis is because the data imputation strategy largely depends on WHY data is\\nmissing.\\nMore speciﬁcally, missingness could be of three types:\\n●Missing completely at random (MCAR).\\n●Missing at random (MAR).\\n●Missing not at random (MNAR).\\nOnly when we determine the reason can we proceed with the appropriate\\nimputation techniques.\\nWhat are MCAR, MAR, and MNAR?\\nLet’s understand them in this chapter in detail.\\n391'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 392}, page_content='DailyDoseofDS.com\\n\\u05f8) іiءآiכճ Ԏoו\\u05fblԶقe\\u05cdځ Ӟt ؟aכԠoו \\u0601MπίR\\u0602\\nMCAR is a situation in which the data is genuinely missing at random and has no\\nrelation to any observed or unobserved variables. In other words, the missing\\ndata points follow no discernable pattern.\\nFor instance, in survey responses with missing values, assuming MCAR would\\nmean that some participants may have unintentionally skipped to answer any\\nrandom question.\\nHowever, in my experience, MCAR has mostly been an unrealistic assumption for\\nmissingness, i.e., data is usually not MCAR for most real-world datasets with\\nmissing values. This is because, in real-world scenarios, the occurrence of\\nmissing data oϔen tends to be inﬂuenced by some factor, either observed or\\nunobserved.\\nHuman behavior, survey administration, or any external events can be motivating\\nfactors for missing values to appear in a dataset. For instance, participants may\\nselectively omit sensitive information, or certain groups may be more prone to\\nnon-response, causing missingness.\\nThat is why assuming MCAR for missing data is not a sensible assumption unless\\nyou know the end-to-end data collection process and/or have domain expertise in.\\nHere, it becomes essential for data scientists to talk to data engineers and\\nunderstand the data collection process. In fact, this is not just about MCAR but\\napplicable to the other two situations as well, which we shall discuss shortly.\\n392'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 393}, page_content='DailyDoseofDS.com\\nUnderstanding the data collection process will NEVER hurt.\\nThat said, nothing stops us from assuming that missing values are MCAR if that\\nappears to be a fair thing to do based on the analysis and/or input from the\\ndomain experts and/or aϔer understanding the data collection mechanism. You\\ncan proceed with the simplest univariate imputation techniques.\\nٗ) іiءآiכճ Ӟt ؟aכԠoו \\u0601MήѺ)\\nMAR is a situation in which the missingness of one feature can be explained by\\nother observed features in the dataset.\\nIn contrast to MCAR, MAR is more practically observed.\\nIn this case, the missingness can be accounted for through appropriate statistical\\nmethods with reasonable accuracy.\\nThus, even though the data is missing, its occurrence can still be (somewhat)\\nestimated based on the information available in the dataset.\\nA common way to determine MAR is by conditioning on another observed\\nfeatures and noticing any increase in the probability of missingness.\\n393'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 394}, page_content='DailyDoseofDS.com\\nFor instance, in an academic performance survey, students with higher grades\\nmight be less likely to disclose information about the number of hours they study\\n(due to competition, maybe).\\nThis is where techniques like kNN imputation, Miss Forest, etc., are quite\\neﬀective. They use other observed features to impute the missing feature.\\nIf you want to learn about these techniques, the chapter is precisely about this\\ntopic.\\nي) іiءآiכճ לoف Ӟt ؟aכԠoו \\u0601MјίR\\u0602\\nMNAR is the most complicated situation of all three. In MNAR, missingness is\\neither attributed to the missing value itself or the feature(s) that we didn’t collect\\ndata for.\\nIn other words, within MNAR, there is a deﬁnite pattern in missing variables.\\nHowever, the pattern is unrelated to any observed feature(s). This is diﬀerent\\nfrom MCAR where there is no deﬁnite pattern.\\n394'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 395}, page_content='DailyDoseofDS.com\\nFor instance, in a health survey, participants with very high stress levels might\\nconsciously choose not to disclose their stress level due to stigma, or fear of\\njudgment. As a result, the missing data on stress level is not random; it is\\ninﬂuenced by the stress level itself.\\nSo, in a way, the higher the stress level, the less likely one will disclose it, and the\\nmore likely the value will be missing from the collected dataset. Thus, the\\nmissingness is directly dependent on the very variable that is missing in the ﬁrst\\nplace. That’s tricky, isn’t it?\\nThere’s not much we can do to address this, except for collecting more\\ndata/features. What’s more, domain expertise become extremely important to\\nsmartly tackle MNAR and improve the data collection process.\\nAt times, I have also preferred proceeding with typical imputation techniques\\n(used in MCAR and MAR) because further data collection can be infeasible in\\nmost cases. But as discussed above, there is a deﬁnite missingness pattern in\\nMNAR, which can be important. But direct imputation will discard that\\ninformation.\\nOne way to retain that missingness pattern is by adding a binary feature,\\nindicating whether the feature was imputed.\\nThis way, the ML algorithm can still access and learn the missing data patterns.\\n395'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 396}, page_content='DailyDoseofDS.com\\nіiءآFת؟eءق Ӟnԟ ׂNј Оm\\u05faٜtӝقiתל\\nThe imputation strategy for missing data largely depends on the type of\\nmissingness, which can be of three types:\\n●Missing completely at random (MCAR): Data is genuinely missing at\\nrandom and has no relation to any observed or unobserved variables.\\n●Missing at random (MAR): The missingness of one feature can be\\nexplained by other observed features in the dataset.\\n●Missing not at random (MNAR): Missingness is either attributed to the\\nmissing value itself or the feature(s) that we didn’t collect data for.\\nI have observed missing at random (MAR) appearing relatively much more in\\npractical situations than the other two, so in this chapter, I want to share two\\nimputation strategies I typically prefer to use in such a case.\\n396'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 397}, page_content='DailyDoseofDS.com\\nש1\\u0602 ׂNј ֖m\\u05faٜtӝقiתל\\nAs the name suggests, it imputes missing values using the k-nearest neighbors\\nalgorithm.\\nMore speciﬁcally, missing features are imputed by running a kNN on\\nnon-missing feature values.\\nThe following image depicts how it works:\\n●Step 1: Select a row (r) with a missing value.\\n●Step 2: Find its k nearest neighbors using the non-missing feature values.\\n●Step 3: Impute the missing feature of the row (r) using the corresponding\\nnon-missing values of k nearest neighbor rows.\\n●Step 4: Repeat for all rows with missing values.\\nIts eﬀectiveness over mean/zero imputation is evident from the demo below.\\nOn the leϔ, we have a feature distribution with missing values. Assuming we\\nhave already validated that the data was missing at random (MAR), using\\nmean/zero alters the summary statistics and distribution, as depicted below.\\n397'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 398}, page_content='DailyDoseofDS.com\\nHowever, as depicted below, kNN imputer appears to be more reliable, and it\\npreserves the summary statistics:\\nש2\\u0602 іiءآFת؟eءق\\nSome major issues with kNN imputation are:\\n1. It has a high run-time for imputation — especially for high-dimensional\\ndatasets.\\n2. It raises issues with distance calculation in the case of categorical\\nnon-missing features.\\n3. It requires feature scaling, etc.\\n398'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 399}, page_content='DailyDoseofDS.com\\nMissForest imputer is another reliable choice for missing value imputation when\\ndata is missing at random (MAR). As the name suggests, it imputes missing\\nvalues using the Random Forest algorithm.\\n1. To begin, impute the missing feature with a random guess — Mean,\\nMedian, etc.\\n2. Model the missing feature using Random Forest.\\n3. Impute ONLY originally missing values using Random Forest’s prediction.\\n4. Back to Step 2. Use the imputed dataset from Step 3 to train the next\\nRandom Forest model.\\n5. Repeat until convergence (or max iterations).\\nIn case of multiple missing features, the idea (somewhat) stays the same:\\n399'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 400}, page_content='DailyDoseofDS.com\\nImpute features sequentially in increasing order missingness — features with\\nfewer missing values are imputed ﬁrst.\\nLike before, on the leϔ, we have a feature distribution with missing values.\\nAgain, assuming we have already validated that the data was missing at random\\n(MAR), using mean/zero alters the summary statistics and distribution:\\nHowever, as depicted below, MissForest appears more reliable and preserves the\\nsummary statistics.\\nkNN imputation and MissForest Jupyter notebook: https://bit.ly/3RW3i8k.\\n400'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 401}, page_content='DailyDoseofDS.com\\nѮiفէa\\u05cd\\u05ces Ӟnԟ іiءԎoכԏԷpف֖oכآ\\nҷhԶל ֖s ѺaכԠoו ѽp\\u05cd֖tف֗לg ϾaفӞl էo؞ іL іoԟԷlءؑ\\nOne thing we hear in almost all introductory ML lessons is to split the given data\\nRANDOMLY into train and validation sets.\\nљoفԷ: Оgכ\\u05ebrԶ قhԶ قeءك آeف էo؞ לoٷ؊\\nRandom splitting makes sense because it ensures that the data is divided without\\nany bias.\\nHowever, I have come across many situations where random splitting is fatal for\\nmodel building. Yet, many people don’t realize it.\\nAnd I am not talking about temporal datasets here.\\nѽcԶלa؞֖o\\nConsider you are building a model that generates captions for images.\\n401'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 402}, page_content='DailyDoseofDS.com\\nDue to the inherent nature of language, every image can have many diﬀerent\\ncaptions:\\nNow, you might realize what would happen if we randomly split this dataset into\\ntrain and validation sets.\\nDuring the random split, the same data point (image) will be available in the train\\nand validation sets.\\nThis is a typical example of data leakage, which results in high overﬁtting!\\nThis type of leakage is also known as group leakage.\\nѽo\\u05cdٜt֕\\u05ebn\\nFrom the above discussion, it is clear that random splitting is the cause of the\\nproblem.\\nGroup shuﬄe split helps us solve this.\\n402'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 403}, page_content='DailyDoseofDS.com\\nThere are two steps:\\n●Group all training instances corresponding to one image (or features that\\nmay result in leakage, any other grouping criteria, etc.).\\n●Aϔer grouping, the whole group must be randomly sent to either the\\ntraining set or the validation set.\\nThis will prevent the group leakage we witnessed earlier and prevent overﬁtting.\\nOne thing to note here is that in the above example, all features in the dataset,\\ni.e., the image pixels, contributed to the grouping criteria.\\nBut more generally speaking, there could only be a subset of features that must be\\ngrouped together for data splitting.\\nFor instance, consider a dataset containing medical imaging data. Each sample\\nconsists of multiple images (e.g., diﬀerent views of the same patient’s body part),\\nand the model is intended to detect the severity of a disease.\\nIn this case, it is crucial to group all images corresponding to the same patient\\ntogether and then perform data splitting. Otherwise, it will result in data leakage\\nand the model will not generalize well to new patients.\\n403'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 404}, page_content='DailyDoseofDS.com\\nόeו\\u05eb\\nIf you use Sklearn, the ЄrתٜpѼ\\u058buﬄԷS\\u05fa\\u05ceiف implements this idea.\\nConsider we have the following dataset:\\n●ٻ1 and ٻ2 are the features.\\n●ځ is the target variable.\\n●group denotes the grouping criteria.\\nFirst, we import the ЄrתٜpѼ\\u058buﬄԷS\\u05fa\\u05ceiف from sklearn and instantiate the object:\\n404'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 405}, page_content='DailyDoseofDS.com\\nThe آp\\u05cd֖t\\u0600\\u0603 method of this object lets us perform group splitting:\\nThis returns a generator, and we can unpack it to get the following output:\\nAs demonstrated above:\\n●The data points in groups “ί” and “ρ” are together in the training set.\\nThe data points in group “λ” are together in the validation/test set.\\n405'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 406}, page_content='DailyDoseofDS.com\\nϾeӝقu؞Է ѽcӝ\\u05ceiכճ ֖s љOҎ ίlٷӞyء љeԍԷsءӞrڀ\\nFeature scaling is commonly used to\\nimprove the performance and stability of\\nML models. This is because it scales the\\ndata to a standard range. This prevents a\\nspeciﬁc feature from having a strong\\ninﬂuence on the model’s output.\\nFor instance, in the image above, the scale of income could massively impact the\\noverall prediction. Scaling both features to the same range can mitigate this and\\nimprove the model’s performance. I am sure you already know this, so we won’t\\nget into more detail here. However, have you ever wondered the following:\\nОs էeӝقu؞Է آcӝ\\u05ceiכճ Ӟlٷӟځs לeԍԷsءӞrڀ ٸhԶל \\u05ebu؞ ԠaفӞsԶق’ء էeӝقu؞Էs \\u058baٱԷ Ӟ\\nԠiٱԷrءԸ ؟aכճeؐ\\nWhile feature scaling is oϔen crucial, we oϔen overlook whether it is even\\nneeded or not. This is because many ML algorithms are unaﬀected by scale.\\nThis is evident from the image,\\nwhich depicts the test accuracy\\nof some classiﬁcation algorithms\\nwith and without feature scaling.\\nLogistic regression (trained using\\nSGD), SVM Classiﬁer, MLP, and\\nkNN do better with feature\\nscaling.\\nDecision trees, Random forests,\\nNaive bayes, and Gradient\\nboosting are unaﬀected by scale.\\n406'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 407}, page_content='DailyDoseofDS.com\\nTo understand better, consider a decision tree. It splits the data based on\\nthresholds determined solely by the feature values, regardless of their scale.\\nAs a result, its performance is unaﬀected by the scale. This makes intuitive sense\\nas well.\\nThus, the takeaway is that when we do feature scaling, it’s important to\\nunderstand not just the nature of our data but also the algorithm we intend to\\nuse. We may never need feature scaling if the algorithm is insensitive to the scale\\nof the data.\\n407'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 408}, page_content='DailyDoseofDS.com\\nί іiءԎoכԏԷpف֖oכ ίbתٜt ѐoղ ҏrӝלsզ\\u05ebrו\\nLog transform is commonly used to eliminate skewness in\\ndata. Yet, it is not always the ideal solution for eliminating\\nskewness. It is important to note that log transform:\\n●DOES NOT eliminate leϔ-skewness.\\n●Only works for right-skewness, that too when the\\nvalues are small and positive.\\nTo understand better, consider the leϔ-skewed distribution and its log transform\\nbelow:\\nIt is clear that log transform did not eliminate skewness.\\nHowever, now consider a right-skewed distribution:\\nThis time, it eliminates the skewness.\\n408'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 409}, page_content=\"DailyDoseofDS.com\\nThis happens because the log function grows faster for lower values. Thus, it\\nstretches out the lower values more than the higher values.\\nMore speciﬁcally, in the case of leϔ-skewness, the tail exists to the leϔ, which\\ngets stretched more than the majority of the probability mass that exists to the\\nright. Thus, skewness isn't aﬀected much.\\nHowever, in the case of right-skewness, the majority of probability mass exists to\\nthe leϔ, which gets stretched out more.\\n409\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 410}, page_content='DailyDoseofDS.com\\nThat said, it is important to note that\\neven if we have a right-skewed\\ndistribution, log transform will not be\\neﬀective if the values are large.\\nThis happens because the stretch eﬀect\\nof the log function diminishes at large\\nvalues:\\nI have oϔen found the box-cox transform to be quite eﬀective at eliminating both\\nright-skewness and leϔ-skewness, as depicted below:\\n410'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 411}, page_content='DailyDoseofDS.com\\nҏhԶ\\nҏrٛԷ\\nѮu؞\\u05fboءԷ\\n\\u05ebf\\nϾeӝقu؞Է\\nѽcӝ\\u05ceiכճ\\nӞnԟ\\nѽtӝלdӝ؟d֕ړaف֖oכ\\nFeature scaling and standardization are two common ways to alter a feature’s\\nrange. For instance:\\n●MinMaxScaler changes the range of a feature to [0,1]:\\n●Standardization makes a feature’s mean zero and standard deviation one:\\nAs you may already know, these operations are necessary because:\\n●They prevent a speciﬁc feature from strongly inﬂuencing the model’s\\noutput.\\n●They ensure that the model is more robust to wide variations in the data.\\nFor instance, in the image below, the scale of “income” could massively impact\\nthe overall prediction.\\n411'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 412}, page_content='DailyDoseofDS.com\\nScaling (or standardizing) the data can mitigate this and improve the model’s\\nperformance. We also learned this in the feature scaling chapter:\\nAs depicted above, feature scaling is necessary for the better performance of\\nmany ML models. So while the importance of feature scaling and standardization\\nis pretty clear and well-known, I have seen many people misinterpreting them as\\ntechniques to eliminate skewness.\\nBut contrary to this common belief, feature scaling and standardization NEVER\\nchange the underlying distribution. Instead, they just alter the range of values.\\nThus, aϔer scaling (or standardization):\\n●Normal distribution →stays Normal\\n●Uniform distribution →stays Uniform\\n●Skewed distribution →stays Skewed\\n●and so on…\\nWe can also verify this from the two illustrations below:\\n412'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 413}, page_content='DailyDoseofDS.com\\nIt is clear that scaling and standardization have no eﬀect on the underlying\\ndistribution. Thus, always remember that if you intend to eliminate skewness,\\nscaling/standardization will never help. Try feature transformations instead.\\nThere are many of them, but the most commonly used transformations are:\\n●Log transform\\n●Sqrt transform\\n●Box-cox transform\\nTheir eﬀectiveness is evident from the image below:\\nAs depicted in this image,\\napplying these operations\\ntransforms the skewed data\\ninto a (somewhat) normally\\ndistributed variable.\\nBefore I conclude, please\\nnote that while log\\ntransform is commonly\\nused to eliminate data\\nskewness, it is not always\\nthe ideal solution.\\n413'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 414}, page_content='DailyDoseofDS.com\\nѐ2 Ѻeղٜlӝ؟iڒӞt֕\\u05ebn ֖s љoف оuءق ҝsԶԠ էo؞\\nѺeղٜlӝ؟iڒӞt֕\\u05ebn\\nAlmost every tutorial/course/blog mentioning L2 regularization I have seen talks\\nabout just one thing:\\nѐ2 ؟eղٜlӝ؟iڒӞt֕\\u05ebn ֖s Ӟ זaԍ\\u058biכԷ \\u05ceeӝ؟n֕לg قeԍ\\u058bn֕؎uԶ قhӝك Ӟvת֖dء \\u05ebvԶ؟ﬁقt֕לg\\nӹy ֖nف؟oԟٜc֕לg Ӟ \\u05fbeכӞlفځ قe؞ז ֖nف\\u05eb قhԶ זoԟԷlؚآ \\u05ceoءآ էuכԎt֕\\u05ebn ӹaءԷd \\u05ebn قhԶ\\nآqٛӞrԶآ \\u05ebf قhԶ זoԟԷlؚآ \\u05fba؞ӞmԶقe؞آ.\\nОn Ԏlӝآs֕ﬁԎaف֖oכ قaءׂsԝ էo؞ ֖nءقaכԎeԝ ֖nԍ؟eӝآiכճ قhԶ ԷﬀԷcف \\u05ebf ؟eղٜlӝ؟iڒӞt֕\\u05ebn\\nٸi\\u05cd\\u05ce \\u05fbrתԠuԍԷ آiו\\u05fblԶ؟ Ԡeԍ֖s֕\\u05ebn ӹoٛלdӝ؟iԶآ.\\nOf course, the above statements are indeed correct, and I am not denying that. In\\nfact, we can also verify this from the diagram below:\\nIn the image above, as we move to the right, the regularization parameter\\nincreases, and the model creates a simpler decision boundary on all 5 datasets.\\n414'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 415}, page_content='DailyDoseofDS.com\\nρoו֖nղ ӹaԍׂ قo قhԶ قo\\u05fa֖c…\\nHowever, what disappoints me the most is that most resources don’t point out\\nthat L2 regularization is a great remedy for multicollinearity.\\nMulticollinearity arises when two (or more) features are highly correlated OR two\\n(or more) features can predict another feature:\\nWhen we use L2 regularization in linear regression, the algorithm is also called\\nRidge regression.\\nBut how does L2 regularization eliminate multicollinearity?\\nIn this chapter, let’s build a demonstrative intuition into this topic, which will\\nalso explain why “ridge regression” is called “ridge regression.”\\n415'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 416}, page_content='DailyDoseofDS.com\\nόuוזy ԠaفӞsԶق\\nFor demonstration purposes, consider this dummy dataset of two features:\\nAs shown above, we have intentionally made featureB highly correlated with\\nfeatureA. This gives us a dummy dataset to work with.\\nЄo֕לg ӞhԶӟԠ, ٸe آhӝ\\u05cel ӹe ֖gכ\\u05ebr֕לg Ӟnڀ ֖nفԷrԍԸ\\u05fbt قe؞ז էo؞ آiו\\u05fbl֕Ԏiفځ.\\nѐiכԷa؞ ؟eղؠԷsء֖oכ ٸiف\\u058boٛق ѐ2 \\u05fbeכӞlفځ\\nDuring regression modeling, the goal is to determine those speciﬁc parameters\\n(θ₁, θ₂), which minimizes the residual sum of squares (RSS):\\nSo how about we do the following:\\n●We shall plot the RSS value for many diﬀerent combinations of (θ₁, θ₂)\\nparameters. This will create a 3D plot:\\n○x-axis →θ₁\\n○y-axis →θ₂\\n○z-axis →RSS value\\n●Next, we shall visually assess this plot to locate those speciﬁc parameters\\n(θ₁, θ₂) that minimize the RSS value.\\n416'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 417}, page_content='DailyDoseofDS.com\\nLet’s do this.\\nWithout the L2 penalty, we get the following plot (it’s the same plot but viewed\\nfrom diﬀerent angles):\\nDid you notice something?\\nThe 3D plot has a valley. There are multiple combinations of parameter values (θ₁,\\nθ₂) for which RSS is minimum.\\nThus, obtaining a single value for the parameters (θ₁, θ₂) that minimize the RSS is\\nimpossible.\\nѐiכԷa؞ ؟eղؠԷsء֖oכ ٸiف\\u058b ѐ2 \\u05fbeכӞlفځ\\nWhen using an L2 penalty, the goal is to minimize the following:\\nCreating the same plot again as we did above, we get the following:\\n417'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 418}, page_content='DailyDoseofDS.com\\nDid you notice something diﬀerent this time?\\nAs depicted above, using L2 regularization removes the valley we saw earlier and\\nprovides a global minima to the RSS error.\\nNow, obtaining a single value for the parameters (θ₁, θ₂) that minimizes the RSS is\\npossible.\\nOut of nowhere, L2 regularization helped us eliminate multicollinearity.\\nҷhڀ قhԶ לaוԷ ؕr֕ԠgԶ ؟eղؠԷsء֖oכؗ?\\nIn fact, this is where “ridge regression” also gets its name from — it eliminates\\nthe ridge in the likelihood function when the L2 penalty is used.\\n418'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 419}, page_content='DailyDoseofDS.com\\nOf course, in the demonstrations we discussed earlier, we noticed a valley, not a\\nridge.\\nHowever, in that case, we were considering the residual sum of error —\\nsomething which is minimized to obtain the optimal parameters.\\nThus, the error function will obviously result in a valley.\\nIf we were to use likelihood instead — something which is maximized, it would\\n(somewhat) invert the graph upside down and result in a ridge instead:\\nApparently, while naming the algorithm, the likelihood function was considered.\\nAnd that is why it was named “ridge regression.”\\nWhen I ﬁrst learned about this some years back, I literally had no idea that such\\ndeep thought went into naming ridge regression.\\n419'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 420}, page_content='DailyDoseofDS.com\\nіiءԎe\\u05cd\\u05ceaכԷoٛآ\\nОs Ӏoٛ؟ іoԟԷl όaفӞ όeﬁԎiԶלtؐ\\nDuring model development, many people ﬁnd themselves in situations where, no\\nmatter how much they try, the model performance barely improves:\\n●Feature engineering is giving marginal improvement.\\n●Trying diﬀerent models does not produce satisfactory results either.\\n●and more…\\nThis is usually (not always) an indicator that the model is data deﬁcient.\\n420'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 421}, page_content='DailyDoseofDS.com\\nIn other words, we don’t have enough data to work with.\\nHowever, gathering new data can be a time-consuming and tedious process. So\\nbefore venturing into that direction, it would be good to get some insights about\\nwhether new data will help. Here’s a trick I have oϔen used to determine this.\\nόaفӞ آuӸأԷtف֖nղ Ӟnԟ іoԟԷl ӹu֕\\u05ced֕לg\\nLet’s say this is your full training and validation set:\\nDivide the training dataset into “k” equal parts. The validation set remains as is.\\n“k” does not have to be super large. Any number between 7 to 12 is ﬁne\\ndepending on how much data you have. If there’s plenty of data, setting a low\\nvalue in this range is recommended (you will understand why shortly).\\n421'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 422}, page_content='DailyDoseofDS.com\\nNext, train models cumulatively on the above subsets and measure their\\nperformance on the validation set:\\n●Train a model on the ﬁrst subset and evaluate the validation set.\\n●Train a model on the ﬁrst two subsets and evaluate the validation set.\\n●Train a model on the ﬁrst three subsets and evaluate the validation set.\\n●And so on…\\nPlotting the validation performance of these models (in order of increasing\\ntraining data) is likely to produce two types of lines:\\n422'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 423}, page_content='DailyDoseofDS.com\\n●Line A conveys that adding more data is likely to increase model\\nperformance.\\n●Line B conveys that the model performance has already saturated. Adding\\nmore data will most likely not result in any considerable gains.\\nNow, you might also understand why I mentioned this above: “If there’s plenty of\\ndata, setting a low value in this range is recommended.”\\n●Because we train multiple models, setting a high value of “k” means more\\nsubsets, which in turn means more models.\\nThis way, you can determine whether the model is data deﬁcient and whether\\ngathering data will be helpful or not.\\n423'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 424}, page_content='DailyDoseofDS.com\\nλaڀԷs֕Ӟn ѡpف֖m֕ړaف֖oכ\\nHyperparameter tuning is a tedious and time-consuming task in training ML\\nmodels.\\nTypically, we use two common approaches for this – Grid search and Random\\nsearch.\\nBut they have many limitations. For instance:\\n●Grid search performs an exhaustive search over all combinations. This is\\ncomputationally expensive.\\n●Grid search and random\\nsearch are restricted to the\\nspeciﬁed hyperparameter\\nrange. Yet, the ideal\\nhyperparameter may exist\\noutside that range.\\n●They can ONLY perform discrete searches, even if the hyperparameter is\\ncontinuous.\\nTo this end, Bayesian Optimization is a highly underappreciated yet immensely\\npowerful approach for tuning hyperparameters.\\n424'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 425}, page_content='DailyDoseofDS.com\\nIt uses Bayesian statistics to estimate the distribution of the best\\nhyperparameters. Here’s how it diﬀers from Grid search and Random Search:\\nBoth Grid search and Random Search evaluate every hyperparameter\\nconﬁguration independently. Thus, they iteratively explore all hyperparameter\\nconﬁgurations to ﬁnd the most optimal one.\\nHowever, Bayesian\\nOptimization takes informed\\nsteps based on the results of the\\nprevious hyperparameter\\nconﬁgurations. This lets it\\nconﬁdently discard non-optimal\\nconﬁgurations. Consequently,\\nthe model converges to an\\noptimal set of hyperparameters\\nmuch faster. The eﬃcacy of\\nBayesian Optimization is\\nevident from the image.\\nBayesian optimization leads the model to the same F1 score but:\\n●it takes 7x fewer iterations\\n●it executes 5x faster\\n●it reaches the optimal conﬁguration earlier\\nPretty cool, isn’t it?\\n425'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 426}, page_content='DailyDoseofDS.com\\nҏrӝ֖n Ӟnԟ ҏeءق-ف֖mԶ όaفӞ ίuղזeכقaف֖oכ\\nData augmentation strategies are typically used during training time.\\nThe idea is to use some clever techniques to create more data from existing data,\\nwhich is especially useful when you don’t have much data to begin with:\\nLet me give you an example.\\nThese days, language-related ML models have become quite advanced and\\ngeneral-purpose. The same model can translate, summarize, identify speech tags\\n(nouns, adjectives, etc.), and much more.\\nBut earlier, models used to be task-speciﬁc (we have them now as well, but they\\nare fewer than we used to have before).\\n●A dedicated model that would translate.\\n●A dedicated model that would summarize, etc.\\nIn one particular use case, I was building a named entity recognition (NER)\\nmodel, and the objective was to identify named entities.\\n426'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 427}, page_content='DailyDoseofDS.com\\nAn example is shown below:\\nI had minimal data — around 8-10k labeled sentences. The dataset was the\\nCoNLL 2003 NER dataset if you know it.\\nHere’s how I approached data augmentation in this case.\\nObƖeƯvƄƗƦoƫ: In NE¶, tƋe faƆƱƘƞl coƕƯƈcƗƫƢsƖ of tƋe seƑƱƈnƆƢư doƈƖ noƗ maƗƱƈr.\\nRevisiting the above example, it would not have mattered if I had the following\\nsentence in the training data:\\nThe sentence is factually incorrect, of course, but that does not matter.\\nThe only thing that matters to the model is that the output labels (named entity\\ntags in this case) must be correct.\\nSo using this observation, I created many more sentences by replacing the named\\nentities in an existing sentence with other named entities in the whole dataset:\\n427'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 428}, page_content='DailyDoseofDS.com\\nFor such substitutions, I could have used named entities from outside. However,\\nit was important to establish a fair comparison with other approaches.\\nThis technique (along with a couple more architectural tweaks) resulted in\\nstate-of-the-art performance.\\nMoving on…\\nThe above discussion was about training data augmentation.\\nBut there’s also test-time augmentation.\\nTest Time Augmentation (TTA) is when we apply data augmentation during\\ntesting.\\nMore speciﬁcally, instead of showing just one test example to the model, we\\nshow multiple versions of the test example by applying diﬀerent operations.\\nThe model makes probability predictions for every version of the test example,\\nwhich are then averaged to generate the ﬁnal prediction:\\n428'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 429}, page_content='DailyDoseofDS.com\\nEssentially, TTA creates an ensemble of predictions by considering multiple\\naugmented versions of the same input, which leads to a more robust ﬁnal\\nprediction.\\nIn fact, in this paper (https://arxiv.org/html/2402.06892v1), the authors proved that\\nthe average model error with TTA never exceeds the average error of the original\\nmodel, which is great.\\nAs you may have guessed, the only catch is that it increases the inference time.\\n●Data augmentation takes some time.\\n●Generating multiple predictions increases the overall prediction run-time.\\nSo, when a low inference time is important to you, think twice about TTA.\\nTo summarize, if you can compromise a bit on inference time, TTA can be a\\npowerful way to improve predictions from an existing model without having to\\nengineer a better model.\\n429'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 430}, page_content='DailyDoseofDS.com\\nόaفӞ ίnӝ\\u05ceyء֖s\\n\\u05f85 ѮaכԠaء ↔Ѯo\\u05cdӞrء ↔ѽQя ↔ѮyѼ\\u05fba؞ׂ ҏrӝלs\\u05cdӞt֕\\u05ebnء\\nThe following visual depicts the 15 most common tabular operations in Pandas\\nand their corresponding translations in SQL, Polars, and PySpark.\\nWhile the motivation for Pandas and SQL is clear and well-known, let me tell you\\nwhy you should care about Polars and PySpark.\\n430'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 431}, page_content='DailyDoseofDS.com\\nҷhڀ Ѯo\\u05cdӞrءؑ\\nPandas has many limitations, which Polars addresses, such as:\\n●Pandas always adheres to single-core computation →Polars is multi-core.\\n●Pandas oﬀers no lazy execution →Polars does.\\n●Pandas creates bulky DataFrames →Polars’ DFs are lightweight.\\n●Pandas is slow on large datasets →Polars is remarkably eﬃcient.\\nIn fact, if we\\nlook at the\\nrun-time\\ncomparison on\\nsome common\\noperations, it’s\\nclear that Polars\\nis much more\\neﬃcient than\\nPandas.\\n431'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 432}, page_content='DailyDoseofDS.com\\nҷhڀ ѽpӝ؟kؐ\\nWhile tabular data space is mainly dominated by Pandas and Sklearn, one can\\nhardly expect any beneﬁt from them beyond some GBs of data due to their\\nsingle-node processing.\\nA more practical solution is to use distributed computing instead — a framework\\nthat disperses the data across many small computers.\\nSpark is among the best technologies used to quickly and eﬃciently analyze,\\nprocess, and train models on big datasets.\\nThat is why most data science roles at big tech demand proﬁciency in Spark. It’s\\nthat important\\n432'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 433}, page_content='DailyDoseofDS.com\\nٗ ίlفԷrכӞt֕ٲeء قo ѮaכԠaء؛ όeءԎr֕ӹe\\nProbably the ﬁrst (or second) thing I do when I load any Pandas or Polars\\nDataFrame is describe it, using the df.describe() method. However, I always ﬁnd\\nits output to be pretty naive and almost of no use. In other words, it hardly\\nhighlights any key information about the data.\\nBut some time back, I came across two pretty cool libraries that IMMENSELY\\nsupercharge this DataFrame summary. Since then, I don’t think I have ever used\\nthe describe() method.\\nש1\\u0602 ѽk֕זpڀ\\nIt is a Jupyter-based\\ntool that provides a\\nstandardized and\\ncomprehensive data\\nsummary.\\nThis includes data\\nshape, column data\\ntypes, column\\nsummary statistics,\\ndistribution charts,\\nmissing stats, etc.:\\n433'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 434}, page_content='DailyDoseofDS.com\\nWhat’s more, the summary is grouped by datatypes for faster analysis. This is the\\ncode to use Skimpy:\\nOne thing I really love about Skimpy is that it works seamlessly with Polars,\\nwhich I have started using more oϔen than I use Pandas these days.\\nש2\\u0602 ѽuוזa؞ځTת\\u05eblء\\nThe second one is\\nSummaryTools, which\\ndoes almost the exact\\nsame thing as Skimpy,\\ni.e., it generates a\\nstandardized report:\\n434'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 435}, page_content='DailyDoseofDS.com\\nThis is the code to use SummaryTools:\\nTwo pretty cool things about SummaryTools are that it can create:\\n1. A collapsible summary of the dataset, as illustrated below:\\n2. A tabbed summary of the dataset, as shown below:\\nThe only thing I don’t like about SummaryTools is that it is not compatible with\\nPolars (yet). Nonetheless, I ﬁnd both of them extremely promising for\\nunderstanding my dataset with more granularity than Pandas’ ԠeءԎr֕ӹe\\u0600\\u0603\\nmethod.\\nTry by downloading this Jupyter notebook: https://bit.ly/45Rheq1.\\n435'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 436}, page_content='DailyDoseofDS.com\\nίcԍԷlԶ؟aفԷ ѮaכԠaء ٸiف\\u058b ЄPҜ ҝs֕לg ѺAѭОDѼ ԎuϋϾ\\nTwo of the biggest problems with Pandas is that:\\n●It always adheres to a single-core computation on a CPU.\\n●It creates bulky DataFrames.\\nWhile many libraries (Polars, for instance) do address these limitations, they are\\nstill limited to CPU-driven computations.\\nNVIDIA’s RAPIDS cuDF library allows Pandas users to supercharge their Pandas\\nworkﬂow with GPUs.\\nHow to use it?\\nWithin a GPU runtime, do the following :\\n●Load the extension: ؈lתӞd٦Էxف Ԏuԟէ.\\u05faӞnԟӟآ\\n●Import Pandas: ֖m\\u05fa\\u05ebrف \\u05fbaכԠaء Ӟs \\u05fbd\\nDone! Use Pandas’ methods as you usually would.\\n436'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 437}, page_content='DailyDoseofDS.com\\nJust loading the\\nextension provides\\nimmense speedups.\\nAs per NVIDIA’s oﬃcial release, this can be as fast as 150x. In my personal\\nexperimentation, however, I mostly observed it to range between 50-70x, which is\\nstill pretty good.\\nThe good thing is that the extension accelerates most Pandas’ methods. Yet, if\\nneeded, it can automatically fall back to the CPU.\\nЕoٷ ԠoԶآ ֖t ٸo؞ׂ?\\nWhenever Ԏuԟէ.\\u05faӞnԟӟآ is enabled, the import pandas as pd statement does not\\nimport the original Pandas library which we use all the time. Instead, it imports\\nanother library that contains GPU-accelerated implementations of all Pandas\\nmethods.\\nThis alternative implementation preserves the entire syntax of Pandas. So if you\\nknow Pandas, you already know how to use cuDF’s Pandas.\\nYou can ﬁnd the code here: https://bit.ly/4cOAPZW.\\n437'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 438}, page_content='DailyDoseofDS.com\\nіiءآiכճ όaفӞ ίnӝ\\u05ceyء֖s ٸiف\\u058b Еeӝقmӝ\\u05fbs\\nReal-world datasets\\nalmost always have\\nmissing values. In most\\ncases, it is unknown to us\\nbeforehand why values\\nare missing.\\nBut it’s good to know that there could be multiple reasons for missing values:\\n●Missing Completely at Random (MCAR): The value is genuinely missing by\\nitself and has no relation to that or any other observation.\\n●Missing at Random (MAR): Data is missing due to another observed\\nvariable. For instance, we may observe that the percentage of missing\\nvalues diﬀers signiﬁcantly based on other variables.\\n●Missing NOT at Random (MNAR): This one is tricky. MNAR occurs when\\nthere is a deﬁnite pattern in the missing variable. However, it is unrelated\\nto any feature we can observe in our data. In fact, this may depend on an\\nunobserved feature.\\nAnd identifying the reason for missingness can be extremely useful for further\\nanalysis, imputation, and modeling.\\nConsider we have a daily sales dataset of a store that has the following\\ninformation:\\n438'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 439}, page_content='DailyDoseofDS.com\\n●Day and Date\\n●Store opening and closing time\\n●Number of customers\\n●Total sales\\n●Account balance at open and close time\\nThe reason for missing values is unknown to us. Here, when doing EDA, many\\nfolks compute the column-wise missing frequency as follows:\\nThe above table just highlights the number of missing values in each column.\\nMore speciﬁcally, we get to know that:\\n●Missing values are relatively high in two columns compared to others.\\n●Missing values in the opening and closing time columns are the same (53).\\nThat’s the only info it provides. However, the problem with this approach is that\\nit hides many important details about missing values, such as:\\n●Their speciﬁc location in the dataset.\\n●Periodicity of missing values (if any).\\n●Missing value correlation across columns, etc.\\n…which can be extremely useful to understand the reason for missingness.\\nTo put it another way, the above table is more like summary statistics, which\\nrarely depict the true picture.\\nWhy?\\n439'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 440}, page_content='DailyDoseofDS.com\\nWe discussed this in the statistics section of this book.\\nBut here’s how I oϔen enrich my missing value analysis with heatmaps. Compare\\nthe missing value table we discussed above with the following heatmap of\\nmissing values:\\n440'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 441}, page_content='DailyDoseofDS.com\\nThe white vertical lines depict the location of missing values in a speciﬁc\\ncolumn.\\nNow, it is immediately clear that:\\n●Values are periodically missing in the opening and closing time columns.\\n●Missing values are correlated in the opening and closing time columns.\\n●The missing values in other columns appear to be (not necessarily though)\\nmissing completely at random.\\nFurther analysis of the opening time lets us discover that the store always\\nremains closed on Sundays:\\nNow, we know why the opening and closing times are missing in our dataset.\\nThis information can be beneﬁcial during its imputation.\\nThis speciﬁc situation is “Missing at Random (MAR).” Essentially, as we saw\\nabove, the missingness is driven by the value of another observed column.\\nAs we know the reason, we can use either the kNN imputation or the MissForest\\ntechniques to impute these values, which we discussed in the ML section of this\\nbook (іiءآFת؟eءق Ӟnԟ ׂNј Оm\\u05faٜtӝقiתל).\\n441'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 442}, page_content=\"DailyDoseofDS.com\\nόaفӞF؞ӞmԶ ѽtڀ\\u05ceiכճ\\nInstead of\\npreviewing raw\\nDataFrames, styling\\ncan make data\\nanalysis much easier\\nand faster.\\nHere's how.\\nJupyter is a\\nweb-based IDE.\\nAnything you print is\\nrendered using\\nHTML and CSS.\\nThis means you can\\nstyle your output in\\nmany diﬀerent ways.\\nTo style Pandas DataFrames, use its Styling API (𝗱𝗳.𝘀𝘁𝘆𝗹𝗲).\\nAs a result, the DataFrame is rendered with the speciﬁed styling.\\nPandas documentation page on Styling: https://bit.ly/3zDmxgE.\\n442\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 443}, page_content='DailyDoseofDS.com\\nՂ ίuف\\u05ebmӝقeԟ ϛDή ҏoת\\u05ces\\nBelow are 8 powerful EDA tools that automate many redundant EDA steps and\\nhelp you proﬁle your data quickly.\\nPlease note that these tools are not the ultimate EDA alternatives that will\\nanswer all your questions about the dataset.\\nBut given that the preliminary EDA steps in almost all projects are the same —\\nplotting the response variable, checking imbalance, running correlation analysis,\\nmissing value analysis, and more, these tools pretty well automate these steps in\\nmy opinion.\\nAlso, at times, manual EDA can be prone to human errors and one may miss out\\non checking a few things. Automated tools eliminate these risks and provide a\\nstandardized report across all projects.\\nFull summary about each tool is available here: https://bit.ly/3xZP4fK.\\n443'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 444}, page_content='DailyDoseofDS.com\\nόaفӞ ұiءٜa\\u05cd֖sӝقiתל\\nіoءق Оm\\u05fa\\u05ebrفӞnف Ѯlתقs ֖n όaفӞ ѽc֕ԷnԍԸ\\nThe visual below depicts the 11 most important and must-know plots in data\\nscience:\\n444'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 445}, page_content='DailyDoseofDS.com\\nLet’s understand them brieﬂy and how they are used.\\nфS Ѯlתق:\\n●It is used to assess the distributional diﬀerences.\\n●The core idea is to measure the maximum distance between the cumulative\\ndistribution functions (CDF) of two distributions.\\n●The lower the maximum distance, the more likely they belong to the same\\ndistribution.\\n●Thus, instead of a “plot”, it is mainly interpreted as a “statistical test” to\\ndetermine distributional diﬀerences.\\nѽHήѮ Ѯlתق:\\n●It summarizes feature importance to a model’s predictions by considering\\ninteractions/dependencies between them.\\n445'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 446}, page_content='DailyDoseofDS.com\\n●It is useful in determining how diﬀerent values (low or high) of a feature\\naﬀect the overall output.\\nѺOπ ρu؞ٲeԛ\\n●It depicts the tradeoﬀbetween the true positive rate (good performance)\\nand the false positive rate (bad performance) across diﬀerent classiﬁcation\\nthresholds.\\n●The idea is to balance TPR (good performance) vs. FPR (bad performance).\\nѮrԶԎiء֖oכ֔RԶԎa\\u05cd\\u05ce ρu؞ٲeԛ\\n●It depicts the tradeoﬀbetween Precision and Recall across diﬀerent\\nclassiﬁcation thresholds.\\n446'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 447}, page_content='DailyDoseofDS.com\\nѷQ Ѯlתق:\\n●It assesses the distributional similarity between observed data and\\ntheoretical distribution.\\n●It plots the quantiles of the two distributions against each other.\\n●Deviations from the straight line indicate a departure from the assumed\\ndistribution.\\nρuוٜlӝقiٱԷ ϛx\\u05fa\\u05cea֕לeԟ ұa؞֖aכԎe Ѯlתق:\\n●It is useful in determining the number of dimensions we can reduce our\\ndata to while preserving max variance during PCA.\\n447'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 448}, page_content='DailyDoseofDS.com\\nϛlӸ\\u05ebw ρu؞ٲeԛ\\n●The plot helps identify the optimal number of clusters for the k-means\\nalgorithm.\\n●The point of the elbow depicts the ideal number of clusters.\\nѽi\\u05cd\\u058boٛԷtفԸ ρu؞ٲeԛ\\n●The Elbow curve is oϔen ineﬀective when you have plenty of clusters.\\n●Silhouette Curve is a better alternative, as depicted above.\\n448'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 449}, page_content='DailyDoseofDS.com\\nЄiכ֖-Нזpٛ؟iفځ Ӟnԟ ϛnف؟o\\u05faځ:\\n●They are used to measure the impurity or disorder of a node or split in a\\ndecision tree.\\n●The plot compares Gini impurity and Entropy across diﬀerent splits.\\n●This provides insights into the tradeoﬀbetween these measures.\\nλiӝآ-ҰӞr֕ӟלcԶ ҏrӝԠeתﬀԜ\\n●It’s probably the most popular plot on this list.\\n●It is used to ﬁnd the right balance between the bias and the variance of a\\nmodel against complexity.\\n449'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 450}, page_content='DailyDoseofDS.com\\nѮa؞قiӝ\\u05ce όe\\u05faԷnԟԸלcڀ Ѯlתقsԛ\\n●Depicts the dependence between target and features.\\n●A plot between the target and one feature forms →1-way PDP.\\n●A plot between the target and two feature forms →2-way PDP.\\n●In the leϔmost plot, an increase in temperature generally results in a\\nhigher target value.\\n450'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 451}, page_content='DailyDoseofDS.com\\nЕoٷ ӞrԶ ѷQ Ѯlתقs ρrԶӞtԶԠ?\\nA QQ plot is a great\\nway to visually assess\\nthe similarity between\\ntwo distributions.\\nIt does this by plotting the quantiles of the two distributions against each other.\\nThe deviations from the straight line indicate the diﬀerences between the two\\ndistributions. The following visual depicts how it is created:\\n451'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 452}, page_content='DailyDoseofDS.com\\nLet’s discuss it in more detail.\\nConsider we have two distributions, D1 and D2.\\nStep 1) Arrange points on axes:\\nAs shown below, we arrange points of D1 on the y-axis and D2 on the x-axis.\\nStep 2) Draw percentile lines\\nNext, for both distributions, we create some percentile lines.\\nFor instance, on both axes, we can mark the points of 10th percentile, 20th\\npercentile, 30th percentile, etc., from both distributions. This is shown below:\\nWe mark the percentile locations for both distributions and intersect the\\ncorresponding lines.\\n●10th percentile of D1 is intersected with 10th percentile of D2.\\n●20th percentile of D1 is intersected with 20th percentile of D2.\\n●and so on.\\n452'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 453}, page_content='DailyDoseofDS.com\\nThe intersection points of these percentile lines gives us the points we typically\\nsee in a QQ plot:\\nNow, we can get rid of the percentile marker lines.\\nIn a gist, the above plot gives us the location where the corresponding percentiles\\nof the two distributions match.\\nStep 3) Add the reference line\\nFinally, we must add a reference line to determine the deviations between the two\\ndistributions.\\nThere are many ways to do this.\\n453'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 454}, page_content='DailyDoseofDS.com\\nFor instance:\\n●The line connecting the 25th and 75th percentiles of both distributions can\\nbe considered as a reference line.\\n●The regression ﬁt on the above scatter plot can be considered as a\\nreference line.\\nTypically, the line connecting the 25-75th percentile is preferred because the\\nregression ﬁt can be inﬂuenced by outliers.\\nAϔer adding the reference line, we get our QQ plot:\\nThe deviations from this reference line indicate that the two distributions diﬀer\\nfrom each other. In other words, the deviations mean that the corresponding\\npercentiles do not align.\\nThis becomes an indicator of distributional dissimilarities.\\n454'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 455}, page_content='DailyDoseofDS.com\\nAnd, of course, the more percentiles we plot, the better and more accurate the\\nQQ plot will be.\\nThere are many applications of the QQ plot.\\nFor instance, say we have an observed distribution and want to determine if it\\nresembles a normal distribution.\\nWe can use a QQ plot for this:\\n●D1: The observed distribution\\n●D2: Normal distribution.\\nIf the percentile points lie closer to the reference line, this would mean that the\\nobserved distribution is more like a normal distribution. This is depicted below:\\n455'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 456}, page_content='DailyDoseofDS.com\\nՂ ϛlԶճaכق ίlفԷrכӞt֕ٲeء قo ҏrӝԠiف֖oכӞl Ѯlתقs\\nScatter plots, bar plots, line plots, box plots, and heatmaps are the most\\nfrequently used plots for data visualization. Although they are simple and known\\nto almost everyone, I believe they are not the right choice to cover every possible\\nscenario.\\nInstead, many other plots originate from these standard plots that can be much\\nmore suitable, if used appropriately. In this chapter, let’s discuss a few\\nalternatives to these popular plots. We will also understand speciﬁc situations\\nwhere they can be more useful over standard plots.\\nThis cƋaƭtƈƕ is noƗ inƗƈƫdƢƇ to diƖƠƒƲraƊƈ tƋe usƈ of tƋeưƈ tƕaơƌtƦoƑƄƩ pƏoƱs. TheƜ wiƏƩ\\nalƚƄƶs haƙƈ tƋeƌƯ pƏaƠƈ.\\nInƖƱeƄd, it is to hiƊƥlƌƊƥt sƓeƠƌﬁc siƗƘƞƱiƒnƖ wƋeƯƈ tƋeƶ caƑ reƓƩƄcƢƇ wiƗƥ beƗƱƈr pƏoƱtƌƑƤ\\nidƈƞƖ.\\nש1\\u0602 ѽiڒԷ-ԶלcתԠeԟ \\u058beӝقmӝ\\u05fbs\\nA traditional heatmap\\nrepresents the values\\nusing a color scale. Yet,\\nmapping the cell color\\nto exact numbers is\\nstill challenging.\\nEmbedding a size\\ncomponent to\\nheatmaps can be\\nextremely helpful in\\nsuch cases. In essence,\\nthe bigger the size, the\\nhigher the absolute\\nvalue\\n456'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 457}, page_content='DailyDoseofDS.com\\nThis is especially useful to make heatmaps cleaner, as many values nearer to zero\\nwill immediately shrink.\\nש2\\u0602 ҷaفԷrզӞl\\u05cd Ԏhӝ؟tء\\nTo visualize the change in value over time, a line (or bar) plot may not always be\\nan apt choice.\\nThis is because a line plot (or bar plot) depicts the actual values in the chart.\\nThus, it is diﬃcult to visually estimate the scale and direction of incremental\\nchanges.\\nInstead, you can use\\na waterfall chart. It\\nelegantly depicts\\nthese rolling\\ndiﬀerences, as\\ndepicted in this\\nimage.\\nHere, the start and\\nﬁnal values are\\nrepresented by the\\nﬁrst and last bars.\\nAlso, the consecutive changes are automatically color-coded, making them easier\\nto interpret.\\n457'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 458}, page_content='DailyDoseofDS.com\\nש3\\u0602 λuו\\u05fb Ԏhӝ؟tء\\nWhen visualizing the change in rank over time of multiple categories, using a bar\\nchart may not be appropriate.\\nThis is because bar charts quickly become cluttered with many categories.\\nInstead, try Bump Charts. They are speciﬁcally used to visualize the rank of\\ndiﬀerent items over time.\\nComparing the bar chart and bump chart above, it is far easier to interpret the\\nchange in rank with a bump chart rather than a bar chart.\\n458'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 459}, page_content='DailyDoseofDS.com\\nש4\\u0602 Ѻa֕לc\\u05cd\\u05ebuԟ Ѯlתقs\\nVisualizing data distributions using box plots and histograms can be misleading\\nat times. This is because:\\n●It is possible to get the same box plot with entirely diﬀerent data.\\n●Altering the number of bins changes the shape of a histogram.\\nThus, to avoid misleading conclusions, it is always recommended to plot the data\\ndistribution as precisely as possible. Raincloud plots provide a concise way to\\ncombine and visualize three diﬀerent types of plots together.\\nThese include:\\n●Box plots for data statistics.\\n●Strip plots for data overview.\\n●KDE plots for the probability distribution of data.\\n459'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 460}, page_content='DailyDoseofDS.com\\nWith Raincloud plots, you can:\\n●Combine multiple plots to prevent incorrect/misleading conclusions\\n●Reduce clutter and enhance clarity\\n●Improve comparisons between groups\\n●Capture diﬀerent aspects of the data through a single plot\\nש5֓ظ) Еeٺӹiכ Ӟnԟ όeכآiفځ Ѯlתقs\\nScatter plots can get too dense to interpret when you have thousands of data\\npoints.\\nInstead, you can replace them with Hexbin plots.\\nHexbin plots bin the area of a chart into hexagonal regions. Each region is\\nassigned a color intensity based on the method of aggregation used (the number\\nof points, for instance).\\n460'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 461}, page_content='DailyDoseofDS.com\\nAnother choice is a density plot, which illustrates the distribution of points in a\\ntwo-dimensional space.\\nA contour is created by connecting points of equal density. In other words, a\\nsingle contour line depicts an equal density of data points.\\nש7֓Ղ) λuӸӹlԶ Ԏhӝ؟tء Ӟnԟ όoف \\u05fblתقs\\nAs discussed above, bar plots quickly get messy and cluttered as the number of\\ncategories increases.\\nA bubble plot is oϔen a better alternative in such cases.\\nThey are like scatter plots but:\\n●with one categorical axis\\n●and one continuous axis\\n461'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 462}, page_content='DailyDoseofDS.com\\nAs depicted above:\\n●It is diﬃcult to interpret the bar plot because it has too many bars packed\\ninto a small space,\\n●But size-encoded bubbles make it pretty easy to visualize the change over\\ntime.\\n462'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 463}, page_content='DailyDoseofDS.com\\nAnother alternative to bar plots in such situations is dot plots.\\nBoth dot plots and bubble charts are based on the idea that, at times, when we\\nhave a bar plot with many bars, we’re oϔen not paying attention to the individual\\nbar lengths.\\nInstead, we mostly consider the individual endpoints that denote the total value.\\nThese plots precisely help us depict that while also eliminating the long bars of\\nlittle to no use.\\n463'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 464}, page_content='DailyDoseofDS.com\\nОnفԷrӝԎt֕ٲe ρoכقrת\\u05ces\\nWhile using Jupyter, one oϔen ﬁnds themselves in situations where they\\nrepeatedly modify a cell and re-rerun it. This makes data exploration\\nirreproducible and time-consuming. What’s more, the notebook also gets messy\\nand cluttered.\\nTo address this, one of the things I\\nactively leverage in my Jupyter\\nnotebooks is interactive controls\\nusing ОPڀٸiԟճeفآ.\\nA single decorator (ӵiכقe؞Ӟcف)\\nallows us to add:\\n●sliders\\n●dropdowns\\n●text ﬁelds, and more.\\nAs a result, one can:\\n●explore the data interactively\\n●speed-up data exploration\\n●avoid repetitive cell\\nmodiﬁcations and executions\\n●organize the data analysis.\\nA Jupyter Notebook is available here to get started: https://bit.ly/3XZtsea.\\n464'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 465}, page_content='DailyDoseofDS.com\\nіoءӞiԍ Ѯlתقs\\nWe mostly use \\u05fblف؊sٛӹp\\u05cd\\u05ebtء\\u0601) method to create subplots using Matplotlib. But\\nthis, at times, gets pretty tedious and cumbersome. For instance, it oﬀers limited\\nﬂexibility to create a custom layout, it is prone to indexing errors, and more.\\nInstead, use the \\u05fblف؊sٛӹp\\u05cd\\u05ebt٦זoءӞiԍ\\u0601)\\nmethod. Here, you can create a plot of\\nany desired layout by deﬁning the plot\\nstructure as a string.\\nFor instance, the string layout:\\n●AB\\n●AC\\n…will create three subplots, wherein:\\n●subplot \"A\" spans 1st column\\n●subplot \"B\" spans top half of the\\n2nd column\\n●subplot \"C\" spans the bottom\\nhalf of 2nd column\\nNext, create a subplot in a speciﬁc\\nregion by indexing the axes dictionary\\nwith its subplot key (\"A\", \"B\", or \"C\").\\n465'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 466}, page_content='DailyDoseofDS.com\\nϛn؞֖c֊\\nіaف\\u05fblתقl֕ӹ\\nѮlתقs\\nٸiف\\u058b\\nОnءԷt\\nίx֕آ\\nӞnԟ\\nίnכ\\u05ebtӝقiתלs\\nWhile creating data visualizations, there are oϔen certain parts that are\\nparticularly important.\\nYet, they may not be immediately obvious to the viewer.\\nA good data storyteller always ensures that the plot guides the viewer’s attention\\nto these key areas.\\nOne great way is to zoom in on speciﬁc regions of interest in a plot, as depicted\\nbelow.\\n466'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 467}, page_content='DailyDoseofDS.com\\nIn contrast to the usual plot, the other plot guides the viewer’s attention to a\\nspeciﬁc area of interest.\\nSuch eﬀorts always ensure that the plot communicates what we intend it to\\ndepict — even if the plot’s creator is not present at that time.\\nIn matplotlib, we can do so using indicate_inset_zoom(). It adds an indicator box,\\nwhich can be zoomed-in for better clarity.\\nThe embedded plot is treated like any other matplotlib plot. Thus, we can add\\naxis labels to it, if needed.\\nAnother great way to provide extra info is by adding text annotations to a plot, as\\ndepicted below:\\n467'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 468}, page_content='DailyDoseofDS.com\\nSuch eﬀorts always ensure that the plot indeed communicates what we intend it\\nto depict — even if the plot’s creator is not present at that time.\\nIn matplotlib, you can use 𝐚𝐧𝐧𝐨𝐭𝐚𝐭𝐞(), as depicted below:\\nIt adds explanatory texts to your plot, which lets you guide a viewer’s attention to\\nspeciﬁc areas and aid their understanding.\\nJupyter notebook for plot annotations is available here: https://bit.ly/4bzQ5Ja.\\n468'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 469}, page_content='DailyDoseofDS.com\\nѮrתէeءآiתלa\\u05cd֖zԶ іaف\\u05fblתقl֕ӹ Ѯlתقs\\nρoכقeٺك\\nI have been using matplotlib for many years now. Based on that experience, I\\nbelieve that one of the best yet underrated potentials of matplotlib is the amount\\nof customizability it oﬀers.\\nBut being unaware of that, most matplotlib users use it as a naive plotting utility\\nwith almost zero customization. And as the default plots never appear\\n“appealing”, they resort to other libraries, Plotly, for instance, to create elegant\\nplots.\\nYet, I believe\\nthat in 90-95% of\\ncases, you would\\nNEVER need to\\nlook beyond\\nmatplotlib. It\\ncan do much\\nmore than what\\nmost users\\nthink. For\\ninstance,\\nconsider the two\\nplots in this\\nimage.\\nYes! Both plots were created using matplotlib.\\nBut some custom formatting makes the second plot much more elegant,\\ninformative, appealing, and easy to follow.\\n●The title and subtitle signiﬁcantly aid the story.\\n469'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 470}, page_content='DailyDoseofDS.com\\n●Also, the footnote oﬀers extra important information, which is nowhere to\\nbe seen in the basic plot.\\n●Lastly, the bold bar immediately draws the viewer’s attention and conveys\\nthe purchase category’s importance.\\nThus, in my opinion, the overwhelming potential for customization makes\\nmatplotlib far more capable than what most users think.\\nρoכԎlٛآiתל\\nOne of the things I always ensure towards being a good storyteller in my data\\nscience projects is that my plot must demand minimal eﬀort from the viewer.\\nThus, I never shy away from putting in that extra eﬀort.\\nThis has been especially true for professional environments. At times, it is also\\ngood to ensure that our visualizations convey the right story, even if they are\\nviewed in our absence. The below plot is a classic example of that.\\nIn this entire chapter, I never discussed what that plot is about — somewhat\\nindicating my absence. Yet, by staring at this plot for a few seconds, you can\\nquickly ﬁgure out what I intended to highlight here, can’t you?\\nYou can download the code notebook for this post here: https://bit.ly/3zv8pWG.\\n470'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 471}, page_content='DailyDoseofDS.com\\nѽaכׂeڀ όiӝճrӝזs\\nMany tabular data analysis tasks can be interpreted as a ﬂow between several\\nsource and target entities.\\nFor instance, consider we have a sports\\npopularity dataset, which lists the\\npopularity index of a sport in a country.\\nIn this dataset:\\n●Countries are entities.\\n●Sports are entities.\\n●Information ﬂowing between them\\nis the popularity value.\\nWhile typical plots, like grouped bar plots, could be used here to understand the\\npopularity distribution of country-wise sports, as shown below:\\n471'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 472}, page_content='DailyDoseofDS.com\\n…in my opinion, however, Sankey diagrams stand out as a pretty cool and elegant\\nalternative to represent such ﬂow datasets:\\nTheir links are represented with arcs whose width is proportional to the value of\\nthe ﬂow. This immensely simpliﬁes the data analysis process. For instance, from\\nthe Sankey diagram above, one can quickly infer that:\\n●The most popular sport in India is Cricket.\\n●Basketball and Football are almost equally popular in the US.\\n●Basketball is hardly popular in India and England.\\n●England’s most popular sport is Football.\\n●Cricket and Football are almost equally popular in Australia.\\n●Overall, Football is the most popular sport in this dataset.\\n●and many many more.\\nImagine doing that by looking at the tabular data or a grouped bar chart.\\n●This process will be time-consuming.\\n●You may miss out on a few insights.\\n●The grouped bar chart can appear pretty cluttered and messy at times.\\n472'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 473}, page_content='DailyDoseofDS.com\\nOf course, Sankey diagrams can have multiple levels as well, as shown below:\\nTo determine when to use Sankey diagrams, see if the data involves any kind of\\nﬂow of resources, energy, or information ﬂow between multiple stages or entities.\\nIf yes, Sankey diagrams could be pretty valuable.\\nThere are multiple ways to create Sankey diagrams:\\n●To generate them programmatically, you may use the ipysankeywidget\\nlibrary: https://github.com/ricklupton/ipysankeywidget.\\n●If you prefer GUI, SankeyMATIC is a pretty cool, and easy-to-use tool to\\ncreate Sankey diagrams which I oϔen use: https://bit.ly/3WdMAnx.\\n473'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 474}, page_content='DailyDoseofDS.com\\nѺiԟճe\\u05cd֖nԶ Ѯlתقs\\nUnderstanding the distributional diﬀerences of distinct groups in a variable is\\nquite useful in uncovering insights around:\\n●behavioral disparities,\\n●feature engineering,\\n●predictive modeling, and more.\\nBut in such situations, many data scientists tend to create group-level\\ndistribution plots (histograms or density plots) on a single axis and compare\\nthem.\\nWhile this is (somewhat) okay when there are limited groups, in the presence of\\nmany groups, it can create cluttered plots, which may not reveal many insights\\nabout distributional diﬀerences:\\nRidgeline plots (shown below) are a pretty compact and elegant way to visualize\\nthe distribution of diﬀerent variables (or categories of a variable).\\n474'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 475}, page_content='DailyDoseofDS.com\\nMore speciﬁcally, the vertical\\nstacking on a common axis\\nprovides an easy comparison\\nbetween groups and reveals many\\ninsights into the shape and\\nvariation of the distributions,\\nwhich otherwise would be\\ndiﬃcult to understand.\\nThis allows us to\\ncompare the\\ndistributions of\\nmultiple groups\\nside by side and\\nunderstand how\\nthey diﬀer. The\\nimage below is\\nanother classic\\nexample of\\nRidgeline plots. It\\ndepicts the search\\ninterest across\\nvarious events that\\nhappened in 2023,\\nand it’s so easy to\\nvisualize.\\nWhile Seaborn provides a way to create Ridgeline plots, I have oϔen found the\\nJoypy library to be pretty useful and easy to use.\\n475'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 476}, page_content='DailyDoseofDS.com\\nҷhԶל قo ԎoכآiԟԷr Ѻiԟճe\\u05cd֖nԶ \\u05fblתق?\\nTypically, creating a Ridgeline plot makes sense when the variable has anything\\nabove 3-4 groups. This is to avoid the overlap that might appear when visualizing\\nthem in a single plot:\\nAlso, Ridgelines plots are relatively more useful when there is a clear pattern\\nand/or ranking on the continuous variable plotted between groups like:\\n●monotonically increasing,\\n●monotonically decreasing,\\n●increasing then decreasing,\\n●decreasing then increasing, etc.\\nThat is why the order in which\\nyou vertically stack the\\ndistribution of groups becomes\\nquite important. For instance,\\nconsider the above “Search\\ntrends” plot again but with a\\nrandom arrangement of groups:\\nI don’t think I have to ask you which one is easier to visualize and understand the\\nﬂow of events in 2023. So these were some points that will help you determine\\nwhether a Ridgeline plot will be a good ﬁt for visualizing your data.\\nI created this notebook for you to get started with Ridgeline plots using JoyPy:\\nhttps://bit.ly/3xMUxXj.\\n476'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 477}, page_content='DailyDoseofDS.com\\nѽpӝ؟k\\u05cd֖nԶ Ѯlתقs\\nWhen doing any data analysis task in Jupyter, we mostly create standalone charts\\nand visualizations. Of course, there’s nothing wrong, but in this chapter, let me\\nintroduce you to another pretty cool and elegant way I oϔen use to create\\nvisualizations in Jupyter Notebook.\\nλaԍׂg؞\\u05ebuכԠ\\nWhenever we display a\\nDataFrame in Jupyter, it is\\nrendered using HTML\\nand CSS.\\nThis means we can format its output just like any other web page. Moreover,\\nwhen we create any plot in Jupyter, it is rendered as an image.\\nHowever, the same image can also be rendered within an HTML image tag in\\nJupyter.\\nAnd magic happens when we combine the above two ideas — rendering plots\\ninside a DataFrame, as depicted below:\\n477'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 478}, page_content='DailyDoseofDS.com\\nFormally, these compact plots are called Sparklines, and they provide a pretty\\nelegant way to visualize data without taking up too much space.\\nUnlike traditional plots, we typically do not create axis ticks and labels in\\nSparklines.\\nOne of the coolest ways I prefer using Sparklines is by adding them to a\\nDataFrame’s cell, as depicted in the image above.\\nA Jupyter Notebook on creating Sparkline plots is available here:\\nhttps://bit.ly/4eVhVmf.\\nIn this notebook, I have provided the step-by-step procedure to embed Sparklines\\nin a DataFrame.\\n478'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 479}, page_content='DailyDoseofDS.com\\nѽQя\\nЄrתٜp֕לg ѽeفآ, Ѻo\\u05cd\\u05ceu\\u05fa Ӟnԟ ρuӸԷ ֖n ѽQя\\nA typical GroupBy query aggregates on just one set of columns. For instance:\\n●Grouping data on column “A” will require one query.\\n●Grouping data on columns “A” and “B” will require a separate query.\\nNext, if the two outputs must be gathered in a single table, we use UNION or\\nUNION ALL (as needed).\\n479'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 480}, page_content='DailyDoseofDS.com\\nBut this is not eﬃcient as it scans the same table twice. Instead, there are three\\nways to run multiple aggregations on the same table by scanning the table just\\nonce. This makes our query much more eﬃcient. These are Grouping Sets,\\nRollup, and Cube, and their usage has been depicted in the image below:\\n480'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 481}, page_content='DailyDoseofDS.com\\nש1\\u0602 Єrתٜp֕לg آeفأ\\nThe GROUPING SETS clause allows us to deﬁne multiple groupings in a single\\nquery. Each grouping set deﬁnes a combination of columns by which the data is\\ngrouped.\\nGiven the above query, here are the group aggregations that will be created:\\n●(A): Aggregated by A, counting all rows across all other columns.\\n●(A, B): Aggregated by (A, B), counting all rows across all other columns.\\n●(C): Aggregated by C, counting all rows across all other columns.\\nA demonstration is shown below:\\n481'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 482}, page_content='DailyDoseofDS.com\\nThe above query:\\n●Generates an aggregation on City column.\\n●Generates another aggregation on Fruit column.\\nש2\\u0602 ѺOяѐUѭ\\nROLLUP creates a result set that includes subtotals and a grand total in addition\\nto the regular grouped results.\\nIt does this by grouping the data at multiple levels of aggregation.\\nGiven the above query, here are the group aggregations that will be created:\\n●(A): Subtotal for each A, aggregated across all B and C.\\n●(A, B): Subtotal for each A and B combination, aggregated across all C.\\n●(A, B, C): Regular group by all three columns.\\n●(): Grand total, aggregated across all A, B, and C.\\nA demonstration is shown below:\\n482'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 483}, page_content='DailyDoseofDS.com\\nThe above query:\\n●Generates an aggregation on Fruit column.\\n●Generates another aggregation on (Fruit, City) column.\\n●Generates a grand total.\\nUnlike Grouping Sets, the order is important in ROLLUP.\\nMore speciﬁcally, ROLLUP (A, B) will not be the same as ROLLUP (B, A).\\nש3\\u0602 ρUκϛ\\nFinally, CUBE creates a result set that includes all possible combinations of\\naggregations for the speciﬁed columns.\\nGiven the above query, here are the group aggregations that will be created:\\n483'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 484}, page_content='DailyDoseofDS.com\\n●(A, B, C): Regular group by all three columns.\\n●(A, B): Subtotal for each A and B combination, aggregated across all C.\\n●(A, C): Subtotal for each A and C combination, aggregated across all B.\\n●(B, C): Subtotal for each B and C combination, aggregated across all A.\\n●(A): Subtotal for each A, aggregated across all B and C.\\n●(B): Subtotal for each B, aggregated across all A and C.\\n●(C): Subtotal for each C, aggregated across all A and B.\\n●(): Grand total, aggregated across all A, B, and C.\\nA demonstration is shown below:\\nThe above query:\\n●Generates an aggregation on Fruit column.\\n●Generates an aggregation on City column.\\n●Generates another aggregation on (Fruit, City) column.\\n●Generates a grand total.\\nTry it out by downloading this Jupyter Notebook: https://bit.ly/4bx4SUX.\\n484'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 485}, page_content='DailyDoseofDS.com\\nѽeו֖, ίnف֖, Ӟnԟ љaفٜrӝ\\u05ce оo֕לs\\nWe all have heard of LEFT JOIN, RIGHT JOIN, INNER JOIN, and OUTER\\nJOIN, haven’t we? These four are the most prevalent types of SQL joins. But\\nthere are more.\\nAnd in this chapter, I want to introduce you to three of them which I ﬁnd pretty\\nhandy at times. These are – Semi Join, Anti Join and Natural Join.\\n485'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 486}, page_content='DailyDoseofDS.com\\nש1\\u0602 ѽeו֖-н\\u05ebiכ\\nSemi-join appears quite similar to a leϔ join, but there are three notable\\ndiﬀerences:\\n1. If the join condition between two rows is TRUE, columns from only the\\nleϔ table are returned. Compare this to the leϔ join, which returns\\ncolumns from both tables.\\n2. If a row in the leϔ table has no match, then that row is not returned. In leϔ\\njoin, however, all rows from the leϔ table are returned irrespective of\\nwhether they have a match or not.\\n3. If a row in the leϔ table has multiple matches, only one entry is returned.\\nIn leϔ join, however, multiple matches are returned an equivalent number\\nof times.\\n486'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 487}, page_content='DailyDoseofDS.com\\nFor instance, consider we have the following two tables:\\nExecuting a semi-join, we get the following results:\\nAs depicted above, unlike leϔ-join:\\n●It only returns columns from the leϔ table.\\n●It only returns the matched rows from the leϔ table.\\nIf a record has multiple matches, like in this case:\\n487'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 488}, page_content='DailyDoseofDS.com\\n…then we notice that semi-join only returns one record from the leϔ table:\\nI ﬁnd semi-join to be particularly useful when I only care about the existence of\\nrecords in another table. Leϔ join returns duplicates which are not of interest at\\nthat point.\\nש2\\u0602 ίnف֖-н\\u05ebiכ\\nThe rows discarded by the semi-join from the leϔ table are the results of an\\nanti-join. So, in a way, we can say that:\\nConsider the above orders and users table again (one in which there were no\\nmultiple matches):\\n488'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 489}, page_content='DailyDoseofDS.com\\nExecuting an anti-join, we get the following results:\\nIt is clear from the semi-join and anti-join results that:\\nOf course, the order can be diﬀerent. When I say “[SEMI JOIN] + [ANTI JOIN] =\\n[LEFT TABLE]”, I mean the collection of all records.\\nI ﬁnd anti-join to be particularly useful when I wish to know which records do\\nnot exist in another table.\\nש3\\u0602 љaفٜrӝ\\u05ce оo֕ל\\nThis one is similar to INNER JOIN, but there’s no need to specify a join\\ncondition explicitly.\\nInstead, it automatically considers a join condition on ALL the matching column\\nnames.\\n489'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 490}, page_content='DailyDoseofDS.com\\nConsider the users and orders table yet again:\\nHere, the User_ID column is present in both tables.\\nExecuting a natural join, we get the following results:\\nAs depicted above, the results are similar to what we would get with INNER\\nJOIN.\\nHowever, we did not have to explicitly specify a JOIN condition, which,\\nadmittedly, could be good or bad.\\n●It is good because it helps us write concise queries.\\n●It is bad because we are not explicit about the columns being joined.\\nThese were three more types of SQL Joins, which I use at times to write concise\\nand elegant SQL queries.\\nIf you wish to experiment with what we discussed, download this Jupyter\\nNotebook: https://bit.ly/4cRGMWg.\\n490'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 491}, page_content='DailyDoseofDS.com\\nҝsԶ ѽQя ؕNѠҏ ОNؖ ҷiف\\u058b ρaٛقiתל\\nIn my experience, many unexpected errors in libraries/tools/languages can be\\nattributed to the presence of missing values. For instance, consider a 1D NumPy\\narray with NaN values.\\nWhen we aggregate this NumPy array to calculate, say, its sum, we get the\\nfollowing output:\\nStrange, right? Although an output of 6 may have made more sense here, NumPy\\nproduces a NaN value instead.\\nAnyway, this is not the topic of this chapter, but I hope you get the point\\nA similar silent mistake can be found in SQL as well, speciﬁcally in the usage of\\n“NOT IN” clause, which many SQL users are not aware of.\\nLet’s understand!\\n491'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 492}, page_content='DailyDoseofDS.com\\nConsider we have the following tables (students and names), in our database:\\nThe task is to select records from students table where ﬁ؟sف٧nӝזe is not in the\\nnames table. One way to do this is by using the љOҎ ОN clause:\\nThis provides the expected results as well.\\nNow, say our names table had a NULL value:\\n492'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 493}, page_content='DailyDoseofDS.com\\nIf we run the above query again, we get no records this time:\\nOn a side note, had\\nwe used the IN\\nclause to select rows\\nfrom the آtٛԠeכقs\\ntable where\\nﬁ؟sف٧nӝזe was in\\nthe names table, we\\nnotice that it works\\nas expected:\\nThat’s strange, isn’t it?\\nҷhڀ Ԡo ٸe ճeف לo ؟eԍ\\u05ebrԟآ ٸiف\\u058b љOҎ ОNؐ\\nThe reason we get no records when we use љOҎ ОN but the ОN clause works as\\nexpected has to do with how these two clauses operate internally.\\nFor simplicity, consider we are\\ncurrently checking the ﬁrst\\nrecord (where ﬁrst_name is\\n“John”) from the students\\ntable:\\n493'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 494}, page_content='DailyDoseofDS.com\\nThe ҷHϚѺE clause needs a boolean value to determine whether a record must be\\nﬁltered or not. When we use “IN”, this boolean value is evaluated using the OR\\noperator as follows:\\nIf any condition is TRUE, the row gets ﬁltered. However, when we use “NOT IN”,\\nthe boolean value is evaluated using the AND operator as follows:\\nFor the above expression to be TRUE, all individual conditions MUST be TRUE.\\nBut the (JOHN != None) condition produces a conﬂict because, typically, this\\ncondition results in an UNKNOWN value. Thus, the entire expression evaluates\\nto UNKNOWN — producing no records. This happens for every record in the\\nstudents table. As a result, the query results in no records:\\n494'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 495}, page_content='DailyDoseofDS.com\\nҷhӝق قo Ԡo ֖nءقeӝԠ?\\nThere are many ways to avoid this. As our task is to select records from آtٛԠeכقs\\ntable where ﬁ؟sف٧nӝזe is not in the names table, we can:\\n●Filter out the NULL values in the sub-query:\\n●Use Anti Joins: Anti join returns only those rows from the leϔ table where\\nno match is found in the right table. This is precisely what we need in our\\ncase and it is implemented below:\\nAt times, such mistakes can take some serious time to debug if you are not aware\\nof them beforehand.\\nIf you wish to experiment with what we discussed about the NOT IN clause,\\ndownload this Jupyter Notebook: https://bit.ly/3xVOIXF.\\n495'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 496}, page_content='DailyDoseofDS.com\\nѮyف\\u058boכ ѡOѭ\\nЄeفقe؞آ Ӟnԟ ѽeفقe؞آ\\nDot notation provides a concise and elegant way to access and modify an object’s\\nattributes.\\nYet, with dot notation, we can not validate the updates made to an attribute. This\\nmeans we can assign invalid values to an instance’s attributes, as shown below:\\n496'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 497}, page_content='DailyDoseofDS.com\\nOne common way to avoid this is by deﬁning a setter (آeف٧s֕Ԡe\\u0600\\u0603), which\\nvalidates the assignment step.\\nBut explicitly invoking a setter method isn’t as elegant as dot notation, is it?\\nIdeally, we would want to use dot notation and still apply those validation checks.\\nThe @𝐩𝐫𝐨𝐩𝐞𝐫𝐭𝐲decorator in Python can help. Here’s how we can use it here.\\nFirst, deﬁne a getter as follows:\\nDeclare a method\\nwith the attribute’s\\nname.\\nThere’s no need to\\nspecify any\\nparameters for this\\nmethod.\\nDecorate it with the\\n@𝐩𝐫𝐨𝐩𝐞𝐫𝐭𝐲decorator.\\n497'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 498}, page_content='DailyDoseofDS.com\\nNext, deﬁne a setter as follows:\\n●Declare a method with the attribute’s name.\\n●Specify the parameter you want to update the attribute with.\\n●Write the conditions as you usually would in any other setter method.\\n●Decorate it with the @𝐚𝐭𝐭𝐫𝐢𝐛𝐮𝐭𝐞-𝐧𝐚𝐦𝐞.𝐬𝐞𝐭𝐭𝐞𝐫decorator.\\nNow, you can use the dot notation while still having validation checks in place.\\nThis approach oﬀers\\nboth the validation and\\ncontrol of explicit setters\\nand getters and the\\nelegance of dot\\nnotations.\\n498'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 499}, page_content='DailyDoseofDS.com\\nόeءԎr֕\\u05fbtת؟s ֖n Ѯyف\\u058boכ\\nIn this chapter, let’s continue our discussion on the above topic and discuss a\\nlimitation of the above approach. Moving on, we shall see how Descriptors in\\nPython provide a much more elegant way of setting and getting values.\\nѐiו֖tӝقiתלs \\u05ebf ӵp؞\\u05ebpԶ؟tڀ Ԡeԍ\\u05ebrӝقo؞\\nConsider the above class implementation again:\\nThe biggest issue here is that we must deﬁne a getter and setter for every\\ninstance-level attribute.\\nSo what if our class has, say, 3 such attributes, and all must be positive?\\nOf course, we will have 3 getters and 3 setters, which makes the overall\\nimplementation long, messy, and redundant.\\n499'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 500}, page_content='DailyDoseofDS.com\\nThere’s redundancy because every setter method will have almost the same lines\\nof code (the if statements for validation).\\nAlso, if you think about it, the getter methods are somewhat redundant and\\nunnecessary too, as they just return an attribute.\\nIf that is clear, there’s one more issue with the above implementation.\\nRecall what I mentioned earlier: “Our class will have 3 such instance-level\\nattributes, and all must be positive?”\\n500'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 501}, page_content='DailyDoseofDS.com\\nSee what happens when we create an object with an invalid input:\\nAs depicted above, Python does not raise any error, when ideally, it should.\\nίn ֖m\\u05faԷrզԸԎt آo\\u05cdٜt֕\\u05ebn\\nOne common way programmers try to eliminate redundancy is by deﬁning\\nexplicit validation functions.\\nFor instance, we can deﬁne a function that just validates the value received, as\\ndemonstrated below:\\n501'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 502}, page_content='DailyDoseofDS.com\\nNext, we can invoke this method wherever needed:\\nBut this does not solve the problem either:\\nWe still have explicit and redundant function calls in each setter method.\\nAll getter methods still do the same thing and have high redundancy.\\nAnd most importantly, the __init__ method is now messed up with multiple\\nfunction calls.\\n502'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 503}, page_content='DailyDoseofDS.com\\nόeءԎr֕\\u05fbtת؟s\\nSimply put, Descriptors are objects with methods (like __get__, __set__, etc.) that\\nare used to manage access to the attributes of another class. So, every descriptor\\nobject is assigned to only one attribute of another class.\\nAnd just to be clear, this “another class” is the class we are primarily interested in\\n— the DummyClass we saw earlier, for instance.\\n●The attribute number1 →gets its own descriptor.\\n●The attribute number2 →gets its own descriptor.\\n●The attribute number3 →gets its own descriptor.\\nA typical Descriptor class is implemented with three methods, as shown below:\\n503'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 504}, page_content='DailyDoseofDS.com\\nThe __set__ method is called when the attribute is assigned a new value. We can\\ndeﬁne the custom checks here.\\nThe __set_name__ method is called when the descriptor object is assigned to a\\nclass attribute. It allows the descriptor to keep track of the name of the attribute\\nit’s assigned to within the class.\\nThe __get__ method is called when the attribute is accessed.\\nAlso:\\nThe instance parameter refers to the object of the desired class — DummyClass().\\nThe owner parameter is the desired class itself — DummyClass.\\nThe value parameter is the value being assigned to an attribute of the desired\\nclass.\\nThe name parameter is the name of the attribute.\\nIf it’s unclear, let me give you a simple demonstration.\\n504'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 505}, page_content='DailyDoseofDS.com\\nConsider this Descriptor class:\\nI’ll explain this implementation shortly, but before that, let’s consider its usage,\\nwhich is demonstrated below:\\n505'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 506}, page_content='DailyDoseofDS.com\\nNow, let’s go back to the DescriptorClass implementation:\\n●__set_name__(self, owner, name): This method is called when the\\ndescriptor is assigned to a class attribute (line 3). It saves the name of the\\nattribute in the descriptor for later use.\\n●__set__(self, instance, value): When a value is assigned to the attribute (line\\n6), this method is called. It raises an error if the value is negative.\\nOtherwise, it stores the value in the instance’s dictionary under the\\nattribute name we deﬁned earlier.\\n●__get__(self, instance, owner): When the attribute is accessed, this method\\nis called. It returns the value from the instance’s dictionary.\\nNow, see how this solution smartly solves all the problems we discussed earlier.\\nLet’s create an object of the DummyClass:\\nAs depicted above, assigning an invalid value to the attribute raises an error.\\nNext, let’s see what happens when the attribute speciﬁed during the initialization\\nis invalid:\\n506'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 507}, page_content='DailyDoseofDS.com\\nGreat! It validates the initialization too.\\nHere, recall that we never deﬁned any explicit checks in the __init__ method,\\nwhich is super cool.\\nMoving on, let’s deﬁne multiple attributes in the DummyClass now:\\nCreating an object and setting an invalid value for any of the attributes raises an\\nerror:\\nWorks seamlessly!\\n507'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 508}, page_content='DailyDoseofDS.com\\nRecall that we never deﬁned multiple getters and setters for each attribute\\nindividually, like we did with the @property decorator earlier.\\nThis is great, isn’t it?\\nI ﬁnd descriptors to be massively helpful in reducing work and code redundancy\\nwhile also making the entire implementation much more elegant.\\nIf you want to try them out, I prepared this notebook for you to get started:\\nhttps://bit.ly/4cqsyM6.\\n508'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 509}, page_content='DailyDoseofDS.com\\nٗ0 іoءق ρoוזoכ іaղ֖c іeف\\u058boԟآ\\nHere are 20 most common magic methods used in Python OOP:\\nIn my experience, these are possibly the only 20 magic methods you would ever\\nneed in most Python projects utilizing OOP.\\nSyntactically, they are preﬁxed and suﬃxed with double underscores, such as\\n٧_\\u05cdԷn٦٧, ٧_ءقr٦٧, and many more. That is why they are also called “Dunder\\nmethods” — short for Double UNDERscore.\\n509'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 510}, page_content='DailyDoseofDS.com\\nіeו\\u05ebrڀ ϛﬃԎiԶלt ρlӝآs ѡbָԷcفآ ٜs֕לg ѽlתقs\\nWhen we deﬁne a class\\nin Python, it is\\npossible to\\ndynamically add new\\nattributes to its objects\\nduring run-time.\\nFor instance, consider\\nthe following Python\\nclass here.\\nHere, it is perfectly legal to add new attributes to any object at run-time, as\\nshown below:\\nHowever, this is not always recommended because:\\n●It may lead to bugs if the code assumes all class instances will always have\\nthe same attributes.\\n●It makes it diﬃcult to debug code when objects keep on accumulating new\\nattributes.\\n●It leads to a conﬂicting schema, etc.\\n510'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 511}, page_content='DailyDoseofDS.com\\nCan we restrict this dynamicity?\\nOf course we can!\\nDeﬁning a slotted class helps us achieve this. Simply put, it allows us to ﬁx the\\ninstance-level attributes a class object can ever possess.\\nA slotted class is declared as follows:\\nDeﬁne a ٧_ء\\u05ceoفآ_٦ attribute with all attributes an object may possess.\\nDone!\\n511'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 512}, page_content='DailyDoseofDS.com\\nNow, if we try to add a new attribute at run-time, it raises an error:\\nDeclaring a slotted class has many advantages as well. For instance, it can help us\\navoid typical code typos. Say we want to change the name of آtٛԠeכقA.\\nHere, instead of referring to the attribute name (with a lowercase n), we\\nmistakenly wrote Name (with an uppercase n). A normal unslloted class will not\\nraise an error, as shown below:\\n512'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 513}, page_content='DailyDoseofDS.com\\nBut a slotted class will catch this typo:\\nDeclaring a slotted class provides memory advantages as well. As shown below,\\nan object of the slotted class consumes ٗ.լٻ less memory than an object of a\\nnormal class:\\nWhy, you may wonder?\\n513'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 514}, page_content='DailyDoseofDS.com\\nTypically, Python creates a dictionary\\nfor every object that maps attributes\\nto their values. As a dictionary is\\nmutable, this is precisely what allows\\nus to add/delete attributes\\ndynamically. But this introduces\\nmemory overheads as Python always\\ntries to keep room for new attributes\\nthat may get added at some point.\\nIn case of slotted class, however, Python can do away with this dictionary:\\nAs a result, the object consumes less memory. As a departing note, if you don’t\\nwant to dynamically add new attributes to an object, it is better to create a slotted\\nclass.\\nIn fact, even if you know the attributes that a class object will ever possess, but\\nsome of these attributes are not available during the object’s initialization, you\\ncan still declare them in the ٧_ء\\u05ceoفآ_٦ class attribute and assign a value to them\\nlater in the program whenever they are available.\\n514'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 515}, page_content='DailyDoseofDS.com\\nҷhڀ όoכ؝t ҷe Оnٱ\\u05ebkԶ זoԟԷl؉էo؞ٸa؞Ԡ(\\u0602 ֖n ѮyҎ\\u05ebrԍ\\u058b?\\nIn PyTorch, the forward pass is implemented in the էo؞ٸa؞Ԡ(\\u0602 method, as\\ndemonstrated below:\\nHere, have you ever wondered that when we want to run the forward pass, we\\nrarely invoke this էo؞ٸa؞Ԡ(\\u0602 method:\\nInstead, we always invoke the model (a class object) as demonstrated below, as it\\nwas a function:\\n515'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 516}, page_content='DailyDoseofDS.com\\nWe can also verify that model is a class object:\\nHow can a class object be invoked like a function and what are we missing here?\\nLet’s understand!\\nί آiו\\u05fblԶ Էxӝזp\\u05cdԷ\\nConsider we want to evaluate the following quadratic:\\n𝑓(𝑥)=𝑎𝑥2+𝑏𝑥+𝑐\\nOne way is to deﬁne a method that accepts the input and returns the value of the\\nquadratic, as shown below:\\nOf course, there is nothing wrong with this approach.\\n516'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 517}, page_content='DailyDoseofDS.com\\nBut there is one smart and elegant way of doing this in Python. Instead of\\nexplicitly invoking a method, we can deﬁne the ٧_ԍӞl\\u05cd٧_\\u0600\\u0603 magic method.\\nThis magic method allows you to deﬁne the behavior of the class object when it\\nis invoked like a function (like this: \\u05ebbָԷcف\\u0601)).\\nLet’s rename the Էvӝ\\u05ceuӝقe\\u0600\\u0603 method to ٧_ԍӞl\\u05cd٧_\\u0600\\u0603.\\nAs a result, we can now invoke the class object directly instead of explicitly\\ninvoking a method. This can have many advantages. For instance:\\n●It allows us to implement objects that can be used in a ﬂexible and\\nintuitive way.\\n●It allows us to use a class object in contexts where a callable object is\\nexpected — using a class object as a decorator, for instance.\\nҷhӝق ֖s Ԏa\\u05cd\\u05ceaӸ\\u05cfԷ?\\nIn Python, a callable is any object\\nthat can be called using parentheses\\nand may return a value. For\\ninstance, a function is a callable\\nobject (one that can be\\ncalled/invoked).\\n517'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 518}, page_content='DailyDoseofDS.com\\nρoו֖nղ ӹaԍׂ قo ѮyҎ\\u05ebrԍ\\u058b\\nThis is what happens when we build deep learning models with PyTorch. For\\ninstance, consider the PyTorch class again:\\nAs you may have already guessed, the model object can be invoked because all\\nPyTorch classes implicitly declare the ٧_ԍӞl\\u05cd٧_\\u0600\\u0603 method themselves. Within\\nthat ٧_ԍӞl\\u05cd٧_\\u0600\\u0603 method, they invoke the user-deﬁned forward pass.\\nA simpliﬁed version of this is depicted below:\\n●PyTorch itself adds the ٧_ԍӞl\\u05cd٧_\\u0600\\u0603 method.\\n●The ٧_ԍӞl\\u05cd٧_\\u0600\\u0603 method invokes the user-deﬁned էo؞ٸa؞Ԡ(\\u0602 method.\\n518'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 519}, page_content='DailyDoseofDS.com\\nThis way, Python gets to know that the model object can be invoked like a\\nfunction — זoԟԷl\\u0600\\u0603.\\nIn fact, we can verify that we will get the same output no matter which way we\\nrun the forward pass:\\nAll three ways return the same output.\\n519'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 520}, page_content='DailyDoseofDS.com\\nҏrٛԷ ѡOѭ ϛnԍӞpءٜlӝقiתל ֖s іiءآiכճ Ͼrתז Ѯyف\\u058boכ\\nUsing access modiﬁers (public, protected, and private) is fundamental to\\nencapsulation in OOP.\\nThis is not just about Python but applicable to OOP in general. However, when\\nwe talk speciﬁcally about Python, we notice that it fails to deliver true\\nencapsulation procedures, which one would want to leverage in their OOP code.\\nHow?\\nLet’s understand this in this chapter.\\nAs you may already know, class attributes in OOP can be of three types:\\n●A public member is accessible everywhere inside and outside the base\\nclass, and it is inherited by all child classes.\\n●A protected member is accessible everywhere inside the base class and\\nchild class(es). It is not accessible outside the class.\\n●A private member is accessible only inside the base class.\\nBut, with Python, there are no such strict enforcements. This is unlike many\\nother programming languages like C++. Thus, protected members behave exactly\\nlike public members.\\nWhat’s more, private members can be (somehow) accessed outside the class as\\nwell.\\n520'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 521}, page_content='DailyDoseofDS.com\\nFor better clarity, consider the following class implementation:\\nSyntactically speaking, in Python:\\n●A public member is declared with 0 leading underscores.\\n●A protected member is declared with 1 leading underscore.\\n●A private member is declared with 2 leading underscores.\\nNext, we instantiate a class object:\\nMoving on, as one would expect, the public attribute is accessible outside the\\nclass:\\n521'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 522}, page_content='DailyDoseofDS.com\\nHowever, in a similar way, even the protected attribute is accessible outside the\\nclass, which, ideally, should not happen:\\nLastly, when declaring private members, Python performs name mangling.\\nIt is a technique used in programming to avoid naming conﬂicts between\\ndiﬀerent classes.\\nPython performs name mangling by attaching underscore-preﬁxed class name\\n(٧Mڀρlӝآs) to all members with two leading underscores.\\nSo, while the private member is not directly accessible using its original name, it\\ncan still be accessed with the modiﬁed name obtained from name mangling.\\nThis is demonstrated below:\\n522'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 523}, page_content='DailyDoseofDS.com\\nAs demonstrated in all the examples above, protected members and private\\nmembers (with name mangling) can be accessed like public members.\\nThus, the point that every Python programmer using OOP must remember here\\nis that Python never enforces encapsulation.\\nInstead, leveraging encapsulation procedures in Python mainly relies on\\nconventions.\\nThey are used to communicate the accessibility protocols of class members to\\nother programmers.\\nThus, it is the programmer’s responsibility to obey these conventions.\\n523'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 524}, page_content='DailyDoseofDS.com\\nί ρoוזoכ іiءԎoכԏԷpف֖oכ ίbתٜt ٧_֕לiف٧_\\u0600\\u0603\\nMost Python programmers misinterpret the ٧_֕לiف٧_\\u0600\\u0603 magic method in\\nPython OOP. They think that it creates a new object, i.e., allocates memory to it.\\nFor instance, consider the Point2D class below:\\nAssume that there is also a ٧_؞Էp؞٧_\\u0600\\u0603 method, as it is not shown here. When\\nwe create an object (shown below), programmers believe that in the background,\\nit is the ٧_֕לiف٧_\\u0600\\u0603 method that is allocating memory to their object:\\nBut that is not true.\\nWhen we create ANY object in Python, the ٧_֕לiف٧_\\u0600\\u0603 method NEVER\\nallocates memory to it.\\n524'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 525}, page_content='DailyDoseofDS.com\\nAs the name suggests, ٧_֕לiف٧_\\u0600\\u0603 only assigns value to an object’s attributes,\\ni.e., initialize the attributes.\\nInstead, it’s the ٧_כԷw٦٧(\\u0602 magic method that creates a new object and\\nallocates memory to it. To understand better, consider the class implementation\\nbelow.\\nHere, we have implemented the ٧_כԷw٦٧(\\u0602 method, which checks if the passed\\narguments are of integer type. Now, if we try to create an object of this class,\\nPython would ﬁrst validate the checks speciﬁed in the ٧_כԷw٦٧(\\u0602 method and\\ncreate a new object only when the speciﬁed conditions are true. This is evident\\nfrom the image below:\\n525'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 526}, page_content='DailyDoseofDS.com\\nAnother popular use case of the ٧_כԷw٦٧ method is to deﬁne singleton classes\\n— classes that can only have one object. For instance, consider the following\\nclass implementation:\\nIn the above code, the ٧_כԷw٦٧ method deﬁne a class variable ٧c\\u05cdӞsء٧cתٜnف.\\nWhen a new object is instantiated, if the value of ٧c\\u05cdӞsء٧cתٜnفՕ0, the value of\\n٧c\\u05cdӞsء٧cתٜnف is updated to 1 and the object is returned. Aϔer creating the ﬁrst\\nobject, a new object can never be created because the value of _class_count will\\nnever be 0. This is evident from the image below:\\n526'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 527}, page_content='DailyDoseofDS.com\\nϾuכԎt֕\\u05ebn ѡvԶ؟lתӞd֕לg ֖n Ѯyف\\u058boכ\\nFunction overloading is critical to\\npolymorphism, wherein, we may have\\nmultiple functions with:\\n●Same name, and\\n●Diﬀerent number (or type) of\\nparameters.\\nHowever, Python provides no native support for function overloading. In other\\nwords, if we deﬁne two (or more) functions with the same name and diﬀerent\\nparameters, Python will only consider the latest deﬁnition corresponding to that\\nfunction name. This is depicted below.\\n527'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 528}, page_content='DailyDoseofDS.com\\nAs depicted above, the latest deﬁnition of add() had three parameters. That is why\\npassing two arguments raised an error. This restricts us from writing good\\npolymorphic code in Python. Of course, there are ways to prevent this error, as\\ndepicted below:\\nAlso, if the same parameter can take multiple data types, we can write\\n(somewhat) polymorphic code by adding if statements using ֖s֕לsفӞnԍԷ(\\u0602.\\nBut this isn’t as elegant as deﬁning multiple functions with diﬀerent data types\\nlike we can do in, say, C++, is it?\\n528'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 529}, page_content='DailyDoseofDS.com\\nC++ automatically invokes the correct method corresponding to the argument\\ndata type. While exploring this for one of my projects, I found a pretty handy way\\nto enable function overloading in Python\\nThe ӵd֕آpӝقc֊ decorator from the Multidispatch library allows us to leverage\\nthe standard and elegant function overloading in Python, as demonstrated below:\\n529'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': '', 'creationdate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'format': 'PDF 1.4', 'title': 'Daily Dose of Data Science Full Archive', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240712201555Z00'00'\", 'trapped': '', 'modDate': \"D:20240712201555Z00'00'\", 'creationDate': \"D:20240712201555Z00'00'\", 'page': 530}, page_content='DailyDoseofDS.com\\nAs depicted above, we have multiple functions with the same name and diﬀerent\\nparameters. The ӵd֕آpӝقc֊ decorator allows us to invoke the correct function\\ncorresponding to the parameters passed during the function call.\\nAnother cool thing about the ӵd֕آpӝقc֊ decorator is that it raises an error when\\nthe function call does not match any of the function deﬁnitions:\\nThis allows us to identify errors which can oϔen go unnoticed.\\nIn a dummy experimentation, I did notice a slight increment in the run time. But\\nthe order of increase was in nanoseconds or so, which can be safely ignored.\\n530'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 0}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 1}, page_content='Hands-On Machine Learning with\\nScikit-Learn, Keras, and\\nTensorFlow\\nTHIRD EDITION\\nConcepts, Tools, and Techniques to Build Intelligent\\nSystems\\nAurélien Géron'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 2}, page_content='Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2023 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\\nSebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales\\npromotional use. Online editions are also available for most titles\\n(https://oreilly.com). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or\\ncorporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editors: Nicole Taché and \\nMichele Cronin\\nProduction Editor: Beth Kelly\\nCopyeditor: Kim Cofer\\nProofreader: Rachel Head\\nIndexer: Potomac Indexing, LLC\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nMarch 2017: First Edition\\nSeptember 2019: Second Edition'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 3}, page_content='October 2022: Third Edition'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 4}, page_content='Revision History for the Third Edition\\n2022-10-03: First Release\\nSee https://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release\\ndetails.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-\\nOn Machine Learning with Scikit-Learn, Keras, and TensorFlow, the cover\\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the author, and do not represent\\nthe publisher’s views. While the publisher and the author have used good\\nfaith efforts to ensure that the information and instructions contained in this\\nwork are accurate, the publisher and the author disclaim all responsibility for\\nerrors or omissions, including without limitation responsibility for damages\\nresulting from the use of or reliance on this work. Use of the information and\\ninstructions contained in this work is at your own risk. If any code samples or\\nother technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to\\nensure that your use thereof complies with such licenses and/or rights.\\n978-1-098-12597-4\\n[LSI]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 5}, page_content='Preface'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 6}, page_content='The Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper\\u2060 \\u2060\\n showing how to train a\\ndeep neural network capable of recognizing handwritten digits with state-of-\\nthe-art precision (>98%). They branded this technique “deep learning”. A\\ndeep neural network is a (very) simplified model of our cerebral cortex,\\ncomposed of a stack of layers of artificial neurons. Training a deep neural net\\nwas widely considered impossible at the time,\\u2060 \\u2060\\n and most researchers had\\nabandoned the idea in the late 1990s. This paper revived the interest of the\\nscientific community, and before long many new papers demonstrated that\\ndeep learning was not only possible, but capable of mind-blowing\\nachievements that no other machine learning (ML) technique could hope to\\nmatch (with the help of tremendous computing power and great amounts of\\ndata). This enthusiasm soon extended to many other areas of machine\\nlearning.\\nA decade later, machine learning had conquered the industry, and today it is\\nat the heart of much of the magic in high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition,\\nrecommending videos, and perhaps even driving your car.\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 7}, page_content='Machine Learning in Your Projects\\nSo, naturally you are excited about machine learning and would love to join\\nthe party!\\nPerhaps you would like to give your homemade robot a brain of its own?\\nMake it recognize faces? Or learn to walk around?\\nOr maybe your company has tons of data (user logs, financial data,\\nproduction data, machine sensor data, hotline stats, HR reports, etc.), and\\nmore than likely you could unearth some hidden gems if you just knew where\\nto look. With machine learning, you could accomplish the following and\\nmuch more:\\nSegment customers and find the best marketing strategy for each group.\\nRecommend products for each client based on what similar clients\\nbought.\\nDetect which transactions are likely to be fraudulent.\\nForecast next year’s revenue.\\nWhatever the reason, you have decided to learn machine learning and\\nimplement it in your projects. Great idea!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 8}, page_content='Objective and Approach\\nThis book assumes that you know close to nothing about machine learning.\\nIts goal is to give you the concepts, tools, and intuition you need to\\nimplement programs capable of learning from data.\\nWe will cover a large number of techniques, from the simplest and most\\ncommonly used (such as linear regression) to some of the deep learning\\ntechniques that regularly win competitions. For this, we will be using\\nproduction-ready Python frameworks:\\nScikit-Learn is very easy to use, yet it implements many machine\\nlearning algorithms efficiently, so it makes for a great entry point to\\nlearning machine learning. It was created by David Cournapeau in 2007,\\nand is now led by a team of researchers at the French Institute for\\nResearch in Computer Science and Automation (Inria).\\nTensorFlow is a more complex library for distributed numerical\\ncomputation. It makes it possible to train and run very large neural\\nnetworks efficiently by distributing the computations across potentially\\nhundreds of multi-GPU (graphics processing unit) servers. TensorFlow\\n(TF) was created at Google and supports many of its large-scale\\nmachine learning applications. It was open sourced in November 2015,\\nand version 2.0 was released in September 2019.\\nKeras is a high-level deep learning API that makes it very simple to train\\nand run neural networks. Keras comes bundled with TensorFlow, and it\\nrelies on TensorFlow for all the intensive computations.\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nmachine learning through concrete working examples and just a little bit of\\ntheory.\\nTIP'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 9}, page_content='While you can read this book without picking up your laptop, I highly recommend you\\nexperiment with the code examples.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 10}, page_content='Code Examples\\nAll the code examples in this book are open source and available online at\\nhttps://github.com/ageron/handson-ml3, as Jupyter notebooks. These are\\ninteractive documents containing text, images, and executable code snippets\\n(Python in our case). The easiest and quickest way to get started is to run\\nthese notebooks using Google Colab: this is a free service that allows you to\\nrun any Jupyter notebook directly online, without having to install anything\\non your machine. All you need is a web browser and a Google account.\\nNOTE\\nIn this book, I will assume that you are using Google Colab, but I have also tested the\\nnotebooks on other online platforms such as Kaggle and Binder, so you can use those if\\nyou prefer. Alternatively, you can install the required libraries and tools (or the Docker\\nimage for this book) and run the notebooks directly on your own machine. See the\\ninstructions at https://homl.info/install.\\nThis book is here to help you get your job done. If you wish to use additional\\ncontent beyond the code examples, and that use falls outside the scope of fair\\nuse guidelines, (such as selling or distributing content from O’Reilly books,\\nor incorporating a significant amount of material from this book into your\\nproduct’s documentation), please reach out to us for permission, at\\npermissions@oreilly.com.\\nWe appreciate, but do not require, attribution. An attribution usually includes\\nthe title, author, publisher, and ISBN. For example: “Hands-On Machine\\nLearning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron.\\nCopyright 2023 Aurélien Géron, 978-1-098-12597-4.”'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 11}, page_content='Prerequisites\\nThis book assumes that you have some Python programming experience. If\\nyou don’t know Python yet, https://learnpython.org is a great place to start.\\nThe official tutorial on Python.org is also quite good.\\nThis book also assumes that you are familiar with Python’s main scientific\\nlibraries—in particular, NumPy, Pandas, and Matplotlib. If you have never\\nused these libraries, don’t worry; they’re easy to learn, and I’ve created a\\ntutorial for each of them. You can access them online at\\nhttps://homl.info/tutorials.\\nMoreover, if you want to fully understand how the machine learning\\nalgorithms work (not just how to use them), then you should have at least a\\nbasic understanding of a few math concepts, especially linear algebra.\\nSpecifically, you should know what vectors and matrices are, and how to\\nperform some simple operations like adding vectors, or transposing and\\nmultiplying matrices. If you need a quick introduction to linear algebra (it’s\\nreally not rocket science!), I provide a tutorial at https://homl.info/tutorials.\\nYou will also find a tutorial on differential calculus, which may be helpful to\\nunderstand how neural networks are trained, but it’s not entirely essential to\\ngrasp the important concepts. This book also uses other mathematical\\nconcepts occasionally, such as exponentials and logarithms, a bit of\\nprobability theory, and some basic statistics concepts, but nothing too\\nadvanced. If you need help on any of these, please check out\\nhttps://khanacademy.org, which offers many excellent and free math courses\\nonline.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 12}, page_content='Roadmap\\nThis book is organized in two parts. Part I, “The Fundamentals of Machine\\nLearning”, covers the following topics:\\nWhat machine learning is, what problems it tries to solve, and the main\\ncategories and fundamental concepts of its systems\\nThe steps in a typical machine learning project\\nLearning by fitting a model to data\\nOptimizing a cost function\\nHandling, cleaning, and preparing data\\nSelecting and engineering features\\nSelecting a model and tuning hyperparameters using cross-validation\\nThe challenges of machine learning, in particular underfitting and\\noverfitting (the bias/variance trade-off)\\nThe most common learning algorithms: linear and polynomial\\nregression, logistic regression, k-nearest neighbors, support vector\\nmachines, decision trees, random forests, and ensemble methods\\nReducing the dimensionality of the training data to fight the “curse of\\ndimensionality”\\nOther unsupervised learning techniques, including clustering, density\\nestimation, and anomaly detection\\nPart II, “Neural Networks and Deep Learning”, covers the following topics:\\nWhat neural nets are and what they’re good for\\nBuilding and training neural nets using TensorFlow and Keras\\nThe most important neural net architectures: feedforward neural nets for'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 13}, page_content='tabular data, convolutional nets for computer vision, recurrent nets and\\nlong short-term memory (LSTM) nets for sequence processing,\\nencoder–decoders and transformers for natural language processing (and\\nmore!), autoencoders, generative adversarial networks (GANs), and\\ndiffusion models for generative learning\\nTechniques for training deep neural nets\\nHow to build an agent (e.g., a bot in a game) that can learn good\\nstrategies through trial and error, using reinforcement learning\\nLoading and preprocessing large amounts of data efficiently\\nTraining and deploying TensorFlow models at scale\\nThe first part is based mostly on Scikit-Learn, while the second part uses\\nTensorFlow and Keras.\\nCAUTION\\nDon’t jump into deep waters too hastily: while deep learning is no doubt one of the most\\nexciting areas in machine learning, you should master the fundamentals first. Moreover,\\nmost problems can be solved quite well using simpler techniques such as random forests\\nand ensemble methods (discussed in Part I). deep learning is best suited for complex\\nproblems such as image recognition, speech recognition, or natural language processing,\\nand it requires a lot of data, computing power, and patience (unless you can leverage a\\npretrained neural network, as you will see).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 14}, page_content='Changes Between the First and the Second Edition\\nIf you have already read the first edition, here are the main changes between\\nthe first and the second edition:\\nAll the code was migrated from TensorFlow 1.x to TensorFlow 2.x, and\\nI replaced most of the low-level TensorFlow code (graphs, sessions,\\nfeature columns, estimators, and so on) with much simpler Keras code.\\nThe second edition introduced the Data API for loading and\\npreprocessing large datasets, the distribution strategies API to train and\\ndeploy TF models at scale, TF Serving and Google Cloud AI Platform to\\nproductionize models, and (briefly) TF Transform, TFLite, TF\\nAddons/Seq2Seq, TensorFlow.js, and TF Agents.\\nIt also introduced many additional ML topics, including a new chapter\\non unsupervised learning, computer vision techniques for object\\ndetection and semantic segmentation, handling sequences using\\nconvolutional neural networks (CNNs), natural language processing\\n(NLP) using recurrent neural networks (RNNs), CNNs and transformers,\\nGANs, and more.\\nSee https://homl.info/changes2 for more details.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 15}, page_content='Changes Between the Second and the Third Edition\\nIf you read the second edition, here are the main changes between the second\\nand the third edition:\\nAll the code was updated to the latest library versions. In particular, this\\nthird edition introduces many new additions to Scikit-Learn (e.g.,\\nfeature name tracking, histogram-based gradient boosting, label\\npropagation, and more). It also introduces the Keras Tuner library for\\nhyperparameter tuning, Hugging Face’s Transformers library for natural\\nlanguage processing, and Keras’s new preprocessing and data\\naugmentation layers.\\nSeveral vision models were added (ResNeXt, DenseNet, MobileNet,\\nCSPNet, and EfficientNet), as well as guidelines for choosing the right\\none.\\nChapter 15 now analyzes the Chicago bus and rail ridership data instead\\nof generated time series, and it introduces the ARMA model and its\\nvariants.\\nChapter 16 on natural language processing now builds an English-to-\\nSpanish translation model, first using an encoder–decoder RNN, then\\nusing a transformer model. The chapter also covers language models\\nsuch as Switch Transformers, DistilBERT, T5, and PaLM (with chain-\\nof-thought prompting). In addition, it introduces vision transformers\\n(ViTs) and gives an overview of a few transformer-based visual models,\\nsuch as data-efficient image transformers (DeiTs), Perceiver, and DINO,\\nas well as a brief overview of some large multimodal models, including\\nCLIP, DALL·E, Flamingo, and GATO.\\nChapter 17 on generative learning now introduces diffusion models, and\\nshows how to implement a denoising diffusion probabilistic model\\n(DDPM) from scratch.\\nChapter 19 migrated from Google Cloud AI Platform to Google Vertex'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 16}, page_content='AI, and uses distributed Keras Tuner for large-scale hyperparameter\\nsearch. The chapter now includes TensorFlow.js code that you can\\nexperiment with online. It also introduces additional distributed training\\ntechniques, including PipeDream and Pathways.\\nIn order to allow for all the new content, some sections were moved\\nonline, including installation instructions, kernel principal component\\nanalysis (PCA), mathematical details of Bayesian Gaussian mixtures, TF\\nAgents, and former appendices A (exercise solutions), C (support vector\\nmachine math), and E (extra neural net architectures).\\nSee https://homl.info/changes3 for more details.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 17}, page_content='Other Resources\\nMany excellent resources are available to learn about machine learning. For\\nexample, Andrew Ng’s ML course on Coursera is amazing, although it\\nrequires a significant time investment.\\nThere are also many interesting websites about machine learning, including\\nScikit-Learn’s exceptional User Guide. You may also enjoy Dataquest, which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on\\nQuora.\\nThere are many other introductory books about machine learning. In\\nparticular:\\nJoel Grus’s Data Science from Scratch, 2nd edition (O’Reilly), presents\\nthe fundamentals of machine learning and implements some of the main\\nalgorithms in pure Python (from scratch, as the name suggests).\\nStephen Marsland’s Machine Learning: An Algorithmic Perspective,\\n2nd edition (Chapman & Hall), is a great introduction to machine\\nlearning, covering a wide range of topics in depth with code examples in\\nPython (also from scratch, but using NumPy).\\nSebastian Raschka’s Python Machine Learning, 3rd edition (Packt\\nPublishing), is also a great introduction to machine learning and\\nleverages Python open source libraries (Pylearn 2 and Theano).\\nFrançois Chollet’s Deep Learning with Python, 2nd edition (Manning),\\nis a very practical book that covers a large range of topics in a clear and\\nconcise way, as you might expect from the author of the excellent Keras\\nlibrary. It favors code examples over mathematical theory.\\nAndriy Burkov’s The Hundred-Page Machine Learning Book (self-\\npublished) is very short but covers an impressive range of topics,\\nintroducing them in approachable terms without shying away from the\\nmath equations.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 18}, page_content='Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin’s\\nLearning from Data (AMLBook) is a rather theoretical approach to ML\\nthat provides deep insights, in particular on the bias/variance trade-off\\n(see Chapter 4).\\nStuart Russell and Peter Norvig’s Artificial Intelligence: A Modern\\nApproach, 4th edition (Pearson), is a great (and huge) book covering an\\nincredible amount of topics, including machine learning. It helps put ML\\ninto perspective.\\nJeremy Howard and Sylvain Gugger’s Deep Learning for Coders with\\nfastai and PyTorch (O’Reilly) provides a wonderfully clear and practical\\nintroduction to deep learning using the fastai and PyTorch libraries.\\nFinally, joining ML competition websites such as Kaggle.com will allow you\\nto practice your skills on real-world problems, with help and insights from\\nsome of the best ML professionals out there.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 19}, page_content='Conventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file\\nextensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to\\nprogram elements such as variable or function names, databases, data\\ntypes, environment variables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values\\ndetermined by context.\\nPunctuation\\nTo avoid any confusion, punctutation appears outside of quotes\\nthroughout the book. My apologies to the purists.\\nTIP\\nThis element signifies a tip or suggestion.\\nNOTE\\nThis element signifies a general note.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 20}, page_content='WARNING\\nThis element indicates a warning or caution.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 21}, page_content='O’Reilly Online Learning\\nNOTE\\nFor more than 40 years, O’Reilly Media has provided technology and business training,\\nknowledge, and insight to help companies succeed.\\nOur unique network of experts and innovators share their knowledge and\\nexpertise through books, articles, and our online learning platform.\\nO’Reilly’s online learning platform gives you on-demand access to live\\ntraining courses, in-depth learning paths, interactive coding environments,\\nand a vast collection of text and video from O’Reilly and 200+ other\\npublishers. For more information, visit https://oreilly.com.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 22}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the\\npublisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any\\nadditional information. You can access this page at https://homl.info/oreilly3.\\nEmail bookquestions@oreilly.com to comment or ask technical questions\\nabout this book.\\nFor news and information about our books and courses, visit\\nhttps://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\\nFollow us on Twitter: https://twitter.com/oreillymedia\\nWatch us on YouTube: https://youtube.com/oreillymedia'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 23}, page_content='Acknowledgments\\nNever in my wildest dreams did I imagine that the first and second editions of\\nthis book would get such a large audience. I received so many messages from\\nreaders, many asking questions, some kindly pointing out errata, and most\\nsending me encouraging words. I cannot express how grateful I am to all\\nthese readers for their tremendous support. Thank you all so very much!\\nPlease do not hesitate to file issues on GitHub if you find errors in the code\\nexamples (or just to ask questions), or to submit errata if you find errors in\\nthe text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on. I\\nfind such feedback incredibly motivating. If you find this book helpful, I\\nwould love it if you could share your story with me, either privately (e.g., via\\nLinkedIn) or publicly (e.g., tweet me at @aureliengeron or write an Amazon\\nreview).\\nHuge thanks as well to all the wonderful people who offered their time and\\nexpertise to review this third edition, correcting errors and making countless\\nsuggestions. This edition is so much better thanks to them: Olzhas\\nAkpambetov, George Bonner, François Chollet, Siddha Gangju, Sam\\nGoodman, Matt Harrison, Sasha Sobran, Lewis Tunstall, Leandro von Werra,\\nand my dear brother Sylvain. You are all amazing!\\nI am also very grateful to the many people who supported me along the way,\\nby answering my questions, suggesting improvements, and contributing to\\nthe code on GitHub: in particular, Yannick Assogba, Ian Beauregard, Ulf\\nBissbort, Rick Chao, Peretz Cohen, Kyle Gallatin, Hannes Hapke, Victor\\nKhaustov, Soonson Kwon, Eric Lebigot, Jason Mayes, Laurence Moroney,\\nSara Robinson, Joaquín Ruales, and Yuefeng Zhou.\\nThis book wouldn’t exist without O’Reilly’s fantastic staff, in particular\\nNicole Taché, who gave me insightful feedback and was always cheerful,\\nencouraging, and helpful: I could not dream of a better editor. Big thanks to\\nMichele Cronin as well, who cheered me on through the final chapters and\\nmanaged to get me past the finish line. Thanks to the whole production team,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 24}, page_content='in particular Elizabeth Kelly and Kristen Brown. Thanks as well to Kim\\nCofer for the thorough copyediting, and to Johnny O’Toole, who managed\\nthe relationship with Amazon and answered many of my questions. Thanks to\\nKate Dullea for greatly improving my illustrations. Thanks to Marie\\nBeaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in\\nthis project and helping me define its scope. Thanks to Matt Hacker and all of\\nthe Atlas team for answering all my technical questions regarding formatting,\\nAsciiDoc, MathML, and LaTeX, and thanks to Nick Adams, Rebecca\\nDemarest, Rachel Head, Judith McConville, Helen Monroe, Karen\\nMontgomery, Rachel Roumeliotis, and everyone else at O’Reilly who\\ncontributed to this book.\\nI’ll never forget all the wonderful people who helped me with the first and\\nsecond editions of this book: friends, colleagues, experts, including many\\nmembers of the TensorFlow team. The list is long: Olzhas Akpambetov,\\nKarmel Allison, Martin Andrews, David Andrzejewski, Paige Bailey, Lukas\\nBiewald, Eugene Brevdo, William Chargin, François Chollet, Clément\\nCourbet, Robert Crowe, Mark Daoust, Daniel “Wolff” Dobson, Julien\\nDubois, Mathias Kende, Daniel Kitachewsky, Nick Felt, Bruce Fontaine,\\nJustin Francis, Goldie Gadde, Irene Giannoumis, Ingrid von Glehn, Vincent\\nGuilbeau, Sandeep Gupta, Priya Gupta, Kevin Haas, Eddy Hung,\\nKonstantinos Katsiapis, Viacheslav Kovalevskyi, Jon Krohn, Allen Lavoie,\\nKarim Matrah, Grégoire Mesnil, Clemens Mewald, Dan Moldovan, Dominic\\nMonn, Sean Morgan, Tom O’Malley, James Pack, Alexander Pak, Haesun\\nPark, Alexandre Passos, Ankur Patel, Josh Patterson, André Susano Pinto,\\nAnthony Platanios, Anosh Raj, Oscar Ramirez, Anna Revinskaya, Saurabh\\nSaxena, Salim Sémaoune, Ryan Sepassi, Vitor Sessak, Jiri Simsa, Iain\\nSmears, Xiaodan Song, Christina Sorokin, Michel Tessier, Wiktor Tomczak,\\nDustin Tran, Todd Wang, Pete Warden, Rich Washington, Martin Wicke,\\nEdd Wilder-James, Sam Witteveen, Jason Zaman, Yuefeng Zhou, and my\\nbrother Sylvain.\\nLast but not least, I am infinitely grateful to my beloved wife, Emmanuelle,\\nand to our three wonderful children, Alexandre, Rémi, and Gabrielle, for\\nencouraging me to work hard on this book. Their insatiable curiosity was'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 25}, page_content='priceless: explaining some of the most difficult concepts in this book to my\\nwife and children helped me clarify my thoughts and directly improved many\\nparts of it. Plus, they keep bringing me cookies and coffee, who could ask for\\nmore?\\n1  Geoffrey E. Hinton et al., “A Fast Learning Algorithm for Deep Belief Nets”, Neural\\nComputation 18 (2006): 1527–1554.\\n2  Despite the fact that Yann LeCun’s deep convolutional neural networks had worked well for\\nimage recognition since the 1990s, although they were not as general-purpose.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 26}, page_content='Part I. The Fundamentals of\\nMachine Learning'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 27}, page_content='Chapter 1. The Machine Learning\\nLandscape\\nNot so long ago, if you had picked up your phone and asked it the way home,\\nit would have ignored you—and people would have questioned your sanity.\\nBut machine learning is no longer science fiction: billions of people use it\\nevery day. And the truth is it has actually been around for decades in some\\nspecialized applications, such as optical character recognition (OCR). The\\nfirst ML application that really became mainstream, improving the lives of\\nhundreds of millions of people, took over the world back in the 1990s: the\\nspam filter. It’s not exactly a self-aware robot, but it does technically qualify\\nas machine learning: it has actually learned so well that you seldom need to\\nflag an email as spam anymore. It was followed by hundreds of ML\\napplications that now quietly power hundreds of products and features that\\nyou use regularly: voice prompts, automatic translation, image search,\\nproduct recommendations, and many more.\\nWhere does machine learning start and where does it end? What exactly does\\nit mean for a machine to learn something? If I download a copy of all\\nWikipedia articles, has my computer really learned something? Is it suddenly\\nsmarter? In this chapter I will start by clarifying what machine learning is and\\nwhy you may want to use it.\\nThen, before we set out to explore the machine learning continent, we will\\ntake a look at the map and learn about the main regions and the most notable\\nlandmarks: supervised versus unsupervised learning and their variants, online\\nversus batch learning, instance-based versus model-based learning. Then we\\nwill look at the workflow of a typical ML project, discuss the main\\nchallenges you may face, and cover how to evaluate and fine-tune a machine\\nlearning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every\\ndata scientist should know by heart. It will be a high-level overview (it’s the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 28}, page_content='only chapter without much code), all rather simple, but my goal is to ensure\\neverything is crystal clear to you before we continue on to the rest of the\\nbook. So grab a coffee and let’s get started!\\nTIP\\nIf you are already familiar with machine learning basics, you may want to skip directly to\\nChapter 2. If you are not sure, try to answer all the questions listed at the end of the\\nchapter before moving on.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 29}, page_content='What Is Machine Learning?\\nMachine learning is the science (and art) of programming computers so they\\ncan learn from data.\\nHere is a slightly more general definition:\\n[Machine learning is the] field of study that gives computers the ability to\\nlearn without being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to\\nsome task T and some performance measure P, if its performance on T, as\\nmeasured by P, improves with experience E.\\n—Tom Mitchell, 1997\\nYour spam filter is a machine learning program that, given examples of spam\\nemails (flagged by users) and examples of regular emails (nonspam, also\\ncalled “ham”), can learn to flag spam. The examples that the system uses to\\nlearn are called the training set. Each training example is called a training\\ninstance (or sample). The part of a machine learning system that learns and\\nmakes predictions is called a model. Neural networks and random forests are\\nexamples of models.\\nIn this case, the task T is to flag spam for new emails, the experience E is the\\ntraining data, and the performance measure P needs to be defined; for\\nexample, you can use the ratio of correctly classified emails. This particular\\nperformance measure is called accuracy, and it is often used in classification\\ntasks.\\nIf you just download a copy of all Wikipedia articles, your computer has a lot\\nmore data, but it is not suddenly better at any task. This is not machine\\nlearning.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 30}, page_content='Why Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming\\ntechniques (Figure 1-1):\\n1. First you would examine what spam typically looks like. You might\\nnotice that some words or phrases (such as “4U”, “credit card”, “free”,\\nand “amazing”) tend to come up a lot in the subject line. Perhaps you\\nwould also notice a few other patterns in the sender’s name, the email’s\\nbody, and other parts of the email.\\n2. You would write a detection algorithm for each of the patterns that you\\nnoticed, and your program would flag emails as spam if a number of\\nthese patterns were detected.\\n3. You would test your program and repeat steps 1 and 2 until it was good\\nenough to launch.\\nFigure 1-1. The traditional approach'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 31}, page_content='Since the problem is difficult, your program will likely become a long list of\\ncomplex rules—pretty hard to maintain.\\nIn contrast, a spam filter based on machine learning techniques automatically\\nlearns which words and phrases are good predictors of spam by detecting\\nunusually frequent patterns of words in the spam examples compared to the\\nham examples (Figure 1-2). The program is much shorter, easier to maintain,\\nand most likely more accurate.\\nFigure 1-2. The machine learning approach\\nWhat if spammers notice that all their emails containing “4U” are blocked?\\nThey might start writing “For U” instead. A spam filter using traditional\\nprogramming techniques would need to be updated to flag “For U” emails. If\\nspammers keep working around your spam filter, you will need to keep\\nwriting new rules forever.\\nIn contrast, a spam filter based on machine learning techniques automatically\\nnotices that “For U” has become unusually frequent in spam flagged by users,\\nand it starts flagging them without your intervention (Figure 1-3).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 32}, page_content='Figure 1-3. Automatically adapting to change\\nAnother area where machine learning shines is for problems that either are\\ntoo complex for traditional approaches or have no known algorithm. For\\nexample, consider speech recognition. Say you want to start simple and write\\na program capable of distinguishing the words “one” and “two”. You might\\nnotice that the word “two” starts with a high-pitch sound (“T”), so you could\\nhardcode an algorithm that measures high-pitch sound intensity and use that\\nto distinguish ones and twos\\u2060 —but obviously this technique will not scale to\\nthousands of words spoken by millions of very different people in noisy\\nenvironments and in dozens of languages. The best solution (at least today) is\\nto write an algorithm that learns by itself, given many example recordings for\\neach word.\\nFinally, machine learning can help humans learn (Figure 1-4). ML models\\ncan be inspected to see what they have learned (although for some models\\nthis can be tricky). For instance, once a spam filter has been trained on\\nenough spam, it can easily be inspected to reveal the list of words and\\ncombinations of words that it believes are the best predictors of spam.\\nSometimes this will reveal unsuspected correlations or new trends, and\\nthereby lead to a better understanding of the problem. Digging into large\\namounts of data to discover hidden patterns is called data mining, and\\nmachine learning excels at it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 33}, page_content='Figure 1-4. Machine learning can help humans learn\\nTo summarize, machine learning is great for:\\nProblems for which existing solutions require a lot of fine-tuning or long\\nlists of rules (a machine learning model can often simplify code and\\nperform better than the traditional approach)\\nComplex problems for which using a traditional approach yields no\\ngood solution (the best machine learning techniques can perhaps find a\\nsolution)\\nFluctuating environments (a machine learning system can easily be\\nretrained on new data, always keeping it up to date)\\nGetting insights about complex problems and large amounts of data'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 34}, page_content='Examples of Applications\\nLet’s look at some concrete examples of machine learning tasks, along with\\nthe techniques that can tackle them:\\nAnalyzing images of products on a production line to automatically classify\\nthem\\nThis is image classification, typically performed using convolutional\\nneural networks (CNNs; see Chapter 14) or sometimes transformers (see\\nChapter 16).\\nDetecting tumors in brain scans\\nThis is semantic image segmentation, where each pixel in the image is\\nclassified (as we want to determine the exact location and shape of\\ntumors), typically using CNNs or transformers.\\nAutomatically classifying news articles\\nThis is natural language processing (NLP), and more specifically text\\nclassification, which can be tackled using recurrent neural networks\\n(RNNs) and CNNs, but transformers work even better (see Chapter 16).\\nAutomatically flagging offensive comments on discussion forums\\nThis is also text classification, using the same NLP tools.\\nSummarizing long documents automatically\\nThis is a branch of NLP called text summarization, again using the same\\ntools.\\nCreating a chatbot or a personal assistant\\nThis involves many NLP components, including natural language\\nunderstanding (NLU) and question-answering modules.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 35}, page_content='Forecasting your company’s revenue next year, based on many performance\\nmetrics\\nThis is a regression task (i.e., predicting values) that may be tackled using\\nany regression model, such as a linear regression or polynomial\\nregression model (see Chapter 4), a regression support vector machine\\n(see Chapter 5), a regression random forest (see Chapter 7), or an\\nartificial neural network (see Chapter 10). If you want to take into\\naccount sequences of past performance metrics, you may want to use\\nRNNs, CNNs, or transformers (see Chapters 15 and 16).\\nMaking your app react to voice commands\\nThis is speech recognition, which requires processing audio samples:\\nsince they are long and complex sequences, they are typically processed\\nusing RNNs, CNNs, or transformers (see Chapters 15 and 16).\\nDetecting credit card fraud\\nThis is anomaly detection, which can be tackled using isolation forests,\\nGaussian mixture models (see Chapter 9), or autoencoders (see\\nChapter 17).\\nSegmenting clients based on their purchases so that you can design a\\ndifferent marketing strategy for each segment\\nThis is clustering, which can be achieved using k-means, DBSCAN, and\\nmore (see Chapter 9).\\nRepresenting a complex, high-dimensional dataset in a clear and insightful\\ndiagram\\nThis is data visualization, often involving dimensionality reduction\\ntechniques (see Chapter 8).\\nRecommending a product that a client may be interested in, based on past\\npurchases'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 36}, page_content='This is a recommender system. One approach is to feed past purchases\\n(and other information about the client) to an artificial neural network\\n(see Chapter 10), and get it to output the most likely next purchase. This\\nneural net would typically be trained on past sequences of purchases\\nacross all clients.\\nBuilding an intelligent bot for a game\\nThis is often tackled using reinforcement learning (RL; see Chapter 18),\\nwhich is a branch of machine learning that trains agents (such as bots) to\\npick the actions that will maximize their rewards over time (e.g., a bot\\nmay get a reward every time the player loses some life points), within a\\ngiven environment (such as the game). The famous AlphaGo program\\nthat beat the world champion at the game of Go was built using RL.\\nThis list could go on and on, but hopefully it gives you a sense of the\\nincredible breadth and complexity of the tasks that machine learning can\\ntackle, and the types of techniques that you would use for each task.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 37}, page_content='Types of Machine Learning Systems\\nThere are so many different types of machine learning systems that it is\\nuseful to classify them in broad categories, based on the following criteria:\\nHow they are supervised during training (supervised, unsupervised,\\nsemi-supervised, self-supervised, and others)\\nWhether or not they can learn incrementally on the fly (online versus\\nbatch learning)\\nWhether they work by simply comparing new data points to known data\\npoints, or instead by detecting patterns in the training data and building a\\npredictive model, much like scientists do (instance-based versus model-\\nbased learning)\\nThese criteria are not exclusive; you can combine them in any way you like.\\nFor example, a state-of-the-art spam filter may learn on the fly using a deep\\nneural network model trained using human-provided examples of spam and\\nham; this makes it an online, model-based, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 38}, page_content='Training Supervision\\nML systems can be classified according to the amount and type of\\nsupervision they get during training. There are many categories, but we’ll\\ndiscuss the main ones: supervised learning, unsupervised learning, self-\\nsupervised learning, semi-supervised learning, and reinforcement learning.\\nSupervised learning\\nIn supervised learning, the training set you feed to the algorithm includes the\\ndesired solutions, called labels (Figure 1-5).\\nFigure 1-5. A labeled training set for spam classification (an example of supervised learning)\\nA typical supervised learning task is classification. The spam filter is a good\\nexample of this: it is trained with many example emails along with their class\\n(spam or ham), and it must learn how to classify new emails.\\nAnother typical task is to predict a target numeric value, such as the price of\\na car, given a set of features (mileage, age, brand, etc.). This sort of task is\\ncalled regression (Figure 1-6).\\u2060\\n To train the system, you need to give it\\nmany examples of cars, including both their features and their targets (i.e.,\\ntheir prices).\\nNote that some regression models can be used for classification as well, and\\nvice versa. For example, logistic regression is commonly used for\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 39}, page_content='classification, as it can output a value that corresponds to the probability of\\nbelonging to a given class (e.g., 20% chance of being spam).\\nFigure 1-6. A regression problem: predict a value, given an input feature (there are usually multiple\\ninput features, and sometimes multiple output values)\\nNOTE\\nThe words target and label are generally treated as synonyms in supervised learning, but\\ntarget is more common in regression tasks and label is more common in classification\\ntasks. Moreover, features are sometimes called predictors or attributes. These terms may\\nrefer to individual samples (e.g., “this car’s mileage feature is equal to 15,000”) or to all\\nsamples (e.g., “the mileage feature is strongly correlated with price”).\\nUnsupervised learning\\nIn unsupervised learning, as you might guess, the training data is unlabeled\\n(Figure 1-7). The system tries to learn without a teacher.\\nFor example, say you have a lot of data about your blog’s visitors. You may\\nwant to run a clustering algorithm to try to detect groups of similar visitors\\n(Figure 1-8). At no point do you tell the algorithm which group a visitor\\nbelongs to: it finds those connections without your help. For example, it\\nmight notice that 40% of your visitors are teenagers who love comic books\\nand generally read your blog after school, while 20% are adults who enjoy'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 40}, page_content='sci-fi and who visit during the weekends. If you use a hierarchical clustering\\nalgorithm, it may also subdivide each group into smaller groups. This may\\nhelp you target your posts for each group.\\nFigure 1-7. An unlabeled training set for unsupervised learning\\nFigure 1-8. Clustering\\nVisualization algorithms are also good examples of unsupervised learning:\\nyou feed them a lot of complex and unlabeled data, and they output a 2D or\\n3D representation of your data that can easily be plotted (Figure 1-9). These\\nalgorithms try to preserve as much structure as they can (e.g., trying to keep'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 41}, page_content='separate clusters in the input space from overlapping in the visualization) so\\nthat you can understand how the data is organized and perhaps identify\\nunsuspected patterns.\\nA related task is dimensionality reduction, in which the goal is to simplify the\\ndata without losing too much information. One way to do this is to merge\\nseveral correlated features into one. For example, a car’s mileage may be\\nstrongly correlated with its age, so the dimensionality reduction algorithm\\nwill merge them into one feature that represents the car’s wear and tear. This\\nis called feature extraction.\\nFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters\\u2060\\nTIP\\nIt is often a good idea to try to reduce the number of dimensions in your training data\\nusing a dimensionality reduction algorithm before you feed it to another machine learning\\nalgorithm (such as a supervised learning algorithm). It will run much faster, the data will\\ntake up less disk and memory space, and in some cases it may also perform better.\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 42}, page_content='Yet another important unsupervised task is anomaly detection—for example,\\ndetecting unusual credit card transactions to prevent fraud, catching\\nmanufacturing defects, or automatically removing outliers from a dataset\\nbefore feeding it to another learning algorithm. The system is shown mostly\\nnormal instances during training, so it learns to recognize them; then, when it\\nsees a new instance, it can tell whether it looks like a normal one or whether\\nit is likely an anomaly (see Figure 1-10). A very similar task is novelty\\ndetection: it aims to detect new instances that look different from all instances\\nin the training set. This requires having a very “clean” training set, devoid of\\nany instance that you would like the algorithm to detect. For example, if you\\nhave thousands of pictures of dogs, and 1% of these pictures represent\\nChihuahuas, then a novelty detection algorithm should not treat new pictures\\nof Chihuahuas as novelties. On the other hand, anomaly detection algorithms\\nmay consider these dogs as so rare and so different from other dogs that they\\nwould likely classify them as anomalies (no offense to Chihuahuas).\\nFigure 1-10. Anomaly detection\\nFinally, another common unsupervised task is association rule learning, in\\nwhich the goal is to dig into large amounts of data and discover interesting\\nrelations between attributes. For example, suppose you own a supermarket.\\nRunning an association rule on your sales logs may reveal that people who\\npurchase barbecue sauce and potato chips also tend to buy steak. Thus, you\\nmay want to place these items close to one another.\\nSemi-supervised learning\\nSince labeling data is usually time-consuming and costly, you will often have'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 43}, page_content='plenty of unlabeled instances, and few labeled instances. Some algorithms\\ncan deal with data that’s partially labeled. This is called semi-supervised\\nlearning (Figure 1-11).\\nFigure 1-11. Semi-supervised learning with two classes (triangles and squares): the unlabeled\\nexamples (circles) help classify a new instance (the cross) into the triangle class rather than the square\\nclass, even though it is closer to the labeled squares\\nSome photo-hosting services, such as Google Photos, are good examples of\\nthis. Once you upload all your family photos to the service, it automatically\\nrecognizes that the same person A shows up in photos 1, 5, and 11, while\\nanother person B shows up in photos 2, 5, and 7. This is the unsupervised\\npart of the algorithm (clustering). Now all the system needs is for you to tell\\nit who these people are. Just add one label per person\\u2060\\n and it is able to\\nname everyone in every photo, which is useful for searching photos.\\nMost semi-supervised learning algorithms are combinations of unsupervised\\nand supervised algorithms. For example, a clustering algorithm may be used\\nto group similar instances together, and then every unlabeled instance can be\\nlabeled with the most common label in its cluster. Once the whole dataset is\\nlabeled, it is possible to use any supervised learning algorithm.\\nSelf-supervised learning\\nAnother approach to machine learning involves actually generating a fully\\nlabeled dataset from a fully unlabeled one. Again, once the whole dataset is\\nlabeled, any supervised learning algorithm can be used. This approach is\\ncalled self-supervised learning.\\nFor example, if you have a large dataset of unlabeled images, you can\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 44}, page_content='randomly mask a small part of each image and then train a model to recover\\nthe original image (Figure 1-12). During training, the masked images are\\nused as the inputs to the model, and the original images are used as the labels.\\nFigure 1-12. Self-supervised learning example: input (left) and target (right)\\nThe resulting model may be quite useful in itself—for example, to repair\\ndamaged images or to erase unwanted objects from pictures. But more often\\nthan not, a model trained using self-supervised learning is not the final goal.\\nYou’ll usually want to tweak and fine-tune the model for a slightly different\\ntask—one that you actually care about.\\nFor example, suppose that what you really want is to have a pet classification\\nmodel: given a picture of any pet, it will tell you what species it belongs to. If\\nyou have a large dataset of unlabeled photos of pets, you can start by training\\nan image-repairing model using self-supervised learning. Once it’s\\nperforming well, it should be able to distinguish different pet species: when it\\nrepairs an image of a cat whose face is masked, it must know not to add a\\ndog’s face. Assuming your model’s architecture allows it (and most neural\\nnetwork architectures do), it is then possible to tweak the model so that it\\npredicts pet species instead of repairing images. The final step consists of\\nfine-tuning the model on a labeled dataset: the model already knows what\\ncats, dogs, and other pet species look like, so this step is only needed so the\\nmodel can learn the mapping between the species it already knows and the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 45}, page_content='labels we expect from it.\\nNOTE\\nTransferring knowledge from one task to another is called transfer learning, and it’s one\\nof the most important techniques in machine learning today, especially when using deep\\nneural networks (i.e., neural networks composed of many layers of neurons). We will\\ndiscuss this in detail in Part II.\\nSome people consider self-supervised learning to be a part of unsupervised\\nlearning, since it deals with fully unlabeled datasets. But self-supervised\\nlearning uses (generated) labels during training, so in that regard it’s closer to\\nsupervised learning. And the term “unsupervised learning” is generally used\\nwhen dealing with tasks like clustering, dimensionality reduction, or anomaly\\ndetection, whereas self-supervised learning focuses on the same tasks as\\nsupervised learning: mainly classification and regression. In short, it’s best to\\ntreat self-supervised learning as its own category.\\nReinforcement learning\\nReinforcement learning is a very different beast. The learning system, called\\nan agent in this context, can observe the environment, select and perform\\nactions, and get rewards in return (or penalties in the form of negative\\nrewards, as shown in Figure 1-13). It must then learn by itself what is the best\\nstrategy, called a policy, to get the most reward over time. A policy defines\\nwhat action the agent should choose when it is in a given situation.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 46}, page_content='Figure 1-13. Reinforcement learning\\nFor example, many robots implement reinforcement learning algorithms to\\nlearn how to walk. DeepMind’s AlphaGo program is also a good example of\\nreinforcement learning: it made the headlines in May 2017 when it beat Ke\\nJie, the number one ranked player in the world at the time, at the game of Go.\\nIt learned its winning policy by analyzing millions of games, and then\\nplaying many games against itself. Note that learning was turned off during\\nthe games against the champion; AlphaGo was just applying the policy it had\\nlearned. As you will see in the next section, this is called offline learning.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 47}, page_content='Batch Versus Online Learning\\nAnother criterion used to classify machine learning systems is whether or not\\nthe system can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning, the system is incapable of learning incrementally: it must\\nbe trained using all the available data. This will generally take a lot of time\\nand computing resources, so it is typically done offline. First the system is\\ntrained, and then it is launched into production and runs without learning\\nanymore; it just applies what it has learned. This is called offline learning.\\nUnfortunately, a model’s performance tends to decay slowly over time,\\nsimply because the world continues to evolve while the model remains\\nunchanged. This phenomenon is often called model rot or data drift. The\\nsolution is to regularly retrain the model on up-to-date data. How often you\\nneed to do that depends on the use case: if the model classifies pictures of\\ncats and dogs, its performance will decay very slowly, but if the model deals\\nwith fast-evolving systems, for example making predictions on the financial\\nmarket, then it is likely to decay quite fast.\\nWARNING\\nEven a model trained to classify pictures of cats and dogs may need to be retrained\\nregularly, not because cats and dogs will mutate overnight, but because cameras keep\\nchanging, along with image formats, sharpness, brightness, and size ratios. Moreover,\\npeople may love different breeds next year, or they may decide to dress their pets with tiny\\nhats—who knows?\\nIf you want a batch learning system to know about new data (such as a new\\ntype of spam), you need to train a new version of the system from scratch on\\nthe full dataset (not just the new data, but also the old data), then replace the\\nold model with the new one. Fortunately, the whole process of training,\\nevaluating, and launching a machine learning system can be automated fairly'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 48}, page_content='easily (as we saw in Figure 1-3), so even a batch learning system can adapt to\\nchange. Simply update the data and train a new version of the system from\\nscratch as often as needed.\\nThis solution is simple and often works fine, but training using the full set of\\ndata can take many hours, so you would typically train a new system only\\nevery 24 hours or even just weekly. If your system needs to adapt to rapidly\\nchanging data (e.g., to predict stock prices), then you need a more reactive\\nsolution.\\nAlso, training on the full set of data requires a lot of computing resources\\n(CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a\\nlot of data and you automate your system to train from scratch every day, it\\nwill end up costing you a lot of money. If the amount of data is huge, it may\\neven be impossible to use a batch learning algorithm.\\nFinally, if your system needs to be able to learn autonomously and it has\\nlimited resources (e.g., a smartphone application or a rover on Mars), then\\ncarrying around large amounts of training data and taking up a lot of\\nresources to train for hours every day is a showstopper.\\nA better option in all these cases is to use algorithms that are capable of\\nlearning incrementally.\\nOnline learning\\nIn online learning, you train the system incrementally by feeding it data\\ninstances sequentially, either individually or in small groups called mini-\\nbatches. Each learning step is fast and cheap, so the system can learn about\\nnew data on the fly, as it arrives (see Figure 1-14).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 49}, page_content='Figure 1-14. In online learning, a model is trained and launched into production, and then it keeps\\nlearning as new data comes in\\nOnline learning is useful for systems that need to adapt to change extremely\\nrapidly (e.g., to detect new patterns in the stock market). It is also a good\\noption if you have limited computing resources; for example, if the model is\\ntrained on a mobile device.\\nAdditionally, online learning algorithms can be used to train models on huge\\ndatasets that cannot fit in one machine’s main memory (this is called out-of-\\ncore learning). The algorithm loads part of the data, runs a training step on\\nthat data, and repeats the process until it has run on all of the data (see\\nFigure 1-15).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 50}, page_content='Figure 1-15. Using online learning to handle huge datasets\\nOne important parameter of online learning systems is how fast they should\\nadapt to changing data: this is called the learning rate. If you set a high\\nlearning rate, then your system will rapidly adapt to new data, but it will also\\ntend to quickly forget the old data (and you don’t want a spam filter to flag\\nonly the latest kinds of spam it was shown). Conversely, if you set a low\\nlearning rate, the system will have more inertia; that is, it will learn more\\nslowly, but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points (outliers).\\nWARNING\\nOut-of-core learning is usually done offline (i.e., not on the live system), so online\\nlearning can be a confusing name. Think of it as incremental learning.\\nA big challenge with online learning is that if bad data is fed to the system,\\nthe system’s performance will decline, possibly quickly (depending on the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 51}, page_content='data quality and learning rate). If it’s a live system, your clients will notice.\\nFor example, bad data could come from a bug (e.g., a malfunctioning sensor\\non a robot), or it could come from someone trying to game the system (e.g.,\\nspamming a search engine to try to rank high in search results). To reduce\\nthis risk, you need to monitor your system closely and promptly switch\\nlearning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. You may also want to monitor the input data and react\\nto abnormal data; for example, using an anomaly detection algorithm (see\\nChapter 9).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 52}, page_content='Instance-Based Versus Model-Based Learning\\nOne more way to categorize machine learning systems is by how they\\ngeneralize. Most machine learning tasks are about making predictions. This\\nmeans that given a number of training examples, the system needs to be able\\nto make good predictions for (generalize to) examples it has never seen\\nbefore. Having a good performance measure on the training data is good, but\\ninsufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you\\nwere to create a spam filter this way, it would just flag all emails that are\\nidentical to emails that have already been flagged by users—not the worst\\nsolution, but certainly not the best.\\nInstead of just flagging emails that are identical to known spam emails, your\\nspam filter could be programmed to also flag emails that are very similar to\\nknown spam emails. This requires a measure of similarity between two\\nemails. A (very basic) similarity measure between two emails could be to\\ncount the number of words they have in common. The system would flag an\\nemail as spam if it has many words in common with a known spam email.\\nThis is called instance-based learning: the system learns the examples by\\nheart, then generalizes to new cases by using a similarity measure to compare\\nthem to the learned examples (or a subset of them). For example, in Figure 1-\\n16 the new instance would be classified as a triangle because the majority of\\nthe most similar instances belong to that class.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 53}, page_content='Figure 1-16. Instance-based learning\\nModel-based learning and a typical machine learning workflow\\nAnother way to generalize from a set of examples is to build a model of these\\nexamples and then use that model to make predictions. This is called model-\\nbased learning (Figure 1-17).\\nFigure 1-17. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so\\nyou download the Better Life Index data from the OECD’s website and\\nWorld Bank stats about gross domestic product (GDP) per capita. Then you\\njoin the tables and sort by GDP per capita. Table 1-1 shows an excerpt of'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 54}, page_content='what you get.\\nTable 1-1. Does money make people happier?\\nCountry\\nGDP per capita (USD) Life satisfaction\\nTurkey\\n28,384\\n5.5\\nHungary\\n31,008\\n5.6\\nFrance\\n42,026\\n6.5\\nUnited States\\n60,236\\n6.9\\nNew Zealand\\n42,404\\n7.3\\nAustralia\\n48,698\\n7.3\\nDenmark\\n55,938\\n7.6\\nLet’s plot the data for these countries (Figure 1-18).\\nFigure 1-18. Do you see a trend here?\\nThere does seem to be a trend here! Although the data is noisy (i.e., partly'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 55}, page_content='random), it looks like life satisfaction goes up more or less linearly as the\\ncountry’s GDP per capita increases. So you decide to model life satisfaction\\nas a linear function of GDP per capita. This step is called model selection:\\nyou selected a linear model of life satisfaction with just one attribute, GDP\\nper capita (Equation 1-1).\\nEquation 1-1. A simple linear model\\nlife_satisfaction = θ 0 + θ 1 × GDP_per_capita\\nThis model has two model parameters, θ  and θ .\\u2060\\n By tweaking these\\nparameters, you can make your model represent any linear function, as shown\\nin Figure 1-19.\\nFigure 1-19. A few possible linear models\\nBefore you can use your model, you need to define the parameter values θ\\nand θ . How can you know which values will make your model perform best?\\nTo answer this question, you need to specify a performance measure. You\\ncan either define a utility function (or fitness function) that measures how\\ngood your model is, or you can define a cost function that measures how bad\\nit is. For linear regression problems, people typically use a cost function that\\nmeasures the distance between the linear model’s predictions and the training\\n0\\n1\\n4\\n0\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 56}, page_content='examples; the objective is to minimize this distance.\\nThis is where the linear regression algorithm comes in: you feed it your\\ntraining examples, and it finds the parameters that make the linear model fit\\nbest to your data. This is called training the model. In our case, the algorithm\\nfinds that the optimal parameter values are θ  = 3.75 and θ  = 6.78 × 10 .\\nWARNING\\nConfusingly, the word “model” can refer to a type of model (e.g., linear regression), to a\\nfully specified model architecture (e.g., linear regression with one input and one output),\\nor to the final trained model ready to be used for predictions (e.g., linear regression with\\none input and one output, using θ  = 3.75 and θ  = 6.78 × 10 ). Model selection consists\\nin choosing the type of model and fully specifying its architecture. Training a model\\nmeans running an algorithm to find the model parameters that will make it best fit the\\ntraining data, and hopefully make good predictions on new data.\\nNow the model fits the training data as closely as possible (for a linear\\nmodel), as you can see in Figure 1-20.\\nFigure 1-20. The linear model that fits the training data best\\nYou are finally ready to run the model to make predictions. For example, say\\n0\\n1\\n–5\\n0\\n1\\n–5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 57}, page_content='you want to know how happy Cypriots are, and the OECD data does not have\\nthe answer. Fortunately, you can use your model to make a good prediction:\\nyou look up Cyprus’s GDP per capita, find $37,655, and then apply your\\nmodel and find that life satisfaction is likely to be somewhere around 3.75 +\\n37,655 × 6.78 × 10  = 6.30.\\nTo whet your appetite, Example 1-1 shows the Python code that loads the\\ndata, separates the inputs X from the labels y, creates a scatterplot for\\nvisualization, and then trains a linear model and makes a prediction.\\u2060\\nExample 1-1. Training and running a linear model using Scikit-Learn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LinearRegression\\n# Download and prepare the data\\ndata_root = \"https://github.com/ageron/data/raw/main/\"\\nlifesat = pd.read_csv(data_root + \"lifesat/lifesat.csv\")\\nX = lifesat[[\"GDP per capita (USD)\"]].values\\ny = lifesat[[\"Life satisfaction\"]].values\\n# Visualize the data\\nlifesat.plot(kind=\\'scatter\\', grid=True,\\n            x=\"GDP per capita (USD)\", y=\"Life satisfaction\")\\nplt.axis([23_500, 62_500, 4, 9])\\nplt.show()\\n# Select a linear model\\nmodel = LinearRegression()\\n# Train the model\\nmodel.fit(X, y)\\n# Make a prediction for Cyprus\\nX_new = [[37_655.2]]  # Cyprus\\' GDP per capita in 2020\\nprint(model.predict(X_new)) # output: [[6.30165767]]\\nNOTE\\nIf you had used an instance-based learning algorithm instead, you would have found that\\nIsrael has the closest GDP per capita to that of Cyprus ($38,341), and since the OECD\\ndata tells us that Israelis’ life satisfaction is 7.2, you would have predicted a life\\n–5\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 58}, page_content='satisfaction of 7.2 for Cyprus. If you zoom out a bit and look at the two next-closest\\ncountries, you will find Lithuania and Slovenia, both with a life satisfaction of 5.9.\\nAveraging these three values, you get 6.33, which is pretty close to your model-based\\nprediction. This simple algorithm is called k-nearest neighbors regression (in this\\nexample, k = 3).\\nReplacing the linear regression model with k-nearest neighbors regression in the previous\\ncode is as easy as replacing these lines:\\nfrom sklearn.linear_model import LinearRegression\\nmodel = LinearRegression()\\nwith these two:\\nfrom sklearn.neighbors import KNeighborsRegressor\\nmodel = KNeighborsRegressor(n_neighbors=3)\\nIf all went well, your model will make good predictions. If not, you may need\\nto use more attributes (employment rate, health, air pollution, etc.), get more\\nor better-quality training data, or perhaps select a more powerful model (e.g.,\\na polynomial regression model).\\nIn summary:\\nYou studied the data.\\nYou selected a model.\\nYou trained it on the training data (i.e., the learning algorithm searched\\nfor the model parameter values that minimize a cost function).\\nFinally, you applied the model to make predictions on new cases (this is\\ncalled inference), hoping that this model will generalize well.\\nThis is what a typical machine learning project looks like. In Chapter 2 you\\nwill experience this firsthand by going through a project end to end.\\nWe have covered a lot of ground so far: you now know what machine\\nlearning is really about, why it is useful, what some of the most common'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 59}, page_content='categories of ML systems are, and what a typical project workflow looks like.\\nNow let’s look at what can go wrong in learning and prevent you from\\nmaking accurate predictions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 60}, page_content='Main Challenges of Machine Learning\\nIn short, since your main task is to select a model and train it on some data,\\nthe two things that can go wrong are “bad model” and “bad data”. Let’s start\\nwith examples of bad data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 61}, page_content='Insufficient Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an\\napple and say “apple” (possibly repeating this procedure a few times). Now\\nthe child is able to recognize apples in all sorts of colors and shapes. Genius.\\nMachine learning is not quite there yet; it takes a lot of data for most machine\\nlearning algorithms to work properly. Even for very simple problems you\\ntypically need thousands of examples, and for complex problems such as\\nimage or speech recognition you may need millions of examples (unless you\\ncan reuse parts of an existing model).\\nTHE UNREASONABLE EFFECTIVENESS OF DATA\\nIn a famous paper published in 2001, Microsoft researchers Michele\\nBanko and Eric Brill showed that very different machine learning\\nalgorithms, including fairly simple ones, performed almost identically\\nwell on a complex problem of natural language disambiguation\\u2060\\n once\\nthey were given enough data (as you can see in Figure 1-21).\\nAs the authors put it, “these results suggest that we may want to\\nreconsider the trade-off between spending time and money on algorithm\\ndevelopment versus spending it on corpus development”.\\nThe idea that data matters more than algorithms for complex problems\\nwas further popularized by Peter Norvig et al. in a paper titled “The\\nUnreasonable Effectiveness of Data”, published in 2009.\\u2060\\n It should be\\nnoted, however, that small and medium-sized datasets are still very\\ncommon, and it is not always easy or cheap to get extra training data\\u2060 —\\nso don’t abandon algorithms just yet.\\n6\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 62}, page_content='Figure 1-21. The importance of data versus algorithms\\u2060\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 63}, page_content='Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be\\nrepresentative of the new cases you want to generalize to. This is true\\nwhether you use instance-based learning or model-based learning.\\nFor example, the set of countries you used earlier for training the linear\\nmodel was not perfectly representative; it did not contain any country with a\\nGDP per capita lower than $23,500 or higher than $62,500. Figure 1-22\\nshows what the data looks like when you add such countries.\\nIf you train a linear model on this data, you get the solid line, while the old\\nmodel is represented by the dotted line. As you can see, not only does adding\\na few missing countries significantly alter the model, but it makes it clear that\\nsuch a simple linear model is probably never going to work well. It seems\\nthat very rich countries are not happier than moderately rich countries (in\\nfact, they seem slightly unhappier!), and conversely some poor countries\\nseem happier than many rich countries.\\nBy using a nonrepresentative training set, you trained a model that is unlikely\\nto make accurate predictions, especially for very poor and very rich countries.\\nFigure 1-22. A more representative training sample\\nIt is crucial to use a training set that is representative of the cases you want to\\ngeneralize to. This is often harder than it sounds: if the sample is too small,\\nyou will have sampling noise (i.e., nonrepresentative data as a result of\\nchance), but even very large samples can be nonrepresentative if the sampling'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 64}, page_content='method is flawed. This is called sampling bias.\\nEXAMPLES OF SAMPLING BIAS\\nPerhaps the most famous example of sampling bias happened during the\\nUS presidential election in 1936, which pitted Landon against Roosevelt:\\nthe Literary Digest conducted a very large poll, sending mail to about 10\\nmillion people. It got 2.4 million answers, and predicted with high\\nconfidence that Landon would get 57% of the votes. Instead, Roosevelt\\nwon with 62% of the votes. The flaw was in the Literary Digest’s\\nsampling method:\\nFirst, to obtain the addresses to send the polls to, the Literary Digest\\nused telephone directories, lists of magazine subscribers, club\\nmembership lists, and the like. All of these lists tended to favor\\nwealthier people, who were more likely to vote Republican (hence\\nLandon).\\nSecond, less than 25% of the people who were polled answered.\\nAgain this introduced a sampling bias, by potentially ruling out\\npeople who didn’t care much about politics, people who didn’t like\\nthe Literary Digest, and other key groups. This is a special type of\\nsampling bias called nonresponse bias.\\nHere is another example: say you want to build a system to recognize\\nfunk music videos. One way to build your training set is to search for\\n“funk music” on YouTube and use the resulting videos. But this assumes\\nthat YouTube’s search engine returns a set of videos that are\\nrepresentative of all the funk music videos on YouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you\\nlive in Brazil you will get a lot of “funk carioca” videos, which sound\\nnothing like James Brown). On the other hand, how else can you get a\\nlarge training set?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 65}, page_content='Poor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due\\nto poor-quality measurements), it will make it harder for the system to detect\\nthe underlying patterns, so your system is less likely to perform well. It is\\noften well worth the effort to spend time cleaning up your training data. The\\ntruth is, most data scientists spend a significant part of their time doing just\\nthat. The following are a couple examples of when you’d want to clean up\\ntraining data:\\nIf some instances are clearly outliers, it may help to simply discard them\\nor try to fix the errors manually.\\nIf some instances are missing a few features (e.g., 5% of your customers\\ndid not specify their age), you must decide whether you want to ignore\\nthis attribute altogether, ignore these instances, fill in the missing values\\n(e.g., with the median age), or train one model with the feature and one\\nmodel without it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 66}, page_content='Irrelevant Features\\nAs the saying goes: garbage in, garbage out. Your system will only be\\ncapable of learning if the training data contains enough relevant features and\\nnot too many irrelevant ones. A critical part of the success of a machine\\nlearning project is coming up with a good set of features to train on. This\\nprocess, called feature engineering, involves the following steps:\\nFeature selection (selecting the most useful features to train on among\\nexisting features)\\nFeature extraction (combining existing features to produce a more\\nuseful one\\u2060 —as we saw earlier, dimensionality reduction algorithms\\ncan help)\\nCreating new features by gathering new data\\nNow that we have looked at many examples of bad data, let’s look at a\\ncouple examples of bad algorithms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 67}, page_content='Overfitting the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. You\\nmight be tempted to say that all taxi drivers in that country are thieves.\\nOvergeneralizing is something that we humans do all too often, and\\nunfortunately machines can fall into the same trap if we are not careful. In\\nmachine learning this is called overfitting: it means that the model performs\\nwell on the training data, but it does not generalize well.\\nFigure 1-23 shows an example of a high-degree polynomial life satisfaction\\nmodel that strongly overfits the training data. Even though it performs much\\nbetter on the training data than the simple linear model, would you really\\ntrust its predictions?\\nFigure 1-23. Overfitting the training data\\nComplex models such as deep neural networks can detect subtle patterns in\\nthe data, but if the training set is noisy, or if it is too small, which introduces\\nsampling noise, then the model is likely to detect patterns in the noise itself\\n(as in the taxi driver example). Obviously these patterns will not generalize to\\nnew instances. For example, say you feed your life satisfaction model many\\nmore attributes, including uninformative ones such as the country’s name. In\\nthat case, a complex model may detect patterns like the fact that all countries\\nin the training data with a w in their name have a life satisfaction greater than\\n7: New Zealand (7.3), Norway (7.6), Sweden (7.3), and Switzerland (7.5).\\nHow confident are you that the w-satisfaction rule generalizes to Rwanda or\\nZimbabwe? Obviously this pattern occurred in the training data by pure'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 68}, page_content='chance, but the model has no way to tell whether a pattern is real or simply\\nthe result of noise in the data.\\nWARNING\\nOverfitting happens when the model is too complex relative to the amount and noisiness\\nof the training data. Here are possible solutions:\\nSimplify the model by selecting one with fewer parameters (e.g., a linear model\\nrather than a high-degree polynomial model), by reducing the number of attributes\\nin the training data, or by constraining the model.\\nGather more training data.\\nReduce the noise in the training data (e.g., fix data errors and remove outliers).\\nConstraining a model to make it simpler and reduce the risk of overfitting is\\ncalled regularization. For example, the linear model we defined earlier has\\ntwo parameters, θ  and θ . This gives the learning algorithm two degrees of\\nfreedom to adapt the model to the training data: it can tweak both the height\\n(θ ) and the slope (θ ) of the line. If we forced θ  = 0, the algorithm would\\nhave only one degree of freedom and would have a much harder time fitting\\nthe data properly: all it could do is move the line up or down to get as close\\nas possible to the training instances, so it would end up around the mean. A\\nvery simple model indeed! If we allow the algorithm to modify θ  but we\\nforce it to keep it small, then the learning algorithm will effectively have\\nsomewhere in between one and two degrees of freedom. It will produce a\\nmodel that’s simpler than one with two degrees of freedom, but more\\ncomplex than one with just one. You want to find the right balance between\\nfitting the training data perfectly and keeping the model simple enough to\\nensure that it will generalize well.\\nFigure 1-24 shows three models. The dotted line represents the original\\nmodel that was trained on the countries represented as circles (without the\\ncountries represented as squares), the solid line is our second model trained\\nwith all countries (circles and squares), and the dashed line is a model trained\\n0\\n1\\n0\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 69}, page_content='with the same data as the first model but with a regularization constraint. You\\ncan see that regularization forced the model to have a smaller slope: this\\nmodel does not fit the training data (circles) as well as the first model, but it\\nactually generalizes better to new examples that it did not see during training\\n(squares).\\nFigure 1-24. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a\\nhyperparameter. A hyperparameter is a parameter of a learning algorithm\\n(not of the model). As such, it is not affected by the learning algorithm itself;\\nit must be set prior to training and remains constant during training. If you set\\nthe regularization hyperparameter to a very large value, you will get an\\nalmost flat model (a slope close to zero); the learning algorithm will almost\\ncertainly not overfit the training data, but it will be less likely to find a good\\nsolution. Tuning hyperparameters is an important part of building a machine\\nlearning system (you will see a detailed example in the next chapter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 70}, page_content='Underfitting the Training Data\\nAs you might guess, underfitting is the opposite of overfitting: it occurs when\\nyour model is too simple to learn the underlying structure of the data. For\\nexample, a linear model of life satisfaction is prone to underfit; reality is just\\nmore complex than the model, so its predictions are bound to be inaccurate,\\neven on the training examples.\\nHere are the main options for fixing this problem:\\nSelect a more powerful model, with more parameters.\\nFeed better features to the learning algorithm (feature engineering).\\nReduce the constraints on the model (for example by reducing the\\nregularization hyperparameter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 71}, page_content='Stepping Back\\nBy now you know a lot about machine learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and\\nlook at the big picture:\\nMachine learning is about making machines get better at some task by\\nlearning from data, instead of having to explicitly code rules.\\nThere are many different types of ML systems: supervised or not, batch\\nor online, instance-based or model-based.\\nIn an ML project you gather data in a training set, and you feed the\\ntraining set to a learning algorithm. If the algorithm is model-based, it\\ntunes some parameters to fit the model to the training set (i.e., to make\\ngood predictions on the training set itself), and then hopefully it will be\\nable to make good predictions on new cases as well. If the algorithm is\\ninstance-based, it just learns the examples by heart and generalizes to\\nnew instances by using a similarity measure to compare them to the\\nlearned instances.\\nThe system will not perform well if your training set is too small, or if\\nthe data is not representative, is noisy, or is polluted with irrelevant\\nfeatures (garbage in, garbage out). Lastly, your model needs to be\\nneither too simple (in which case it will underfit) nor too complex (in\\nwhich case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model,\\nyou don’t want to just “hope” it generalizes to new cases. You want to\\nevaluate it and fine-tune it if necessary. Let’s see how to do that.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 72}, page_content='Testing and Validating\\nThe only way to know how well a model will generalize to new cases is to\\nactually try it out on new cases. One way to do that is to put your model in\\nproduction and monitor how well it performs. This works well, but if your\\nmodel is horribly bad, your users will complain—not the best idea.\\nA better option is to split your data into two sets: the training set and the test\\nset. As these names imply, you train your model using the training set, and\\nyou test it using the test set. The error rate on new cases is called the\\ngeneralization error (or out-of-sample error), and by evaluating your model\\non the test set, you get an estimate of this error. This value tells you how well\\nyour model will perform on instances it has never seen before.\\nIf the training error is low (i.e., your model makes few mistakes on the\\ntraining set) but the generalization error is high, it means that your model is\\noverfitting the training data.\\nTIP\\nIt is common to use 80% of the data for training and hold out 20% for testing. However,\\nthis depends on the size of the dataset: if it contains 10 million instances, then holding out\\n1% means your test set will contain 100,000 instances, probably more than enough to get a\\ngood estimate of the generalization error.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 73}, page_content='Hyperparameter Tuning and Model Selection\\nEvaluating a model is simple enough: just use a test set. But suppose you are\\nhesitating between two types of models (say, a linear model and a polynomial\\nmodel): how can you decide between them? One option is to train both and\\ncompare how well they generalize using the test set.\\nNow suppose that the linear model generalizes better, but you want to apply\\nsome regularization to avoid overfitting. The question is, how do you choose\\nthe value of the regularization hyperparameter? One option is to train 100\\ndifferent models using 100 different values for this hyperparameter. Suppose\\nyou find the best hyperparameter value that produces a model with the lowest\\ngeneralization error\\u2060 —say, just 5% error. You launch this model into\\nproduction, but unfortunately it does not perform as well as expected and\\nproduces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on\\nthe test set, and you adapted the model and hyperparameters to produce the\\nbest model for that particular set. This means the model is unlikely to\\nperform as well on new data.\\nA common solution to this problem is called holdout validation (Figure 1-\\n25): you simply hold out part of the training set to evaluate several candidate\\nmodels and select the best one. The new held-out set is called the validation\\nset (or the development set, or dev set). More specifically, you train multiple\\nmodels with various hyperparameters on the reduced training set (i.e., the full\\ntraining set minus the validation set), and you select the model that performs\\nbest on the validation set. After this holdout validation process, you train the\\nbest model on the full training set (including the validation set), and this\\ngives you the final model. Lastly, you evaluate this final model on the test set\\nto get an estimate of the generalization error.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 74}, page_content='Figure 1-25. Model selection using holdout validation\\nThis solution usually works quite well. However, if the validation set is too\\nsmall, then the model evaluations will be imprecise: you may end up\\nselecting a suboptimal model by mistake. Conversely, if the validation set is\\ntoo large, then the remaining training set will be much smaller than the full\\ntraining set. Why is this bad? Well, since the final model will be trained on\\nthe full training set, it is not ideal to compare candidate models trained on a\\nmuch smaller training set. It would be like selecting the fastest sprinter to\\nparticipate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation, using many small validation sets. Each model is\\nevaluated once per validation set after it is trained on the rest of the data. By\\naveraging out all the evaluations of a model, you get a much more accurate\\nmeasure of its performance. There is a drawback, however: the training time\\nis multiplied by the number of validation sets.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 75}, page_content='Data Mismatch\\nIn some cases, it’s easy to get a large amount of data for training, but this\\ndata probably won’t be perfectly representative of the data that will be used\\nin production. For example, suppose you want to create a mobile app to take\\npictures of flowers and automatically determine their species. You can easily\\ndownload millions of pictures of flowers on the web, but they won’t be\\nperfectly representative of the pictures that will actually be taken using the\\napp on a mobile device. Perhaps you only have 1,000 representative pictures\\n(i.e., actually taken with the app).\\nIn this case, the most important rule to remember is that both the validation\\nset and the test set must be as representative as possible of the data you\\nexpect to use in production, so they should be composed exclusively of\\nrepresentative pictures: you can shuffle them and put half in the validation set\\nand half in the test set (making sure that no duplicates or near-duplicates end\\nup in both sets). After training your model on the web pictures, if you\\nobserve that the performance of the model on the validation set is\\ndisappointing, you will not know whether this is because your model has\\noverfit the training set, or whether this is just due to the mismatch between\\nthe web pictures and the mobile app pictures.\\nOne solution is to hold out some of the training pictures (from the web) in yet\\nanother set that Andrew Ng dubbed the train-dev set (Figure 1-26). After the\\nmodel is trained (on the training set, not on the train-dev set), you can\\nevaluate it on the train-dev set. If the model performs poorly, then it must\\nhave overfit the training set, so you should try to simplify or regularize the\\nmodel, get more training data, and clean up the training data. But if it\\nperforms well on the train-dev set, then you can evaluate the model on the\\ndev set. If it performs poorly, then the problem must be coming from the data\\nmismatch. You can try to tackle this problem by preprocessing the web\\nimages to make them look more like the pictures that will be taken by the\\nmobile app, and then retraining the model. Once you have a model that\\nperforms well on both the train-dev set and the dev set, you can evaluate it\\none last time on the test set to know how well it is likely to perform in'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 76}, page_content='production.\\nFigure 1-26. When real data is scarce (right), you may use similar abundant data (left) for training and\\nhold out some of it in a train-dev set to evaluate overfitting; the real data is then used to evaluate data\\nmismatch (dev set) and to evaluate the final model’s performance (test set)\\nNO FREE LUNCH THEOREM\\nA model is a simplified representation of the data. The simplifications are\\nmeant to discard the superfluous details that are unlikely to generalize to\\nnew instances. When you select a particular type of model, you are\\nimplicitly making assumptions about the data. For example, if you choose\\na linear model, you are implicitly assuming that the data is fundamentally\\nlinear and that the distance between the instances and the straight line is\\njust noise, which can safely be ignored.\\nIn a famous 1996 paper,\\u2060\\n David Wolpert demonstrated that if you make\\nabsolutely no assumption about the data, then there is no reason to prefer\\none model over any other. This is called the No Free Lunch (NFL)\\ntheorem. For some datasets the best model is a linear model, while for\\nother datasets it is a neural network. There is no model that is a priori\\nguaranteed to work better (hence the name of the theorem). The only way\\nto know for sure which model is best is to evaluate them all. Since this is\\nnot possible, in practice you make some reasonable assumptions about\\nthe data and evaluate only a few reasonable models. For example, for\\nsimple tasks you may evaluate linear models with various levels of\\nregularization, and for a complex problem you may evaluate various\\nneural networks.\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 77}, page_content='Exercises\\nIn this chapter we have covered some of the most important concepts in\\nmachine learning. In the next chapters we will dive deeper and write more\\ncode, but before we do, make sure you can answer the following questions:\\n1. How would you define machine learning?\\n2. Can you name four types of applications where it shines?\\n3. What is a labeled training set?\\n4. What are the two most common supervised tasks?\\n5. Can you name four common unsupervised tasks?\\n6. What type of algorithm would you use to allow a robot to walk in\\nvarious unknown terrains?\\n7. What type of algorithm would you use to segment your customers into\\nmultiple groups?\\n8. Would you frame the problem of spam detection as a supervised\\nlearning problem or an unsupervised learning problem?\\n9. What is an online learning system?\\n10. What is out-of-core learning?\\n11. What type of algorithm relies on a similarity measure to make\\npredictions?\\n12. What is the difference between a model parameter and a model\\nhyperparameter?\\n13. What do model-based algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14. Can you name four of the main challenges in machine learning?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 78}, page_content='15. If your model performs great on the training data but generalizes poorly\\nto new instances, what is happening? Can you name three possible\\nsolutions?\\n16. What is a test set, and why would you want to use it?\\n17. What is the purpose of a validation set?\\n18. What is the train-dev set, when do you need it, and how do you use it?\\n19. What can go wrong if you tune hyperparameters using the test set?\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was\\nstudying the fact that the children of tall people tend to be shorter than their parents. Since the\\nchildren were shorter, he called this regression to the mean. This name was then applied to the\\nmethods he used to analyze correlations between variables.\\n2  Notice how animals are rather well separated from vehicles and how horses are close to deer but\\nfar from birds. Figure reproduced with permission from Richard Socher et al., “Zero-Shot\\nLearning Through Cross-Modal Transfer”, Proceedings of the 26th International Conference on\\nNeural Information Processing Systems 1 (2013): 935–943.\\n3  That’s when the system works perfectly. In practice it often creates a few clusters per person,\\nand sometimes mixes up two people who look alike, so you may need to provide a few labels per\\nperson and manually clean up some clusters.\\n4  By convention, the Greek letter θ (theta) is frequently used to represent model parameters.\\n5  It’s OK if you don’t understand all the code yet; I will present Scikit-Learn in the following\\nchapters.\\n6  For example, knowing whether to write “to”, “two”, or “too”, depending on the context.\\n7  Peter Norvig et al., “The Unreasonable Effectiveness of Data”, IEEE Intelligent Systems 24, no.\\n2 (2009): 8–12.\\n8  Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very Very\\nLarge Corpora for Natural Language Disambiguation”, Proceedings of the 39th Annual Meeting\\nof the Association for Computational Linguistics (2001): 26–33.\\n9  David Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms”, Neural\\nComputation 8, no. 7 (1996): 1341–1390.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 79}, page_content='Chapter 2. End-to-End Machine\\nLearning Project\\nIn this chapter you will work through an example project end to end,\\npretending to be a recently hired data scientist at a real estate company. This\\nexample is fictitious; the goal is to illustrate the main steps of a machine\\nlearning project, not to learn anything about the real estate business. Here are\\nthe main steps we will walk through:\\n1. Look at the big picture.\\n2. Get the data.\\n3. Explore and visualize the data to gain insights.\\n4. Prepare the data for machine learning algorithms.\\n5. Select a model and train it.\\n6. Fine-tune your model.\\n7. Present your solution.\\n8. Launch, monitor, and maintain your system.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 80}, page_content='Working with Real Data\\nWhen you are learning about machine learning, it is best to experiment with\\nreal-world data, not artificial datasets. Fortunately, there are thousands of\\nopen datasets to choose from, ranging across all sorts of domains. Here are a\\nfew places you can look to get data:\\nPopular open data repositories:\\nOpenML.org\\nKaggle.com\\nPapersWithCode.com\\nUC Irvine Machine Learning Repository\\nAmazon’s AWS datasets\\nTensorFlow datasets\\nMeta portals (they list open data repositories):\\nDataPortals.org\\nOpenDataMonitor.eu\\nOther pages listing many popular open data repositories:\\nWikipedia’s list of machine learning datasets\\nQuora.com\\nThe datasets subreddit\\nIn this chapter we’ll use the California Housing Prices dataset from the\\nStatLib repository\\u2060\\n (see Figure 2-1). This dataset is based on data from the\\n1990 California census. It is not exactly recent (a nice house in the Bay Area\\nwas still affordable at the time), but it has many qualities for learning, so we\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 81}, page_content='will pretend it is recent data. For teaching purposes I’ve added a categorical\\nattribute and removed a few features.\\nFigure 2-1. California housing prices'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 82}, page_content='Look at the Big Picture\\nWelcome to the Machine Learning Housing Corporation! Your first task is to\\nuse California census data to build a model of housing prices in the state.\\nThis data includes metrics such as the population, median income, and\\nmedian housing price for each block group in California. Block groups are\\nthe smallest geographical unit for which the US Census Bureau publishes\\nsample data (a block group typically has a population of 600 to 3,000\\npeople). I will call them “districts” for short.\\nYour model should learn from this data and be able to predict the median\\nhousing price in any district, given all the other metrics.\\nTIP\\nSince you are a well-organized data scientist, the first thing you should do is pull out your\\nmachine learning project checklist. You can start with the one in Appendix A; it should\\nwork reasonably well for most machine learning projects, but make sure to adapt it to your\\nneeds. In this chapter we will go through many checklist items, but we will also skip a\\nfew, either because they are self-explanatory or because they will be discussed in later\\nchapters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 83}, page_content='Frame the Problem\\nThe first question to ask your boss is what exactly the business objective is.\\nBuilding a model is probably not the end goal. How does the company expect\\nto use and benefit from this model? Knowing the objective is important\\nbecause it will determine how you frame the problem, which algorithms you\\nwill select, which performance measure you will use to evaluate your model,\\nand how much effort you will spend tweaking it.\\nYour boss answers that your model’s output (a prediction of a district’s\\nmedian housing price) will be fed to another machine learning system (see\\nFigure 2-2), along with many other signals.\\u2060\\n This downstream system will\\ndetermine whether it is worth investing in a given area. Getting this right is\\ncritical, as it directly affects revenue.\\nThe next question to ask your boss is what the current solution looks like (if\\nany). The current situation will often give you a reference for performance, as\\nwell as insights on how to solve the problem. Your boss answers that the\\ndistrict housing prices are currently estimated manually by experts: a team\\ngathers up-to-date information about a district, and when they cannot get the\\nmedian housing price, they estimate it using complex rules.\\nFigure 2-2. A machine learning pipeline for real estate investments\\nThis is costly and time-consuming, and their estimates are not great; in cases\\nwhere they manage to find out the actual median housing price, they often\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 84}, page_content='realize that their estimates were off by more than 30%. This is why the\\ncompany thinks that it would be useful to train a model to predict a district’s\\nmedian housing price, given other data about that district. The census data\\nlooks like a great dataset to exploit for this purpose, since it includes the\\nmedian housing prices of thousands of districts, as well as other data.\\nPIPELINES\\nA sequence of data processing components is called a data pipeline.\\nPipelines are very common in machine learning systems, since there is a\\nlot of data to manipulate and many data transformations to apply.\\nComponents typically run asynchronously. Each component pulls in a\\nlarge amount of data, processes it, and spits out the result in another data\\nstore. Then, some time later, the next component in the pipeline pulls in\\nthis data and spits out its own output. Each component is fairly self-\\ncontained: the interface between components is simply the data store.\\nThis makes the system simple to grasp (with the help of a data flow\\ngraph), and different teams can focus on different components. Moreover,\\nif a component breaks down, the downstream components can often\\ncontinue to run normally (at least for a while) by just using the last output\\nfrom the broken component. This makes the architecture quite robust.\\nOn the other hand, a broken component can go unnoticed for some time if\\nproper monitoring is not implemented. The data gets stale and the overall\\nsystem’s performance drops.\\nWith all this information, you are now ready to start designing your system.\\nFirst, determine what kind of training supervision the model will need: is it a\\nsupervised, unsupervised, semi-supervised, self-supervised, or reinforcement\\nlearning task? And is it a classification task, a regression task, or something\\nelse? Should you use batch learning or online learning techniques? Before\\nyou read on, pause and try to answer these questions for yourself.\\nHave you found the answers? Let’s see. This is clearly a typical supervised\\nlearning task, since the model can be trained with labeled examples (each'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 85}, page_content='instance comes with the expected output, i.e., the district’s median housing\\nprice). It is a typical regression task, since the model will be asked to predict\\na value. More specifically, this is a multiple regression problem, since the\\nsystem will use multiple features to make a prediction (the district’s\\npopulation, the median income, etc.). It is also a univariate regression\\nproblem, since we are only trying to predict a single value for each district. If\\nwe were trying to predict multiple values per district, it would be a\\nmultivariate regression problem. Finally, there is no continuous flow of data\\ncoming into the system, there is no particular need to adjust to changing data\\nrapidly, and the data is small enough to fit in memory, so plain batch learning\\nshould do just fine.\\nTIP\\nIf the data were huge, you could either split your batch learning work across multiple\\nservers (using the MapReduce technique) or use an online learning technique.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 86}, page_content='Select a Performance Measure\\nYour next step is to select a performance measure. A typical performance\\nmeasure for regression problems is the root mean square error (RMSE). It\\ngives an idea of how much error the system typically makes in its predictions,\\nwith a higher weight given to large errors. Equation 2-1 shows the\\nmathematical formula to compute the RMSE.\\nEquation 2-1. Root mean square error (RMSE)\\nRMSE ( X , h ) = 1 m ∑ i=1 m h(x (i) )-y (i) 2\\nNOTATIONS\\nThis equation introduces several very common machine learning\\nnotations that I will use throughout this book:\\nm is the number of instances in the dataset you are measuring the\\nRMSE on.\\nFor example, if you are evaluating the RMSE on a validation\\nset of 2,000 districts, then m = 2,000.\\nx  is a vector of all the feature values (excluding the label) of the i\\ninstance in the dataset, and y  is its label (the desired output value\\nfor that instance).\\nFor example, if the first district in the dataset is located at\\nlongitude –118.29°, latitude 33.91°, and it has 1,416 inhabitants\\nwith a median income of $38,372, and the median house value\\nis $156,400 (ignoring other features for now), then:\\nx (1) = - 118.29 33.91 1,416 38,372\\nand:\\ny (1) = 156,400\\n(i)\\nth\\n(i)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 87}, page_content='X is a matrix containing all the feature values (excluding labels) of\\nall instances in the dataset. There is one row per instance, and the i\\nrow is equal to the transpose of x , noted (x ) .\\u2060\\nFor example, if the first district is as just described, then the\\nmatrix X looks like this:\\nX = (x (1) ) ⊺ (x (2) ) ⊺ ⋮ (x (1999) ) ⊺ (x (2000) ) ⊺ = - 118.29\\n33.91 1,416 38,372 ⋮ ⋮ ⋮ ⋮\\nh is your system’s prediction function, also called a hypothesis.\\nWhen your system is given an instance’s feature vector x , it\\noutputs a predicted value ŷ  = h(x ) for that instance (ŷ is\\npronounced “y-hat”).\\nFor example, if your system predicts that the median housing\\nprice in the first district is $158,400, then ŷ\\n = h(x\\n) =\\n158,400. The prediction error for this district is ŷ\\n – y\\n =\\n2,000.\\nRMSE(X,h) is the cost function measured on the set of examples\\nusing your hypothesis h.\\nWe use lowercase italic font for scalar values (such as m or y ) and\\nfunction names (such as h), lowercase bold font for vectors (such as x ),\\nand uppercase bold font for matrices (such as X).\\nAlthough the RMSE is generally the preferred performance measure for\\nregression tasks, in some contexts you may prefer to use another function.\\nFor example, if there are many outlier districts. In that case, you may\\nconsider using the mean absolute error (MAE, also called the average\\nabsolute deviation), shown in Equation 2-2:\\nEquation 2-2. Mean absolute error (MAE)\\nMAE ( X , h ) = 1 m ∑ i=1 m h ( x (i) ) - y (i)\\nBoth the RMSE and the MAE are ways to measure the distance between two\\nth\\n(i)\\n(i) ⊺\\n3\\n(i)\\n(i)\\n(i)\\n(1)\\n(1)\\n(1)\\n(1)\\n(i)\\n(i)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 88}, page_content='vectors: the vector of predictions and the vector of target values. Various\\ndistance measures, or norms, are possible:\\nComputing the root of a sum of squares (RMSE) corresponds to the\\nEuclidean norm: this is the notion of distance we are all familiar with. It\\nis also called the ℓ norm, noted ∥ · ∥ (or just ∥ · ∥).\\nComputing the sum of absolutes (MAE) corresponds to the ℓ norm,\\nnoted ∥ · ∥. This is sometimes called the Manhattan norm because it\\nmeasures the distance between two points in a city if you can only travel\\nalong orthogonal city blocks.\\nMore generally, the ℓ norm of a vector v containing n elements is\\ndefined as ∥v∥ = (|v |  + |v |  + ... + |v | )\\n. ℓ gives the number of\\nnonzero elements in the vector, and ℓ gives the maximum absolute\\nvalue in the vector.\\nThe higher the norm index, the more it focuses on large values and neglects\\nsmall ones. This is why the RMSE is more sensitive to outliers than the\\nMAE. But when outliers are exponentially rare (like in a bell-shaped curve),\\nthe RMSE performs very well and is generally preferred.\\n2\\n2\\n1\\n1\\nk\\nk\\n1 k\\n2 k\\nn k 1/k\\n0\\n∞'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 89}, page_content='Check the Assumptions\\nLastly, it is good practice to list and verify the assumptions that have been\\nmade so far (by you or others); this can help you catch serious issues early\\non. For example, the district prices that your system outputs are going to be\\nfed into a downstream machine learning system, and you assume that these\\nprices are going to be used as such. But what if the downstream system\\nconverts the prices into categories (e.g., “cheap”, “medium”, or “expensive”)\\nand then uses those categories instead of the prices themselves? In this case,\\ngetting the price perfectly right is not important at all; your system just needs\\nto get the category right. If that’s so, then the problem should have been\\nframed as a classification task, not a regression task. You don’t want to find\\nthis out after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system,\\nyou are confident that they do indeed need the actual prices, not just\\ncategories. Great! You’re all set, the lights are green, and you can start coding\\nnow!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 90}, page_content='Get the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and\\nwalk through the code examples. As I mentioned in the preface, all the code\\nexamples in this book are open source and available online as Jupyter\\nnotebooks, which are interactive documents containing text, images, and\\nexecutable code snippets (Python in our case). In this book I will assume you\\nare running these notebooks on Google Colab, a free service that lets you run\\nany Jupyter notebook directly online, without having to install anything on\\nyour machine. If you want to use another online platform (e.g., Kaggle) or if\\nyou want to install everything locally on your own machine, please see the\\ninstructions on the book’s GitHub page.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 91}, page_content='Running the Code Examples Using Google Colab\\nFirst, open a web browser and visit https://homl.info/colab3: this will lead\\nyou to Google Colab, and it will display the list of Jupyter notebooks for this\\nbook (see Figure 2-3). You will find one notebook per chapter, plus a few\\nextra notebooks and tutorials for NumPy, Matplotlib, Pandas, linear algebra,\\nand differential calculus. For example, if you click\\n02_end_to_end_machine_learning_project.ipynb, the notebook from\\nChapter 2 will open up in Google Colab (see Figure 2-4).\\nA Jupyter notebook is composed of a list of cells. Each cell contains either\\nexecutable code or text. Try double-clicking the first text cell (which contains\\nthe sentence “Welcome to Machine Learning Housing Corp.!”). This will\\nopen the cell for editing. Notice that Jupyter notebooks use Markdown syntax\\nfor formatting (e.g., **bold**, *italics*, # Title, [url](link text), and so on).\\nTry modifying this text, then press Shift-Enter to see the result.\\nFigure 2-3. List of notebooks in Google Colab'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 92}, page_content='Figure 2-4. Your notebook in Google Colab\\nNext, create a new code cell by selecting Insert → “Code cell” from the\\nmenu. Alternatively, you can click the + Code button in the toolbar, or hover\\nyour mouse over the bottom of a cell until you see + Code and + Text appear,\\nthen click + Code. In the new code cell, type some Python code, such as\\nprint(\"Hello World\"), then press Shift-Enter to run this code (or click the ▷\\nbutton on the left side of the cell).\\nIf you’re not logged in to your Google account, you’ll be asked to log in now\\n(if you don’t already have a Google account, you’ll need to create one). Once\\nyou are logged in, when you try to run the code you’ll see a security warning\\ntelling you that this notebook was not authored by Google. A malicious\\nperson could create a notebook that tries to trick you into entering your\\nGoogle credentials so they can access your personal data, so before you run a\\nnotebook, always make sure you trust its author (or double-check what each\\ncode cell will do before running it). Assuming you trust me (or you plan to\\ncheck every code cell), you can now click “Run anyway”.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 93}, page_content='Colab will then allocate a new runtime for you: this is a free virtual machine\\nlocated on Google’s servers that contains a bunch of tools and Python\\nlibraries, including everything you’ll need for most chapters (in some\\nchapters, you’ll need to run a command to install additional libraries). This\\nwill take a few seconds. Next, Colab will automatically connect to this\\nruntime and use it to execute your new code cell. Importantly, the code runs\\non the runtime, not on your machine. The code’s output will be displayed\\nunder the cell. Congrats, you’ve run some Python code on Colab!\\nTIP\\nTo insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed by A\\n(to insert above the active cell) or B (to insert below). There are many other keyboard\\nshortcuts available: you can view and edit them by typing Ctrl-M (or Cmd-M) then H. If\\nyou choose to run the notebooks on Kaggle or on your own machine using JupyterLab or\\nan IDE such as Visual Studio Code with the Jupyter extension, you will see some minor\\ndifferences—runtimes are called kernels, the user interface and keyboard shortcuts are\\nslightly different, etc.—but switching from one Jupyter environment to another is not too\\nhard.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 94}, page_content='Saving Your Code Changes and Your Data\\nYou can make changes to a Colab notebook, and they will persist for as long\\nas you keep your browser tab open. But once you close it, the changes will be\\nlost. To avoid this, make sure you save a copy of the notebook to your\\nGoogle Drive by selecting File → “Save a copy in Drive”. Alternatively, you\\ncan download the notebook to your computer by selecting File → Download\\n→ “Download .ipynb”. Then you can later visit\\nhttps://colab.research.google.com and open the notebook again (either from\\nGoogle Drive or by uploading it from your computer).\\nWARNING\\nGoogle Colab is meant only for interactive use: you can play around in the notebooks and\\ntweak the code as you like, but you cannot let the notebooks run unattended for a long\\nperiod of time, or else the runtime will be shut down and all of its data will be lost.\\nIf the notebook generates data that you care about, make sure you download\\nthis data before the runtime shuts down. To do this, click the Files icon (see\\nstep 1 in Figure 2-5), find the file you want to download, click the vertical\\ndots next to it (step 2), and click Download (step 3). Alternatively, you can\\nmount your Google Drive on the runtime, allowing the notebook to read and\\nwrite files directly to Google Drive as if it were a local directory. For this,\\nclick the Files icon (step 1), then click the Google Drive icon (circled in\\nFigure 2-5) and follow the on-screen instructions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 95}, page_content='Figure 2-5. Downloading a file from a Google Colab runtime (steps 1 to 3), or mounting your Google\\nDrive (circled icon)\\nBy default, your Google Drive will be mounted at /content/drive/MyDrive. If\\nyou want to back up a data file, simply copy it to this directory by running\\n!cp /content/my_great_model /content/drive/MyDrive. Any command\\nstarting with a bang (!) is treated as a shell command, not as Python code: cp\\nis the Linux shell command to copy a file from one path to another. Note that\\nColab runtimes run on Linux (specifically, Ubuntu).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 96}, page_content='The Power and Danger of Interactivity\\nJupyter notebooks are interactive, and that’s a great thing: you can run each\\ncell one by one, stop at any point, insert a cell, play with the code, go back\\nand run the same cell again, etc., and I highly encourage you to do so. If you\\njust run the cells one by one without ever playing around with them, you\\nwon’t learn as fast. However, this flexibility comes at a price: it’s very easy\\nto run cells in the wrong order, or to forget to run a cell. If this happens, the\\nsubsequent code cells are likely to fail. For example, the very first code cell\\nin each notebook contains setup code (such as imports), so make sure you run\\nit first, or else nothing will work.\\nTIP\\nIf you ever run into a weird error, try restarting the runtime (by selecting Runtime →\\n“Restart runtime” from the menu) and then run all the cells again from the beginning of\\nthe notebook. This often solves the problem. If not, it’s likely that one of the changes you\\nmade broke the notebook: just revert to the original notebook and try again. If it still fails,\\nplease file an issue on GitHub.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 97}, page_content='Book Code Versus Notebook Code\\nYou may sometimes notice some little differences between the code in this\\nbook and the code in the notebooks. This may happen for several reasons:\\nA library may have changed slightly by the time you read these lines, or\\nperhaps despite my best efforts I made an error in the book. Sadly, I\\ncannot magically fix the code in your copy of this book (unless you are\\nreading an electronic copy and you can download the latest version), but\\nI can fix the notebooks. So, if you run into an error after copying code\\nfrom this book, please look for the fixed code in the notebooks: I will\\nstrive to keep them error-free and up-to-date with the latest library\\nversions.\\nThe notebooks contain some extra code to beautify the figures (adding\\nlabels, setting font sizes, etc.) and to save them in high resolution for\\nthis book. You can safely ignore this extra code if you want.\\nI optimized the code for readability and simplicity: I made it as linear and flat\\nas possible, defining very few functions or classes. The goal is to ensure that\\nthe code you are running is generally right in front of you, and not nested\\nwithin several layers of abstractions that you have to search through. This\\nalso makes it easier for you to play with the code. For simplicity, there’s\\nlimited error handling, and I placed some of the least common imports right\\nwhere they are needed (instead of placing them at the top of the file, as is\\nrecommended by the PEP 8 Python style guide). That said, your production\\ncode will not be very different: just a bit more modular, and with additional\\ntests and error handling.\\nOK! Once you’re comfortable with Colab, you’re ready to download the data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 98}, page_content='Download the Data\\nIn typical environments your data would be available in a relational database\\nor some other common data store, and spread across multiple\\ntables/documents/files. To access it, you would first need to get your\\ncredentials and access authorizations\\u2060\\n and familiarize yourself with the data\\nschema. In this project, however, things are much simpler: you will just\\ndownload a single compressed file, housing.tgz, which contains a comma-\\nseparated values (CSV) file called housing.csv with all the data.\\nRather than manually downloading and decompressing the data, it’s usually\\npreferable to write a function that does it for you. This is useful in particular\\nif the data changes regularly: you can write a small script that uses the\\nfunction to fetch the latest data (or you can set up a scheduled job to do that\\nautomatically at regular intervals). Automating the process of fetching the\\ndata is also useful if you need to install the dataset on multiple machines.\\nHere is the function to fetch and load the data:\\nfrom pathlib import Path\\nimport pandas as pd\\nimport tarfile\\nimport urllib.request\\ndef load_housing_data():\\n    tarball_path = Path(\"datasets/housing.tgz\")\\n    if not tarball_path.is_file():\\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\\n        urllib.request.urlretrieve(url, tarball_path)\\n        with tarfile.open(tarball_path) as housing_tarball:\\n            housing_tarball.extractall(path=\"datasets\")\\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\\nhousing = load_housing_data()\\nWhen load_housing_data() is called, it looks for the datasets/housing.tgz file.\\nIf it does not find it, it creates the datasets directory inside the current\\ndirectory (which is /content by default, in Colab), downloads the housing.tgz\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 99}, page_content='file from the ageron/data GitHub repository, and extracts its content into the\\ndatasets directory; this creates the datasets/housing directory with the\\nhousing.csv file inside it. Lastly, the function loads this CSV file into a\\nPandas DataFrame object containing all the data, and returns it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 100}, page_content=\"Take a Quick Look at the Data Structure\\nYou start by looking at the top five rows of data using the DataFrame’s\\nhead() method (see Figure 2-6).\\nFigure 2-6. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (they are not all\\nshown in the screenshot): longitude, latitude, housing_median_age,\\ntotal_rooms, total_bedrooms, population, households, median_income,\\nmedian_house_value, and ocean_proximity.\\nThe info() method is useful to get a quick description of the data, in particular\\nthe total number of rows, each attribute’s type, and the number of non-null\\nvalues:\\n>>> housing.info()\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 20640 entries, 0 to 20639\\nData columns (total 10 columns):\\n #   Column              Non-Null Count  Dtype\\n---  ------              --------------  -----\\n 0   longitude           20640 non-null  float64\\n 1   latitude            20640 non-null  float64\\n 2   housing_median_age  20640 non-null  float64\\n 3   total_rooms         20640 non-null  float64\\n 4   total_bedrooms      20433 non-null  float64\\n 5   population          20640 non-null  float64\\n 6   households          20640 non-null  float64\\n 7   median_income       20640 non-null  float64\\n 8   median_house_value  20640 non-null  float64\\n 9   ocean_proximity     20640 non-null  object\\ndtypes: float64(9), object(1)\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 101}, page_content='memory usage: 1.6+ MB\\nNOTE\\nIn this book, when a code example contains a mix of code and outputs, as is the case here,\\nit is formatted like in the Python interpreter, for better readability: the code lines are\\nprefixed with >>> (or ... for indented blocks), and the outputs have no prefix.\\nThere are 20,640 instances in the dataset, which means that it is fairly small\\nby machine learning standards, but it’s perfect to get started. You notice that\\nthe total_bedrooms attribute has only 20,433 non-null values, meaning that\\n207 districts are missing this feature. You will need to take care of this later.\\nAll attributes are numerical, except for ocean_proximity. Its type is object, so\\nit could hold any kind of Python object. But since you loaded this data from a\\nCSV file, you know that it must be a text attribute. When you looked at the\\ntop five rows, you probably noticed that the values in the ocean_proximity\\ncolumn were repetitive, which means that it is probably a categorical\\nattribute. You can find out what categories exist and how many districts\\nbelong to each category by using the value_counts() method:\\n>>> housing[\"ocean_proximity\"].value_counts()\\n<1H OCEAN     9136\\nINLAND        6551\\nNEAR OCEAN    2658\\nNEAR BAY      2290\\nISLAND           5\\nName: ocean_proximity, dtype: int64\\nLet’s look at the other fields. The describe() method shows a summary of the\\nnumerical attributes (Figure 2-7).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 102}, page_content='Figure 2-7. Summary of each numerical attribute\\nThe count, mean, min, and max rows are self-explanatory. Note that the null\\nvalues are ignored (so, for example, the count of total_bedrooms is 20,433,\\nnot 20,640). The std row shows the standard deviation, which measures how\\ndispersed the values are.\\u2060\\n The 25%, 50%, and 75% rows show the\\ncorresponding percentiles: a percentile indicates the value below which a\\ngiven percentage of observations in a group of observations fall. For\\nexample, 25% of the districts have a housing_median_age lower than 18,\\nwhile 50% are lower than 29 and 75% are lower than 37. These are often\\ncalled the 25th percentile (or first quartile), the median, and the 75th\\npercentile (or third quartile).\\nAnother quick way to get a feel of the type of data you are dealing with is to\\nplot a histogram for each numerical attribute. A histogram shows the number\\nof instances (on the vertical axis) that have a given value range (on the\\nhorizontal axis). You can either plot this one attribute at a time, or you can\\ncall the hist() method on the whole dataset (as shown in the following code\\nexample), and it will plot a histogram for each numerical attribute (see\\nFigure 2-8):\\nimport matplotlib.pyplot as plt\\nhousing.hist(bins=50, figsize=(12, 8))\\nplt.show()\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 103}, page_content='Figure 2-8. A histogram for each numerical attribute\\nLooking at these histograms, you notice a few things:\\nFirst, the median income attribute does not look like it is expressed in\\nUS dollars (USD). After checking with the team that collected the data,\\nyou are told that the data has been scaled and capped at 15 (actually,\\n15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for\\nlower median incomes. The numbers represent roughly tens of\\nthousands of dollars (e.g., 3 actually means about $30,000). Working\\nwith preprocessed attributes is common in machine learning, and it is\\nnot necessarily a problem, but you should try to understand how the data\\nwas computed.\\nThe housing median age and the median house value were also capped.\\nThe latter may be a serious problem since it is your target attribute (your\\nlabels). Your machine learning algorithms may learn that prices never\\ngo beyond that limit. You need to check with your client team (the team\\nthat will use your system’s output) to see if this is a problem or not. If\\nthey tell you that they need precise predictions even beyond $500,000,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 104}, page_content='then you have two options:\\nCollect proper labels for the districts whose labels were capped.\\nRemove those districts from the training set (and also from the test\\nset, since your system should not be evaluated poorly if it predicts\\nvalues beyond $500,000).\\nThese attributes have very different scales. We will discuss this later in\\nthis chapter, when we explore feature scaling.\\nFinally, many histograms are skewed right: they extend much farther to\\nthe right of the median than to the left. This may make it a bit harder for\\nsome machine learning algorithms to detect patterns. Later, you’ll try\\ntransforming these attributes to have more symmetrical and bell-shaped\\ndistributions.\\nYou should now have a better understanding of the kind of data you’re\\ndealing with.\\nWARNING\\nWait! Before you look at the data any further, you need to create a test set, put it aside, and\\nnever look at it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 105}, page_content='Create a Test Set\\nIt may seem strange to voluntarily set aside part of the data at this stage. After\\nall, you have only taken a quick glance at the data, and surely you should\\nlearn a whole lot more about it before you decide what algorithms to use,\\nright? This is true, but your brain is an amazing pattern detection system,\\nwhich also means that it is highly prone to overfitting: if you look at the test\\nset, you may stumble upon some seemingly interesting pattern in the test data\\nthat leads you to select a particular kind of machine learning model. When\\nyou estimate the generalization error using the test set, your estimate will be\\ntoo optimistic, and you will launch a system that will not perform as well as\\nexpected. This is called data snooping bias.\\nCreating a test set is theoretically simple; pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set\\nthem aside:\\nimport numpy as np\\ndef shuffle_and_split_data(data, test_ratio):\\n    shuffled_indices = np.random.permutation(len(data))\\n    test_set_size = int(len(data) * test_ratio)\\n    test_indices = shuffled_indices[:test_set_size]\\n    train_indices = shuffled_indices[test_set_size:]\\n    return data.iloc[train_indices], data.iloc[test_indices]\\nYou can then use this function like this:\\n>>> train_set, test_set = shuffle_and_split_data(housing, 0.2)\\n>>> len(train_set)\\n16512\\n>>> len(test_set)\\n4128\\nWell, this works, but it is not perfect: if you run the program again, it will\\ngenerate a different test set! Over time, you (or your machine learning\\nalgorithms) will get to see the whole dataset, which is what you want to\\navoid.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 106}, page_content='One solution is to save the test set on the first run and then load it in\\nsubsequent runs. Another option is to set the random number generator’s seed\\n(e.g., with np.random.seed(42))\\u2060\\n before calling np.random.permutation() so\\nthat it always generates the same shuffled indices.\\nHowever, both these solutions will break the next time you fetch an updated\\ndataset. To have a stable train/test split even after updating the dataset, a\\ncommon solution is to use each instance’s identifier to decide whether or not\\nit should go in the test set (assuming instances have unique and immutable\\nidentifiers). For example, you could compute a hash of each instance’s\\nidentifier and put that instance in the test set if the hash is lower than or equal\\nto 20% of the maximum hash value. This ensures that the test set will remain\\nconsistent across multiple runs, even if you refresh the dataset. The new test\\nset will contain 20% of the new instances, but it will not contain any instance\\nthat was previously in the training set.\\nHere is a possible implementation:\\nfrom zlib import crc32\\ndef is_id_in_test_set(identifier, test_ratio):\\n    return crc32(np.int64(identifier)) < test_ratio * 2**32\\ndef split_data_with_id_hash(data, test_ratio, id_column):\\n    ids = data[id_column]\\n    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\\n    return data.loc[~in_test_set], data.loc[in_test_set]\\nUnfortunately, the housing dataset does not have an identifier column. The\\nsimplest solution is to use the row index as the ID:\\nhousing_with_id = housing.reset_index()  # adds an `index` column\\ntrain_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")\\nIf you use the row index as a unique identifier, you need to make sure that\\nnew data gets appended to the end of the dataset and that no row ever gets\\ndeleted. If this is not possible, then you can try to use the most stable features\\nto build a unique identifier. For example, a district’s latitude and longitude\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 107}, page_content='are guaranteed to be stable for a few million years, so you could combine\\nthem into an ID like so:\\u2060\\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\\ntrain_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")\\nScikit-Learn provides a few functions to split datasets into multiple subsets in\\nvarious ways. The simplest function is train_test_split(), which does pretty\\nmuch the same thing as the shuffle_and_split_data() function we defined\\nearlier, with a couple of additional features. First, there is a random_state\\nparameter that allows you to set the random generator seed. Second, you can\\npass it multiple datasets with an identical number of rows, and it will split\\nthem on the same indices (this is very useful, for example, if you have a\\nseparate DataFrame for labels):\\nfrom sklearn.model_selection import train_test_split\\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\\nSo far we have considered purely random sampling methods. This is\\ngenerally fine if your dataset is large enough (especially relative to the\\nnumber of attributes), but if it is not, you run the risk of introducing a\\nsignificant sampling bias. When employees at a survey company decides to\\ncall 1,000 people to ask them a few questions, they don’t just pick 1,000\\npeople randomly in a phone book. They try to ensure that these 1,000 people\\nare representative of the whole population, with regard to the questions they\\nwant to ask. For example, the US population is 51.1% females and 48.9%\\nmales, so a well-conducted survey in the US would try to maintain this ratio\\nin the sample: 511 females and 489 males (at least if it seems possible that\\nthe answers may vary across genders). This is called stratified sampling: the\\npopulation is divided into homogeneous subgroups called strata, and the right\\nnumber of instances are sampled from each stratum to guarantee that the test\\nset is representative of the overall population. If the people running the\\nsurvey used purely random sampling, there would be about a 10.7% chance\\nof sampling a skewed test set with less than 48.5% female or more than\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 108}, page_content='53.5% female participants. Either way, the survey results would likely be\\nquite biased.\\nSuppose you’ve chatted with some experts who told you that the median\\nincome is a very important attribute to predict median housing prices. You\\nmay want to ensure that the test set is representative of the various categories\\nof incomes in the whole dataset. Since the median income is a continuous\\nnumerical attribute, you first need to create an income category attribute.\\nLet’s look at the median income histogram more closely (back in Figure 2-8):\\nmost median income values are clustered around 1.5 to 6 (i.e., $15,000–\\n$60,000), but some median incomes go far beyond 6. It is important to have a\\nsufficient number of instances in your dataset for each stratum, or else the\\nestimate of a stratum’s importance may be biased. This means that you\\nshould not have too many strata, and each stratum should be large enough.\\nThe following code uses the pd.cut() function to create an income category\\nattribute with five categories (labeled from 1 to 5); category 1 ranges from 0\\nto 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\\n                               labels=[1, 2, 3, 4, 5])\\nThese income categories are represented in Figure 2-9:\\nhousing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\\nplt.xlabel(\"Income category\")\\nplt.ylabel(\"Number of districts\")\\nplt.show()\\nNow you are ready to do stratified sampling based on the income category.\\nScikit-Learn provides a number of splitter classes in the\\nsklearn.model_selection package that implement various strategies to split\\nyour dataset into a training set and a test set. Each splitter has a split() method\\nthat returns an iterator over different training/test splits of the same data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 109}, page_content='Figure 2-9. Histogram of income categories\\nTo be precise, the split() method yields the training and test indices, not the\\ndata itself. Having multiple splits can be useful if you want to better estimate\\nthe performance of your model, as you will see when we discuss cross-\\nvalidation later in this chapter. For example, the following code generates 10\\ndifferent stratified splits of the same dataset:\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\nsplitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\\nstrat_splits = []\\nfor train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\\n    strat_train_set_n = housing.iloc[train_index]\\n    strat_test_set_n = housing.iloc[test_index]\\n    strat_splits.append([strat_train_set_n, strat_test_set_n])\\nFor now, you can just use the first split:\\nstrat_train_set, strat_test_set = strat_splits[0]\\nOr, since stratified sampling is fairly common, there’s a shorter way to get a\\nsingle split using the train_test_split() function with the stratify argument:\\nstrat_train_set, strat_test_set = train_test_split(\\n    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 110}, page_content='Let’s see if this worked as expected. You can start by looking at the income\\ncategory proportions in the test set:\\n>>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114341\\n1    0.039971\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the\\nfull dataset. Figure 2-10 compares the income category proportions in the\\noverall dataset, in the test set generated with stratified sampling, and in a test\\nset generated using purely random sampling. As you can see, the test set\\ngenerated using stratified sampling has income category proportions almost\\nidentical to those in the full dataset, whereas the test set generated using\\npurely random sampling is skewed.\\nFigure 2-10. Sampling bias comparison of stratified versus purely random sampling\\nYou won’t use the income_cat column again, so you might as well drop it,\\nreverting the data back to its original state:\\nfor set_ in (strat_train_set, strat_test_set):\\n    set_.drop(\"income_cat\", axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an\\noften neglected but critical part of a machine learning project. Moreover,\\nmany of these ideas will be useful later when we discuss cross-validation.\\nNow it’s time to move on to the next stage: exploring the data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 111}, page_content='Explore and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general\\nunderstanding of the kind of data you are manipulating. Now the goal is to go\\ninto a little more depth.\\nFirst, make sure you have put the test set aside and you are only exploring the\\ntraining set. Also, if the training set is very large, you may want to sample an\\nexploration set, to make manipulations easy and fast during the exploration\\nphase. In this case, the training set is quite small, so you can just work\\ndirectly on the full set. Since you’re going to experiment with various\\ntransformations of the full training set, you should make a copy of the\\noriginal so you can revert to it afterwards:\\nhousing = strat_train_set.copy()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 112}, page_content='Visualizing Geographical Data\\nBecause the dataset includes geographical information (latitude and\\nlongitude), it is a good idea to create a scatterplot of all the districts to\\nvisualize the data (Figure 2-11):\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\\nplt.show()\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any\\nparticular pattern. Setting the alpha option to 0.2 makes it much easier to\\nvisualize the places where there is a high density of data points (Figure 2-12):\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\\nplt.show()\\nNow that’s much better: you can clearly see the high-density areas, namely'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 113}, page_content='the Bay Area and around Los Angeles and San Diego, plus a long line of\\nfairly high-density areas in the Central Valley (in particular, around\\nSacramento and Fresno).\\nOur brains are very good at spotting patterns in pictures, but you may need to\\nplay around with visualization parameters to make the patterns stand out.\\nFigure 2-12. A better visualization that highlights high-density areas\\nNext, you look at the housing prices (Figure 2-13). The radius of each circle\\nrepresents the district’s population (option s), and the color represents the\\nprice (option c). Here you use a predefined color map (option cmap) called\\njet, which ranges from blue (low values) to red (high prices):\\u2060\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\\n             s=housing[\"population\"] / 100, label=\"population\",\\n             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\\n             legend=True, sharex=False, figsize=(10, 7))\\nplt.show()\\nThis image tells you that the housing prices are very much related to the\\nlocation (e.g., close to the ocean) and to the population density, as you\\nprobably knew already. A clustering algorithm should be useful for detecting\\nthe main cluster and for adding new features that measure the proximity to\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 114}, page_content='the cluster centers. The ocean proximity attribute may be useful as well,\\nalthough in Northern California the housing prices in coastal districts are not\\ntoo high, so it is not a simple rule.\\nFigure 2-13. California housing prices: red is expensive, blue is cheap, larger circles indicate areas\\nwith a larger population'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 115}, page_content='Look for Correlations\\nSince the dataset is not too large, you can easily compute the standard\\ncorrelation coefficient (also called Pearson’s r) between every pair of\\nattributes using the corr() method:\\ncorr_matrix = housing.corr()\\nNow you can look at how much each attribute correlates with the median\\nhouse value:\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\\nmedian_house_value    1.000000\\nmedian_income         0.688380\\ntotal_rooms           0.137455\\nhousing_median_age    0.102175\\nhouseholds            0.071426\\ntotal_bedrooms        0.054635\\npopulation           -0.020153\\nlongitude            -0.050859\\nlatitude             -0.139584\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it\\nmeans that there is a strong positive correlation; for example, the median\\nhouse value tends to go up when the median income goes up. When the\\ncoefficient is close to –1, it means that there is a strong negative correlation;\\nyou can see a small negative correlation between the latitude and the median\\nhouse value (i.e., prices have a slight tendency to go down when you go\\nnorth). Finally, coefficients close to 0 mean that there is no linear correlation.\\nAnother way to check for correlation between attributes is to use the Pandas\\nscatter_matrix() function, which plots every numerical attribute against every\\nother numerical attribute. Since there are now 11 numerical attributes, you\\nwould get 11  = 121 plots, which would not fit on a page—so you decide to\\nfocus on a few promising attributes that seem most correlated with the\\nmedian housing value (Figure 2-14):\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 116}, page_content='from pandas.plotting import scatter_matrix\\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\\n              \"housing_median_age\"]\\nscatter_matrix(housing[attributes], figsize=(12, 8))\\nplt.show()\\nFigure 2-14. This scatter matrix plots every numerical attribute against every other numerical\\nattribute, plus a histogram of each numerical attribute’s values on the main diagonal (top left to bottom\\nright)\\nThe main diagonal would be full of straight lines if Pandas plotted each\\nvariable against itself, which would not be very useful. So instead, the Pandas\\ndisplays a histogram of each attribute (other options are available; see the\\nPandas documentation for more details).\\nLooking at the correlation scatterplots, it seems like the most promising\\nattribute to predict the median house value is the median income, so you\\nzoom in on their scatterplot (Figure 2-15):\\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\\n             alpha=0.1, grid=True)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 117}, page_content='plt.show()\\nFigure 2-15. Median income versus median house value\\nThis plot reveals a few things. First, the correlation is indeed quite strong;\\nyou can clearly see the upward trend, and the points are not too dispersed.\\nSecond, the price cap you noticed earlier is clearly visible as a horizontal line\\nat $500,000. But the plot also reveals other less obvious straight lines: a\\nhorizontal line around $450,000, another around $350,000, perhaps one\\naround $280,000, and a few more below that. You may want to try removing\\nthe corresponding districts to prevent your algorithms from learning to\\nreproduce these data quirks.\\nWARNING\\nThe correlation coefficient only measures linear correlations (“as x goes up, y generally\\ngoes up/down”). It may completely miss out on nonlinear relationships (e.g., “as x\\napproaches 0, y generally goes up”). Figure 2-16 shows a variety of datasets along with\\ntheir correlation coefficient. Note how all the plots of the bottom row have a correlation\\ncoefficient equal to 0, despite the fact that their axes are clearly not independent: these are\\nexamples of nonlinear relationships. Also, the second row shows examples where the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 118}, page_content='correlation coefficient is equal to 1 or –1; notice that this has nothing to do with the slope.\\nFor example, your height in inches has a correlation coefficient of 1 with your height in\\nfeet or in nanometers.\\nFigure 2-16. Standard correlation coefficient of various datasets (source: Wikipedia; public domain\\nimage)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 119}, page_content='Experiment with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can\\nexplore the data and gain insights. You identified a few data quirks that you\\nmay want to clean up before feeding the data to a machine learning\\nalgorithm, and you found interesting correlations between attributes, in\\nparticular with the target attribute. You also noticed that some attributes have\\na skewed-right distribution, so you may want to transform them (e.g., by\\ncomputing their logarithm or square root). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before preparing the data for machine\\nlearning algorithms is to try out various attribute combinations. For example,\\nthe total number of rooms in a district is not very useful if you don’t know\\nhow many households there are. What you really want is the number of\\nrooms per household. Similarly, the total number of bedrooms by itself is not\\nvery useful: you probably want to compare it to the number of rooms. And\\nthe population per household also seems like an interesting attribute\\ncombination to look at. You create these new attributes as follows:\\nhousing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\\nhousing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\\nAnd then you look at the correlation matrix again:\\n>>> corr_matrix = housing.corr()\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\\nmedian_house_value    1.000000\\nmedian_income         0.688380\\nrooms_per_house       0.143663\\ntotal_rooms           0.137455\\nhousing_median_age    0.102175\\nhouseholds            0.071426\\ntotal_bedrooms        0.054635\\npopulation           -0.020153\\npeople_per_house     -0.038224\\nlongitude            -0.050859'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 120}, page_content='latitude             -0.139584\\nbedrooms_ratio       -0.256397\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_ratio attribute is much more correlated\\nwith the median house value than the total number of rooms or bedrooms.\\nApparently houses with a lower bedroom/room ratio tend to be more\\nexpensive. The number of rooms per household is also more informative than\\nthe total number of rooms in a district—obviously the larger the houses, the\\nmore expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point\\nis to start off on the right foot and quickly gain insights that will help you get\\na first reasonably good prototype. But this is an iterative process: once you\\nget a prototype up and running, you can analyze its output to gain more\\ninsights and come back to this exploration step.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 121}, page_content='Prepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your machine learning algorithms. Instead of\\ndoing this manually, you should write functions for this purpose, for several\\ngood reasons:\\nThis will allow you to reproduce these transformations easily on any\\ndataset (e.g., the next time you get a fresh dataset).\\nYou will gradually build a library of transformation functions that you\\ncan reuse in future projects.\\nYou can use these functions in your live system to transform the new\\ndata before feeding it to your algorithms.\\nThis will make it possible for you to easily try various transformations\\nand see which combination of transformations works best.\\nBut first, revert to a clean training set (by copying strat_train_set once again).\\nYou should also separate the predictors and the labels, since you don’t\\nnecessarily want to apply the same transformations to the predictors and the\\ntarget values (note that drop() creates a copy of the data and does not affect\\nstrat_train_set):\\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\\nhousing_labels = strat_train_set[\"median_house_value\"].copy()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 122}, page_content='Clean the Data\\nMost machine learning algorithms cannot work with missing features, so\\nyou’ll need to take care of these. For example, you noticed earlier that the\\ntotal_bedrooms attribute has some missing values. You have three options to\\nfix this:\\n1. Get rid of the corresponding districts.\\n2. Get rid of the whole attribute.\\n3. Set the missing values to some value (zero, the mean, the median, etc.).\\nThis is called imputation.\\nYou can accomplish these easily using the Pandas DataFrame’s dropna(),\\ndrop(), and fillna() methods:\\nhousing.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\\nhousing.drop(\"total_bedrooms\", axis=1)  # option 2\\nmedian = housing[\"total_bedrooms\"].median()  # option 3\\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\\nYou decide to go for option 3 since it is the least destructive, but instead of\\nthe preceding code, you will use a handy Scikit-Learn class: SimpleImputer.\\nThe benefit is that it will store the median value of each feature: this will\\nmake it possible to impute missing values not only on the training set, but\\nalso on the validation set, the test set, and any new data fed to the model. To\\nuse it, first you need to create a SimpleImputer instance, specifying that you\\nwant to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute import SimpleImputer\\nimputer = SimpleImputer(strategy=\"median\")\\nSince the median can only be computed on numerical attributes, you then'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 123}, page_content='need to create a copy of the data with only the numerical attributes (this will\\nexclude the text attribute ocean_proximity):\\nhousing_num = housing.select_dtypes(include=[np.number])\\nNow you can fit the imputer instance to the training data using the fit()\\nmethod:\\nimputer.fit(housing_num)\\nThe imputer has simply computed the median of each attribute and stored the\\nresult in its statistics_ instance variable. Only the total_bedrooms attribute\\nhad missing values, but you cannot be sure that there won’t be any missing\\nvalues in new data after the system goes live, so it is safer to apply the\\nimputer to all the numerical attributes:\\n>>> imputer.statistics_\\narray([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])\\n>>> housing_num.median().values\\narray([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])\\nNow you can use this “trained” imputer to transform the training set by\\nreplacing missing values with the learned medians:\\nX = imputer.transform(housing_num)\\nMissing values can also be replaced with the mean value (strategy=\"mean\"),\\nor with the most frequent value (strategy=\"most_frequent\"), or with a\\nconstant value (strategy=\"constant\", fill_value=…). The last two strategies\\nsupport non-numerical data.\\nTIP\\nThere are also more powerful imputers available in the sklearn.impute package (both for\\nnumerical features only):\\nKNNImputer replaces each missing value with the mean of the k-nearest neighbors’'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 124}, page_content='values for that feature. The distance is based on all the available features.\\nIterativeImputer trains a regression model per feature to predict the missing values\\nbased on all the other available features. It then trains the model again on the\\nupdated data, and repeats the process several times, improving the models and the\\nreplacement values at each iteration.\\nSCIKIT-LEARN DESIGN\\nScikit-Learn’s API is remarkably well designed. These are the main\\ndesign principles:\\u2060\\nConsistency\\nAll objects share a consistent and simple interface:\\nEstimators\\nAny object that can estimate some parameters based on a dataset is\\ncalled an estimator (e.g., a SimpleImputer is an estimator). The\\nestimation itself is performed by the fit() method, and it takes a\\ndataset as a parameter, or two for supervised learning algorithms—the\\nsecond dataset contains the labels. Any other parameter needed to\\nguide the estimation process is considered a hyperparameter (such as\\na SimpleImputer’s strategy), and it must be set as an instance variable\\n(generally via a constructor parameter).\\nTransformers\\nSome estimators (such as a SimpleImputer) can also transform a\\ndataset; these are called transformers. Once again, the API is simple:\\nthe transformation is performed by the transform() method with the\\ndataset to transform as a parameter. It returns the transformed dataset.\\nThis transformation generally relies on the learned parameters, as is\\nthe case for a SimpleImputer. All transformers also have a\\nconvenience method called fit_transform(), which is equivalent to\\ncalling fit() and then transform() (but sometimes fit_transform() is\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 125}, page_content='optimized and runs much faster).\\nPredictors\\nFinally, some estimators, given a dataset, are capable of making\\npredictions; they are called predictors. For example, the\\nLinearRegression model in the previous chapter was a predictor:\\ngiven a country’s GDP per capita, it predicted life satisfaction. A\\npredictor has a predict() method that takes a dataset of new instances\\nand returns a dataset of corresponding predictions. It also has a\\nscore() method that measures the quality of the predictions, given a\\ntest set (and the corresponding labels, in the case of supervised\\nlearning algorithms).\\nInspection\\nAll the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy), and all the estimator’s learned\\nparameters are accessible via public instance variables with an\\nunderscore suffix (e.g., imputer.statistics_).\\nNonproliferation of classes\\nDatasets are represented as NumPy arrays or SciPy sparse matrices,\\ninstead of homemade classes. Hyperparameters are just regular Python\\nstrings or numbers.\\nComposition\\nExisting building blocks are reused as much as possible. For example, it\\nis easy to create a Pipeline estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as you will see.\\nSensible defaults\\nScikit-Learn provides reasonable default values for most parameters,\\nmaking it easy to quickly create a baseline working system.\\nScikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse\\nmatrices) even when they are fed Pandas DataFrames as input.\\u2060\\n So, the\\n10\\n11'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 126}, page_content='output of imputer.transform(housing_num) is a NumPy array: X has neither\\ncolumn names nor index. Luckily, it’s not too hard to wrap X in a DataFrame\\nand recover the column names and index from housing_num:\\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\\n                          index=housing_num.index)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 127}, page_content='Handling Text and Categorical Attributes\\nSo far we have only dealt with numerical attributes, but your data may also\\ncontain text attributes. In this dataset, there is just one: the ocean_proximity\\nattribute. Let’s look at its value for the first few instances:\\n>>> housing_cat = housing[[\"ocean_proximity\"]]\\n>>> housing_cat.head(8)\\n      ocean_proximity\\n13096        NEAR BAY\\n14973       <1H OCEAN\\n3785           INLAND\\n14689          INLAND\\n20507      NEAR OCEAN\\n1286           INLAND\\n18078       <1H OCEAN\\n4396         NEAR BAY\\nIt’s not arbitrary text: there are a limited number of possible values, each of\\nwhich represents a category. So this attribute is a categorical attribute. Most\\nmachine learning algorithms prefer to work with numbers, so let’s convert\\nthese categories from text to numbers. For this, we can use Scikit-Learn’s\\nOrdinalEncoder class:\\nfrom sklearn.preprocessing import OrdinalEncoder\\nordinal_encoder = OrdinalEncoder()\\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\\nHere’s what the first few encoded values in housing_cat_encoded look like:\\n>>> housing_cat_encoded[:8]\\narray([[3.],\\n       [0.],\\n       [1.],\\n       [1.],\\n       [4.],\\n       [1.],\\n       [0.],\\n       [3.]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 128}, page_content='You can get the list of categories using the categories_ instance variable. It is\\na list containing a 1D array of categories for each categorical attribute (in this\\ncase, a list containing a single array since there is just one categorical\\nattribute):\\n>>> ordinal_encoder.categories_\\n[array([\\'<1H OCEAN\\', \\'INLAND\\', \\'ISLAND\\', \\'NEAR BAY\\', \\'NEAR OCEAN\\'],\\n       dtype=object)]\\nOne issue with this representation is that ML algorithms will assume that two\\nnearby values are more similar than two distant values. This may be fine in\\nsome cases (e.g., for ordered categories such as “bad”, “average”, “good”,\\nand “excellent”), but it is obviously not the case for the ocean_proximity\\ncolumn (for example, categories 0 and 4 are clearly more similar than\\ncategories 0 and 1). To fix this issue, a common solution is to create one\\nbinary attribute per category: one attribute equal to 1 when the category is \"\\n<1H OCEAN\" (and 0 otherwise), another attribute equal to 1 when the\\ncategory is \"INLAND\" (and 0 otherwise), and so on. This is called one-hot\\nencoding, because only one attribute will be equal to 1 (hot), while the others\\nwill be 0 (cold). The new attributes are sometimes called dummy attributes.\\nScikit-Learn provides a OneHotEncoder class to convert categorical values\\ninto one-hot vectors:\\nfrom sklearn.preprocessing import OneHotEncoder\\ncat_encoder = OneHotEncoder()\\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\\nBy default, the output of a OneHotEncoder is a SciPy sparse matrix, instead\\nof a NumPy array:\\n>>> housing_cat_1hot\\n<16512x5 sparse matrix of type \\'<class \\'numpy.float64\\'>\\'\\n with 16512 stored elements in Compressed Sparse Row format>\\nA sparse matrix is a very efficient representation for matrices that contain\\nmostly zeros. Indeed, internally it only stores the nonzero values and their'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 129}, page_content='positions. When a categorical attribute has hundreds or thousands of\\ncategories, one-hot encoding it results in a very large matrix full of 0s except\\nfor a single 1 per row. In this case, a sparse matrix is exactly what you need:\\nit will save plenty of memory and speed up computations. You can use a\\nsparse matrix mostly like a normal 2D array,\\u2060\\n but if you want to convert it\\nto a (dense) NumPy array, just call the toarray() method:\\n>>> housing_cat_1hot.toarray()\\narray([[0., 0., 0., 1., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 1., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., 0., 1.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.]])\\nAlternatively, you can set sparse=False when creating the OneHotEncoder, in\\nwhich case the transform() method will return a regular (dense) NumPy array\\ndirectly.\\nAs with the OrdinalEncoder, you can get the list of categories using the\\nencoder’s categories_ instance variable:\\n>>> cat_encoder.categories_\\n[array([\\'<1H OCEAN\\', \\'INLAND\\', \\'ISLAND\\', \\'NEAR BAY\\', \\'NEAR OCEAN\\'],\\n       dtype=object)]\\nPandas has a function called get_dummies(), which also converts each\\ncategorical feature into a one-hot representation, with one binary feature per\\ncategory:\\n>>> df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\\n>>> pd.get_dummies(df_test)\\n   ocean_proximity_INLAND  ocean_proximity_NEAR BAY\\n0                       1                         0\\n1                       0                         1\\nIt looks nice and simple, so why not use it instead of OneHotEncoder? Well,\\nthe advantage of OneHotEncoder is that it remembers which categories it was\\n12'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 130}, page_content='trained on. This is very important because once your model is in production,\\nit should be fed exactly the same features as during training: no more, no less.\\nLook what our trained cat_encoder outputs when we make it transform the\\nsame df_test (using transform(), not fit_transform()):\\n>>> cat_encoder.transform(df_test)\\narray([[0., 1., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nSee the difference? get_dummies() saw only two categories, so it output two\\ncolumns, whereas OneHotEncoder output one column per learned category,\\nin the right order. Moreover, if you feed get_dummies() a DataFrame\\ncontaining an unknown category (e.g., \"<2H OCEAN\"), it will happily\\ngenerate a column for it:\\n>>> df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\\n>>> pd.get_dummies(df_test_unknown)\\n   ocean_proximity_<2H OCEAN  ocean_proximity_ISLAND\\n0                          1                       0\\n1                          0                       1\\nBut OneHotEncoder is smarter: it will detect the unknown category and raise\\nan exception. If you prefer, you can set the handle_unknown hyperparameter\\nto \"ignore\", in which case it will just represent the unknown category with\\nzeros:\\n>>> cat_encoder.handle_unknown = \"ignore\"\\n>>> cat_encoder.transform(df_test_unknown)\\narray([[0., 0., 0., 0., 0.],\\n       [0., 0., 1., 0., 0.]])\\nTIP\\nIf a categorical attribute has a large number of possible categories (e.g., country code,\\nprofession, species), then one-hot encoding will result in a large number of input features.\\nThis may slow down training and degrade performance. If this happens, you may want to\\nreplace the categorical input with useful numerical features related to the categories: for\\nexample, you could replace the ocean_proximity feature with the distance to the ocean\\n(similarly, a country code could be replaced with the country’s population and GDP per'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 131}, page_content=\"capita). Alternatively, you can use one of the encoders provided by the category_encoders\\npackage on GitHub. Or, when dealing with neural networks, you can replace each\\ncategory with a learnable, low-dimensional vector called an embedding. This is an\\nexample of representation learning (see Chapters 13 and 17 for more details).\\nWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator\\nstores the column names in the feature_names_in_ attribute. Scikit-Learn\\nthen ensures that any DataFrame fed to this estimator after that (e.g., to\\ntransform() or predict()) has the same column names. Transformers also\\nprovide a get_feature_names_out() method that you can use to build a\\nDataFrame around the transformer’s output:\\n>>> cat_encoder.feature_names_in_\\narray(['ocean_proximity'], dtype=object)\\n>>> cat_encoder.get_feature_names_out()\\narray(['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\\n       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\\n       'ocean_proximity_NEAR OCEAN'], dtype=object)\\n>>> df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\\n...                          columns=cat_encoder.get_feature_names_out(),\\n...                          index=df_test_unknown.index)\\n...\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 132}, page_content='Feature Scaling and Transformation\\nOne of the most important transformations you need to apply to your data is\\nfeature scaling. With few exceptions, machine learning algorithms don’t\\nperform well when the input numerical attributes have very different scales.\\nThis is the case for the housing data: the total number of rooms ranges from\\nabout 6 to 39,320, while the median incomes only range from 0 to 15.\\nWithout any scaling, most models will be biased toward ignoring the median\\nincome and focusing more on the number of rooms.\\nThere are two common ways to get all attributes to have the same scale: min-\\nmax scaling and standardization.\\nWARNING\\nAs with all estimators, it is important to fit the scalers to the training data only: never use\\nfit() or fit_transform() for anything else than the training set. Once you have a trained\\nscaler, you can then use it to transform() any other set, including the validation set, the test\\nset, and new data. Note that while the training set values will always be scaled to the\\nspecified range, if new data contains outliers, these may end up scaled outside the range. If\\nyou want to avoid this, just set the clip hyperparameter to True.\\nMin-max scaling (many people call this normalization) is the simplest: for\\neach attribute, the values are shifted and rescaled so that they end up ranging\\nfrom 0 to 1. This is performed by subtracting the min value and dividing by\\nthe difference between the min and the max. Scikit-Learn provides a\\ntransformer called MinMaxScaler for this. It has a feature_range\\nhyperparameter that lets you change the range if, for some reason, you don’t\\nwant 0–1 (e.g., neural networks work best with zero-mean inputs, so a range\\nof –1 to 1 is preferable). It’s quite easy to use:\\nfrom sklearn.preprocessing import MinMaxScaler\\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 133}, page_content='Standardization is different: first it subtracts the mean value (so standardized\\nvalues have a zero mean), then it divides the result by the standard deviation\\n(so standardized values have a standard deviation equal to 1). Unlike min-\\nmax scaling, standardization does not restrict values to a specific range.\\nHowever, standardization is much less affected by outliers. For example,\\nsuppose a district has a median income equal to 100 (by mistake), instead of\\nthe usual 0–15. Min-max scaling to the 0–1 range would map this outlier\\ndown to 1 and it would crush all the other values down to 0–0.15, whereas\\nstandardization would not be much affected. Scikit-Learn provides a\\ntransformer called StandardScaler for standardization:\\nfrom sklearn.preprocessing import StandardScaler\\nstd_scaler = StandardScaler()\\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\\nTIP\\nIf you want to scale a sparse matrix without converting it to a dense matrix first, you can\\nuse a StandardScaler with its with_mean hyperparameter set to False: it will only divide\\nthe data by the standard deviation, without subtracting the mean (as this would break\\nsparsity).\\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the\\nmean are not exponentially rare), both min-max scaling and standardization\\nwill squash most values into a small range. Machine learning models\\ngenerally don’t like this at all, as you will see in Chapter 4. So before you\\nscale the feature, you should first transform it to shrink the heavy tail, and if\\npossible to make the distribution roughly symmetrical. For example, a\\ncommon way to do this for positive features with a heavy tail to the right is to\\nreplace the feature with its square root (or raise the feature to a power\\nbetween 0 and 1). If the feature has a really long and heavy tail, such as a\\npower law distribution, then replacing the feature with its logarithm may\\nhelp. For example, the population feature roughly follows a power law:\\ndistricts with 10,000 inhabitants are only 10 times less frequent than districts'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 134}, page_content='with 1,000 inhabitants, not exponentially less frequent. Figure 2-17 shows\\nhow much better this feature looks when you compute its log: it’s very close\\nto a Gaussian distribution (i.e., bell-shaped).\\nFigure 2-17. Transforming a feature to make it closer to a Gaussian distribution\\nAnother approach to handle heavy-tailed features consists in bucketizing the\\nfeature. This means chopping its distribution into roughly equal-sized\\nbuckets, and replacing each feature value with the index of the bucket it\\nbelongs to, much like we did to create the income_cat feature (although we\\nonly used it for stratified sampling). For example, you could replace each\\nvalue with its percentile. Bucketizing with equal-sized buckets results in a\\nfeature with an almost uniform distribution, so there’s no need for further\\nscaling, or you can just divide by the number of buckets to force the values to\\nthe 0–1 range.\\nWhen a feature has a multimodal distribution (i.e., with two or more clear\\npeaks, called modes), such as the housing_median_age feature, it can also be\\nhelpful to bucketize it, but this time treating the bucket IDs as categories,\\nrather than as numerical values. This means that the bucket indices must be\\nencoded, for example using a OneHotEncoder (so you usually don’t want to\\nuse too many buckets). This approach will allow the regression model to\\nmore easily learn different rules for different ranges of this feature value. For\\nexample, perhaps houses built around 35 years ago have a peculiar style that\\nfell out of fashion, and therefore they’re cheaper than their age alone would\\nsuggest.\\nAnother approach to transforming multimodal distributions is to add a feature'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 135}, page_content='for each of the modes (at least the main ones), representing the similarity\\nbetween the housing median age and that particular mode. The similarity\\nmeasure is typically computed using a radial basis function (RBF)—any\\nfunction that depends only on the distance between the input value and a\\nfixed point. The most commonly used RBF is the Gaussian RBF, whose\\noutput value decays exponentially as the input value moves away from the\\nfixed point. For example, the Gaussian RBF similarity between the housing\\nage x and 35 is given by the equation exp(–γ(x – 35)²). The hyperparameter γ\\n(gamma) determines how quickly the similarity measure decays as x moves\\naway from 35. Using Scikit-Learn’s rbf_kernel() function, you can create a\\nnew Gaussian RBF feature measuring the similarity between the housing\\nmedian age and 35:\\nfrom sklearn.metrics.pairwise import rbf_kernel\\nage_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)\\nFigure 2-18 shows this new feature as a function of the housing median age\\n(solid line). It also shows what the feature would look like if you used a\\nsmaller gamma value. As the chart shows, the new age similarity feature\\npeaks at 35, right around the spike in the housing median age distribution: if\\nthis particular age group is well correlated with lower prices, there’s a good\\nchance that this new feature will help.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 136}, page_content='Figure 2-18. Gaussian RBF feature measuring the similarity between the housing median age and 35\\nSo far we’ve only looked at the input features, but the target values may also\\nneed to be transformed. For example, if the target distribution has a heavy\\ntail, you may choose to replace the target with its logarithm. But if you do,\\nthe regression model will now predict the log of the median house value, not\\nthe median house value itself. You will need to compute the exponential of\\nthe model’s prediction if you want the predicted median house value.\\nLuckily, most of Scikit-Learn’s transformers have an inverse_transform()\\nmethod, making it easy to compute the inverse of their transformations. For\\nexample, the following code example shows how to scale the labels using a\\nStandardScaler (just like we did for inputs), then train a simple linear\\nregression model on the resulting scaled labels and use it to make predictions\\non some new data, which we transform back to the original scale using the\\ntrained scaler’s inverse_transform() method. Note that we convert the labels\\nfrom a Pandas Series to a DataFrame, since the StandardScaler expects 2D\\ninputs. Also, in this example we just train the model on a single raw input\\nfeature (median income), for simplicity:\\nfrom sklearn.linear_model import LinearRegression\\ntarget_scaler = StandardScaler()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 137}, page_content='scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\\nmodel = LinearRegression()\\nmodel.fit(housing[[\"median_income\"]], scaled_labels)\\nsome_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\\nscaled_predictions = model.predict(some_new_data)\\npredictions = target_scaler.inverse_transform(scaled_predictions)\\nThis works fine, but a simpler option is to use a\\nTransformedTargetRegressor. We just need to construct it, giving it the\\nregression model and the label transformer, then fit it on the training set,\\nusing the original unscaled labels. It will automatically use the transformer to\\nscale the labels and train the regression model on the resulting scaled labels,\\njust like we did previously. Then, when we want to make a prediction, it will\\ncall the regression model’s predict() method and use the scaler’s\\ninverse_transform() method to produce the prediction:\\nfrom sklearn.compose import TransformedTargetRegressor\\nmodel = TransformedTargetRegressor(LinearRegression(),\\n                                   transformer=StandardScaler())\\nmodel.fit(housing[[\"median_income\"]], housing_labels)\\npredictions = model.predict(some_new_data)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 138}, page_content='Custom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to\\nwrite your own for tasks such as custom transformations, cleanup operations,\\nor combining specific attributes.\\nFor transformations that don’t require any training, you can just write a\\nfunction that takes a NumPy array as input and outputs the transformed array.\\nFor example, as discussed in the previous section, it’s often a good idea to\\ntransform features with heavy-tailed distributions by replacing them with\\ntheir logarithm (assuming the feature is positive and the tail is on the right).\\nLet’s create a log-transformer and apply it to the population feature:\\nfrom sklearn.preprocessing import FunctionTransformer\\nlog_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\\nlog_pop = log_transformer.transform(housing[[\"population\"]])\\nThe inverse_func argument is optional. It lets you specify an inverse\\ntransform function, e.g., if you plan to use your transformer in a\\nTransformedTargetRegressor.\\nYour transformation function can take hyperparameters as additional\\narguments. For example, here’s how to create a transformer that computes the\\nsame Gaussian RBF similarity measure as earlier:\\nrbf_transformer = FunctionTransformer(rbf_kernel,\\n                                      kw_args=dict(Y=[[35.]], gamma=0.1))\\nage_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\\nNote that there’s no inverse function for the RBF kernel, since there are\\nalways two values at a given distance from a fixed point (except at distance\\n0). Also note that rbf_kernel() does not treat the features separately. If you\\npass it an array with two features, it will measure the 2D distance (Euclidean)\\nto measure similarity. For example, here’s how to add a feature that will\\nmeasure the geographic similarity between each district and San Francisco:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 139}, page_content='sf_coords = 37.7749, -122.41\\nsf_transformer = FunctionTransformer(rbf_kernel,\\n                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\\nsf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])\\nCustom transformers are also useful to combine features. For example, here’s\\na FunctionTransformer that computes the ratio between the input features 0\\nand 1:\\n>>> ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\\n>>> ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\\narray([[0.5 ],\\n       [0.75]])\\nFunctionTransformer is very handy, but what if you would like your\\ntransformer to be trainable, learning some parameters in the fit() method and\\nusing them later in the transform() method? For this, you need to write a\\ncustom class. Scikit-Learn relies on duck typing, so this class does not have\\nto inherit from any particular base class. All it needs is three methods: fit()\\n(which must return self), transform(), and fit_transform().\\nYou can get fit_transform() for free by simply adding TransformerMixin as a\\nbase class: the default implementation will just call fit() and then transform().\\nIf you add BaseEstimator as a base class (and avoid using *args and\\n**kwargs in your constructor), you will also get two extra methods:\\nget_params() and set_params(). These will be useful for automatic\\nhyperparameter tuning.\\nFor example, here’s a custom transformer that acts much like the\\nStandardScaler:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nfrom sklearn.utils.validation import check_array, check_is_fitted\\nclass StandardScalerClone(BaseEstimator, TransformerMixin):\\n    def __init__(self, with_mean=True):  # no *args or **kwargs!\\n        self.with_mean = with_mean\\n    def fit(self, X, y=None):  # y is required even though we don\\'t use it\\n        X = check_array(X)  # checks that X is an array with finite float values'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 140}, page_content='self.mean_ = X.mean(axis=0)\\n        self.scale_ = X.std(axis=0)\\n        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\\n        return self  # always return self!\\n    def transform(self, X):\\n        check_is_fitted(self)  # looks for learned attributes (with trailing _)\\n        X = check_array(X)\\n        assert self.n_features_in_ == X.shape[1]\\n        if self.with_mean:\\n            X = X - self.mean_\\n        return X / self.scale_\\nHere are a few things to note:\\nThe sklearn.utils.validation package contains several functions we can\\nuse to validate the inputs. For simplicity, we will skip such tests in the\\nrest of this book, but production code should have them.\\nScikit-Learn pipelines require the fit() method to have two arguments X\\nand y, which is why we need the y=None argument even though we\\ndon’t use y.\\nAll Scikit-Learn estimators set n_features_in_ in the fit() method, and\\nthey ensure that the data passed to transform() or predict() has this\\nnumber of features.\\nThe fit() method must return self.\\nThis implementation is not 100% complete: all estimators should set\\nfeature_names_in_ in the fit() method when they are passed a\\nDataFrame. Moreover, all transformers should provide a\\nget_feature_names_out() method, as well as an inverse_transform()\\nmethod when their transformation can be reversed. See the last exercise\\nat the end of this chapter for more details.\\nA custom transformer can (and often does) use other estimators in its\\nimplementation. For example, the following code demonstrates custom\\ntransformer that uses a KMeans clusterer in the fit() method to identify the\\nmain clusters in the training data, and then uses rbf_kernel() in the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 141}, page_content='transform() method to measure how similar each sample is to each cluster\\ncenter:\\nfrom sklearn.cluster import KMeans\\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\\n        self.n_clusters = n_clusters\\n        self.gamma = gamma\\n        self.random_state = random_state\\n    def fit(self, X, y=None, sample_weight=None):\\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\\n        self.kmeans_.fit(X, sample_weight=sample_weight)\\n        return self  # always return self!\\n    def transform(self, X):\\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\\n    def get_feature_names_out(self, names=None):\\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\\nTIP\\nYou can check whether your custom estimator respects Scikit-Learn’s API by passing an\\ninstance to check_estimator() from the sklearn.utils.estimator_checks package. For the full\\nAPI, check out https://scikit-learn.org/stable/developers.\\nAs you will see in Chapter 9, k-means is a clustering algorithm that locates\\nclusters in the data. How many it searches for is controlled by the n_clusters\\nhyperparameter. After training, the cluster centers are available via the\\ncluster_centers_ attribute. The fit() method of KMeans supports an optional\\nargument sample_weight, which lets the user specify the relative weights of\\nthe samples. k-means is a stochastic algorithm, meaning that it relies on\\nrandomness to locate the clusters, so if you want reproducible results, you\\nmust set the random_state parameter. As you can see, despite the complexity\\nof the task, the code is fairly straightforward. Now let’s use this custom\\ntransformer:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 142}, page_content='cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\\n                                           sample_weight=housing_labels)\\nThis code creates a ClusterSimilarity transformer, setting the number of\\nclusters to 10. Then it calls fit_transform() with the latitude and longitude of\\nevery district in the training set, weighting each district by its median house\\nvalue. The transformer uses k-means to locate the clusters, then measures the\\nGaussian RBF similarity between each district and all 10 cluster centers. The\\nresult is a matrix with one row per district, and one column per cluster. Let’s\\nlook at the first three rows, rounding to two decimal places:\\n>>> similarities[:3].round(2)\\narray([[0.  , 0.14, 0.  , 0.  , 0.  , 0.08, 0.  , 0.99, 0.  , 0.6 ],\\n       [0.63, 0.  , 0.99, 0.  , 0.  , 0.  , 0.04, 0.  , 0.11, 0.  ],\\n       [0.  , 0.29, 0.  , 0.  , 0.01, 0.44, 0.  , 0.7 , 0.  , 0.3 ]])\\nFigure 2-19 shows the 10 cluster centers found by k-means. The districts are\\ncolored according to their geographic similarity to their closest cluster center.\\nAs you can see, most clusters are located in highly populated and expensive\\nareas.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 143}, page_content='Figure 2-19. Gaussian RBF similarity to the nearest cluster center'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 144}, page_content='Transformation Pipelines\\nAs you can see, there are many data transformation steps that need to be\\nexecuted in the right order. Fortunately, Scikit-Learn provides the Pipeline\\nclass to help with such sequences of transformations. Here is a small pipeline\\nfor numerical attributes, which will first impute then scale the input features:\\nfrom sklearn.pipeline import Pipeline\\nnum_pipeline = Pipeline([\\n    (\"impute\", SimpleImputer(strategy=\"median\")),\\n    (\"standardize\", StandardScaler()),\\n])\\nThe Pipeline constructor takes a list of name/estimator pairs (2-tuples)\\ndefining a sequence of steps. The names can be anything you like, as long as\\nthey are unique and don’t contain double underscores (__). They will be\\nuseful later, when we discuss hyperparameter tuning. The estimators must all\\nbe transformers (i.e., they must have a fit_transform() method), except for the\\nlast one, which can be anything: a transformer, a predictor, or any other type\\nof estimator.\\nTIP\\nIn a Jupyter notebook, if you import sklearn and run sklearn.\\nset_config(display=\"diagram\"), all Scikit-Learn estimators will be rendered as interactive\\ndiagrams. This is particularly useful for visualizing pipelines. To visualize num_pipeline,\\nrun a cell with num_pipeline as the last line. Clicking an estimator will show more details.\\nIf you don’t want to name the transformers, you can use the make_pipeline()\\nfunction instead; it takes transformers as positional arguments and creates a\\nPipeline using the names of the transformers’ classes, in lowercase and\\nwithout underscores (e.g., \"simpleimputer\"):\\nfrom sklearn.pipeline import make_pipeline'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 145}, page_content='num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\\nIf multiple transformers have the same name, an index is appended to their\\nnames (e.g., \"foo-1\", \"foo-2\", etc.).\\nWhen you call the pipeline’s fit() method, it calls fit_transform() sequentially\\non all the transformers, passing the output of each call as the parameter to the\\nnext call until it reaches the final estimator, for which it just calls the fit()\\nmethod.\\nThe pipeline exposes the same methods as the final estimator. In this example\\nthe last estimator is a StandardScaler, which is a transformer, so the pipeline\\nalso acts like a transformer. If you call the pipeline’s transform() method, it\\nwill sequentially apply all the transformations to the data. If the last estimator\\nwere a predictor instead of a transformer, then the pipeline would have a\\npredict() method rather than a transform() method. Calling it would\\nsequentially apply all the transformations to the data and pass the result to the\\npredictor’s predict() method.\\nLet’s call the pipeline’s fit_transform() method and look at the output’s first\\ntwo rows, rounded to two decimal places:\\n>>> housing_num_prepared = num_pipeline.fit_transform(housing_num)\\n>>> housing_num_prepared[:2].round(2)\\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\\n       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])\\nAs you saw earlier, if you want to recover a nice DataFrame, you can use the\\npipeline’s get_feature_names_out() method:\\ndf_housing_num_prepared = pd.DataFrame(\\n    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\\n    index=housing_num.index)\\nPipelines support indexing; for example, pipeline[1] returns the second\\nestimator in the pipeline, and pipeline[:-1] returns a Pipeline object\\ncontaining all but the last estimator. You can also access the estimators via\\nthe steps attribute, which is a list of name/estimator pairs, or via the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 146}, page_content='named_steps dictionary attribute, which maps the names to the estimators.\\nFor example, num_pipeline[\"simpleimputer\"] returns the estimator named\\n\"simpleimputer\".\\nSo far, we have handled the categorical columns and the numerical columns\\nseparately. It would be more convenient to have a single transformer capable\\nof handling all columns, applying the appropriate transformations to each\\ncolumn. For this, you can use a ColumnTransformer. For example, the\\nfollowing ColumnTransformer will apply num_pipeline (the one we just\\ndefined) to the numerical attributes and cat_pipeline to the categorical\\nattribute:\\nfrom sklearn.compose import ColumnTransformer\\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\\n               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\\ncat_attribs = [\"ocean_proximity\"]\\ncat_pipeline = make_pipeline(\\n    SimpleImputer(strategy=\"most_frequent\"),\\n    OneHotEncoder(handle_unknown=\"ignore\"))\\npreprocessing = ColumnTransformer([\\n    (\"num\", num_pipeline, num_attribs),\\n    (\"cat\", cat_pipeline, cat_attribs),\\n])\\nFirst we import the ColumnTransformer class, then we define the list of\\nnumerical and categorical column names and construct a simple pipeline for\\ncategorical attributes. Lastly, we construct a ColumnTransformer. Its\\nconstructor requires a list of triplets (3-tuples), each containing a name\\n(which must be unique and not contain double underscores), a transformer,\\nand a list of names (or indices) of columns that the transformer should be\\napplied to.\\nTIP\\nInstead of using a transformer, you can specify the string \"drop\" if you want the columns\\nto be dropped, or you can specify \"passthrough\" if you want the columns to be left'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 147}, page_content='untouched. By default, the remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder hyperparameter to any transformer (or to\\n\"passthrough\") if you want these columns to be handled differently.\\nSince listing all the column names is not very convenient, Scikit-Learn\\nprovides a make_column_selector() function that returns a selector function\\nyou can use to automatically select all the features of a given type, such as\\nnumerical or categorical. You can pass this selector function to the\\nColumnTransformer instead of column names or indices. Moreover, if you\\ndon’t care about naming the transformers, you can use\\nmake_column_transformer(), which chooses the names for you, just like\\nmake_pipeline() does. For example, the following code creates the same\\nColumnTransformer as earlier, except the transformers are automatically\\nnamed \"pipeline-1\" and \"pipeline-2\" instead of \"num\" and \"cat\":\\nfrom sklearn.compose import make_column_selector, make_column_transformer\\npreprocessing = make_column_transformer(\\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\\n    (cat_pipeline, make_column_selector(dtype_include=object)),\\n)\\nNow we’re ready to apply this ColumnTransformer to the housing data:\\nhousing_prepared = preprocessing.fit_transform(housing)\\nGreat! We have a preprocessing pipeline that takes the entire training dataset\\nand applies each transformer to the appropriate columns, then concatenates\\nthe transformed columns horizontally (transformers must never change the\\nnumber of rows). Once again this returns a NumPy array, but you can get the\\ncolumn names using preprocessing.get_feature_names_out() and wrap the\\ndata in a nice DataFrame as we did before.\\nNOTE\\nThe OneHotEncoder returns a sparse matrix and the num_pipeline returns a dense matrix.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 148}, page_content='When there is such a mix of sparse and dense matrices, the ColumnTransformer estimates\\nthe density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse\\nmatrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). In\\nthis example, it returns a dense matrix.\\nYour project is going really well and you’re almost ready to train some\\nmodels! You now want to create a single pipeline that will perform all the\\ntransformations you’ve experimented with up to now. Let’s recap what the\\npipeline will do and why:\\nMissing values in numerical features will be imputed by replacing them\\nwith the median, as most ML algorithms don’t expect missing values. In\\ncategorical features, missing values will be replaced by the most\\nfrequent category.\\nThe categorical feature will be one-hot encoded, as most ML algorithms\\nonly accept numerical inputs.\\nA few ratio features will be computed and added: bedrooms_ratio,\\nrooms_per_house, and people_per_house. Hopefully these will better\\ncorrelate with the median house value, and thereby help the ML models.\\nA few cluster similarity features will also be added. These will likely be\\nmore useful to the model than latitude and longitude.\\nFeatures with a long tail will be replaced by their logarithm, as most\\nmodels prefer features with roughly uniform or Gaussian distributions.\\nAll numerical features will be standardized, as most ML algorithms\\nprefer when all features have roughly the same scale.\\nThe code that builds the pipeline to do all of this should look familiar to you\\nby now:\\ndef column_ratio(X):\\n    return X[:, [0]] / X[:, [1]]\\ndef ratio_name(function_transformer, feature_names_in):\\n    return [\"ratio\"]  # feature names out'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 149}, page_content='def ratio_pipeline():\\n    return make_pipeline(\\n        SimpleImputer(strategy=\"median\"),\\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\\n        StandardScaler())\\nlog_pipeline = make_pipeline(\\n    SimpleImputer(strategy=\"median\"),\\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\\n    StandardScaler())\\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\\n                                     StandardScaler())\\npreprocessing = ColumnTransformer([\\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\\n                               \"households\", \"median_income\"]),\\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\\n    ],\\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\\nIf you run this ColumnTransformer, it performs all the transformations and\\noutputs a NumPy array with 24 features:\\n>>> housing_prepared = preprocessing.fit_transform(housing)\\n>>> housing_prepared.shape\\n(16512, 24)\\n>>> preprocessing.get_feature_names_out()\\narray([\\'bedrooms__ratio\\', \\'rooms_per_house__ratio\\',\\n       \\'people_per_house__ratio\\', \\'log__total_bedrooms\\',\\n       \\'log__total_rooms\\', \\'log__population\\', \\'log__households\\',\\n       \\'log__median_income\\', \\'geo__Cluster 0 similarity\\', [...],\\n       \\'geo__Cluster 9 similarity\\', \\'cat__ocean_proximity_<1H OCEAN\\',\\n       \\'cat__ocean_proximity_INLAND\\', \\'cat__ocean_proximity_ISLAND\\',\\n       \\'cat__ocean_proximity_NEAR BAY\\', \\'cat__ocean_proximity_NEAR OCEAN\\',\\n       \\'remainder__housing_median_age\\'], dtype=object)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 150}, page_content='Select and Train a Model\\nAt last! You framed the problem, you got the data and explored it, you\\nsampled a training set and a test set, and you wrote a preprocessing pipeline\\nto automatically clean up and prepare your data for machine learning\\nalgorithms. You are now ready to select and train a machine learning model.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 151}, page_content='Train and Evaluate on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going\\nto be easy! You decide to train a very basic linear regression model to get\\nstarted:\\nfrom sklearn.linear_model import LinearRegression\\nlin_reg = make_pipeline(preprocessing, LinearRegression())\\nlin_reg.fit(housing, housing_labels)\\nDone! You now have a working linear regression model. You try it out on the\\ntraining set, looking at the first five predictions and comparing them to the\\nlabels:\\n>>> housing_predictions = lin_reg.predict(housing)\\n>>> housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\\narray([243700., 372400., 128800.,  94400., 328300.])\\n>>> housing_labels.iloc[:5].values\\narray([458300., 483800., 101700.,  96100., 361800.])\\nWell, it works, but not always: the first prediction is way off (by over\\n$200,000!), while the other predictions are better: two are off by about 25%,\\nand two are off by less than 10%. Remember that you chose to use the RMSE\\nas your performance measure, so you want to measure this regression\\nmodel’s RMSE on the whole training set using Scikit-Learn’s\\nmean_squared_error() function, with the squared argument set to False:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> lin_rmse = mean_squared_error(housing_labels, housing_predictions,\\n...                               squared=False)\\n...\\n>>> lin_rmse\\n68687.89176589991\\nThis is better than nothing, but clearly not a great score: the\\nmedian_housing_values of most districts range between $120,000 and\\n$265,000, so a typical prediction error of $68,628 is really not very'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 152}, page_content='satisfying. This is an example of a model underfitting the training data. When\\nthis happens it can mean that the features do not provide enough information\\nto make good predictions, or that the model is not powerful enough. As we\\nsaw in the previous chapter, the main ways to fix underfitting are to select a\\nmore powerful model, to feed the training algorithm with better features, or to\\nreduce the constraints on the model. This model is not regularized, which\\nrules out the last option. You could try to add more features, but first you\\nwant to try a more complex model to see how it does.\\nYou decide to try a DecisionTreeRegressor, as this is a fairly powerful model\\ncapable of finding complex nonlinear relationships in the data (decision trees\\nare presented in more detail in Chapter 6):\\nfrom sklearn.tree import DecisionTreeRegressor\\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\\ntree_reg.fit(housing, housing_labels)\\nNow that the model is trained, you evaluate it on the training set:\\n>>> housing_predictions = tree_reg.predict(housing)\\n>>> tree_rmse = mean_squared_error(housing_labels, housing_predictions,\\n...                                squared=False)\\n...\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect?\\nOf course, it is much more likely that the model has badly overfit the data.\\nHow can you be sure? As you saw earlier, you don’t want to touch the test set\\nuntil you are ready to launch a model you are confident about, so you need to\\nuse part of the training set for training and part of it for model validation.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 153}, page_content='Better Evaluation Using Cross-Validation\\nOne way to evaluate the decision tree model would be to use the train_\\ntest_split() function to split the training set into a smaller training set and a\\nvalidation set, then train your models against the smaller training set and\\nevaluate them against the validation set. It’s a bit of effort, but nothing too\\ndifficult, and it would work fairly well.\\nA great alternative is to use Scikit-Learn’s k_-fold cross-validation feature.\\nThe following code randomly splits the training set into 10 nonoverlapping\\nsubsets called folds, then it trains and evaluates the decision tree model 10\\ntimes, picking a different fold for evaluation every time and using the other 9\\nfolds for training. The result is an array containing the 10 evaluation scores:\\nfrom sklearn.model_selection import cross_val_score\\ntree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\\n                              scoring=\"neg_root_mean_squared_error\", cv=10)\\nWARNING\\nScikit-Learn’s cross-validation features expect a utility function (greater is better) rather\\nthan a cost function (lower is better), so the scoring function is actually the opposite of the\\nRMSE. It’s a negative value, so you need to switch the sign of the output to get the RMSE\\nscores.\\nLet’s look at the results:\\n>>> pd.Series(tree_rmses).describe()\\ncount       10.000000\\nmean     66868.027288\\nstd       2060.966425\\nmin      63649.536493\\n25%      65338.078316\\n50%      66801.953094\\n75%      68229.934454\\nmax      70094.778246\\ndtype: float64'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 154}, page_content='Now the decision tree doesn’t look as good as it did earlier. In fact, it seems\\nto perform almost as poorly as the linear regression model! Notice that cross-\\nvalidation allows you to get not only an estimate of the performance of your\\nmodel, but also a measure of how precise this estimate is (i.e., its standard\\ndeviation). The decision tree has an RMSE of about 66,868, with a standard\\ndeviation of about 2,061. You would not have this information if you just\\nused one validation set. But cross-validation comes at the cost of training the\\nmodel several times, so it is not always feasible.\\nIf you compute the same metric for the linear regression model, you will find\\nthat the mean RMSE is 69,858 and the standard deviation is 4,182. So the\\ndecision tree model seems to perform very slightly better than the linear\\nmodel, but the difference is minimal due to severe overfitting. We know\\nthere’s an overfitting problem because the training error is low (actually zero)\\nwhile the validation error is high.\\nLet’s try one last model now: the RandomForestRegressor. As you will see in\\nChapter 7, random forests work by training many decision trees on random\\nsubsets of the features, then averaging out their predictions. Such models\\ncomposed of many other models are called ensembles: they are capable of\\nboosting the performance of the underlying model (in this case, decision\\ntrees). The code is much the same as earlier:\\nfrom sklearn.ensemble import RandomForestRegressor\\nforest_reg = make_pipeline(preprocessing,\\n                           RandomForestRegressor(random_state=42))\\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\\n                                scoring=\"neg_root_mean_squared_error\", cv=10)\\nLet’s look at the scores:\\n>>> pd.Series(forest_rmses).describe()\\ncount       10.000000\\nmean     47019.561281\\nstd       1033.957120\\nmin      45458.112527\\n25%      46464.031184\\n50%      46967.596354'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 155}, page_content='75%      47325.694987\\nmax      49243.765795\\ndtype: float64\\nWow, this is much better: random forests really look very promising for this\\ntask! However, if you train a RandomForest and measure the RMSE on the\\ntraining set, you will find roughly 17,474: that’s much lower, meaning that\\nthere’s still quite a lot of overfitting going on. Possible solutions are to\\nsimplify the model, constrain it (i.e., regularize it), or get a lot more training\\ndata. Before you dive much deeper into random forests, however, you should\\ntry out many other models from various categories of machine learning\\nalgorithms (e.g., several support vector machines with different kernels, and\\npossibly a neural network), without spending too much time tweaking the\\nhyperparameters. The goal is to shortlist a few (two to five) promising\\nmodels.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 156}, page_content='Fine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. You now\\nneed to fine-tune them. Let’s look at a few ways you can do that.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 157}, page_content='Grid Search\\nOne option would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very\\ntedious work, and you may not have time to explore many combinations.\\nInstead, you can use Scikit-Learn’s GridSearchCV class to search for you.\\nAll you need to do is tell it which hyperparameters you want it to experiment\\nwith and what values to try out, and it will use cross-validation to evaluate all\\nthe possible combinations of hyperparameter values. For example, the\\nfollowing code searches for the best combination of hyperparameter values\\nfor the RandomForestRegressor:\\nfrom sklearn.model_selection import GridSearchCV\\nfull_pipeline = Pipeline([\\n    (\"preprocessing\", preprocessing),\\n    (\"random_forest\", RandomForestRegressor(random_state=42)),\\n])\\nparam_grid = [\\n    {\\'preprocessing__geo__n_clusters\\': [5, 8, 10],\\n     \\'random_forest__max_features\\': [4, 6, 8]},\\n    {\\'preprocessing__geo__n_clusters\\': [10, 15],\\n     \\'random_forest__max_features\\': [6, 8, 10]},\\n]\\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\\n                           scoring=\\'neg_root_mean_squared_error\\')\\ngrid_search.fit(housing, housing_labels)\\nNotice that you can refer to any hyperparameter of any estimator in a\\npipeline, even if this estimator is nested deep inside several pipelines and\\ncolumn transformers. For example, when Scikit-Learn sees\\n\"preprocessing__geo__n_clusters\", it splits this string at the double\\nunderscores, then it looks for an estimator named \"preprocessing\" in the\\npipeline and finds the preprocessing ColumnTransformer. Next, it looks for a\\ntransformer named \"geo\" inside this ColumnTransformer and finds the\\nClusterSimilarity transformer we used on the latitude and longitude attributes.\\nThen it finds this transformer’s n_clusters hyperparameter. Similarly,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 158}, page_content='random_forest__max_features refers to the max_features hyperparameter of\\nthe estimator named \"random_forest\", which is of course the RandomForest\\nmodel (the max_features hyperparameter will be explained in Chapter 7).\\nTIP\\nWrapping preprocessing steps in a Scikit-Learn pipeline allows you to tune the\\npreprocessing hyperparameters along with the model hyperparameters. This is a good\\nthing since they often interact. For example, perhaps increasing n_clusters requires\\nincreasing max_features as well. If fitting the pipeline transformers is computationally\\nexpensive, you can set the pipeline’s memory hyperparameter to the path of a caching\\ndirectory: when you first fit the pipeline, Scikit-Learn will save the fitted transformers to\\nthis directory. If you then fit the pipeline again with the same hyperparameters, Scikit-\\nLearn will just load the cached transformers.\\nThere are two dictionaries in this param_grid, so GridSearchCV will first\\nevaluate all 3 × 3 = 9 combinations of n_clusters and max_features\\nhyperparameter values specified in the first dict, then it will try all 2 × 3 = 6\\ncombinations of hyperparameter values in the second dict. So in total the grid\\nsearch will explore 9 + 6 = 15 combinations of hyperparameter values, and it\\nwill train the pipeline 3 times per combination, since we are using 3-fold\\ncross validation. This means there will be a grand total of 15 × 3 = 45 rounds\\nof training! It may take a while, but when it is done you can get the best\\ncombination of parameters like this:\\n>>> grid_search.best_params_\\n{\\'preprocessing__geo__n_clusters\\': 15, \\'random_forest__max_features\\': 6}\\nIn this example, the best model is obtained by setting n_clusters to 15 and\\nsetting max_features to 8.\\nTIP\\nSince 15 is the maximum value that was evaluated for n_clusters, you should probably try\\nsearching again with higher values; the score may continue to improve.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 159}, page_content='You can access the best estimator using grid_search.best_estimator_. If\\nGridSearchCV is initialized with refit=True (which is the default), then once\\nit finds the best estimator using cross-validation, it retrains it on the whole\\ntraining set. This is usually a good idea, since feeding it more data will likely\\nimprove its performance.\\nThe evaluation scores are available using grid_search.cv_results_. This is a\\ndictionary, but if you wrap it in a DataFrame you get a nice list of all the test\\nscores for each combination of hyperparameters and for each cross-validation\\nsplit, as well as the mean test score across all splits:\\n>>> cv_res = pd.DataFrame(grid_search.cv_results_)\\n>>> cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\\n>>> [...]  # change column names to fit on this page, and show rmse = -score\\n>>> cv_res.head()  # note: the 1st column is the row ID\\n   n_clusters max_features  split0  split1  split2  mean_test_rmse\\n12         15            6   43460   43919   44748           44042\\n13         15            8   44132   44075   45010           44406\\n14         15           10   44374   44286   45316           44659\\n7          10            6   44683   44655   45657           44999\\n9          10            6   44683   44655   45657           44999\\nThe mean test RMSE score for the best model is 44,042, which is better than\\nthe score you got earlier using the default hyperparameter values (which was\\n47,019). Congratulations, you have successfully fine-tuned your best model!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 160}, page_content=\"Randomized Search\\nThe grid search approach is fine when you are exploring relatively few\\ncombinations, like in the previous example, but RandomizedSearchCV is\\noften preferable, especially when the hyperparameter search space is large.\\nThis class can be used in much the same way as the GridSearchCV class, but\\ninstead of trying out all possible combinations it evaluates a fixed number of\\ncombinations, selecting a random value for each hyperparameter at every\\niteration. This may sound surprising, but this approach has several benefits:\\nIf some of your hyperparameters are continuous (or discrete but with\\nmany possible values), and you let randomized search run for, say, 1,000\\niterations, then it will explore 1,000 different values for each of these\\nhyperparameters, whereas grid search would only explore the few values\\nyou listed for each one.\\nSuppose a hyperparameter does not actually make much difference, but\\nyou don’t know it yet. If it has 10 possible values and you add it to your\\ngrid search, then training will take 10 times longer. But if you add it to a\\nrandom search, it will not make any difference.\\nIf there are 6 hyperparameters to explore, each with 10 possible values,\\nthen grid search offers no other choice than training the model a million\\ntimes, whereas random search can always run for any number of\\niterations you choose.\\nFor each hyperparameter, you must provide either a list of possible values, or\\na probability distribution:\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom scipy.stats import randint\\nparam_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\\n                  'random_forest__max_features': randint(low=2, high=20)}\\nrnd_search = RandomizedSearchCV(\\n    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\\n    scoring='neg_root_mean_squared_error', random_state=42)\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 161}, page_content='rnd_search.fit(housing, housing_labels)\\nScikit-Learn also has HalvingRandomSearchCV and HalvingGridSearchCV\\nhyperparameter search classes. Their goal is to use the computational\\nresources more efficiently, either to train faster or to explore a larger\\nhyperparameter space. Here’s how they work: in the first round, many\\nhyperparameter combinations (called “candidates”) are generated using either\\nthe grid approach or the random approach. These candidates are then used to\\ntrain models that are evaluated using cross-validation, as usual. However,\\ntraining uses limited resources, which speeds up this first round considerably.\\nBy default, “limited resources” means that the models are trained on a small\\npart of the training set. However, other limitations are possible, such as\\nreducing the number of training iterations if the model has a hyperparameter\\nto set it. Once every candidate has been evaluated, only the best ones go on to\\nthe second round, where they are allowed more resources to compete. After\\nseveral rounds, the final candidates are evaluated using full resources. This\\nmay save you some time tuning hyperparameters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 162}, page_content='Ensemble Methods\\nAnother way to fine-tune your system is to try to combine the models that\\nperform best. The group (or “ensemble”) will often perform better than the\\nbest individual model—just like random forests perform better than the\\nindividual decision trees they rely on—especially if the individual models\\nmake very different types of errors. For example, you could train and fine-\\ntune a k-nearest neighbors model, then create an ensemble model that just\\npredicts the mean of the random forest prediction and that model’s\\nprediction. We will cover this topic in more detail in Chapter 7.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 163}, page_content='Analyzing the Best Models and Their Errors\\nYou will often gain good insights on the problem by inspecting the best\\nmodels. For example, the RandomForestRegressor can indicate the relative\\nimportance of each attribute for making accurate predictions:\\n>>> final_model = rnd_search.best_estimator_  # includes preprocessing\\n>>> feature_importances = final_model[\"random_forest\"].feature_importances_\\n>>> feature_importances.round(2)\\narray([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, [...], 0.01])\\nLet’s sort these importance scores in descending order and display them next\\nto their corresponding attribute names:\\n>>> sorted(zip(feature_importances,\\n...            final_model[\"preprocessing\"].get_feature_names_out()),\\n...            reverse=True)\\n...\\n[(0.18694559869103852, \\'log__median_income\\'),\\n (0.0748194905715524, \\'cat__ocean_proximity_INLAND\\'),\\n (0.06926417748515576, \\'bedrooms__ratio\\'),\\n (0.05446998753775219, \\'rooms_per_house__ratio\\'),\\n (0.05262301809680712, \\'people_per_house__ratio\\'),\\n (0.03819415873915732, \\'geo__Cluster 0 similarity\\'),\\n [...]\\n (0.00015061247730531558, \\'cat__ocean_proximity_NEAR BAY\\'),\\n (7.301686597099842e-05, \\'cat__ocean_proximity_ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful\\nfeatures (e.g., apparently only one ocean_proximity category is really useful,\\nso you could try dropping the others).\\nTIP\\nThe sklearn.feature_selection.SelectFromModel transformer can automatically drop the\\nleast useful features for you: when you fit it, it trains a model (typically a random forest),\\nlooks at its feature_importances_ attribute, and selects the most useful features. Then\\nwhen you call transform(), it drops the other features.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 164}, page_content='You should also look at the specific errors that your system makes, then try to\\nunderstand why it makes them and what could fix the problem: adding extra\\nfeatures or getting rid of uninformative ones, cleaning up outliers, etc.\\nNow is also a good time to ensure that your model not only works well on\\naverage, but also on all categories of districts, whether they’re rural or urban,\\nrich or poor, northern or southern, minority or not, etc. Creating subsets of\\nyour validation set for each category takes a bit of work, but it’s important: if\\nyour model performs poorly on a whole category of districts, then it should\\nprobably not be deployed until the issue is solved, or at least it should not be\\nused to make predictions for that category, as it may do more harm than\\ngood.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 165}, page_content='Evaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that\\nperforms sufficiently well. You are ready to evaluate the final model on the\\ntest set. There is nothing special about this process; just get the predictors and\\nthe labels from your test set and run your final_model to transform the data\\nand make predictions, then evaluate these predictions:\\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\\ny_test = strat_test_set[\"median_house_value\"].copy()\\nfinal_predictions = final_model.predict(X_test)\\nfinal_rmse = mean_squared_error(y_test, final_predictions, squared=False)\\nprint(final_rmse)  # prints 41424.40026462184\\nIn some cases, such a point estimate of the generalization error will not be\\nquite enough to convince you to launch: what if it is just 0.1% better than the\\nmodel currently in production? You might want to have an idea of how\\nprecise this estimate is. For this, you can compute a 95% confidence interval\\nfor the generalization error using scipy.stats.t.interval(). You get a fairly large\\ninterval from 39,275 to 43,467, and your previous point estimate of 41,424 is\\nroughly in the middle of it:\\n>>> from scipy import stats\\n>>> confidence = 0.95\\n>>> squared_errors = (final_predictions - y_test) ** 2\\n>>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\\n...                          loc=squared_errors.mean(),\\n...                          scale=stats.sem(squared_errors)))\\n...\\narray([39275.40861216, 43467.27680583])\\nIf you did a lot of hyperparameter tuning, the performance will usually be\\nslightly worse than what you measured using cross-validation. That’s because\\nyour system ends up fine-tuned to perform well on the validation data and\\nwill likely not perform as well on unknown datasets. That’s not the case in\\nthis example since the test RMSE is lower than the validation RMSE, but'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 166}, page_content='when it happens you must resist the temptation to tweak the hyperparameters\\nto make the numbers look good on the test set; the improvements would be\\nunlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution\\n(highlighting what you have learned, what worked and what did not, what\\nassumptions were made, and what your system’s limitations are), document\\neverything, and create nice presentations with clear visualizations and easy-\\nto-remember statements (e.g., “the median income is the number one\\npredictor of housing prices”). In this California housing example, the final\\nperformance of the system is not much better than the experts’ price\\nestimates, which were often off by 30%, but it may still be a good idea to\\nlaunch it, especially if this frees up some time for the experts so they can\\nwork on more interesting and productive tasks.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 167}, page_content='Launch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! You now need to get your solution ready\\nfor production (e.g., polish the code, write documentation and tests, and so\\non). Then you can deploy your model to your production environment. The\\nmost basic way to do this is just to save the best model you trained, transfer\\nthe file to your production environment, and load it. To save the model, you\\ncan use the joblib library like this:\\nimport joblib\\njoblib.dump(final_model, \"my_california_housing_model.pkl\")\\nTIP\\nIt’s often a good idea to save every model you experiment with so that you can come back\\neasily to any model you want. You may also save the cross-validation scores and perhaps\\nthe actual predictions on the validation set. This will allow you to easily compare scores\\nacross model types, and compare the types of errors they make.\\nOnce your model is transferred to production, you can load it and use it. For\\nthis you must first import any custom classes and functions the model relies\\non (which means transferring the code to production), then load the model\\nusing joblib and use it to make predictions:\\nimport joblib\\n[...]  # import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.\\ndef column_ratio(X): [...]\\ndef ratio_name(function_transformer, feature_names_in): [...]\\nclass ClusterSimilarity(BaseEstimator, TransformerMixin): [...]\\nfinal_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\\nnew_data = [...]  # some new districts to make predictions for\\npredictions = final_model_reloaded.predict(new_data)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 168}, page_content='For example, perhaps the model will be used within a website: the user will\\ntype in some data about a new district and click the Estimate Price button.\\nThis will send a query containing the data to the web server, which will\\nforward it to your web application, and finally your code will simply call the\\nmodel’s predict() method (you want to load the model upon server startup,\\nrather than every time the model is used). Alternatively, you can wrap the\\nmodel within a dedicated web service that your web application can query\\nthrough a REST API\\u2060\\n (see Figure 2-20). This makes it easier to upgrade\\nyour model to new versions without interrupting the main application. It also\\nsimplifies scaling, since you can start as many web services as needed and\\nload-balance the requests coming from your web application across these\\nweb services. Moreover, it allows your web application to use any\\nprogramming language, not just Python.\\nFigure 2-20. A model deployed as a web service and used by a web application\\nAnother popular strategy is to deploy your model to the cloud, for example\\non Google’s Vertex AI (formerly known as Google Cloud AI Platform and\\nGoogle Cloud ML Engine): just save your model using joblib and upload it to\\nGoogle Cloud Storage (GCS), then head over to Vertex AI and create a new\\nmodel version, pointing it to the GCS file. That’s it! This gives you a simple\\nweb service that takes care of load balancing and scaling for you. It takes\\nJSON requests containing the input data (e.g., of a district) and returns JSON\\nresponses containing the predictions. You can then use this web service in\\nyour website (or whatever production environment you are using). As you\\nwill see in Chapter 19, deploying TensorFlow models on Vertex AI is not\\nmuch different from deploying Scikit-Learn models.\\nBut deployment is not the end of the story. You also need to write monitoring\\ncode to check your system’s live performance at regular intervals and trigger\\nalerts when it drops. It may drop very quickly, for example if a component\\nbreaks in your infrastructure, but be aware that it could also decay very\\nslowly, which can easily go unnoticed for a long time. This is quite common\\n13'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 169}, page_content='because of model rot: if the model was trained with last year’s data, it may\\nnot be adapted to today’s data.\\nSo, you need to monitor your model’s live performance. But how do you do\\nthat? Well, it depends. In some cases, the model’s performance can be\\ninferred from downstream metrics. For example, if your model is part of a\\nrecommender system and it suggests products that the users may be interested\\nin, then it’s easy to monitor the number of recommended products sold each\\nday. If this number drops (compared to non-recommended products), then the\\nprime suspect is the model. This may be because the data pipeline is broken,\\nor perhaps the model needs to be retrained on fresh data (as we will discuss\\nshortly).\\nHowever, you may also need human analysis to assess the model’s\\nperformance. For example, suppose you trained an image classification\\nmodel (we’ll look at these in Chapter 3) to detect various product defects on a\\nproduction line. How can you get an alert if the model’s performance drops,\\nbefore thousands of defective products get shipped to your clients? One\\nsolution is to send to human raters a sample of all the pictures that the model\\nclassified (especially pictures that the model wasn’t so sure about).\\nDepending on the task, the raters may need to be experts, or they could be\\nnonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon\\nMechanical Turk). In some applications they could even be the users\\nthemselves, responding, for example, via surveys or repurposed captchas.\\u2060\\nEither way, you need to put in place a monitoring system (with or without\\nhuman raters to evaluate the live model), as well as all the relevant processes\\nto define what to do in case of failures and how to prepare for them.\\nUnfortunately, this can be a lot of work. In fact, it is often much more work\\nthan building and training a model.\\nIf the data keeps evolving, you will need to update your datasets and retrain\\nyour model regularly. You should probably automate the whole process as\\nmuch as possible. Here are a few things you can automate:\\nCollect fresh data regularly and label it (e.g., using human raters).\\n14'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 170}, page_content='Write a script to train the model and fine-tune the hyperparameters\\nautomatically. This script could run automatically, for example every\\nday or every week, depending on your needs.\\nWrite another script that will evaluate both the new model and the\\nprevious model on the updated test set, and deploy the model to\\nproduction if the performance has not decreased (if it did, make sure you\\ninvestigate why). The script should probably test the performance of\\nyour model on various subsets of the test set, such as poor or rich\\ndistricts, rural or urban districts, etc.\\nYou should also make sure you evaluate the model’s input data quality.\\nSometimes performance will degrade slightly because of a poor-quality\\nsignal (e.g., a malfunctioning sensor sending random values, or another\\nteam’s output becoming stale), but it may take a while before your system’s\\nperformance degrades enough to trigger an alert. If you monitor your model’s\\ninputs, you may catch this earlier. For example, you could trigger an alert if\\nmore and more inputs are missing a feature, or the mean or standard\\ndeviation drifts too far from the training set, or a categorical feature starts\\ncontaining new categories.\\nFinally, make sure you keep backups of every model you create and have the\\nprocess and tools in place to roll back to a previous model quickly, in case\\nthe new model starts failing badly for some reason. Having backups also\\nmakes it possible to easily compare new models with previous ones.\\nSimilarly, you should keep backups of every version of your datasets so that\\nyou can roll back to a previous dataset if the new one ever gets corrupted\\n(e.g., if the fresh data that gets added to it turns out to be full of outliers).\\nHaving backups of your datasets also allows you to evaluate any model\\nagainst any previous dataset.\\nAs you can see, machine learning involves quite a lot of infrastructure.\\nChapter 19 discusses some aspects of this, but it’s a very broad topic called\\nML Operations (MLOps), which deserves its own book. So don’t be\\nsurprised if your first ML project takes a lot of effort and time to build and\\ndeploy to production. Fortunately, once all the infrastructure is in place,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 171}, page_content='going from idea to production will be much faster.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 172}, page_content='Try It Out!\\nHopefully this chapter gave you a good idea of what a machine learning\\nproject looks like as well as showing you some of the tools you can use to\\ntrain a great system. As you can see, much of the work is in the data\\npreparation step: building monitoring tools, setting up human evaluation\\npipelines, and automating regular model training. The machine learning\\nalgorithms are important, of course, but it is probably preferable to be\\ncomfortable with the overall process and know three or four algorithms well\\nrather than to spend all your time exploring advanced algorithms.\\nSo, if you have not already done so, now is a good time to pick up a laptop,\\nselect a dataset that you are interested in, and try to go through the whole\\nprocess from A to Z. A good place to start is on a competition website such\\nas Kaggle: you will have a dataset to play with, a clear goal, and people to\\nshare the experience with. Have fun!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 173}, page_content='Exercises\\nThe following exercises are based on this chapter’s housing dataset:\\n1. Try a support vector machine regressor (sklearn.svm.SVR) with various\\nhyperparameters, such as kernel=\"linear\" (with various values for the C\\nhyperparameter) or kernel=\"rbf\" (with various values for the C and\\ngamma hyperparameters). Note that support vector machines don’t scale\\nwell to large datasets, so you should probably train your model on just\\nthe first 5,000 instances of the training set and use only 3-fold cross-\\nvalidation, or else it will take hours. Don’t worry about what the\\nhyperparameters mean for now; we’ll discuss them in Chapter 5. How\\ndoes the best SVR predictor perform?\\n2. Try replacing the GridSearchCV with a RandomizedSearchCV.\\n3. Try adding a SelectFromModel transformer in the preparation pipeline\\nto select only the most important attributes.\\n4. Try creating a custom transformer that trains a k-nearest neighbors\\nregressor (sklearn.neighbors.KNeighborsRegressor) in its fit() method,\\nand outputs the model’s predictions in its transform() method. Then add\\nthis feature to the preprocessing pipeline, using latitude and longitude as\\nthe inputs to this transformer. This will add a feature in the model that\\ncorresponds to the housing median price of the nearest districts.\\n5. Automatically explore some preparation options using GridSearchCV.\\n6. Try to implement the StandardScalerClone class again from scratch,\\nthen add support for the inverse_transform() method: executing scaler.\\ninverse_transform(scaler.fit_transform(X)) should return an array very\\nclose to X. Then add support for feature names: set feature_names_in_\\nin the fit() method if the input is a DataFrame. This attribute should be a\\nNumPy array of column names. Lastly, implement the\\nget_feature_names_out() method: it should have one optional\\ninput_features=None argument. If passed, the method should check that'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 174}, page_content='its length matches n_features_in_, and it should match\\nfeature_names_in_ if it is defined; then input_features should be\\nreturned. If input_features is None, then the method should either return\\nfeature_names_in_ if it is defined or np.array([\"x0\", \"x1\", ...]) with\\nlength n_features_in_ otherwise.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial\\nAutoregressions”, Statistics & Probability Letters 33, no. 3 (1997): 291–297.\\n2  A piece of information fed to a machine learning system is often called a signal, in reference to\\nClaude Shannon’s information theory, which he developed at Bell Labs to improve\\ntelecommunications. His theory: you want a high signal-to-noise ratio.\\n3  Recall that the transpose operator flips a column vector into a row vector (and vice versa).\\n4  You might also need to check legal constraints, such as private fields that should never be\\ncopied to unsafe data stores.\\n5  The standard deviation is generally denoted σ (the Greek letter sigma), and it is the square root\\nof the variance, which is the average of the squared deviation from the mean. When a feature has\\na bell-shaped normal distribution (also called a Gaussian distribution), which is very common,\\nthe “68-95-99.7” rule applies: about 68% of the values fall within 1σ of the mean, 95% within 2σ,\\nand 99.7% within 3σ.\\n6  You will often see people set the random seed to 42. This number has no special property, other\\nthan being the Answer to the Ultimate Question of Life, the Universe, and Everything.\\n7  The location information is actually quite coarse, and as a result many districts will have the\\nexact same ID, so they will end up in the same set (test or train). This introduces some\\nunfortunate sampling bias.\\n8  If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from\\nthe Bay Area down to San Diego (as you might expect). You can add a patch of yellow around\\nSacramento as well.\\n9  For more details on the design principles, see Lars Buitinck et al., “API Design for Machine\\nLearning Software: Experiences from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238\\n(2013).\\n10  Some predictors also provide methods to measure the confidence of their predictions.\\n11  By the time you read these lines, it may be possible to make all transformers output Pandas\\nDataFrames when they receive a DataFrame as input: Pandas in, Pandas out. There will likely be\\na global configuration option for this: sklearn.set_config(pandas_in_out=True).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 175}, page_content='12  See SciPy’s documentation for more details.\\n13  In a nutshell, a REST (or RESTful) API is an HTTP-based API that follows some conventions,\\nsuch as using standard HTTP verbs to read, update, create, or delete resources (GET, POST,\\nPUT, and DELETE) and using JSON for the inputs and outputs.\\n14  A captcha is a test to ensure a user is not a robot. These tests have often been used as a cheap\\nway to label training data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 176}, page_content='Chapter 3. Classification\\nIn Chapter 1 I mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In\\nChapter 2 we explored a regression task, predicting housing values, using\\nvarious algorithms such as linear regression, decision trees, and random\\nforests (which will be explained in further detail in later chapters). Now we\\nwill turn our attention to classification systems.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 177}, page_content='MNIST\\nIn this chapter we will be using the MNIST dataset, which is a set of 70,000\\nsmall images of digits handwritten by high school students and employees of\\nthe US Census Bureau. Each image is labeled with the digit it represents.\\nThis set has been studied so much that it is often called the “hello world” of\\nmachine learning: whenever people come up with a new classification\\nalgorithm they are curious to see how it will perform on MNIST, and anyone\\nwho learns machine learning tackles this dataset sooner or later.\\nScikit-Learn provides many helper functions to download popular datasets.\\nMNIST is one of them. The following code fetches the MNIST dataset from\\nOpenML.org:\\u2060\\nfrom sklearn.datasets import fetch_openml\\nmnist = fetch_openml(\\'mnist_784\\', as_frame=False)\\nThe sklearn.datasets package contains mostly three types of functions:\\nfetch_* functions such as fetch_openml() to download real-life datasets,\\nload_* functions to load small toy datasets bundled with Scikit-Learn (so\\nthey don’t need to be downloaded over the internet), and make_* functions to\\ngenerate fake datasets, useful for tests. Generated datasets are usually\\nreturned as an (X, y) tuple containing the input data and the targets, both as\\nNumPy arrays. Other datasets are returned as sklearn.utils.Bunch objects,\\nwhich are dictionaries whose entries can also be accessed as attributes. They\\ngenerally contain the following entries:\\n\"DESCR\"\\nA description of the dataset\\n\"data\"\\nThe input data, usually as a 2D NumPy array\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 178}, page_content='\"target\"\\nThe labels, usually as a 1D NumPy array\\nThe fetch_openml() function is a bit unusual since by default it returns the\\ninputs as a Pandas DataFrame and the labels as a Pandas Series (unless the\\ndataset is sparse). But the MNIST dataset contains images, and DataFrames\\naren’t ideal for that, so it’s preferable to set as_frame=False to get the data as\\nNumPy arrays instead. Let’s look at these arrays:\\n>>> X, y = mnist.data, mnist.target\\n>>> X\\narray([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]])\\n>>> X.shape\\n(70000, 784)\\n>>> y\\narray([\\'5\\', \\'0\\', \\'4\\', ..., \\'4\\', \\'5\\', \\'6\\'], dtype=object)\\n>>> y.shape\\n(70000,)\\nThere are 70,000 images, and each image has 784 features. This is because\\neach image is 28 × 28 pixels, and each feature simply represents one pixel’s\\nintensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from\\nthe dataset (Figure 3-1). All we need to do is grab an instance’s feature\\nvector, reshape it to a 28 × 28 array, and display it using Matplotlib’s\\nimshow() function. We use cmap=\"binary\" to get a grayscale color map\\nwhere 0 is white and 255 is black:\\nimport matplotlib.pyplot as plt\\ndef plot_digit(image_data):\\n    image = image_data.reshape(28, 28)\\n    plt.imshow(image, cmap=\"binary\")\\n    plt.axis(\"off\")'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 179}, page_content=\"some_digit = X[0]\\nplot_digit(some_digit)\\nplt.show()\\nFigure 3-1. Example of an MNIST image\\nThis looks like a 5, and indeed that’s what the label tells us:\\n>>> y[0]\\n'5'\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 180}, page_content='To give you a feel for the complexity of the classification task, Figure 3-2\\nshows a few more images from the MNIST dataset.\\nBut wait! You should always create a test set and set it aside before\\ninspecting the data closely. The MNIST dataset returned by fetch_openml() is\\nactually already split into a training set (the first 60,000 images) and a test set\\n(the last 10,000 images):\\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\\nThe training set is already shuffled for us, which is good because this\\nguarantees that all cross-validation folds will be similar (we don’t want one\\nfold to be missing some digits). Moreover, some learning algorithms are\\nsensitive to the order of the training instances, and they perform poorly if\\nthey get many similar instances in a row. Shuffling the dataset ensures that\\nthis won’t happen.\\u2060\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 181}, page_content='Figure 3-2. Digits from the MNIST dataset'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 182}, page_content=\"Training a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for\\nexample, the number 5. This “5-detector” will be an example of a binary\\nclassifier, capable of distinguishing between just two classes, 5 and non-5.\\nFirst we’ll create the target vectors for this classification task:\\ny_train_5 = (y_train == '5')  # True for all 5s, False for all other digits\\ny_test_5 = (y_test == '5')\\nNow let’s pick a classifier and train it. A good place to start is with a\\nstochastic gradient descent (SGD, or stochastic GD) classifier, using Scikit-\\nLearn’s SGDClassifier class. This classifier is capable of handling very large\\ndatasets efficiently. This is in part because SGD deals with training instances\\nindependently, one at a time, which also makes SGD well suited for online\\nlearning, as you will see later. Let’s create an SGDClassifier and train it on\\nthe whole training set:\\nfrom sklearn.linear_model import SGDClassifier\\nsgd_clf = SGDClassifier(random_state=42)\\nsgd_clf.fit(X_train, y_train_5)\\nNow we can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit])\\narray([ True])\\nThe classifier guesses that this image represents a 5 (True). Looks like it\\nguessed right in this particular case! Now, let’s evaluate this model’s\\nperformance.\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 183}, page_content='Performance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a\\nregressor, so we will spend a large part of this chapter on this topic. There are\\nmany performance measures available, so grab another coffee and get ready\\nto learn a bunch of new concepts and acronyms!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 184}, page_content='Measuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in\\nChapter 2. Let’s use the cross_val_score() function to evaluate our\\nSGDClassifier model, using k-fold cross-validation with three folds.\\nRemember that k-fold cross-validation means splitting the training set into k\\nfolds (in this case, three), then training the model k times, holding out a\\ndifferent fold each time for evaluation (see Chapter 2):\\n>>> from sklearn.model_selection import cross_val_score\\n>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\\narray([0.95035, 0.96035, 0.9604 ])\\nWow! Above 95% accuracy (ratio of correct predictions) on all cross-\\nvalidation folds? This looks amazing, doesn’t it? Well, before you get too\\nexcited, let’s look at a dummy classifier that just classifies every single image\\nin the most frequent class, which in this case is the negative class (i.e., non\\n5):\\nfrom sklearn.dummy import DummyClassifier\\ndummy_clf = DummyClassifier()\\ndummy_clf.fit(X_train, y_train_5)\\nprint(any(dummy_clf.predict(X_train)))  # prints False: no 5s detected\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\\narray([0.90965, 0.90965, 0.90965])\\nThat’s right, it has over 90% accuracy! This is simply because only about\\n10% of the images are 5s, so if you always guess that an image is not a 5, you\\nwill be right about 90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance\\nmeasure for classifiers, especially when you are dealing with skewed datasets\\n(i.e., when some classes are much more frequent than others). A much better'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 185}, page_content='way to evaluate the performance of a classifier is to look at the confusion\\nmatrix (CM).\\nIMPLEMENTING CROSS-VALIDATION\\nOccasionally you will need more control over the cross-validation\\nprocess than what Scikit-Learn provides off the shelf. In these cases, you\\ncan implement cross-validation yourself. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score() function, and\\nit prints the same result:\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.base import clone\\nskfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is\\n                                       # not already shuffled\\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\\n    clone_clf = clone(sgd_clf)\\n    X_train_folds = X_train[train_index]\\n    y_train_folds = y_train_5[train_index]\\n    X_test_fold = X_train[test_index]\\n    y_test_fold = y_train_5[test_index]\\n    clone_clf.fit(X_train_folds, y_train_folds)\\n    y_pred = clone_clf.predict(X_test_fold)\\n    n_correct = sum(y_pred == y_test_fold)\\n    print(n_correct / len(y_pred))  # prints 0.95035, 0.96035, and 0.9604\\nThe StratifiedKFold class performs stratified sampling (as explained in\\nChapter 2) to produce folds that contain a representative ratio of each\\nclass. At each iteration the code creates a clone of the classifier, trains\\nthat clone on the training folds, and makes predictions on the test fold.\\nThen it counts the number of correct predictions and outputs the ratio of\\ncorrect predictions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 186}, page_content='Confusion Matrices\\nThe general idea of a confusion matrix is to count the number of times\\ninstances of class A are classified as class B, for all A/B pairs. For example,\\nto know the number of times the classifier confused images of 8s with 0s, you\\nwould look at row #8, column #0 of the confusion matrix.\\nTo compute the confusion matrix, you first need to have a set of predictions\\nso that they can be compared to the actual targets. You could make\\npredictions on the test set, but it’s best to keep that untouched for now\\n(remember that you want to use the test set only at the very end of your\\nproject, once you have a classifier that you are ready to launch). Instead, you\\ncan use the cross_val_predict() function:\\nfrom sklearn.model_selection import cross_val_predict\\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\\nJust like the cross_val_score() function, cross_val_predict() performs k-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the\\npredictions made on each test fold. This means that you get a clean prediction\\nfor each instance in the training set (by “clean” I mean “out-of-sample”: the\\nmodel makes predictions on data that it never saw during training).\\nNow you are ready to get the confusion matrix using the confusion_matrix()\\nfunction. Just pass it the target classes (y_train_5) and the predicted classes\\n(y_train_pred):\\n>>> from sklearn.metrics import confusion_matrix\\n>>> cm = confusion_matrix(y_train_5, y_train_pred)\\n>>> cm\\narray([[53892,   687],\\n       [ 1891,  3530]])\\nEach row in a confusion matrix represents an actual class, while each column\\nrepresents a predicted class. The first row of this matrix considers non-5\\nimages (the negative class): 53,892 of them were correctly classified as non-'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 187}, page_content='5s (they are called true negatives), while the remaining 687 were wrongly\\nclassified as 5s (false positives, also called type I errors). The second row\\nconsiders the images of 5s (the positive class): 1,891 were wrongly classified\\nas non-5s (false negatives, also called type II errors), while the remaining\\n3,530 were correctly classified as 5s (true positives). A perfect classifier\\nwould only have true positives and true negatives, so its confusion matrix\\nwould have nonzero values only on its main diagonal (top left to bottom\\nright):\\n>>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection\\n>>> confusion_matrix(y_train_5, y_train_perfect_predictions)\\narray([[54579,     0],\\n       [    0,  5421]])\\nThe confusion matrix gives you a lot of information, but sometimes you may\\nprefer a more concise metric. An interesting one to look at is the accuracy of\\nthe positive predictions; this is called the precision of the classifier (Equation\\n3-1).\\nEquation 3-1. Precision\\nprecision = TP TP+FP\\nTP is the number of true positives, and FP is the number of false positives.\\nA trivial way to have perfect precision is to create a classifier that always\\nmakes negative predictions, except for one single positive prediction on the\\ninstance it’s most confident about. If this one prediction is correct, then the\\nclassifier has 100% precision (precision = 1/1 = 100%). Obviously, such a\\nclassifier would not be very useful, since it would ignore all but one positive\\ninstance. So, precision is typically used along with another metric named\\nrecall, also called sensitivity or the true positive rate (TPR): this is the ratio\\nof positive instances that are correctly detected by the classifier (Equation 3-\\n2).\\nEquation 3-2. Recall\\nrecall = TP TP+FN'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 188}, page_content='FN is, of course, the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-3 may help.\\nFigure 3-3. An illustrated confusion matrix showing examples of true negatives (top left), false positives\\n(top right), false negatives (lower left), and true positives (lower right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 189}, page_content='Precision and Recall\\nScikit-Learn provides several functions to compute classifier metrics,\\nincluding precision and recall:\\n>>> from sklearn.metrics import precision_score, recall_score\\n>>> precision_score(y_train_5, y_train_pred)  # == 3530 / (687 + 3530)\\n0.8370879772350012\\n>>> recall_score(y_train_5, y_train_pred)  # == 3530 / (1891 + 3530)\\n0.6511713705958311\\nNow our 5-detector does not look as shiny as it did when we looked at its\\naccuracy. When it claims an image represents a 5, it is correct only 83.7% of\\nthe time. Moreover, it only detects 65.1% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric\\ncalled the F  score, especially when you need a single metric to compare two\\nclassifiers. The F  score is the harmonic mean of precision and recall\\n(Equation 3-3). Whereas the regular mean treats all values equally, the\\nharmonic mean gives much more weight to low values. As a result, the\\nclassifier will only get a high F  score if both recall and precision are high.\\nEquation 3-3. F  score\\nF 1 = 2 1 precision+1 recall = 2 × precision×recall precision+recall = TP\\nTP+FN+FP 2\\nTo compute the F  score, simply call the f1_score() function:\\n>>> from sklearn.metrics import f1_score\\n>>> f1_score(y_train_5, y_train_pred)\\n0.7325171197343846\\nThe F  score favors classifiers that have similar precision and recall. This is\\nnot always what you want: in some contexts you mostly care about precision,\\nand in other contexts you really care about recall. For example, if you trained\\na classifier to detect videos that are safe for kids, you would probably prefer a\\nclassifier that rejects many good videos (low recall) but keeps only safe ones\\n1\\n1\\n1\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 190}, page_content='(high precision), rather than a classifier that has a much higher recall but lets\\na few really bad videos show up in your product (in such cases, you may\\neven want to add a human pipeline to check the classifier’s video selection).\\nOn the other hand, suppose you train a classifier to detect shoplifters in\\nsurveillance images: it is probably fine if your classifier only has 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few\\nfalse alerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces\\nrecall, and vice versa. This is called the precision/recall trade-off.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 191}, page_content='The Precision/Recall Trade-off\\nTo understand this trade-off, let’s look at how the SGDClassifier makes its\\nclassification decisions. For each instance, it computes a score based on a\\ndecision function. If that score is greater than a threshold, it assigns the\\ninstance to the positive class; otherwise it assigns it to the negative class.\\nFigure 3-4 shows a few digits positioned from the lowest score on the left to\\nthe highest score on the right. Suppose the decision threshold is positioned at\\nthe central arrow (between the two 5s): you will find 4 true positives (actual\\n5s) on the right of that threshold, and 1 false positive (actually a 6).\\nTherefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you\\nraise the threshold (move it to the arrow on the right), the false positive (the\\n6) becomes a true negative, thereby increasing the precision (up to 100% in\\nthis case), but one true positive becomes a false negative, decreasing recall\\ndown to 50%. Conversely, lowering the threshold increases recall and\\nreduces precision.\\nFigure 3-4. The precision/recall trade-off: images are ranked by their classifier score, and those above\\nthe chosen decision threshold are considered positive; the higher the threshold, the lower the recall,\\nbut (in general) the higher the precision\\nScikit-Learn does not let you set the threshold directly, but it does give you\\naccess to the decision scores that it uses to make predictions. Instead of\\ncalling the classifier’s predict() method, you can call its decision_function()\\nmethod, which returns a score for each instance, and then use any threshold\\nyou want to make predictions based on those scores:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 192}, page_content='>>> y_scores = sgd_clf.decision_function([some_digit])\\n>>> y_scores\\narray([2164.22030239])\\n>>> threshold = 0\\n>>> y_some_digit_pred = (y_scores > threshold)\\narray([ True])\\nThe SGDClassifier uses a threshold equal to 0, so the preceding code returns\\nthe same result as the predict() method (i.e., True). Let’s raise the threshold:\\n>>> threshold = 3000\\n>>> y_some_digit_pred = (y_scores > threshold)\\n>>> y_some_digit_pred\\narray([False])\\nThis confirms that raising the threshold decreases recall. The image actually\\nrepresents a 5, and the classifier detects it when the threshold is 0, but it\\nmisses it when the threshold is increased to 3,000.\\nHow do you decide which threshold to use? First, use the cross_val_predict()\\nfunction to get the scores of all instances in the training set, but this time\\nspecify that you want to return decision scores instead of predictions:\\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\\n                             method=\"decision_function\")\\nWith these scores, use the precision_recall_curve() function to compute\\nprecision and recall for all possible thresholds (the function adds a last\\nprecision of 0 and a last recall of 1, corresponding to an infinite threshold):\\nfrom sklearn.metrics import precision_recall_curve\\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\\nFinally, use Matplotlib to plot precision and recall as functions of the\\nthreshold value (Figure 3-5). Let’s show the threshold of 3,000 we selected:\\nplt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\\nplt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\\nplt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 193}, page_content='[...]  # beautify the figure: add grid, legend, axis, labels, and circles\\nplt.show()\\nFigure 3-5. Precision and recall versus the decision threshold\\nNOTE\\nYou may wonder why the precision curve is bumpier than the recall curve in Figure 3-5.\\nThe reason is that precision may sometimes go down when you raise the threshold\\n(although in general it will go up). To understand why, look back at Figure 3-4 and notice\\nwhat happens when you start from the central threshold and move it just one digit to the\\nright: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can\\nonly go down when the threshold is increased, which explains why its curve looks smooth.\\nAt this threshold value, precision is near 90% and recall is around 50%.\\nAnother way to select a good precision/recall trade-off is to plot precision\\ndirectly against recall, as shown in Figure 3-6 (the same threshold is shown):\\nplt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\\n[...]  # beautify the figure: add labels, grid, legend, arrow, and text\\nplt.show()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 194}, page_content='Figure 3-6. Precision versus recall\\nYou can see that precision really starts to fall sharply at around 80% recall.\\nYou will probably want to select a precision/recall trade-off just before that\\ndrop—for example, at around 60% recall. But of course, the choice depends\\non your project.\\nSuppose you decide to aim for 90% precision. You could use the first plot to\\nfind the threshold you need to use, but that’s not very precise. Alternatively,\\nyou can search for the lowest threshold that gives you at least 90% precision.\\nFor this, you can use the NumPy array’s argmax() method. This returns the\\nfirst index of the maximum value, which in this case means the first True\\nvalue:\\n>>> idx_for_90_precision = (precisions >= 0.90).argmax()\\n>>> threshold_for_90_precision = thresholds[idx_for_90_precision]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 195}, page_content='>>> threshold_for_90_precision\\n3370.0194991439557\\nTo make predictions (on the training set for now), instead of calling the\\nclassifier’s predict() method, you can run this code:\\ny_train_pred_90 = (y_scores >= threshold_for_90_precision)\\nLet’s check these predictions’ precision and recall:\\n>>> precision_score(y_train_5, y_train_pred_90)\\n0.9000345901072293\\n>>> recall_at_90_precision = recall_score(y_train_5, y_train_pred_90)\\n>>> recall_at_90_precision\\n0.4799852425751706\\nGreat, you have a 90% precision classifier! As you can see, it is fairly easy to\\ncreate a classifier with virtually any precision you want: just set a high\\nenough threshold, and you’re done. But wait, not so fast–a high-precision\\nclassifier is not very useful if its recall is too low! For many applications,\\n48% recall wouldn’t be great at all.\\nTIP\\nIf someone says, “Let’s reach 99% precision”, you should ask, “At what recall?”'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 196}, page_content='The ROC Curve\\nThe receiver operating characteristic (ROC) curve is another common tool\\nused with binary classifiers. It is very similar to the precision/recall curve, but\\ninstead of plotting precision versus recall, the ROC curve plots the true\\npositive rate (another name for recall) against the false positive rate (FPR).\\nThe FPR (also called the fall-out) is the ratio of negative instances that are\\nincorrectly classified as positive. It is equal to 1 – the true negative rate\\n(TNR), which is the ratio of negative instances that are correctly classified as\\nnegative. The TNR is also called specificity. Hence, the ROC curve plots\\nsensitivity (recall) versus 1 – specificity.\\nTo plot the ROC curve, you first use the roc_curve() function to compute the\\nTPR and FPR for various threshold values:\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\\nThen you can plot the FPR against the TPR using Matplotlib. The following\\ncode produces the plot in Figure 3-7. To find the point that corresponds to\\n90% precision, we need to look for the index of the desired threshold. Since\\nthresholds are listed in decreasing order in this case, we use <= instead of >=\\non the first line:\\nidx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\\ntpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\\nplt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\\nplt.plot([0, 1], [0, 1], \\'k:\\', label=\"Random classifier\\'s ROC curve\")\\nplt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\\n[...]  # beautify the figure: add labels, grid, legend, arrow, and text\\nplt.show()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 197}, page_content='Figure 3-7. A ROC curve plotting the false positive rate against the true positive rate for all possible\\nthresholds; the black circle highlights the chosen ratio (at 90% precision and 48% recall)\\nOnce again there is a trade-off: the higher the recall (TPR), the more false\\npositives (FPR) the classifier produces. The dotted line represents the ROC\\ncurve of a purely random classifier; a good classifier stays as far away from\\nthat line as possible (toward the top-left corner).\\nOne way to compare classifiers is to measure the area under the curve\\n(AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a\\npurely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn\\nprovides a function to estimate the ROC AUC:\\n>>> from sklearn.metrics import roc_auc_score\\n>>> roc_auc_score(y_train_5, y_scores)\\n0.9604938554008616'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 198}, page_content='TIP\\nSince the ROC curve is so similar to the precision/recall (PR) curve, you may wonder how\\nto decide which one to use. As a rule of thumb, you should prefer the PR curve whenever\\nthe positive class is rare or when you care more about the false positives than the false\\nnegatives. Otherwise, use the ROC curve. For example, looking at the previous ROC\\ncurve (and the ROC AUC score), you may think that the classifier is really good. But this\\nis mostly because there are few positives (5s) compared to the negatives (non-5s). In\\ncontrast, the PR curve makes it clear that the classifier has room for improvement: the\\ncurve could really be closer to the top-right corner (see Figure 3-6 again).\\nLet’s now create a RandomForestClassifier, whose PR curve and F  score we\\ncan compare to those of the SGDClassifier:\\nfrom sklearn.ensemble import RandomForestClassifier\\nforest_clf = RandomForestClassifier(random_state=42)\\nThe precision_recall_curve() function expects labels and scores for each\\ninstance, so we need to train the random forest classifier and make it assign a\\nscore to each instance. But the RandomForestClassifier class does not have a\\ndecision_function() method, due to the way it works (we will cover this in\\nChapter 7). Luckily, it has a predict_proba() method that returns class\\nprobabilities for each instance, and we can just use the probability of the\\npositive class as a score, so it will work fine.  We can call the\\ncross_val_predict() function to train the RandomForestClassifier using cross-\\nvalidation and make it predict class probabilities for every image as follows:\\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\\n                                    method=\"predict_proba\")\\nLet’s look at the class probabilities for the first two images in the training set:\\n>>> y_probas_forest[:2]\\narray([[0.11, 0.89],\\n       [0.99, 0.01]])\\nThe model predicts that the first image is positive with 89% probability, and\\n1\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 199}, page_content='it predicts that the second image is negative with 99% probability. Since each\\nimage is either positive or negative, the probabilities in each row add up to\\n100%.\\nWARNING\\nThese are estimated probabilities, not actual probabilities. For example, if you look at all\\nthe images that the model classified as positive with an estimated probability between 50%\\nand 60%, roughly 94% of them are actually positive. So, the model’s estimated\\nprobabilities were much too low in this case—but models can be overconfident as well.\\nThe sklearn.calibration package contains tools to calibrate the estimated probabilities and\\nmake them much closer to actual probabilities. See the extra material section in this\\nchapter’s notebook for more details.\\nThe second column contains the estimated probabilities for the positive class,\\nso let’s pass them to the precision_recall_curve() function:\\ny_scores_forest = y_probas_forest[:, 1]\\nprecisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\\n    y_train_5, y_scores_forest)\\nNow we’re ready to plot the PR curve. It is useful to plot the first PR curve as\\nwell to see how they compare (Figure 3-8):\\nplt.plot(recalls_forest, precisions_forest, \"b-\", linewidth=2,\\n         label=\"Random Forest\")\\nplt.plot(recalls, precisions, \"--\", linewidth=2, label=\"SGD\")\\n[...]  # beautify the figure: add labels, grid, and legend\\nplt.show()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 200}, page_content='Figure 3-8. Comparing PR curves: the random forest classifier is superior to the SGD classifier\\nbecause its PR curve is much closer to the top-right corner, and it has a greater AUC\\nAs you can see in Figure 3-8, the RandomForestClassifier’s PR curve looks\\nmuch better than the SGDClassifier’s: it comes much closer to the top-right\\ncorner. Its F  score and ROC AUC score are also significantly better:\\n>>> y_train_pred_forest = y_probas_forest[:, 1] >= 0.5  # positive proba ≥ 50%\\n>>> f1_score(y_train_5, y_pred_forest)\\n0.9242275142688446\\n>>> roc_auc_score(y_train_5, y_scores_forest)\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find about 99.1%\\nprecision and 86.6% recall. Not too bad!\\nYou now know how to train binary classifiers, choose the appropriate metric\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 201}, page_content='for your task, evaluate your classifiers using cross-validation, select the\\nprecision/recall trade-off that fits your needs, and use several metrics and\\ncurves to compare various models. You’re ready to try to detect more than\\njust the 5s.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 202}, page_content='Multiclass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass\\nclassifiers (also called multinomial classifiers) can distinguish between more\\nthan two classes.\\nSome Scikit-Learn classifiers (e.g., LogisticRegression,\\nRandomForestClassifier, and GaussianNB) are capable of handling multiple\\nclasses natively. Others are strictly binary classifiers (e.g., SGDClassifier and\\nSVC). However, there are various strategies that you can use to perform\\nmulticlass classification with multiple binary classifiers.\\nOne way to create a system that can classify the digit images into 10 classes\\n(from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector,\\na 1-detector, a 2-detector, and so on). Then when you want to classify an\\nimage, you get the decision score from each classifier for that image and you\\nselect the class whose classifier outputs the highest score. This is called the\\none-versus-the-rest (OvR) strategy, or sometimes one-versus-all (OvA).\\nAnother strategy is to train a binary classifier for every pair of digits: one to\\ndistinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s,\\nand so on. This is called the one-versus-one (OvO) strategy. If there are N\\nclasses, you need to train N × (N – 1) / 2 classifiers. For the MNIST problem,\\nthis means training 45 binary classifiers! When you want to classify an\\nimage, you have to run the image through all 45 classifiers and see which\\nclass wins the most duels. The main advantage of OvO is that each classifier\\nonly needs to be trained on the part of the training set containing the two\\nclasses that it must distinguish.\\nSome algorithms (such as support vector machine classifiers) scale poorly\\nwith the size of the training set. For these algorithms OvO is preferred\\nbecause it is faster to train many classifiers on small training sets than to train\\nfew classifiers on large training sets. For most binary classification\\nalgorithms, however, OvR is preferred.\\nScikit-Learn detects when you try to use a binary classification algorithm for'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 203}, page_content=\"a multiclass classification task, and it automatically runs OvR or OvO,\\ndepending on the algorithm. Let’s try this with a support vector machine\\nclassifier using the sklearn.svm.SVC class (see Chapter 5). We’ll only train\\non the first 2,000 images, or else it will take a very long time:\\nfrom sklearn.svm import SVC\\nsvm_clf = SVC(random_state=42)\\nsvm_clf.fit(X_train[:2000], y_train[:2000])  # y_train, not y_train_5\\nThat was easy! We trained the SVC using the original target classes from 0 to\\n9 (y_train), instead of the 5-versus-the-rest target classes (y_train_5). Since\\nthere are 10 classes (i.e., more than 2), Scikit-Learn used the OvO strategy\\nand trained 45 binary classifiers. Now let’s make a prediction on an image:\\n>>> svm_clf.predict([some_digit])\\narray(['5'], dtype=object)\\nThat’s correct! This code actually made 45 predictions—one per pair of\\nclasses—and it selected the class that won the most duels. If you call the\\ndecision_function() method, you will see that it returns 10 scores per\\ninstance: one per class. Each class gets a score equal to the number of won\\nduels plus or minus a small tweak (max ±0.33) to break ties, based on the\\nclassifier scores:\\n>>> some_digit_scores = svm_clf.decision_function([some_digit])\\n>>> some_digit_scores.round(2)\\narray([[ 3.79,  0.73,  6.06,  8.3 , -0.29,  9.3 ,  1.75,  2.77,  7.21,\\n         4.82]])\\nThe highest score is 9.3, and it’s indeed the one corresponding to class 5:\\n>>> class_id = some_digit_scores.argmax()\\n>>> class_id\\n5\\nWhen a classifier is trained, it stores the list of target classes in its classes_\\nattribute, ordered by value. In the case of MNIST, the index of each class in\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 204}, page_content=\"the classes_ array conveniently matches the class itself (e.g., the class at\\nindex 5 happens to be class '5'), but in general you won’t be so lucky; you\\nwill need to look up the class label like this:\\n>>> svm_clf.classes_\\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)\\n>>> svm_clf.classes_[class_id]\\n'5'\\nIf you want to force Scikit-Learn to use one-versus-one or one-versus-the-\\nrest, you can use the OneVsOneClassifier or OneVsRestClassifier classes.\\nSimply create an instance and pass a classifier to its constructor (it doesn’t\\neven have to be a binary classifier). For example, this code creates a\\nmulticlass classifier using the OvR strategy, based on an SVC:\\nfrom sklearn.multiclass import OneVsRestClassifier\\novr_clf = OneVsRestClassifier(SVC(random_state=42))\\novr_clf.fit(X_train[:2000], y_train[:2000])\\nLet’s make a prediction, and check the number of trained classifiers:\\n>>> ovr_clf.predict([some_digit])\\narray(['5'], dtype='<U1')\\n>>> len(ovr_clf.estimators_)\\n10\\nTraining an SGDClassifier on a multiclass dataset and using it to make\\npredictions is just as easy:\\n>>> sgd_clf = SGDClassifier(random_state=42)\\n>>> sgd_clf.fit(X_train, y_train)\\n>>> sgd_clf.predict([some_digit])\\narray(['3'], dtype='<U1')\\nOops, that’s incorrect. Prediction errors do happen! This time Scikit-Learn\\nused the OvR strategy under the hood: since there are 10 classes, it trained 10\\nbinary classifiers. The decision_function() method now returns one value per\\nclass. Let’s look at the scores that the SGD classifier assigned to each class:\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 205}, page_content='>>> sgd_clf.decision_function([some_digit]).round()\\narray([[-31893., -34420.,  -9531.,   1824., -22320.,  -1386., -26189.,\\n        -16148.,  -4604., -12051.]])\\nYou can see that the classifier is not very confident about its prediction:\\nalmost all scores are very negative, while class 3 has a score of +1,824, and\\nclass 5 is not too far behind at –1,386. Of course, you’ll want to evaluate this\\nclassifier on more than one image. Since there are roughly the same number\\nof images in each class, the accuracy metric is fine. As usual, you can use the\\ncross_val_score() function to evaluate the model:\\n>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\\narray([0.87365, 0.85835, 0.8689 ])\\nIt gets over 85.8% on all test folds. If you used a random classifier, you\\nwould get 10% accuracy, so this is not such a bad score, but you can still do\\nmuch better. Simply scaling the inputs (as discussed in Chapter 2) increases\\naccuracy above 89.1%:\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> scaler = StandardScaler()\\n>>> X_train_scaled = scaler.fit_transform(X_train.astype(\"float64\"))\\n>>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\\narray([0.8983, 0.891 , 0.9018])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 206}, page_content='Error Analysis\\nIf this were a real project, you would now follow the steps in your machine\\nlearning project checklist (see Appendix A). You’d explore data preparation\\noptions, try out multiple models, shortlist the best ones, fine-tune their\\nhyperparameters using GridSearchCV, and automate as much as possible.\\nHere, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors\\nit makes.\\nFirst, look at the confusion matrix. For this, you first need to make\\npredictions using the cross_val_predict() function; then you can pass the\\nlabels and predictions to the confusion_matrix() function, just like you did\\nearlier. However, since there are now 10 classes instead of 2, the confusion\\nmatrix will contain quite a lot of numbers, and it may be hard to read.\\nA colored diagram of the confusion matrix is much easier to analyze. To plot\\nsuch a diagram, use the ConfusionMatrixDisplay.from_predictions() function\\nlike this:\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\\nplt.show()\\nThis produces the left diagram in Figure 3-9. This confusion matrix looks\\npretty good: most images are on the main diagonal, which means that they\\nwere classified correctly. Notice that the cell on the diagonal in row #5 and\\ncolumn #5 looks slightly darker than the other digits. This could be because\\nthe model made more errors on 5s, or because there are fewer 5s in the\\ndataset than the other digits. That’s why it’s important to normalize the\\nconfusion matrix by dividing each value by the total number of images in the\\ncorresponding (true) class (i.e., divide by the row’s sum). This can be done\\nsimply by setting normalize=\"true\". We can also specify the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 207}, page_content='values_format=\".0%\" argument to show percentages with no decimals. The\\nfollowing code produces the diagram on the right in Figure 3-9:\\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\\n                                        normalize=\"true\", values_format=\".0%\")\\nplt.show()\\nNow we can easily see that only 82% of the images of 5s were classified\\ncorrectly. The most common error the model made with images of 5s was to\\nmisclassify them as 8s: this happened for 10% of all 5s. But only 2% of 8s\\ngot misclassified as 5s; confusion matrices are generally not symmetrical! If\\nyou look carefully, you will notice that many digits have been misclassified\\nas 8s, but this is not immediately obvious from this diagram. If you want to\\nmake the errors stand out more, you can try putting zero weight on the correct\\npredictions. The following code does just that and produces the diagram on\\nthe left in Figure 3-10:\\nsample_weight = (y_train_pred != y_train)\\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\\n                                        sample_weight=sample_weight,\\n                                        normalize=\"true\", values_format=\".0%\")\\nplt.show()\\nFigure 3-9. Confusion matrix (left) and the same CM normalized by row (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 208}, page_content='Figure 3-10. Confusion matrix with errors only, normalized by row (left) and by column (right)\\nNow you can see much more clearly the kinds of errors the classifier makes.\\nThe column for class 8 is now really bright, which confirms that many\\nimages got misclassified as 8s. In fact this is the most common\\nmisclassification for almost all classes. But be careful how you interpret the\\npercentages in this diagram: remember that we’ve excluded the correct\\npredictions. For example, the 36% in row #7, column #9 does not mean that\\n36% of all images of 7s were misclassified as 9s. It means that 36% of the\\nerrors the model made on images of 7s were misclassifications as 9s. In\\nreality, only 3% of images of 7s were misclassified as 9s, as you can see in\\nthe diagram on the right in Figure 3-9.\\nIt is also possible to normalize the confusion matrix by column rather than by\\nrow: if you set normalize=\"pred\", you get the diagram on the right in\\nFigure 3-10. For example, you can see that 56% of misclassified 7s are\\nactually 9s.\\nAnalyzing the confusion matrix often gives you insights into ways to improve\\nyour classifier. Looking at these plots, it seems that your efforts should be\\nspent on reducing the false 8s. For example, you could try to gather more\\ntraining data for digits that look like 8s (but are not) so that the classifier can\\nlearn to distinguish them from real 8s. Or you could engineer new features\\nthat would help the classifier—for example, writing an algorithm to count the\\nnumber of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you could'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 209}, page_content=\"preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make\\nsome patterns, such as closed loops, stand out more.\\nAnalyzing individual errors can also be a good way to gain insights into what\\nyour classifier is doing and why it is failing. For example, let’s plot examples\\nof 3s and 5s in a confusion matrix style (Figure 3-11):\\ncl_a, cl_b = '3', '5'\\nX_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\\nX_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\\nX_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\\nX_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\\n[...]  # plot all images in X_aa, X_ab, X_ba, X_bb in a confusion matrix style\\nFigure 3-11. Some images of 3s and 5s organized like a confusion matrix\\nAs you can see, some of the digits that the classifier gets wrong (i.e., in the\\nbottom-left and top-right blocks) are so badly written that even a human\\nwould have trouble classifying them. However, most misclassified images\\nseem like obvious errors to us. It may be hard to understand why the\\nclassifier made the mistakes it did, but remember that the human brain is a\\nfantastic pattern recognition system, and our visual system does a lot of\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 210}, page_content='complex preprocessing before any information even reaches our\\nconsciousness. So, the fact that this task feels simple does not mean that it is.\\nRecall that we used a simple SGDClassifier, which is just a linear model: all\\nit does is assign a weight per class to each pixel, and when it sees a new\\nimage it just sums up the weighted pixel intensities to get a score for each\\nclass. Since 3s and 5s differ by only a few pixels, this model will easily\\nconfuse them.\\nThe main difference between 3s and 5s is the position of the small line that\\njoins the top line to the bottom arc. If you draw a 3 with the junction slightly\\nshifted to the left, the classifier might classify it as a 5, and vice versa. In\\nother words, this classifier is quite sensitive to image shifting and rotation.\\nOne way to reduce the 3/5 confusion is to preprocess the images to ensure\\nthat they are well centered and not too rotated. However, this may not be easy\\nsince it requires predicting the correct rotation of each image. A much\\nsimpler approach consists of augmenting the training set with slightly shifted\\nand rotated variants of the training images. This will force the model to learn\\nto be more tolerant to such variations. This is called data augmentation (we’ll\\ncover this in Chapter 14; also see exercise 2 at the end of this chapter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 211}, page_content=\"Multilabel Classification\\nUntil now, each instance has always been assigned to just one class. But in\\nsome cases you may want your classifier to output multiple classes for each\\ninstance. Consider a face-recognition classifier: what should it do if it\\nrecognizes several people in the same picture? It should attach one tag per\\nperson it recognizes. Say the classifier has been trained to recognize three\\nfaces: Alice, Bob, and Charlie. Then when the classifier is shown a picture of\\nAlice and Charlie, it should output [True, False, True] (meaning “Alice yes,\\nBob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler\\nexample, just for illustration purposes:\\nimport numpy as np\\nfrom sklearn.neighbors import KNeighborsClassifier\\ny_train_large = (y_train >= '7')\\ny_train_odd = (y_train.astype('int8') % 2 == 1)\\ny_multilabel = np.c_[y_train_large, y_train_odd]\\nknn_clf = KNeighborsClassifier()\\nknn_clf.fit(X_train, y_multilabel)\\nThis code creates a y_multilabel array containing two target labels for each\\ndigit image: the first indicates whether or not the digit is large (7, 8, or 9), and\\nthe second indicates whether or not it is odd. Then the code creates a\\nKNeighborsClassifier instance, which supports multilabel classification (not\\nall classifiers do), and trains this model using the multiple targets array. Now\\nyou can make a prediction, and notice that it outputs two labels:\\n>>> knn_clf.predict([some_digit])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large (False) and odd (True).\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 212}, page_content='There are many ways to evaluate a multilabel classifier, and selecting the\\nright metric really depends on your project. One approach is to measure the\\nF  score for each individual label (or any other binary classifier metric\\ndiscussed earlier), then simply compute the average score. The following\\ncode computes the average F  score across all labels:\\n>>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\\n>>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\\n0.976410265560605\\nThis approach assumes that all labels are equally important, which may not\\nbe the case. In particular, if you have many more pictures of Alice than of\\nBob or Charlie, you may want to give more weight to the classifier’s score on\\npictures of Alice. One simple option is to give each label a weight equal to its\\nsupport (i.e., the number of instances with that target label). To do this,\\nsimply set average=\"weighted\" when calling the f1_score() function.\\u2060\\nIf you wish to use a classifier that does not natively support multilabel\\nclassification, such as SVC, one possible strategy is to train one model per\\nlabel. However, this strategy may have a hard time capturing the\\ndependencies between the labels. For example, a large digit (7, 8, or 9) is\\ntwice more likely to be odd than even, but the classifier for the “odd” label\\ndoes not know what the classifier for the “large” label predicted. To solve this\\nissue, the models can be organized in a chain: when a model makes a\\nprediction, it uses the input features plus all the predictions of the models that\\ncome before it in the chain.\\nThe good news is that Scikit-Learn has a class called ChainClassifier that\\ndoes just that! By default it will use the true labels for training, feeding each\\nmodel the appropriate labels depending on their position in the chain. But if\\nyou set the cv hyperparameter, it will use cross-validation to get “clean” (out-\\nof-sample) predictions from each trained model for every instance in the\\ntraining set, and these predictions will then be used to train all the models\\nlater in the chain. Here’s an example showing how to create and train a\\nChainClassifier using the cross-validation strategy. As earlier, we’ll just use\\nthe first 2,000 images in the training set to speed things up:\\n1\\n1\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 213}, page_content='from sklearn.multioutput import ClassifierChain\\nchain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\\nchain_clf.fit(X_train[:2000], y_multilabel[:2000])\\nNow we can use this ChainClassifier to make predictions:\\n>>> chain_clf.predict([some_digit])\\narray([[0., 1.]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 214}, page_content='Multioutput Classification\\nThe last type of classification task we’ll discuss here is called multioutput–\\nmulticlass classification (or just multioutput classification). It is a\\ngeneralization of multilabel classification where each label can be multiclass\\n(i.e., it can have more than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will\\ntake as input a noisy digit image, and it will (hopefully) output a clean digit\\nimage, represented as an array of pixel intensities, just like the MNIST\\nimages. Notice that the classifier’s output is multilabel (one label per pixel)\\nand each label can have multiple values (pixel intensity ranges from 0 to\\n255). This is thus an example of a multioutput classification system.\\nNOTE\\nThe line between classification and regression is sometimes blurry, such as in this\\nexample. Arguably, predicting pixel intensity is more akin to regression than to\\nclassification. Moreover, multioutput systems are not limited to classification tasks; you\\ncould even have a system that outputs multiple labels per instance, including both class\\nlabels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images\\nand adding noise to their pixel intensities with NumPy’s randint() function.\\nThe target images will be the original images:\\nnp.random.seed(42)  # to make this code example reproducible\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod = X_test + noise\\ny_train_mod = X_train\\ny_test_mod = X_test\\nLet’s take a peek at the first image from the test set (Figure 3-12). Yes, we’re\\nsnooping on the test data, so you should be frowning right now.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 215}, page_content='Figure 3-12. A noisy image (left) and the target clean image (right)\\nOn the left is the noisy input image, and on the right is the clean target image.\\nNow let’s train the classifier and make it clean up this image (Figure 3-13):\\nknn_clf = KNeighborsClassifier()\\nknn_clf.fit(X_train_mod, y_train_mod)\\nclean_digit = knn_clf.predict([X_test_mod[0]])\\nplot_digit(clean_digit)\\nplt.show()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 216}, page_content='Figure 3-13. The cleaned-up image\\nLooks close enough to the target! This concludes our tour of classification.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 217}, page_content='You now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall trade-off, compare classifiers, and more generally\\nbuild good classification systems for a variety of tasks. In the next chapters,\\nyou’ll learn how all these machine learning models you’ve been using\\nactually work.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 218}, page_content='Exercises\\n1. Try to build a classifier for the MNIST dataset that achieves over 97%\\naccuracy on the test set. Hint: the KNeighborsClassifier works quite well\\nfor this task; you just need to find good hyperparameter values (try a\\ngrid search on the weights and n_neighbors hyperparameters).\\n2. Write a function that can shift an MNIST image in any direction (left,\\nright, up, or down) by one pixel.\\u2060\\n Then, for each image in the training\\nset, create four shifted copies (one per direction) and add them to the\\ntraining set. Finally, train your best model on this expanded training set\\nand measure its accuracy on the test set. You should observe that your\\nmodel performs even better now! This technique of artificially growing\\nthe training set is called data augmentation or training set expansion.\\n3. Tackle the Titanic dataset. A great place to start is on Kaggle.\\nAlternatively, you can download the data from\\nhttps://homl.info/titanic.tgz and unzip this tarball like you did for the\\nhousing data in Chapter 2. This will give you two CSV files, train.csv\\nand test.csv, which you can load using pandas.read_csv(). The goal is to\\ntrain a classifier that can predict the Survived column based on the other\\ncolumns.\\n4. Build a spam classifier (a more challenging exercise):\\na. Download examples of spam and ham from Apache\\nSpamAssassin’s public datasets.\\nb. Unzip the datasets and familiarize yourself with the data format.\\nc. Split the data into a training set and a test set.\\nd. Write a data preparation pipeline to convert each email into a\\nfeature vector. Your preparation pipeline should transform an email\\ninto a (sparse) vector that indicates the presence or absence of each\\npossible word. For example, if all emails only ever contain four\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 219}, page_content='words, “Hello”, “how”, “are”, “you”, then the email “Hello you\\nHello Hello you” would be converted into a vector [1, 0, 0, 1]\\n(meaning [“Hello” is present, “how” is absent, “are” is absent,\\n“you” is present]), or [3, 0, 0, 2] if you prefer to count the number\\nof occurrences of each word.\\nYou may want to add hyperparameters to your preparation pipeline\\nto control whether or not to strip off email headers, convert each\\nemail to lowercase, remove punctuation, replace all URLs with\\n“URL”, replace all numbers with “NUMBER”, or even perform\\nstemming (i.e., trim off word endings; there are Python libraries\\navailable to do this).\\ne. Finally, try out several classifiers and see if you can build a great\\nspam classifier, with both high recall and high precision.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  By default Scikit-Learn caches downloaded datasets in a directory called scikit_learn_data in\\nyour home directory.\\n2  Datasets returned by fetch_openml() are not always shuffled or split.\\n3  Shuffling may be a bad idea in some contexts—for example, if you are working on time series\\ndata (such as stock market prices or weather conditions). We will explore this in Chapter 15.\\n4  Scikit-Learn classifiers always have either a decision_function() method or a predict_proba()\\nmethod, or sometimes both.\\n5  Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the\\ndocumentation for more details.\\n6  You can use the shift() function from the scipy.ndimage.interpolation module. For example,\\nshift(image, [2, 1], cval=0) shifts the image two pixels down and one pixel to the right.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 220}, page_content='Chapter 4. Training Models\\nSo far we have treated machine learning models and their training algorithms\\nmostly like black boxes. If you went through some of the exercises in the\\nprevious chapters, you may have been surprised by how much you can get\\ndone without knowing anything about what’s under the hood: you optimized\\na regression system, you improved a digit image classifier, and you even built\\na spam classifier from scratch, all without knowing how they actually work.\\nIndeed, in many situations you don’t really need to know the implementation\\ndetails.\\nHowever, having a good understanding of how things work can help you\\nquickly home in on the appropriate model, the right training algorithm to use,\\nand a good set of hyperparameters for your task. Understanding what’s under\\nthe hood will also help you debug issues and perform error analysis more\\nefficiently. Lastly, most of the topics discussed in this chapter will be\\nessential in understanding, building, and training neural networks (discussed\\nin Part II of this book).\\nIn this chapter we will start by looking at the linear regression model, one of\\nthe simplest models there is. We will discuss two very different ways to train\\nit:\\nUsing a “closed-form” equation\\u2060\\n that directly computes the model\\nparameters that best fit the model to the training set (i.e., the model\\nparameters that minimize the cost function over the training set).\\nUsing an iterative optimization approach called gradient descent (GD)\\nthat gradually tweaks the model parameters to minimize the cost\\nfunction over the training set, eventually converging to the same set of\\nparameters as the first method. We will look at a few variants of gradient\\ndescent that we will use again and again when we study neural networks\\nin Part II: batch GD, mini-batch GD, and stochastic GD.\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 221}, page_content='Next we will look at polynomial regression, a more complex model that can\\nfit nonlinear datasets. Since this model has more parameters than linear\\nregression, it is more prone to overfitting the training data. We will explore\\nhow to detect whether or not this is the case using learning curves, and then\\nwe will look at several regularization techniques that can reduce the risk of\\noverfitting the training set.\\nFinally, we will examine two more models that are commonly used for\\nclassification tasks: logistic regression and softmax regression.\\nWARNING\\nThere will be quite a few math equations in this chapter, using basic notions of linear\\nalgebra and calculus. To understand these equations, you will need to know what vectors\\nand matrices are; how to transpose them, multiply them, and inverse them; and what\\npartial derivatives are. If you are unfamiliar with these concepts, please go through the\\nlinear algebra and calculus introductory tutorials available as Jupyter notebooks in the\\nonline supplemental material. For those who are truly allergic to mathematics, you should\\nstill go through this chapter and simply skip the equations; hopefully, the text will be\\nsufficient to help you understand most of the concepts.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 222}, page_content='Linear Regression\\nIn Chapter 1 we looked at a simple regression model of life satisfaction:\\nlife_satisfaction = θ  + θ  × GDP_per_capita\\nThis model is just a linear function of the input feature GDP_per_capita. θ\\nand θ  are the model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a\\nweighted sum of the input features, plus a constant called the bias term (also\\ncalled the intercept term), as shown in Equation 4-1.\\nEquation 4-1. Linear regression model prediction\\ny ^ = θ 0 + θ 1 x 1 + θ 2 x 2 + ⋯ + θ n x n\\nIn this equation:\\nŷ is the predicted value.\\nn is the number of features.\\nx  is the i  feature value.\\nθ  is the j  model parameter, including the bias term θ  and the feature\\nweights θ , θ , ⋯, θ .\\nThis can be written much more concisely using a vectorized form, as shown\\nin Equation 4-2.\\nEquation 4-2. Linear regression model prediction (vectorized form)\\ny^=hθ(x)=θ·x\\nIn this equation:\\nh  is the hypothesis function, using the model parameters θ.\\nθ is the model’s parameter vector, containing the bias term θ  and the\\nfeature weights θ  to θ .\\n0\\n1\\n0\\n1\\ni\\nth\\nj\\nth\\n0\\n1\\n2\\nn\\nθ\\n0\\n1\\nn'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 223}, page_content='x is the instance’s feature vector, containing x  to x , with x  always\\nequal to 1.\\nθ · x is the dot product of the vectors θ and x, which is equal to θ x  +\\nθ x  + θ x  + ... + θ x .\\nNOTE\\nIn machine learning, vectors are often represented as column vectors, which are 2D arrays\\nwith a single column. If θ and x are column vectors, then the prediction is y^=θ⊺x, where\\nθ⊺ is the transpose of θ (a row vector instead of a column vector) and θ⊺x is the matrix\\nmultiplication of θ⊺ and x. It is of course the same prediction, except that it is now\\nrepresented as a single-cell matrix rather than a scalar value. In this book I will use this\\nnotation to avoid switching between dot products and matrix multiplications.\\nOK, that’s the linear regression model—but how do we train it? Well, recall\\nthat training a model means setting its parameters so that the model best fits\\nthe training set. For this purpose, we first need a measure of how well (or\\npoorly) the model fits the training data. In Chapter 2 we saw that the most\\ncommon performance measure of a regression model is the root mean square\\nerror (Equation 2-1). Therefore, to train a linear regression model, we need to\\nfind the value of θ that minimizes the RMSE. In practice, it is simpler to\\nminimize the mean squared error (MSE) than the RMSE, and it leads to the\\nsame result (because the value that minimizes a positive function also\\nminimizes its square root).\\nWARNING\\nLearning algorithms will often optimize a different loss function during training than the\\nperformance measure used to evaluate the final model. This is generally because the\\nfunction is easier to optimize and/or because it has extra terms needed during training only\\n(e.g., for regularization). A good performance metric is as close as possible to the final\\nbusiness objective. A good training loss is easy to optimize and strongly correlated with\\nthe metric. For example, classifiers are often trained using a cost function such as the log\\nloss (as you will see later in this chapter) but evaluated using precision/recall. The log loss\\nis easy to minimize, and doing so will usually improve precision/recall.\\n0\\nn\\n0\\n0 0\\n1 1\\n2 2\\nn n'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 224}, page_content='The MSE of a linear regression hypothesis h  on a training set X is calculated\\nusing Equation 4-3.\\nEquation 4-3. MSE cost function for a linear regression model\\nMSE ( X , h θ ) = 1 m ∑ i=1 m (θ ⊺ x (i) -y (i) ) 2\\nMost of these notations were presented in Chapter 2 (see “Notations”). The\\nonly difference is that we write h  instead of just h to make it clear that the\\nmodel is parametrized by the vector θ. To simplify notations, we will just\\nwrite MSE(θ) instead of MSE(X, h ).\\nθ\\nθ\\nθ'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 225}, page_content='The Normal Equation\\nTo find the value of θ that minimizes the MSE, there exists a closed-form\\nsolution—in other words, a mathematical equation that gives the result\\ndirectly. This is called the Normal equation (Equation 4-4).\\nEquation 4-4. Normal equation\\nθ ^ = (X ⊺ X) -1   X ⊺   y\\nIn this equation:\\nθ^ is the value of θ that minimizes the cost function.\\ny is the vector of target values containing y\\n to y\\n.\\nLet’s generate some linear-looking data to test this equation on (Figure 4-1):\\nimport numpy as np\\nnp.random.seed(42)  # to make this code example reproducible\\nm = 100  # number of instances\\nX = 2 * np.random.rand(m, 1)  # column vector\\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\\n(1)\\n(m)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 226}, page_content='Figure 4-1. A randomly generated linear dataset\\nNow let’s compute θ^ using the Normal equation. We will use the inv()\\nfunction from NumPy’s linear algebra module (np.linalg) to compute the\\ninverse of a matrix, and the dot() method for matrix multiplication:\\nfrom sklearn.preprocessing import add_dummy_feature\\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\\nNOTE\\nThe @ operator performs matrix multiplication. If A and B are NumPy arrays, then A @ B\\nis equivalent to np.matmul(A, B). Many other libraries, like TensorFlow, PyTorch, and\\nJAX, support the @ operator as well. However, you cannot use @ on pure Python arrays\\n(i.e., lists of lists).\\nThe function that we used to generate the data is y = 4 + 3x  + Gaussian\\nnoise. Let’s see what the equation found:\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 227}, page_content='>>> theta_best\\narray([[4.21509616],\\n       [2.77011339]])\\nWe would have hoped for θ  = 4 and θ  = 3 instead of θ  = 4.215 and θ  =\\n2.770. Close enough, but the noise made it impossible to recover the exact\\nparameters of the original function. The smaller and noisier the dataset, the\\nharder it gets.\\nNow we can make predictions using θ^:\\n>>> X_new = np.array([[0], [2]])\\n>>> X_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\\n>>> y_predict = X_new_b @ theta_best\\n>>> y_predict\\narray([[4.21509616],\\n       [9.75532293]])\\nLet’s plot this model’s predictions (Figure 4-2):\\nimport matplotlib.pyplot as plt\\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\\nplt.plot(X, y, \"b.\")\\n[...]  # beautify the figure: add labels, axis, grid, and legend\\nplt.show()\\n0\\n1\\n0\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 228}, page_content='Figure 4-2. Linear regression model predictions\\nPerforming linear regression using Scikit-Learn is relatively straightforward:\\n>>> from sklearn.linear_model import LinearRegression\\n>>> lin_reg = LinearRegression()\\n>>> lin_reg.fit(X, y)\\n>>> lin_reg.intercept_, lin_reg.coef_\\n(array([4.21509616]), array([[2.77011339]]))\\n>>> lin_reg.predict(X_new)\\narray([[4.21509616],\\n       [9.75532293]])\\nNotice that Scikit-Learn separates the bias term (intercept_) from the feature\\nweights (coef_). The LinearRegression class is based on the\\nscipy.linalg.lstsq() function (the name stands for “least squares”), which you\\ncould call directly:\\n>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\\n>>> theta_best_svd\\narray([[4.21509616],\\n       [2.77011339]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 229}, page_content='This function computes θ^=X+y, where X+ is the pseudoinverse of X\\n(specifically, the Moore–Penrose inverse). You can use np.linalg.pinv() to\\ncompute the pseudoinverse directly:\\n>>> np.linalg.pinv(X_b) @ y\\narray([[4.21509616],\\n       [2.77011339]])\\nThe pseudoinverse itself is computed using a standard matrix factorization\\ntechnique called singular value decomposition (SVD) that can decompose the\\ntraining set matrix X into the matrix multiplication of three matrices U Σ V\\n(see numpy.linalg.svd()). The pseudoinverse is computed as X+=VΣ+U⊺. To\\ncompute the matrix Σ+, the algorithm takes Σ and sets to zero all values\\nsmaller than a tiny threshold value, then it replaces all the nonzero values\\nwith their inverse, and finally it transposes the resulting matrix. This\\napproach is more efficient than computing the Normal equation, plus it\\nhandles edge cases nicely: indeed, the Normal equation may not work if the\\nmatrix X X is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\n⊺\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 230}, page_content='Computational Complexity\\nThe Normal equation computes the inverse of X  X, which is an (n + 1) × (n\\n+ 1) matrix (where n is the number of features). The computational\\ncomplexity of inverting such a matrix is typically about O(n\\n) to O(n ),\\ndepending on the implementation. In other words, if you double the number\\nof features, you multiply the computation time by roughly 2\\n = 5.3 to 2  =\\n8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression class is about\\nO(n ). If you double the number of features, you multiply the computation\\ntime by roughly 4.\\nWARNING\\nBoth the Normal equation and the SVD approach get very slow when the number of\\nfeatures grows large (e.g., 100,000). On the positive side, both are linear with regard to the\\nnumber of instances in the training set (they are O(m)), so they handle large training sets\\nefficiently, provided they can fit in memory.\\nAlso, once you have trained your linear regression model (using the Normal\\nequation or any other algorithm), predictions are very fast: the computational\\ncomplexity is linear with regard to both the number of instances you want to\\nmake predictions on and the number of features. In other words, making\\npredictions on twice as many instances (or twice as many features) will take\\nroughly twice as much time.\\nNow we will look at a very different way to train a linear regression model,\\nwhich is better suited for cases where there are a large number of features or\\ntoo many training instances to fit in memory.\\n⊺\\n2.4\\n3\\n2.4\\n3\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 231}, page_content='Gradient Descent\\nGradient descent is a generic optimization algorithm capable of finding\\noptimal solutions to a wide range of problems. The general idea of gradient\\ndescent is to tweak parameters iteratively in order to minimize a cost\\nfunction.\\nSuppose you are lost in the mountains in a dense fog, and you can only feel\\nthe slope of the ground below your feet. A good strategy to get to the bottom\\nof the valley quickly is to go downhill in the direction of the steepest slope.\\nThis is exactly what gradient descent does: it measures the local gradient of\\nthe error function with regard to the parameter vector θ, and it goes in the\\ndirection of descending gradient. Once the gradient is zero, you have reached\\na minimum!\\nIn practice, you start by filling θ with random values (this is called random\\ninitialization). Then you improve it gradually, taking one baby step at a time,\\neach step attempting to decrease the cost function (e.g., the MSE), until the\\nalgorithm converges to a minimum (see Figure 4-3).\\nFigure 4-3. In this depiction of gradient descent, the model parameters are initialized randomly and get'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 232}, page_content='tweaked repeatedly to minimize the cost function; the learning step size is proportional to the slope of\\nthe cost function, so the steps gradually get smaller as the cost approaches the minimum\\nAn important parameter in gradient descent is the size of the steps,\\ndetermined by the learning rate hyperparameter. If the learning rate is too\\nsmall, then the algorithm will have to go through many iterations to\\nconverge, which will take a long time (see Figure 4-4).\\nFigure 4-4. Learning rate too small\\nOn the other hand, if the learning rate is too high, you might jump across the\\nvalley and end up on the other side, possibly even higher up than you were\\nbefore. This might make the algorithm diverge, with larger and larger values,\\nfailing to find a good solution (see Figure 4-5).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 233}, page_content='Figure 4-5. Learning rate too high\\nAdditionally, not all cost functions look like nice, regular bowls. There may\\nbe holes, ridges, plateaus, and all sorts of irregular terrain, making\\nconvergence to the minimum difficult. Figure 4-6 shows the two main\\nchallenges with gradient descent. If the random initialization starts the\\nalgorithm on the left, then it will converge to a local minimum, which is not\\nas good as the global minimum. If it starts on the right, then it will take a very\\nlong time to cross the plateau. And if you stop too early, you will never reach\\nthe global minimum.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 234}, page_content='Figure 4-6. Gradient descent pitfalls\\nFortunately, the MSE cost function for a linear regression model happens to\\nbe a convex function, which means that if you pick any two points on the\\ncurve, the line segment joining them is never below the curve. This implies\\nthat there are no local minima, just one global minimum. It is also a\\ncontinuous function with a slope that never changes abruptly.\\u2060\\n These two\\nfacts have a great consequence: gradient descent is guaranteed to approach\\narbitrarily closely the global minimum (if you wait long enough and if the\\nlearning rate is not too high).\\nWhile the cost function has the shape of a bowl, it can be an elongated bowl\\nif the features have very different scales. Figure 4-7 shows gradient descent\\non a training set where features 1 and 2 have the same scale (on the left), and\\non a training set where feature 1 has much smaller values than feature 2 (on\\nthe right).\\u2060\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 235}, page_content='Figure 4-7. Gradient descent with (left) and without (right) feature scaling\\nAs you can see, on the left the gradient descent algorithm goes straight\\ntoward the minimum, thereby reaching it quickly, whereas on the right it first\\ngoes in a direction almost orthogonal to the direction of the global minimum,\\nand it ends with a long march down an almost flat valley. It will eventually\\nreach the minimum, but it will take a long time.\\nWARNING\\nWhen using gradient descent, you should ensure that all features have a similar scale (e.g.,\\nusing Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching\\nfor a combination of model parameters that minimizes a cost function (over\\nthe training set). It is a search in the model’s parameter space. The more\\nparameters a model has, the more dimensions this space has, and the harder\\nthe search is: searching for a needle in a 300-dimensional haystack is much\\ntrickier than in 3 dimensions. Fortunately, since the cost function is convex in\\nthe case of linear regression, the needle is simply at the bottom of the bowl.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 236}, page_content='Batch Gradient Descent\\nTo implement gradient descent, you need to compute the gradient of the cost\\nfunction with regard to each model parameter θ . In other words, you need to\\ncalculate how much the cost function will change if you change θ  just a little\\nbit. This is called a partial derivative. It is like asking, “What is the slope of\\nthe mountain under my feet if I face east”? and then asking the same question\\nfacing north (and so on for all other dimensions, if you can imagine a\\nuniverse with more than three dimensions). Equation 4-5 computes the partial\\nderivative of the MSE with regard to parameter θ , noted ∂ MSE(θ) / ∂θ .\\nEquation 4-5. Partial derivatives of the cost function\\n∂ ∂θ j MSE ( θ ) = 2 m ∑ i=1 m ( θ ⊺ x (i) - y (i) ) x j (i)\\nInstead of computing these partial derivatives individually, you can use\\nEquation 4-6 to compute them all in one go. The gradient vector, noted\\n∇MSE(θ), contains all the partial derivatives of the cost function (one for\\neach model parameter).\\nEquation 4-6. Gradient vector of the cost function\\n∇ θ MSE ( θ ) = ∂ ∂θ 0 MSE ( θ ) ∂ ∂θ 1 MSE ( θ ) ⋮ ∂ ∂θ n MSE ( θ ) = 2 m\\nX ⊺ ( X θ - y )\\nWARNING\\nNotice that this formula involves calculations over the full training set X, at each gradient\\ndescent step! This is why the algorithm is called batch gradient descent: it uses the whole\\nbatch of training data at every step (actually, full gradient descent would probably be a\\nbetter name). As a result, it is terribly slow on very large training sets (we will look at\\nsome much faster gradient descent algorithms shortly). However, gradient descent scales\\nwell with the number of features; training a linear regression model when there are\\nhundreds of thousands of features is much faster using gradient descent than using the\\nNormal equation or SVD decomposition.\\nOnce you have the gradient vector, which points uphill, just go in the\\nj\\nj\\nj\\nj\\nθ'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 237}, page_content='opposite direction to go downhill. This means subtracting ∇MSE(θ) from θ.\\nThis is where the learning rate η comes into play:\\u2060\\n multiply the gradient\\nvector by η to determine the size of the downhill step (Equation 4-7).\\nEquation 4-7. Gradient descent step\\nθ(next step)=θ-η∇θ\\u200aMSE(θ)\\nLet’s look at a quick implementation of this algorithm:\\neta = 0.1  # learning rate\\nn_epochs = 1000\\nm = len(X_b)  # number of instances\\nnp.random.seed(42)\\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\\nfor epoch in range(n_epochs):\\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\\n    theta = theta - eta * gradients\\nThat wasn’t too hard! Each iteration over the training set is called an epoch.\\nLet’s look at the resulting theta:\\n>>> theta\\narray([[4.21509616],\\n       [2.77011339]])\\nHey, that’s exactly what the Normal equation found! Gradient descent\\nworked perfectly. But what if you had used a different learning rate (eta)?\\nFigure 4-8 shows the first 20 steps of gradient descent using three different\\nlearning rates. The line at the bottom of each plot represents the random\\nstarting point, then each epoch is represented by a darker and darker line.\\nθ\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 238}, page_content='Figure 4-8. Gradient descent with various learning rates\\nOn the left, the learning rate is too low: the algorithm will eventually reach\\nthe solution, but it will take a long time. In the middle, the learning rate looks\\npretty good: in just a few epochs, it has already converged to the solution. On\\nthe right, the learning rate is too high: the algorithm diverges, jumping all\\nover the place and actually getting further and further away from the solution\\nat every step.\\nTo find a good learning rate, you can use grid search (see Chapter 2).\\nHowever, you may want to limit the number of epochs so that grid search can\\neliminate models that take too long to converge.\\nYou may wonder how to set the number of epochs. If it is too low, you will\\nstill be far away from the optimal solution when the algorithm stops; but if it\\nis too high, you will waste time while the model parameters do not change\\nanymore. A simple solution is to set a very large number of epochs but to\\ninterrupt the algorithm when the gradient vector becomes tiny—that is, when\\nits norm becomes smaller than a tiny number ϵ (called the tolerance)—\\nbecause this happens when gradient descent has (almost) reached the\\nminimum.\\nCONVERGENCE RATE\\nWhen the cost function is convex and its slope does not change abruptly\\n(as is the case for the MSE cost function), batch gradient descent with a\\nfixed learning rate will eventually converge to the optimal solution, but'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 239}, page_content='you may have to wait a while: it can take O(1/ϵ) iterations to reach the\\noptimum within a range of ϵ, depending on the shape of the cost function.\\nIf you divide the tolerance by 10 to have a more precise solution, then the\\nalgorithm may have to run about 10 times longer.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 240}, page_content='Stochastic Gradient Descent\\nThe main problem with batch gradient descent is the fact that it uses the\\nwhole training set to compute the gradients at every step, which makes it very\\nslow when the training set is large. At the opposite extreme, stochastic\\ngradient descent picks a random instance in the training set at every step and\\ncomputes the gradients based only on that single instance. Obviously,\\nworking on a single instance at a time makes the algorithm much faster\\nbecause it has very little data to manipulate at every iteration. It also makes it\\npossible to train on huge training sets, since only one instance needs to be in\\nmemory at each iteration (stochastic GD can be implemented as an out-of-\\ncore algorithm; see Chapter 1).\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is\\nmuch less regular than batch gradient descent: instead of gently decreasing\\nuntil it reaches the minimum, the cost function will bounce up and down,\\ndecreasing only on average. Over time it will end up very close to the\\nminimum, but once it gets there it will continue to bounce around, never\\nsettling down (see Figure 4-9). Once the algorithm stops, the final parameter\\nvalues will be good, but not optimal.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 241}, page_content='Figure 4-9. With stochastic gradient descent, each training step is much faster but also much more\\nstochastic than when using batch gradient descent\\nWhen the cost function is very irregular (as in Figure 4-6), this can actually\\nhelp the algorithm jump out of local minima, so stochastic gradient descent\\nhas a better chance of finding the global minimum than batch gradient\\ndescent does.\\nTherefore, randomness is good to escape from local optima, but bad because\\nit means that the algorithm can never settle at the minimum. One solution to\\nthis dilemma is to gradually reduce the learning rate. The steps start out large\\n(which helps make quick progress and escape local minima), then get smaller\\nand smaller, allowing the algorithm to settle at the global minimum. This\\nprocess is akin to simulated annealing, an algorithm inspired by the process\\nin metallurgy of annealing, where molten metal is slowly cooled down. The\\nfunction that determines the learning rate at each iteration is called the\\nlearning schedule. If the learning rate is reduced too quickly, you may get\\nstuck in a local minimum, or even end up frozen halfway to the minimum. If'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 242}, page_content='the learning rate is reduced too slowly, you may jump around the minimum\\nfor a long time and end up with a suboptimal solution if you halt training too\\nearly.\\nThis code implements stochastic gradient descent using a simple learning\\nschedule:\\nn_epochs = 50\\nt0, t1 = 5, 50  # learning schedule hyperparameters\\ndef learning_schedule(t):\\n    return t0 / (t + t1)\\nnp.random.seed(42)\\ntheta = np.random.randn(2, 1)  # random initialization\\nfor epoch in range(n_epochs):\\n    for iteration in range(m):\\n        random_index = np.random.randint(m)\\n        xi = X_b[random_index : random_index + 1]\\n        yi = y[random_index : random_index + 1]\\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\\n        eta = learning_schedule(epoch * m + iteration)\\n        theta = theta - eta * gradients\\nBy convention we iterate by rounds of m iterations; each round is called an\\nepoch, as earlier. While the batch gradient descent code iterated 1,000 times\\nthrough the whole training set, this code goes through the training set only 50\\ntimes and reaches a pretty good solution:\\n>>> theta\\narray([[4.21076011],\\n       [2.74856079]])\\nFigure 4-10 shows the first 20 steps of training (notice how irregular the steps\\nare).\\nNote that since instances are picked randomly, some instances may be picked\\nseveral times per epoch, while others may not be picked at all. If you want to\\nbe sure that the algorithm goes through every instance at each epoch, another\\napproach is to shuffle the training set (making sure to shuffle the input'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 243}, page_content='features and the labels jointly), then go through it instance by instance, then\\nshuffle it again, and so on. However, this approach is more complex, and it\\ngenerally does not improve the result.\\nFigure 4-10. The first 20 steps of stochastic gradient descent\\nWARNING\\nWhen using stochastic gradient descent, the training instances must be independent and\\nidentically distributed (IID) to ensure that the parameters get pulled toward the global\\noptimum, on average. A simple way to ensure this is to shuffle the instances during\\ntraining (e.g., pick each instance randomly, or shuffle the training set at the beginning of\\neach epoch). If you do not shuffle the instances—for example, if the instances are sorted\\nby label—then SGD will start by optimizing for one label, then the next, and so on, and it\\nwill not settle close to the global minimum.\\nTo perform linear regression using stochastic GD with Scikit-Learn, you can\\nuse the SGDRegressor class, which defaults to optimizing the MSE cost\\nfunction. The following code runs for maximum 1,000 epochs (max_iter) or\\nuntil the loss drops by less than 10  (tol) during 100 epochs\\n–5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 244}, page_content='(n_iter_no_change). It starts with a learning rate of 0.01 (eta0), using the\\ndefault learning schedule (different from the one we used). Lastly, it does not\\nuse any regularization (penalty=None; more details on this shortly):\\nfrom sklearn.linear_model import SGDRegressor\\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\\n                       n_iter_no_change=100, random_state=42)\\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\\nOnce again, you find a solution quite close to the one returned by the Normal\\nequation:\\n>>> sgd_reg.intercept_, sgd_reg.coef_\\n(array([4.21278812]), array([2.77270267]))\\nTIP\\nAll Scikit-Learn estimators can be trained using the fit() method, but some estimators also\\nhave a partial_fit() method that you can call to run a single round of training on one or\\nmore instances (it ignores hyperparameters like max_iter or tol). Repeatedly calling\\npartial_fit() will gradually train the model. This is useful when you need more control over\\nthe training process. Other models have a warm_start hyperparameter instead (and some\\nhave both): if you set warm_start=True, calling the fit() method on a trained model will\\nnot reset the model; it will just continue training where it left off, respecting\\nhyperparameters like max_iter and tol. Note that fit() resets the iteration counter used by\\nthe learning schedule, while partial_fit() does not.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 245}, page_content='Mini-Batch Gradient Descent\\nThe last gradient descent algorithm we will look at is called mini-batch\\ngradient descent. It is straightforward once you know batch and stochastic\\ngradient descent: at each step, instead of computing the gradients based on\\nthe full training set (as in batch GD) or based on just one instance (as in\\nstochastic GD), mini-batch GD computes the gradients on small random sets\\nof instances called mini-batches. The main advantage of mini-batch GD over\\nstochastic GD is that you can get a performance boost from hardware\\noptimization of matrix operations, especially when using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with\\nstochastic GD, especially with fairly large mini-batches. As a result, mini-\\nbatch GD will end up walking around a bit closer to the minimum than\\nstochastic GD—but it may be harder for it to escape from local minima (in\\nthe case of problems that suffer from local minima, unlike linear regression\\nwith the MSE cost function). Figure 4-11 shows the paths taken by the three\\ngradient descent algorithms in parameter space during training. They all end\\nup near the minimum, but batch GD’s path actually stops at the minimum,\\nwhile both stochastic GD and mini-batch GD continue to walk around.\\nHowever, don’t forget that batch GD takes a lot of time to take each step, and\\nstochastic GD and mini-batch GD would also reach the minimum if you used\\na good learning schedule.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 246}, page_content='Figure 4-11. Gradient descent paths in parameter space\\nTable 4-1 compares the algorithms we’ve discussed so far for linear\\nregression\\u2060\\n (recall that m is the number of training instances and n is the\\nnumber of features).\\nTable 4-1. Comparison of algorithms for linear regression\\nAlgorithm\\nLarge m\\nOut-of-core\\nsupport\\nLarge n\\nHyperparams\\nNormal equation\\nFast\\nNo\\nSlow\\n0\\nSVD\\nFast\\nNo\\nSlow\\n0\\nBatch GD\\nSlow\\nNo\\nFast\\n2\\nStochastic GD\\nFast\\nYes\\nFast\\n≥2\\nMini-batch GD\\nFast\\nYes\\nFast\\n≥2\\nThere is almost no difference after training: all these algorithms end up with\\nvery similar models and make predictions in exactly the same way.\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 247}, page_content='Polynomial Regression\\nWhat if your data is more complex than a straight line? Surprisingly, you can\\nuse a linear model to fit nonlinear data. A simple way to do this is to add\\npowers of each feature as new features, then train a linear model on this\\nextended set of features. This technique is called polynomial regression.\\nLet’s look at an example. First, we’ll generate some nonlinear data (see\\nFigure 4-12), based on a simple quadratic equation—that’s an equation of\\nthe form y = ax² + bx + c—plus some noise:\\nnp.random.seed(42)\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\\nFigure 4-12. Generated nonlinear and noisy dataset\\nClearly, a straight line will never fit this data properly. So let’s use Scikit-\\nLearn’s PolynomialFeatures class to transform our training data, adding the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 248}, page_content='square (second-degree polynomial) of each feature in the training set as a new\\nfeature (in this case there is just one feature):\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\\n>>> X_poly = poly_features.fit_transform(X)\\n>>> X[0]\\narray([-0.75275929])\\n>>> X_poly[0]\\narray([-0.75275929,  0.56664654])\\nX_poly now contains the original feature of X plus the square of this feature.\\nNow we can fit a LinearRegression model to this extended training data\\n(Figure 4-13):\\n>>> lin_reg = LinearRegression()\\n>>> lin_reg.fit(X_poly, y)\\n>>> lin_reg.intercept_, lin_reg.coef_\\n(array([1.78134581]), array([[0.93366893, 0.56456263]]))\\nFigure 4-13. Polynomial regression model predictions'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 249}, page_content='Not bad: the model estimates y ^ = 0.56 x 1 2 + 0.93 x 1 + 1.78 when in fact\\nthe original function was y = 0.5 x 1 2 + 1.0 x 1 + 2.0 + Gaussian noise .\\nNote that when there are multiple features, polynomial regression is capable\\nof finding relationships between features, which is something a plain linear\\nregression model cannot do. This is made possible by the fact that\\nPolynomialFeatures also adds all combinations of features up to the given\\ndegree. For example, if there were two features a and b, PolynomialFeatures\\nwith degree=3 would not only add the features a , a , b , and b , but also the\\ncombinations ab, a b, and ab .\\nWARNING\\nPolynomialFeatures(degree=d) transforms an array containing n features into an array\\ncontaining (n + d)! / d!n! features, where n! is the factorial of n, equal to 1 × 2 × 3 × ⋯ × n.\\nBeware of the combinatorial explosion of the number of features!\\n2\\n3\\n2\\n3\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 250}, page_content='Learning Curves\\nIf you perform high-degree polynomial regression, you will likely fit the\\ntraining data much better than with plain linear regression. For example,\\nFigure 4-14 applies a 300-degree polynomial model to the preceding training\\ndata, and compares the result with a pure linear model and a quadratic model\\n(second-degree polynomial). Notice how the 300-degree polynomial model\\nwiggles around to get as close as possible to the training instances.\\nFigure 4-14. High-degree polynomial regression\\nThis high-degree polynomial regression model is severely overfitting the\\ntraining data, while the linear model is underfitting it. The model that will\\ngeneralize best in this case is the quadratic model, which makes sense\\nbecause the data was generated using a quadratic model. But in general you\\nwon’t know what function generated the data, so how can you decide how\\ncomplex your model should be? How can you tell that your model is\\noverfitting or underfitting the data?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 251}, page_content='In Chapter 2 you used cross-validation to get an estimate of a model’s\\ngeneralization performance. If a model performs well on the training data but\\ngeneralizes poorly according to the cross-validation metrics, then your model\\nis overfitting. If it performs poorly on both, then it is underfitting. This is one\\nway to tell when a model is too simple or too complex.\\nAnother way to tell is to look at the learning curves, which are plots of the\\nmodel’s training error and validation error as a function of the training\\niteration: just evaluate the model at regular intervals during training on both\\nthe training set and the validation set, and plot the results. If the model cannot\\nbe trained incrementally (i.e., if it does not support partial_fit() or\\nwarm_start), then you must train it several times on gradually larger subsets\\nof the training set.\\nScikit-Learn has a useful learning_curve() function to help with this: it trains\\nand evaluates the model using cross-validation. By default it retrains the\\nmodel on growing subsets of the training set, but if the model supports\\nincremental learning you can set exploit_incremental_learning=True when\\ncalling learning_curve() and it will train the model incrementally instead. The\\nfunction returns the training set sizes at which it evaluated the model, and the\\ntraining and validation scores it measured for each size and for each cross-\\nvalidation fold. Let’s use this function to look at the learning curves of the\\nplain linear regression model (see Figure 4-15):\\nfrom sklearn.model_selection import learning_curve\\ntrain_sizes, train_scores, valid_scores = learning_curve(\\n    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\\n    scoring=\"neg_root_mean_squared_error\")\\ntrain_errors = -train_scores.mean(axis=1)\\nvalid_errors = -valid_scores.mean(axis=1)\\nplt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\\nplt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\\n[...]  # beautify the figure: add labels, axis, grid, and legend\\nplt.show()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 252}, page_content='Figure 4-15. Learning curves\\nThis model is underfitting. To see why, first let’s look at the training error.\\nWhen there are just one or two instances in the training set, the model can fit\\nthem perfectly, which is why the curve starts at zero. But as new instances are\\nadded to the training set, it becomes impossible for the model to fit the\\ntraining data perfectly, both because the data is noisy and because it is not\\nlinear at all. So the error on the training data goes up until it reaches a\\nplateau, at which point adding new instances to the training set doesn’t make\\nthe average error much better or worse. Now let’s look at the validation error.\\nWhen the model is trained on very few training instances, it is incapable of\\ngeneralizing properly, which is why the validation error is initially quite\\nlarge. Then, as the model is shown more training examples, it learns, and thus\\nthe validation error slowly goes down. However, once again a straight line\\ncannot do a good job of modeling the data, so the error ends up at a plateau,\\nvery close to the other curve.\\nThese learning curves are typical of a model that’s underfitting. Both curves\\nhave reached a plateau; they are close and fairly high.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 253}, page_content='TIP\\nIf your model is underfitting the training data, adding more training examples will not\\nhelp. You need to use a better model or come up with better features.\\nNow let’s look at the learning curves of a 10th-degree polynomial model on\\nthe same data (Figure 4-16):\\nfrom sklearn.pipeline import make_pipeline\\npolynomial_regression = make_pipeline(\\n    PolynomialFeatures(degree=10, include_bias=False),\\n    LinearRegression())\\ntrain_sizes, train_scores, valid_scores = learning_curve(\\n    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\\n    scoring=\"neg_root_mean_squared_error\")\\n[...]  # same as earlier\\nFigure 4-16. Learning curves for the 10th-degree polynomial model'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 254}, page_content='These learning curves look a bit like the previous ones, but there are two very\\nimportant differences:\\nThe error on the training data is much lower than before.\\nThere is a gap between the curves. This means that the model performs\\nsignificantly better on the training data than on the validation data,\\nwhich is the hallmark of an overfitting model. If you used a much larger\\ntraining set, however, the two curves would continue to get closer.\\nTIP\\nOne way to improve an overfitting model is to feed it more training data until the\\nvalidation error reaches the training error.\\nTHE BIAS/VARIANCE TRADE-OFF\\nAn important theoretical result of statistics and machine learning is the\\nfact that a model’s generalization error can be expressed as the sum of\\nthree very different errors:\\nBias\\nThis part of the generalization error is due to wrong assumptions,\\nsuch as assuming that the data is linear when it is actually quadratic.\\nA high-bias model is most likely to underfit the training data.\\u2060\\nVariance\\nThis part is due to the model’s excessive sensitivity to small\\nvariations in the training data. A model with many degrees of\\nfreedom (such as a high-degree polynomial model) is likely to have\\nhigh variance and thus overfit the training data.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 255}, page_content='reduce this part of the error is to clean up the data (e.g., fix the data\\nsources, such as broken sensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and\\nreduce its bias. Conversely, reducing a model’s complexity increases its\\nbias and reduces its variance. This is why it is called a trade-off.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 256}, page_content='Regularized Linear Models\\nAs you saw in Chapters 1 and 2, a good way to reduce overfitting is to\\nregularize the model (i.e., to constrain it): the fewer degrees of freedom it\\nhas, the harder it will be for it to overfit the data. A simple way to regularize\\na polynomial model is to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the\\nweights of the model. We will now look at ridge regression, lasso regression,\\nand elastic net regression, which implement three different ways to constrain\\nthe weights.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 257}, page_content='Ridge Regression\\nRidge regression (also called Tikhonov regularization) is a regularized\\nversion of linear regression: a regularization term equal to αm∑i=1nθi2 is\\nadded to the MSE. This forces the learning algorithm to not only fit the data\\nbut also keep the model weights as small as possible. Note that the\\nregularization term should only be added to the cost function during training.\\nOnce the model is trained, you want to use the unregularized MSE (or the\\nRMSE) to evaluate the model’s performance.\\nThe hyperparameter α controls how much you want to regularize the model.\\nIf α = 0, then ridge regression is just linear regression. If α is very large, then\\nall weights end up very close to zero and the result is a flat line going through\\nthe data’s mean. Equation 4-8 presents the ridge regression cost function.\\u2060\\nEquation 4-8. Ridge regression cost function\\nJ(θ)=MSE(θ)+αm∑i=1nθi2\\nNote that the bias term θ  is not regularized (the sum starts at i = 1, not 0). If\\nwe define w as the vector of feature weights (θ  to θ ), then the regularization\\nterm is equal to α(∥ w ∥)  / m, where ∥ w ∥ represents the ℓ norm of\\nthe weight vector.\\u2060\\n For batch gradient descent, just add 2αw / m to the part\\nof the MSE gradient vector that corresponds to the feature weights, without\\nadding anything to the gradient of the bias term (see Equation 4-6).\\nWARNING\\nIt is important to scale the data (e.g., using a StandardScaler) before performing ridge\\nregression, as it is sensitive to the scale of the input features. This is true of most\\nregularized models.\\nFigure 4-17 shows several ridge models that were trained on some very noisy\\nlinear data using different α values. On the left, plain ridge models are used,\\nleading to linear predictions. On the right, the data is first expanded using\\n7\\n0\\n1\\nn\\n2 2\\n2\\n2\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 258}, page_content='PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, and\\nfinally the ridge models are applied to the resulting features: this is\\npolynomial regression with ridge regularization. Note how increasing α leads\\nto flatter (i.e., less extreme, more reasonable) predictions, thus reducing the\\nmodel’s variance but increasing its bias.\\nFigure 4-17. Linear (left) and a polynomial (right) models, both with various levels of ridge\\nregularization\\nAs with linear regression, we can perform ridge regression either by\\ncomputing a closed-form equation or by performing gradient descent. The\\npros and cons are the same. Equation 4-9 shows the closed-form solution,\\nwhere A is the (n + 1) × (n + 1) identity matrix,\\u2060\\n except with a 0 in the top-\\nleft cell, corresponding to the bias term.\\nEquation 4-9. Ridge regression closed-form solution\\nθ ^ = (X ⊺ X+αA) -1   X ⊺   y\\nHere is how to perform ridge regression with Scikit-Learn using a closed-\\nform solution (a variant of Equation 4-9 that uses a matrix factorization\\ntechnique by André-Louis Cholesky):\\n>>> from sklearn.linear_model import Ridge\\n>>> ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\\n>>> ridge_reg.fit(X, y)\\n>>> ridge_reg.predict([[1.5]])\\narray([[1.55325833]])\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 259}, page_content='And using stochastic gradient descent:\\u2060\\n>>> sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\\n...                        max_iter=1000, eta0=0.01, random_state=42)\\n...\\n>>> sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\\n>>> sgd_reg.predict([[1.5]])\\narray([1.55302613])\\nThe penalty hyperparameter sets the type of regularization term to use.\\nSpecifying \"l2\" indicates that you want SGD to add a regularization term to\\nthe MSE cost function equal to alpha times the square of the ℓ norm of the\\nweight vector. This is just like ridge regression, except there’s no division by\\nm in this case; that’s why we passed alpha=0.1 / m, to get the same result as\\nRidge(alpha=0.1).\\nTIP\\nThe RidgeCV class also performs ridge regression, but it automatically tunes\\nhyperparameters using cross-validation. It’s roughly equivalent to using GridSearchCV,\\nbut it’s optimized for ridge regression and runs much faster. Several other estimators\\n(mostly linear) also have efficient CV variants, such as LassoCV and ElasticNetCV.\\n10\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 260}, page_content='Lasso Regression\\nLeast absolute shrinkage and selection operator regression (usually simply\\ncalled lasso regression) is another regularized version of linear regression:\\njust like ridge regression, it adds a regularization term to the cost function,\\nbut it uses the ℓ norm of the weight vector instead of the square of the ℓ\\nnorm (see Equation 4-10). Notice that the ℓ norm is multiplied by 2α,\\nwhereas the ℓ norm was multiplied by α / m in ridge regression. These\\nfactors were chosen to ensure that the optimal α value is independent from\\nthe training set size: different norms lead to different factors (see Scikit-Learn\\nissue #15657 for more details).\\nEquation 4-10. Lasso regression cost function\\nJ(θ)=MSE(θ)+2α∑i=1nθi\\nFigure 4-18 shows the same thing as Figure 4-17 but replaces the ridge\\nmodels with lasso models and uses different α values.\\nFigure 4-18. Linear (left) and polynomial (right) models, both using various levels of lasso\\nregularization\\nAn important characteristic of lasso regression is that it tends to eliminate the\\nweights of the least important features (i.e., set them to zero). For example,\\nthe dashed line in the righthand plot in Figure 4-18 (with α = 0.01) looks\\nroughly cubic: all the weights for the high-degree polynomial features are\\nequal to zero. In other words, lasso regression automatically performs feature\\n1\\n2\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 261}, page_content='selection and outputs a sparse model with few nonzero feature weights.\\nYou can get a sense of why this is the case by looking at Figure 4-19: the\\naxes represent two model parameters, and the background contours represent\\ndifferent loss functions. In the top-left plot, the contours represent the ℓ loss\\n(|θ | + |θ |), which drops linearly as you get closer to any axis. For example, if\\nyou initialize the model parameters to θ  = 2 and θ  = 0.5, running gradient\\ndescent will decrement both parameters equally (as represented by the dashed\\nyellow line); therefore θ  will reach 0 first (since it was closer to 0 to begin\\nwith). After that, gradient descent will roll down the gutter until it reaches θ\\n= 0 (with a bit of bouncing around, since the gradients of ℓ never get close to\\n0: they are either –1 or 1 for each parameter). In the top-right plot, the\\ncontours represent lasso regression’s cost function (i.e., an MSE cost function\\nplus an ℓ loss). The small white circles show the path that gradient descent\\ntakes to optimize some model parameters that were initialized around θ  =\\n0.25 and θ  = –1: notice once again how the path quickly reaches θ  = 0, then\\nrolls down the gutter and ends up bouncing around the global optimum\\n(represented by the red square). If we increased α, the global optimum would\\nmove left along the dashed yellow line, while if we decreased α, the global\\noptimum would move right (in this example, the optimal parameters for the\\nunregularized MSE are θ  = 2 and θ  = 0.5).\\n1\\n1\\n2\\n1\\n2\\n2\\n1\\n1\\n1\\n1\\n2\\n2\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 262}, page_content='Figure 4-19. Lasso versus ridge regularization\\nThe two bottom plots show the same thing but with an ℓ penalty instead. In\\nthe bottom-left plot, you can see that the ℓ loss decreases as we get closer to\\nthe origin, so gradient descent just takes a straight path toward that point. In\\nthe bottom-right plot, the contours represent ridge regression’s cost function\\n(i.e., an MSE cost function plus an ℓ loss). As you can see, the gradients get\\nsmaller as the parameters approach the global optimum, so gradient descent\\nnaturally slows down. This limits the bouncing around, which helps ridge\\nconverge faster than lasso regression. Also note that the optimal parameters\\n(represented by the red square) get closer and closer to the origin when you\\nincrease α, but they never get eliminated entirely.\\nTIP\\nTo keep gradient descent from bouncing around the optimum at the end when using lasso\\n2\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 263}, page_content='regression, you need to gradually reduce the learning rate during training. It will still\\nbounce around the optimum, but the steps will get smaller and smaller, so it will converge.\\nThe lasso cost function is not differentiable at θ  = 0 (for i = 1, 2, ⋯, n), but\\ngradient descent still works if you use a subgradient vector g\\u2060  instead when\\nany θ  = 0. Equation 4-11 shows a subgradient vector equation you can use\\nfor gradient descent with the lasso cost function.\\nEquation 4-11. Lasso regression subgradient vector\\ng(θ,J)=∇θ\\u200a\\nMSE(θ)+2αsign(θ1)sign(θ2)⋮sign(θn)  where sign(θi)=-1if θi<00if θi=0+1if θi\\nHere is a small Scikit-Learn example using the Lasso class:\\n>>> from sklearn.linear_model import Lasso\\n>>> lasso_reg = Lasso(alpha=0.1)\\n>>> lasso_reg.fit(X, y)\\n>>> lasso_reg.predict([[1.5]])\\narray([1.53788174])\\nNote that you could instead use SGDRegressor(penalty=\"l1\", alpha=0.1).\\ni\\n11\\ni'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 264}, page_content='Elastic Net Regression\\nElastic net regression is a middle ground between ridge regression and lasso\\nregression. The regularization term is a weighted sum of both ridge and\\nlasso’s regularization terms, and you can control the mix ratio r. When r = 0,\\nelastic net is equivalent to ridge regression, and when r = 1, it is equivalent to\\nlasso regression (Equation 4-12).\\nEquation 4-12. Elastic net cost function\\nJ(θ)=MSE(θ)+r2α∑i=1nθi+(1-r)αm∑i=1nθi2\\nSo when should you use elastic net regression, or ridge, lasso, or plain linear\\nregression (i.e., without any regularization)? It is almost always preferable to\\nhave at least a little bit of regularization, so generally you should avoid plain\\nlinear regression. Ridge is a good default, but if you suspect that only a few\\nfeatures are useful, you should prefer lasso or elastic net because they tend to\\nreduce the useless features’ weights down to zero, as discussed earlier. In\\ngeneral, elastic net is preferred over lasso because lasso may behave\\nerratically when the number of features is greater than the number of training\\ninstances or when several features are strongly correlated.\\nHere is a short example that uses Scikit-Learn’s ElasticNet (l1_ratio\\ncorresponds to the mix ratio r):\\n>>> from sklearn.linear_model import ElasticNet\\n>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\\n>>> elastic_net.fit(X, y)\\n>>> elastic_net.predict([[1.5]])\\narray([1.54333232])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 265}, page_content='Early Stopping\\nA very different way to regularize iterative learning algorithms such as\\ngradient descent is to stop training as soon as the validation error reaches a\\nminimum. This is called early stopping. Figure 4-20 shows a complex model\\n(in this case, a high-degree polynomial regression model) being trained with\\nbatch gradient descent on the quadratic dataset we used earlier. As the epochs\\ngo by, the algorithm learns, and its prediction error (RMSE) on the training\\nset goes down, along with its prediction error on the validation set. After a\\nwhile, though, the validation error stops decreasing and starts to go back up.\\nThis indicates that the model has started to overfit the training data. With\\nearly stopping you just stop training as soon as the validation error reaches\\nthe minimum. It is such a simple and efficient regularization technique that\\nGeoffrey Hinton called it a “beautiful free lunch”.\\nFigure 4-20. Early stopping regularization\\nTIP'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 266}, page_content=\"With stochastic and mini-batch gradient descent, the curves are not so smooth, and it may\\nbe hard to know whether you have reached the minimum or not. One solution is to stop\\nonly after the validation error has been above the minimum for some time (when you are\\nconfident that the model will not do any better), then roll back the model parameters to the\\npoint where the validation error was at a minimum.\\nHere is a basic implementation of early stopping:\\nfrom copy import deepcopy\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.preprocessing import StandardScaler\\nX_train, y_train, X_valid, y_valid = [...]  # split the quadratic dataset\\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\\n                              StandardScaler())\\nX_train_prep = preprocessing.fit_transform(X_train)\\nX_valid_prep = preprocessing.transform(X_valid)\\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\\nn_epochs = 500\\nbest_valid_rmse = float('inf')\\nfor epoch in range(n_epochs):\\n    sgd_reg.partial_fit(X_train_prep, y_train)\\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\\n    if val_error < best_valid_rmse:\\n        best_valid_rmse = val_error\\n        best_model = deepcopy(sgd_reg)\\nThis code first adds the polynomial features and scales all the input features,\\nboth for the training set and for the validation set (the code assumes that you\\nhave split the original training set into a smaller training set and a validation\\nset). Then it creates an SGDRegressor model with no regularization and a\\nsmall learning rate. In the training loop, it calls partial_fit() instead of fit(), to\\nperform incremental learning. At each epoch, it measures the RMSE on the\\nvalidation set. If it is lower than the lowest RMSE seen so far, it saves a copy\\nof the model in the best_model variable. This implementation does not\\nactually stop training, but it lets you revert to the best model after training.\\nNote that the model is copied using copy.deepcopy(), because it copies both\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 267}, page_content='the model’s hyperparameters and the learned parameters. In contrast,\\nsklearn.base.clone() only copies the model’s hyperparameters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 268}, page_content='Logistic Regression\\nAs discussed in Chapter 1, some regression algorithms can be used for\\nclassification (and vice versa). Logistic regression (also called logit\\nregression) is commonly used to estimate the probability that an instance\\nbelongs to a particular class (e.g., what is the probability that this email is\\nspam?). If the estimated probability is greater than a given threshold\\n(typically 50%), then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), and otherwise it predicts that it does\\nnot (i.e., it belongs to the negative class, labeled “0”). This makes it a binary\\nclassifier.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 269}, page_content='Estimating Probabilities\\nSo how does logistic regression work? Just like a linear regression model, a\\nlogistic regression model computes a weighted sum of the input features (plus\\na bias term), but instead of outputting the result directly like the linear\\nregression model does, it outputs the logistic of this result (see Equation 4-\\n13).\\nEquation 4-13. Logistic regression model estimated probability (vectorized form)\\np ^ = h θ ( x ) = σ ( θ ⊺ x )\\nThe logistic—noted σ(·)—is a sigmoid function (i.e., S-shaped) that outputs a\\nnumber between 0 and 1. It is defined as shown in Equation 4-14 and\\nFigure 4-21.\\nEquation 4-14. Logistic function\\nσ ( t ) = 1 1+exp(-t)\\nFigure 4-21. Logistic function\\nOnce the logistic regression model has estimated the probability p^ = h (x)\\nthat an instance x belongs to the positive class, it can make its prediction ŷ\\neasily (see Equation 4-15).\\nEquation 4-15. Logistic regression model prediction using a 50% threshold probability\\ny ^ = 0 if p ^ < 0.5 1 if p ^ ≥ 0.5\\nθ'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 270}, page_content='Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a logistic\\nregression model using the default threshold of 50% probability predicts 1 if\\nθ  x is positive and 0 if it is negative.\\nNOTE\\nThe score t is often called the logit. The name comes from the fact that the logit function,\\ndefined as logit(p) = log(p / (1 – p)), is the inverse of the logistic function. Indeed, if you\\ncompute the logit of the estimated probability p, you will find that the result is t. The logit\\nis also called the log-odds, since it is the log of the ratio between the estimated probability\\nfor the positive class and the estimated probability for the negative class.\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 271}, page_content='Training and Cost Function\\nNow you know how a logistic regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set\\nthe parameter vector θ so that the model estimates high probabilities for\\npositive instances (y = 1) and low probabilities for negative instances (y = 0).\\nThis idea is captured by the cost function shown in Equation 4-16 for a single\\ntraining instance x.\\nEquation 4-16. Cost function of a single training instance\\nc(θ)=-log(p^)if y=1-log(1-p^)if y=0\\nThis cost function makes sense because –log(t) grows very large when t\\napproaches 0, so the cost will be large if the model estimates a probability\\nclose to 0 for a positive instance, and it will also be large if the model\\nestimates a probability close to 1 for a negative instance. On the other hand, –\\nlog(t) is close to 0 when t is close to 1, so the cost will be close to 0 if the\\nestimated probability is close to 0 for a negative instance or close to 1 for a\\npositive instance, which is precisely what we want.\\nThe cost function over the whole training set is the average cost over all\\ntraining instances. It can be written in a single expression called the log loss,\\nshown in Equation 4-17.\\nEquation 4-17. Logistic regression cost function (log loss)\\nJ(θ)=-1m∑i=1my(i)logp^(i)+(1-y(i))log1-p^(i)\\nWARNING\\nThe log loss was not just pulled out of a hat. It can be shown mathematically (using\\nBayesian inference) that minimizing this loss will result in the model with the maximum\\nlikelihood of being optimal, assuming that the instances follow a Gaussian distribution\\naround the mean of their class. When you use the log loss, this is the implicit assumption\\nyou are making. The more wrong this assumption is, the more biased the model will be.\\nSimilarly, when we used the MSE to train linear regression models, we were implicitly\\nassuming that the data was purely linear, plus some Gaussian noise. So, if the data is not\\nlinear (e.g., if it’s quadratic) or if the noise is not Gaussian (e.g., if outliers are not'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 272}, page_content='exponentially rare), then the model will be biased.\\nThe bad news is that there is no known closed-form equation to compute the\\nvalue of θ that minimizes this cost function (there is no equivalent of the\\nNormal equation). But the good news is that this cost function is convex, so\\ngradient descent (or any other optimization algorithm) is guaranteed to find\\nthe global minimum (if the learning rate is not too large and you wait long\\nenough). The partial derivatives of the cost function with regard to the j\\nmodel parameter θ  are given by Equation 4-18.\\nEquation 4-18. Logistic cost function partial derivatives\\n∂ ∂θ j J ( θ ) = 1 m ∑ i=1 m σ ( θ ⊺ x (i) ) - y (i) x j (i)\\nThis equation looks very much like Equation 4-5: for each instance it\\ncomputes the prediction error and multiplies it by the j  feature value, and\\nthen it computes the average over all training instances. Once you have the\\ngradient vector containing all the partial derivatives, you can use it in the\\nbatch gradient descent algorithm. That’s it: you now know how to train a\\nlogistic regression model. For stochastic GD you would take one instance at a\\ntime, and for mini-batch GD you would use a mini-batch at a time.\\nth\\nj\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 273}, page_content=\"Decision Boundaries\\nWe can use the iris dataset to illustrate logistic regression. This is a famous\\ndataset that contains the sepal and petal length and width of 150 iris flowers\\nof three different species: Iris setosa, Iris versicolor, and Iris virginica (see\\nFigure 4-22).\\nFigure 4-22. Flowers of three iris plant species\\u2060\\nLet’s try to build a classifier to detect the Iris virginica type based only on the\\npetal width feature. The first step is to load the data and take a quick peek:\\n>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris(as_frame=True)\\n>>> list(iris)\\n['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names',\\n 'filename', 'data_module']\\n>>> iris.data.head(3)\\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\\n0                5.1               3.5                1.4               0.2\\n1                4.9               3.0                1.4               0.2\\n2                4.7               3.2                1.3               0.2\\n>>> iris.target.head(3)  # note that the instances are not shuffled\\n0    0\\n1    0\\n12\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 274}, page_content='2    0\\nName: target, dtype: int64\\n>>> iris.target_names\\narray([\\'setosa\\', \\'versicolor\\', \\'virginica\\'], dtype=\\'<U10\\')\\nNext we’ll split the data and train a logistic regression model on the training\\nset:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nX = iris.data[[\"petal width (cm)\"]].values\\ny = iris.target_names[iris.target] == \\'virginica\\'\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nlog_reg = LogisticRegression(random_state=42)\\nlog_reg.fit(X_train, y_train)\\nLet’s look at the model’s estimated probabilities for flowers with petal widths\\nvarying from 0 cm to 3 cm (Figure 4-23):\\u2060\\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\\ny_proba = log_reg.predict_proba(X_new)\\ndecision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\\n         label=\"Not Iris virginica proba\")\\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\\n         label=\"Decision boundary\")\\n[...] # beautify the figure: add grid, labels, axis, legend, arrows, and samples\\nplt.show()\\n13'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 275}, page_content='Figure 4-23. Estimated probabilities and decision boundary\\nThe petal width of Iris virginica flowers (represented as triangles) ranges\\nfrom 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares)\\ngenerally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice\\nthat there is a bit of overlap. Above about 2 cm the classifier is highly\\nconfident that the flower is an Iris virginica (it outputs a high probability for\\nthat class), while below 1 cm it is highly confident that it is not an Iris\\nvirginica (high probability for the “Not Iris virginica” class). In between\\nthese extremes, the classifier is unsure. However, if you ask it to predict the\\nclass (using the predict() method rather than the predict_proba() method), it\\nwill return whichever class is the most likely. Therefore, there is a decision\\nboundary at around 1.6 cm where both probabilities are equal to 50%: if the\\npetal width is greater than 1.6 cm the classifier will predict that the flower is\\nan Iris virginica, and otherwise it will predict that it is not (even if it is not\\nvery confident):\\n>>> decision_boundary\\n1.6516516516516517\\n>>> log_reg.predict([[1.7], [1.5]])\\narray([ True, False])\\nFigure 4-24 shows the same dataset, but this time displaying two features:\\npetal width and length. Once trained, the logistic regression classifier can,\\nbased on these two features, estimate the probability that a new flower is an\\nIris virginica. The dashed line represents the points where the model\\nestimates a 50% probability: this is the model’s decision boundary. Note that'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 276}, page_content='it is a linear boundary.\\u2060\\n Each parallel line represents the points where the\\nmodel outputs a specific probability, from 15% (bottom left) to 90% (top\\nright). All the flowers beyond the top-right line have over 90% chance of\\nbeing Iris virginica, according to the model.\\nFigure 4-24. Linear decision boundary\\nNOTE\\nThe hyperparameter controlling the regularization strength of a Scikit-Learn\\nLogisticRegression model is not alpha (as in other linear models), but its inverse: C. The\\nhigher the value of C, the less the model is regularized.\\nJust like the other linear models, logistic regression models can be\\nregularized using ℓ or ℓ penalties. Scikit-Learn actually adds an ℓ penalty\\nby default.\\n14\\n1\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 277}, page_content='Softmax Regression\\nThe logistic regression model can be generalized to support multiple classes\\ndirectly, without having to train and combine multiple binary classifiers (as\\ndiscussed in Chapter 3). This is called softmax regression, or multinomial\\nlogistic regression.\\nThe idea is simple: when given an instance x, the softmax regression model\\nfirst computes a score s (x) for each class k, then estimates the probability of\\neach class by applying the softmax function (also called the normalized\\nexponential) to the scores. The equation to compute s (x) should look\\nfamiliar, as it is just like the equation for linear regression prediction (see\\nEquation 4-19).\\nEquation 4-19. Softmax score for class k\\ns k ( x ) = (θ (k) ) ⊺ x\\nNote that each class has its own dedicated parameter vector θ\\n. All these\\nvectors are typically stored as rows in a parameter matrix Θ.\\nOnce you have computed the score of every class for the instance x, you can\\nestimate the probability p^k that the instance belongs to class k by running\\nthe scores through the softmax function (Equation 4-20). The function\\ncomputes the exponential of every score, then normalizes them (dividing by\\nthe sum of all the exponentials). The scores are generally called logits or log-\\nodds (although they are actually unnormalized log-odds).\\nEquation 4-20. Softmax function\\np ^ k = σ s(x) k = exps k (x) ∑ j=1 K exps j (x)\\nIn this equation:\\nK is the number of classes.\\ns(x) is a vector containing the scores of each class for the instance x.\\nσ(s(x))  is the estimated probability that the instance x belongs to class k,\\nk\\nk\\n(k)\\nk'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 278}, page_content='given the scores of each class for that instance.\\nJust like the logistic regression classifier, by default the softmax regression\\nclassifier predicts the class with the highest estimated probability (which is\\nsimply the class with the highest score), as shown in Equation 4-21.\\nEquation 4-21. Softmax regression classifier prediction\\ny ^ = argmax k σ s(x) k = argmax k s k ( x ) = argmax k (θ (k) ) ⊺ x\\nThe argmax operator returns the value of a variable that maximizes a\\nfunction. In this equation, it returns the value of k that maximizes the\\nestimated probability σ(s(x)) .\\nTIP\\nThe softmax regression classifier predicts only one class at a time (i.e., it is multiclass, not\\nmultioutput), so it should be used only with mutually exclusive classes, such as different\\nspecies of plants. You cannot use it to recognize multiple people in one picture.\\nNow that you know how the model estimates probabilities and makes\\npredictions, let’s take a look at training. The objective is to have a model that\\nestimates a high probability for the target class (and consequently a low\\nprobability for the other classes). Minimizing the cost function shown in\\nEquation 4-22, called the cross entropy, should lead to this objective because\\nit penalizes the model when it estimates a low probability for a target class.\\nCross entropy is frequently used to measure how well a set of estimated class\\nprobabilities matches the target classes.\\nEquation 4-22. Cross entropy cost function\\nJ(Θ)=-1m∑i=1m∑k=1Kyk(i)logp^k(i)\\nIn this equation, yk(i) is the target probability that the i  instance belongs to\\nclass k. In general, it is either equal to 1 or 0, depending on whether the\\ninstance belongs to the class or not.\\nNotice that when there are just two classes (K = 2), this cost function is\\nk\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 279}, page_content='equivalent to the logistic regression cost function (log loss; see Equation 4-\\n17).\\nCROSS ENTROPY\\nCross entropy originated from Claude Shannon’s information theory.\\nSuppose you want to efficiently transmit information about the weather\\nevery day. If there are eight options (sunny, rainy, etc.), you could encode\\neach option using 3 bits, because 2  = 8. However, if you think it will be\\nsunny almost every day, it would be much more efficient to code “sunny”\\non just one bit (0) and the other seven options on four bits (starting with a\\n1). Cross entropy measures the average number of bits you actually send\\nper option. If your assumption about the weather is perfect, cross entropy\\nwill be equal to the entropy of the weather itself (i.e., its intrinsic\\nunpredictability). But if your assumption is wrong (e.g., if it rains often),\\ncross entropy will be greater by an amount called the Kullback–Leibler\\n(KL) divergence.\\nThe cross entropy between two probability distributions p and q is\\ndefined as H(p,q) = –Σ \\u2009p(x) log q(x) (at least when the distributions are\\ndiscrete). For more details, check out my video on the subject.\\nThe gradient vector of this cost function with regard to θ\\n is given by\\nEquation 4-23.\\nEquation 4-23. Cross entropy gradient vector for class k\\n∇ θ (k) J ( Θ ) = 1 m ∑ i=1 m p ^ k (i) - y k (i) x (i)\\nNow you can compute the gradient vector for every class, then use gradient\\ndescent (or any other optimization algorithm) to find the parameter matrix Θ\\nthat minimizes the cost function.\\nLet’s use softmax regression to classify the iris plants into all three classes.\\nScikit-Learn’s LogisticRegression classifier uses softmax regression\\nautomatically when you train it on more than two classes (assuming you use\\nsolver=\"lbfgs\", which is the default). It also applies ℓ regularization by\\n3\\nx\\n(k)\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 280}, page_content='default, which you can control using the hyperparameter C, as mentioned\\nearlier:\\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\\ny = iris[\"target\"]\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\\nsoftmax_reg.fit(X_train, y_train)\\nSo the next time you find an iris with petals that are 5 cm long and 2 cm\\nwide, you can ask your model to tell you what type of iris it is, and it will\\nanswer Iris virginica (class 2) with 96% probability (or Iris versicolor with\\n4% probability):\\n>>> softmax_reg.predict([[5, 2]])\\narray([2])\\n>>> softmax_reg.predict_proba([[5, 2]]).round(2)\\narray([[0.  , 0.04, 0.96]])\\nFigure 4-25 shows the resulting decision boundaries, represented by the\\nbackground colors. Notice that the decision boundaries between any two\\nclasses are linear. The figure also shows the probabilities for the Iris\\nversicolor class, represented by the curved lines (e.g., the line labeled with\\n0.30 represents the 30% probability boundary). Notice that the model can\\npredict a class that has an estimated probability below 50%. For example, at\\nthe point where all decision boundaries meet, all classes have an equal\\nestimated probability of 33%.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 281}, page_content='Figure 4-25. Softmax regression decision boundaries\\nIn this chapter, you learned various ways to train linear models, both for\\nregression and for classification. You used a closed-form equation to solve\\nlinear regression, as well as gradient descent, and you learned how various\\npenalties can be added to the cost function during training to regularize the\\nmodel. Along the way, you also learned how to plot learning curves and\\nanalyze them, and how to implement early stopping. Finally, you learned\\nhow logistic regression and softmax regression work. We’ve opened up the\\nfirst machine learning black boxes! In the next chapters we will open many\\nmore, starting with support vector machines.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 282}, page_content='Exercises\\n1. Which linear regression training algorithm can you use if you have a\\ntraining set with millions of features?\\n2. Suppose the features in your training set have very different scales.\\nWhich algorithms might suffer from this, and how? What can you do\\nabout it?\\n3. Can gradient descent get stuck in a local minimum when training a\\nlogistic regression model?\\n4. Do all gradient descent algorithms lead to the same model, provided you\\nlet them run long enough?\\n5. Suppose you use batch gradient descent and you plot the validation error\\nat every epoch. If you notice that the validation error consistently goes\\nup, what is likely going on? How can you fix this?\\n6. Is it a good idea to stop mini-batch gradient descent immediately when\\nthe validation error goes up?\\n7. Which gradient descent algorithm (among those we discussed) will\\nreach the vicinity of the optimal solution the fastest? Which will actually\\nconverge? How can you make the others converge as well?\\n8. Suppose you are using polynomial regression. You plot the learning\\ncurves and you notice that there is a large gap between the training error\\nand the validation error. What is happening? What are three ways to\\nsolve this?\\n9. Suppose you are using ridge regression and you notice that the training\\nerror and the validation error are almost equal and fairly high. Would\\nyou say that the model suffers from high bias or high variance? Should\\nyou increase the regularization hyperparameter α or reduce it?\\n10. Why would you want to use:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 283}, page_content='a. Ridge regression instead of plain linear regression (i.e., without any\\nregularization)?\\nb. Lasso instead of ridge regression?\\nc. Elastic net instead of lasso regression?\\n11. Suppose you want to classify pictures as outdoor/indoor and\\ndaytime/nighttime. Should you implement two logistic regression\\nclassifiers or one softmax regression classifier?\\n12. Implement batch gradient descent with early stopping for softmax\\nregression without using Scikit-Learn, only NumPy. Use it on a\\nclassification task such as the iris dataset.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  A closed-form equation is only composed of a finite number of constants, variables, and\\nstandard operations: for example, a = sin(b – c). No infinite sums, no limits, no integrals, etc.\\n2  Technically speaking, its derivative is Lipschitz continuous.\\n3  Since feature 1 is smaller, it takes a larger change in θ  to affect the cost function, which is why\\nthe bowl is elongated along the θ  axis.\\n4  Eta (η) is the seventh letter of the Greek alphabet.\\n5  While the Normal equation can only perform linear regression, the gradient descent algorithms\\ncan be used to train many other models, as you’ll see.\\n6  This notion of bias is not to be confused with the bias term of linear models.\\n7  It is common to use the notation J(θ) for cost functions that don’t have a short name; I’ll often\\nuse this notation throughout the rest of this book. The context will make it clear which cost\\nfunction is being discussed.\\n8  Norms are discussed in Chapter 2.\\n9  A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).\\n10  Alternatively, you can use the Ridge class with the \"sag\" solver. Stochastic average GD is a\\nvariant of stochastic GD. For more details, see the presentation “Minimizing Finite Sums with\\nthe Stochastic Average Gradient Algorithm” by Mark Schmidt et al. from the University of\\nBritish Columbia.\\n11  You can think of a subgradient vector at a nondifferentiable point as an intermediate vector\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 284}, page_content='between the gradient vectors around that point.\\n12  Photos reproduced from the corresponding Wikipedia pages. Iris virginica photo by Frank\\nMayfield (Creative Commons BY-SA 2.0), Iris versicolor photo by D. Gordon E. Robertson\\n(Creative Commons BY-SA 3.0), Iris setosa photo public domain.\\n13  NumPy’s reshape() function allows one dimension to be –1, which means “automatic”: the\\nvalue is inferred from the length of the array and the remaining dimensions.\\n14  It is the set of points x such that θ  + θ x  + θ x  = 0, which defines a straight line.\\n0\\n1 1\\n2 2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 285}, page_content='Chapter 5. Support Vector\\nMachines\\nA support vector machine (SVM) is a powerful and versatile machine\\nlearning model, capable of performing linear or nonlinear classification,\\nregression, and even novelty detection. SVMs shine with small to medium-\\nsized nonlinear datasets (i.e., hundreds to thousands of instances), especially\\nfor classification tasks. However, they don’t scale very well to very large\\ndatasets, as you will see.\\nThis chapter will explain the core concepts of SVMs, how to use them, and\\nhow they work. Let’s jump right in!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 286}, page_content='Linear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some visuals.\\nFigure 5-1 shows part of the iris dataset that was introduced at the end of\\nChapter 4. The two classes can clearly be separated easily with a straight line\\n(they are linearly separable). The left plot shows the decision boundaries of\\nthree possible linear classifiers. The model whose decision boundary is\\nrepresented by the dashed line is so bad that it does not even separate the\\nclasses properly. The other two models work perfectly on this training set,\\nbut their decision boundaries come so close to the instances that these models\\nwill probably not perform as well on new instances. In contrast, the solid line\\nin the plot on the right represents the decision boundary of an SVM classifier;\\nthis line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. You can think of an SVM classifier as\\nfitting the widest possible street (represented by the parallel dashed lines)\\nbetween the classes. This is called large margin classification.\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the\\ndecision boundary at all: it is fully determined (or “supported”) by the\\ninstances located on the edge of the street. These instances are called the\\nsupport vectors (they are circled in Figure 5-1).\\nWARNING\\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2. In the left plot, the\\nvertical scale is much larger than the horizontal scale, so the widest possible street is close\\nto horizontal. After feature scaling (e.g., using Scikit-Learn’s StandardScaler), the decision'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 287}, page_content='boundary in the right plot looks much better.\\nFigure 5-2. Sensitivity to feature scales'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 288}, page_content='Soft Margin Classification\\nIf we strictly impose that all instances must be off the street and on the\\ncorrect side, this is called hard margin classification. There are two main\\nissues with hard margin classification. First, it only works if the data is\\nlinearly separable. Second, it is sensitive to outliers. Figure 5-3 shows the iris\\ndataset with just one additional outlier: on the left, it is impossible to find a\\nhard margin; on the right, the decision boundary ends up very different from\\nthe one we saw in Figure 5-1 without the outlier, and the model will probably\\nnot generalize as well.\\nFigure 5-3. Hard margin sensitivity to outliers\\nTo avoid these issues, we need to use a more flexible model. The objective is\\nto find a good balance between keeping the street as large as possible and\\nlimiting the margin violations (i.e., instances that end up in the middle of the\\nstreet or even on the wrong side). This is called soft margin classification.\\nWhen creating an SVM model using Scikit-Learn, you can specify several\\nhyperparameters, including the regularization hyperparameter C. If you set it\\nto a low value, then you end up with the model on the left of Figure 5-4. With\\na high value, you get the model on the right. As you can see, reducing C\\nmakes the street larger, but it also leads to more margin violations. In other\\nwords, reducing C results in more instances supporting the street, so there’s\\nless risk of overfitting. But if you reduce it too much, then the model ends up\\nunderfitting, as seems to be the case here: the model with C=100 looks like it\\nwill generalize better than the one with C=1.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 289}, page_content='Figure 5-4. Large margin (left) versus fewer margin violations (right)\\nTIP\\nIf your SVM model is overfitting, you can try regularizing it by reducing C.\\nThe following Scikit-Learn code loads the iris dataset and trains a linear\\nSVM classifier to detect Iris virginica flowers. The pipeline first scales the\\nfeatures, then uses a LinearSVC with C=1:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import LinearSVC\\niris = load_iris(as_frame=True)\\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\\ny = (iris.target == 2)  # Iris virginica\\nsvm_clf = make_pipeline(StandardScaler(),\\n                        LinearSVC(C=1, random_state=42))\\nsvm_clf.fit(X, y)\\nThe resulting model is represented on the left in Figure 5-4.\\nThen, as usual, you can use the model to make predictions:\\n>>> X_new = [[5.5, 1.7], [5.0, 1.5]]\\n>>> svm_clf.predict(X_new)\\narray([ True, False])\\nThe first plant is classified as an Iris virginica, while the second is not. Let’s'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 290}, page_content='look at the scores that the SVM used to make these predictions. These\\nmeasure the signed distance between each instance and the decision\\nboundary:\\n>>> svm_clf.decision_function(X_new)\\narray([ 0.66163411, -0.22036063])\\nUnlike LogisticRegression, LinearSVC doesn’t have a predict_proba()\\nmethod to estimate the class probabilities. That said, if you use the SVC class\\n(discussed shortly) instead of LinearSVC, and if you set its probability\\nhyperparameter to True, then the model will fit an extra model at the end of\\ntraining to map the SVM decision function scores to estimated probabilities.\\nUnder the hood, this requires using 5-fold cross-validation to generate out-of-\\nsample predictions for every instance in the training set, then training a\\nLogisticRegression model, so it will slow down training considerably. After\\nthat, the predict_proba() and predict_log_proba() methods will be available.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 291}, page_content='Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and often work surprisingly\\nwell, many datasets are not even close to being linearly separable. One\\napproach to handling nonlinear datasets is to add more features, such as\\npolynomial features (as we did in Chapter 4); in some cases this can result in\\na linearly separable dataset. Consider the lefthand plot in Figure 5-5: it\\nrepresents a simple dataset with just one feature, x . This dataset is not\\nlinearly separable, as you can see. But if you add a second feature x  = (x ) ,\\nthe resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a pipeline\\ncontaining a PolynomialFeatures transformer (discussed in “Polynomial\\nRegression”), followed by a StandardScaler and a LinearSVC classifier. Let’s\\ntest this on the moons dataset, a toy dataset for binary classification in which\\nthe data points are shaped as two interleaving crescent moons (see Figure 5-\\n6). You can generate this dataset using the make_moons() function:\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.preprocessing import PolynomialFeatures\\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\\npolynomial_svm_clf = make_pipeline(\\n    PolynomialFeatures(degree=3),\\n    StandardScaler(),\\n    LinearSVC(C=10, max_iter=10_000, random_state=42)\\n1\\n2\\n1 2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 292}, page_content=')\\npolynomial_svm_clf.fit(X, y)\\nFigure 5-6. Linear SVM classifier using polynomial features'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 293}, page_content='Polynomial Kernel\\nAdding polynomial features is simple to implement and can work great with\\nall sorts of machine learning algorithms (not just SVMs). That said, at a low\\npolynomial degree this method cannot deal with very complex datasets, and\\nwith a high polynomial degree it creates a huge number of features, making\\nthe model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous\\nmathematical technique called the kernel trick (which is explained later in\\nthis chapter). The kernel trick makes it possible to get the same result as if\\nyou had added many polynomial features, even with a very high degree,\\nwithout actually having to add them. This means there’s no combinatorial\\nexplosion of the number of features. This trick is implemented by the SVC\\nclass. Let’s test it on the moons dataset:\\nfrom sklearn.svm import SVC\\npoly_kernel_svm_clf = make_pipeline(StandardScaler(),\\n                                    SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\npoly_kernel_svm_clf.fit(X, y)\\nThis code trains an SVM classifier using a third-degree polynomial kernel,\\nrepresented on the left in Figure 5-7. On the right is another SVM classifier\\nusing a 10th-degree polynomial kernel. Obviously, if your model is\\noverfitting, you might want to reduce the polynomial degree. Conversely, if it\\nis underfitting, you can try increasing it. The hyperparameter coef0 controls\\nhow much the model is influenced by high-degree terms versus low-degree\\nterms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 294}, page_content='Figure 5-7. SVM classifiers with a polynomial kernel\\nTIP\\nAlthough hyperparameters will generally be tuned automatically (e.g., using randomized\\nsearch), it’s good to have a sense of what each hyperparameter actually does and how it\\nmay interact with other hyperparameters: this way, you can narrow the search to a much\\nsmaller space.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 295}, page_content='Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed\\nusing a similarity function, which measures how much each instance\\nresembles a particular landmark, as we did in Chapter 2 when we added the\\ngeographic similarity features. For example, let’s take the 1D dataset from\\nearlier and add two landmarks to it at x  = –2 and x  = 1 (see the left plot in\\nFigure 5-8). Next, we’ll define the similarity function to be the Gaussian RBF\\nwith γ = 0.3. This is a bell-shaped function varying from 0 (very far away\\nfrom the landmark) to 1 (at the landmark).\\nNow we are ready to compute the new features. For example, let’s look at the\\ninstance x  = –1: it is located at a distance of 1 from the first landmark and 2\\nfrom the second landmark. Therefore, its new features are x  = exp(–0.3 × 1 )\\n≈ 0.74 and x  = exp(–0.3 × 2 ) ≈ 0.30. The plot on the right in Figure 5-8\\nshows the transformed dataset (dropping the original features). As you can\\nsee, it is now linearly separable.\\nFigure 5-8. Similarity features using the Gaussian RBF\\nYou may wonder how to select the landmarks. The simplest approach is to\\ncreate a landmark at the location of each and every instance in the dataset.\\nDoing that creates many dimensions and thus increases the chances that the\\ntransformed training set will be linearly separable. The downside is that a\\ntraining set with m instances and n features gets transformed into a training\\nset with m instances and m features (assuming you drop the original features).\\n1\\n1\\n1\\n2\\n2\\n3\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 296}, page_content='If your training set is very large, you end up with an equally large number of\\nfeatures.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 297}, page_content='Gaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can\\nbe useful with any machine learning algorithm, but it may be computationally\\nexpensive to compute all the additional features (especially on large training\\nsets). Once again the kernel trick does its SVM magic, making it possible to\\nobtain a similar result as if you had added many similarity features, but\\nwithout actually doing so. Let’s try the SVC class with the Gaussian RBF\\nkernel:\\nrbf_kernel_svm_clf = make_pipeline(StandardScaler(),\\n                                   SVC(kernel=\"rbf\", gamma=5, C=0.001))\\nrbf_kernel_svm_clf.fit(X, y)\\nThis model is represented at the bottom left in Figure 5-9. The other plots\\nshow models trained with different values of hyperparameters gamma (γ) and\\nC. Increasing gamma makes the bell-shaped curve narrower (see the lefthand\\nplots in Figure 5-8). As a result, each instance’s range of influence is smaller:\\nthe decision boundary ends up being more irregular, wiggling around\\nindividual instances. Conversely, a small gamma value makes the bell-shaped\\ncurve wider: instances have a larger range of influence, and the decision\\nboundary ends up smoother. So γ acts like a regularization hyperparameter: if\\nyour model is overfitting, you should reduce γ; if it is underfitting, you should\\nincrease γ (similar to the C hyperparameter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 298}, page_content='Figure 5-9. SVM classifiers using an RBF kernel\\nOther kernels exist but are used much more rarely. Some kernels are\\nspecialized for specific data structures. String kernels are sometimes used\\nwhen classifying text documents or DNA sequences (e.g., using the string\\nsubsequence kernel or kernels based on the Levenshtein distance).\\nTIP\\nWith so many kernels to choose from, how can you decide which one to use? As a rule of\\nthumb, you should always try the linear kernel first. The LinearSVC class is much faster\\nthan SVC(kernel=\"linear\"), especially if the training set is very large. If it is not too large,\\nyou should also try kernelized SVMs, starting with the Gaussian RBF kernel; it often\\nworks really well. Then, if you have spare time and computing power, you can experiment\\nwith a few other kernels using hyperparameter search. If there are kernels specialized for\\nyour training set’s data structure, make sure to give them a try too.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 299}, page_content='SVM Classes and Computational Complexity\\nThe LinearSVC class is based on the liblinear library, which implements an\\noptimized algorithm for linear SVMs.\\u2060\\n It does not support the kernel trick,\\nbut it scales almost linearly with the number of training instances and the\\nnumber of features. Its training time complexity is roughly O(m × n). The\\nalgorithm takes longer if you require very high precision. This is controlled\\nby the tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most\\nclassification tasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm library, which implements an algorithm\\nthat supports the kernel trick.\\u2060\\n The training time complexity is usually\\nbetween O(m  × n) and O(m  × n). Unfortunately, this means that it gets\\ndreadfully slow when the number of training instances gets large (e.g.,\\nhundreds of thousands of instances), so this algorithm is best for small or\\nmedium-sized nonlinear training sets. It scales well with the number of\\nfeatures, especially with sparse features (i.e., when each instance has few\\nnonzero features). In this case, the algorithm scales roughly with the average\\nnumber of nonzero features per instance.\\nThe SGDClassifier class also performs large margin classification by default,\\nand its hyperparameters–especially the regularization hyperparameters (alpha\\nand penalty) and the learning_rate–can be adjusted to produce similar results\\nas the linear SVMs. For training it uses stochastic gradient descent (see\\nChapter 4), which allows incremental learning and uses little memory, so you\\ncan use it to train a model on a large dataset that does not fit in RAM (i.e., for\\nout-of-core learning). Moreover, it scales very well, as its computational\\ncomplexity is O(m × n). Table 5-1 compares Scikit-Learn’s SVM\\nclassification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass\\nTime complexity\\nOut-of-core\\nsupport\\nScaling required\\nKernel trick\\nLinearSVC\\nO(m × n)\\nNo\\nYes\\nNo\\n1\\n2\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 300}, page_content='SVC\\nO(m² × n) to O(m³\\n× n)\\nNo\\nYes\\nYes\\nSGDClassifier\\nO(m × n)\\nYes\\nYes\\nNo\\nNow let’s see how the SVM algorithms can also be used for linear and\\nnonlinear regression.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 301}, page_content='SVM Regression\\nTo use SVMs for regression instead of classification, the trick is to tweak the\\nobjective: instead of trying to fit the largest possible street between two\\nclasses while limiting margin violations, SVM regression tries to fit as many\\ninstances as possible on the street while limiting margin violations (i.e.,\\ninstances off the street). The width of the street is controlled by a\\nhyperparameter, ϵ. Figure 5-10 shows two linear SVM regression models\\ntrained on some linear data, one with a small margin (ϵ = 0.5) and the other\\nwith a larger margin (ϵ = 1.2).\\nFigure 5-10. SVM regression\\nReducing ϵ increases the number of support vectors, which regularizes the\\nmodel. Moreover, if you add more training instances within the margin, it\\nwill not affect the model’s predictions; thus, the model is said to be ϵ-\\ninsensitive.\\nYou can use Scikit-Learn’s LinearSVR class to perform linear SVM\\nregression. The following code produces the model represented on the left in\\nFigure 5-10:\\nfrom sklearn.svm import LinearSVR\\nX, y = [...]  # a linear dataset\\nsvm_reg = make_pipeline(StandardScaler(),'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 302}, page_content='LinearSVR(epsilon=0.5, random_state=42))\\nsvm_reg.fit(X, y)\\nTo tackle nonlinear regression tasks, you can use a kernelized SVM model.\\nFigure 5-11 shows SVM regression on a random quadratic training set, using\\na second-degree polynomial kernel. There is some regularization in the left\\nplot (i.e., a small C value), and much less in the right plot (i.e., a large C\\nvalue).\\nFigure 5-11. SVM regression using a second-degree polynomial kernel\\nThe following code uses Scikit-Learn’s SVR class (which supports the kernel\\ntrick) to produce the model represented on the left in Figure 5-11:\\nfrom sklearn.svm import SVR\\nX, y = [...]  # a quadratic dataset\\nsvm_poly_reg = make_pipeline(StandardScaler(),\\n                             SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1))\\nsvm_poly_reg.fit(X, y)\\nThe SVR class is the regression equivalent of the SVC class, and the\\nLinearSVR class is the regression equivalent of the LinearSVC class. The\\nLinearSVR class scales linearly with the size of the training set (just like the\\nLinearSVC class), while the SVR class gets much too slow when the training\\nset grows very large (just like the SVC class).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 303}, page_content='NOTE\\nSVMs can also be used for novelty detection, as you will see in Chapter 9.\\nThe rest of this chapter explains how SVMs make predictions and how their\\ntraining algorithms work, starting with linear SVM classifiers. If you are just\\ngetting started with machine learning, you can safely skip this and go straight\\nto the exercises at the end of this chapter, and come back later when you want\\nto get a deeper understanding of SVMs.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 304}, page_content='Under the Hood of Linear SVM Classifiers\\nA linear SVM classifier predicts the class of a new instance x by first\\ncomputing the decision function θ  x = θ  x  + ⋯ + θ  x , where x  is the bias\\nfeature (always equal to 1). If the result is positive, then the predicted class ŷ\\nis the positive class (1); otherwise it is the negative class (0). This is exactly\\nlike LogisticRegression (discussed in Chapter 4).\\nNOTE\\nUp to now, I have used the convention of putting all the model parameters in one vector θ,\\nincluding the bias term θ  and the input feature weights θ  to θ . This required adding a\\nbias input x  = 1 to all instances. Another very common convention is to separate the bias\\nterm b (equal to θ ) and the feature weights vector w (containing θ  to θ ). In this case, no\\nbias feature needs to be added to the input feature vectors, and the linear SVM’s decision\\nfunction is equal to w  x + b = w  x  + ⋯ + w  x  + b. I will use this convention throughout\\nthe rest of this book.\\nSo, making predictions with a linear SVM classifier is quite straightforward.\\nHow about training? This requires finding the weights vector w and the bias\\nterm b that make the street, or margin, as wide as possible while limiting the\\nnumber of margin violations. Let’s start with the width of the street: to make\\nit larger, we need to make w smaller. This may be easier to visualize in 2D,\\nas shown in Figure 5-12. Let’s define the borders of the street as the points\\nwhere the decision function is equal to –1 or +1. In the left plot the weight w\\nis 1, so the points at which w  x  = –1 or +1 are x  = –1 and +1: therefore the\\nmargin’s size is 2. In the right plot the weight is 0.5, so the points at which w\\nx  = –1 or +1 are x  = –2 and +2: the margin’s size is 4. So, we need to keep\\nw as small as possible. Note that the bias term b has no influence on the size\\nof the margin: tweaking it just shifts the margin around, without affecting its\\nsize.\\n⊺\\n0\\n0\\nn\\nn\\n0\\n0\\n1\\nn\\n0\\n0\\n1\\nn\\n⊺\\n1\\n1\\nn\\nn\\n1\\n1\\n1\\n1\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 305}, page_content='Figure 5-12. A smaller weight vector results in a larger margin\\nWe also want to avoid margin violations, so we need the decision function to\\nbe greater than 1 for all positive training instances and lower than –1 for\\nnegative training instances. If we define t  = –1 for negative instances (when\\ny  = 0) and t  = 1 for positive instances (when y  = 1), then we can write\\nthis constraint as t (w  x  + b) ≥ 1 for all instances.\\nWe can therefore express the hard margin linear SVM classifier objective as\\nthe constrained optimization problem in Equation 5-1.\\nEquation 5-1. Hard margin linear SVM classifier objective\\nminimize w,b 1 2 w ⊺ w subject to t (i) ( w ⊺ x (i) + b ) ≥ 1 for i = 1 , 2 , ⋯ , m\\nNOTE\\nWe are minimizing ½ w  w, which is equal to ½∥ w ∥, rather than minimizing ∥ w ∥\\n(the norm of w). Indeed, ½∥ w ∥ has a nice, simple derivative (it is just w), while ∥ w\\n∥ is not differentiable at w = 0. Optimization algorithms often work much better on\\ndifferentiable functions.\\nTo get the soft margin objective, we need to introduce a slack variable ζ  ≥ 0\\nfor each instance:\\u2060\\n ζ  measures how much the i  instance is allowed to\\nviolate the margin. We now have two conflicting objectives: make the slack\\nvariables as small as possible to reduce the margin violations, and make ½ w\\nw as small as possible to increase the margin. This is where the C\\nhyperparameter comes in: it allows us to define the trade-off between these\\n(i)\\n(i)\\n(i)\\n(i)\\n(i)\\n⊺\\n(i)\\n⊺\\n2\\n2\\n(i)\\n3\\n(i)\\nth\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 306}, page_content='two objectives. This gives us the constrained optimization problem in\\nEquation 5-2.\\nEquation 5-2. Soft margin linear SVM classifier objective\\nminimize w,b,ζ 1 2 w ⊺ w + C ∑ i=1 m ζ (i) subject to t (i) ( w ⊺ x (i) + b ) ≥\\n1 - ζ (i) and ζ (i) ≥ 0 for i = 1 , 2 , ⋯ , m\\nThe hard margin and soft margin problems are both convex quadratic\\noptimization problems with linear constraints. Such problems are known as\\nquadratic programming (QP) problems. Many off-the-shelf solvers are\\navailable to solve QP problems by using a variety of techniques that are\\noutside the scope of this book.\\u2060\\nUsing a QP solver is one way to train an SVM. Another is to use gradient\\ndescent to minimize the hinge loss or the squared hinge loss (see Figure 5-\\n13). Given an instance x of the positive class (i.e., with t = 1), the loss is 0 if\\nthe output s of the decision function (s = w  x + b) is greater than or equal to\\n1. This happens when the instance is off the street and on the positive side.\\nGiven an instance of the negative class (i.e., with t = –1), the loss is 0 if s ≤ –\\n1. This happens when the instance is off the street and on the negative side.\\nThe further away an instance is from the correct side of the margin, the\\nhigher the loss: it grows linearly for the hinge loss, and quadratically for the\\nsquared hinge loss. This makes the squared hinge loss more sensitive to\\noutliers. However, if the dataset is clean, it tends to converge faster. By\\ndefault, LinearSVC uses the squared hinge loss, while SGDClassifier uses the\\nhinge loss. Both classes let you choose the loss by setting the loss\\nhyperparameter to \"hinge\" or \"squared_hinge\". The SVC class’s optimization\\nalgorithm finds a similar solution as minimizing the hinge loss.\\n4\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 307}, page_content='Figure 5-13. The hinge loss (left) and the squared hinge loss (right)\\nNext, we’ll look at yet another way to train a linear SVM classifier: solving\\nthe dual problem.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 308}, page_content='The Dual Problem\\nGiven a constrained optimization problem, known as the primal problem, it is\\npossible to express a different but closely related problem, called its dual\\nproblem. The solution to the dual problem typically gives a lower bound to\\nthe solution of the primal problem, but under some conditions it can have the\\nsame solution as the primal problem. Luckily, the SVM problem happens to\\nmeet these conditions,\\u2060\\n so you can choose to solve the primal problem or\\nthe dual problem; both will have the same solution. Equation 5-3 shows the\\ndual form of the linear SVM objective. If you are interested in knowing how\\nto derive the dual problem from the primal problem, see the extra material\\nsection in this chapter’s notebook.\\nEquation 5-3. Dual form of the linear SVM objective\\nminimize α 12∑i=1m\\u200a\\n∑j=1mα(i)α(j)t(i)t(j)x(i)⊺x(j)  -  ∑i=1mα(i)subject to α(i)≥0 for all i=1,2,\\n…,m and ∑i=1mα(i)t(i)=0\\nOnce you find the vector α ^ that minimizes this equation (using a QP\\nsolver), use Equation 5-4 to compute the w ^ and b^ that minimize the primal\\nproblem. In this equation, n  represents the number of support vectors.\\nEquation 5-4. From the dual solution to the primal solution\\nw ^ = ∑ i=1 m α ^ (i) t (i) x (i) b ^ = 1 n s ∑ i=1 α ^ (i) >0 m t (i) - w ^ ⊺ x (i)\\nThe dual problem is faster to solve than the primal one when the number of\\ntraining instances is smaller than the number of features. More importantly,\\nthe dual problem makes the kernel trick possible, while the primal problem\\ndoes not. So what is this kernel trick, anyway?\\n5\\ns'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 309}, page_content='Kernelized SVMs\\nSuppose you want to apply a second-degree polynomial transformation to a\\ntwo-dimensional training set (such as the moons training set), then train a\\nlinear SVM classifier on the transformed training set. Equation 5-5 shows the\\nsecond-degree polynomial mapping function ϕ that you want to apply.\\nEquation 5-5. Second-degree polynomial mapping\\nϕ x = ϕ x 1 x 2 = x 1 2 2 x 1 x 2 x 2 2\\nNotice that the transformed vector is 3D instead of 2D. Now let’s look at\\nwhat happens to a couple of 2D vectors, a and b, if we apply this second-\\ndegree polynomial mapping and then compute the dot product\\u2060\\n of the\\ntransformed vectors (see Equation 5-6).\\nEquation 5-6. Kernel trick for a second-degree polynomial mapping\\nϕ (a) ⊺ ϕ ( b ) = a 1 2 2a 1 a 2 a 2 2 ⊺ b 1 2 2 b 1 b 2 b 2 2 = a 1 2 b 1 2 + 2 a 1\\nb 1 a 2 b 2 + a 2 2 b 2 2 = a 1 b 1 +a 2 b 2 2 = a 1 a 2 ⊺ b 1 b 2 2 = (a ⊺ b) 2\\nHow about that? The dot product of the transformed vectors is equal to the\\nsquare of the dot product of the original vectors: ϕ(a)  ϕ(b) = (a  b) .\\nHere is the key insight: if you apply the transformation ϕ to all training\\ninstances, then the dual problem (see Equation 5-3) will contain the dot\\nproduct ϕ(x )  ϕ(x ). But if ϕ is the second-degree polynomial\\ntransformation defined in Equation 5-5, then you can replace this dot product\\nof transformed vectors simply by (x (i) ⊺ x (j) ) 2 . So, you don’t need to\\ntransform the training instances at all; just replace the dot product by its\\nsquare in Equation 5-3. The result will be strictly the same as if you had gone\\nthrough the trouble of transforming the training set and then fitting a linear\\nSVM algorithm, but this trick makes the whole process much more\\ncomputationally efficient.\\nThe function K(a, b) = (a  b)  is a second-degree polynomial kernel. In\\nmachine learning, a kernel is a function capable of computing the dot product\\nϕ(a)  ϕ(b), based only on the original vectors a and b, without having to\\n6\\n⊺\\n⊺\\n2\\n(i) ⊺\\n(j)\\n⊺\\n2\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 310}, page_content='compute (or even to know about) the transformation ϕ. Equation 5-7 lists\\nsome of the most commonly used kernels.\\nEquation 5-7. Common kernels\\nLinear: K ( a , b ) = a ⊺ b Polynomial: K ( a , b ) = γa ⊺ b+r d Gaussian RBF:\\nK ( a , b ) = exp ( - γ a-b 2 ) Sigmoid: K ( a , b ) = tanh γ a ⊺ b + r\\nMERCER’S THEOREM\\nAccording to Mercer’s theorem, if a function K(a, b) respects a few\\nmathematical conditions called Mercer’s conditions (e.g., K must be\\ncontinuous and symmetric in its arguments so that K(a, b) = K(b, a),\\netc.), then there exists a function ϕ that maps a and b into another space\\n(possibly with much higher dimensions) such that K(a, b) = ϕ(a)  ϕ(b).\\nYou can use K as a kernel because you know ϕ exists, even if you don’t\\nknow what ϕ is. In the case of the Gaussian RBF kernel, it can be shown\\nthat ϕ maps each training instance to an infinite-dimensional space, so it’s\\na good thing you don’t need to actually perform the mapping!\\nNote that some frequently used kernels (such as the sigmoid kernel) don’t\\nrespect all of Mercer’s conditions, yet they generally work well in\\npractice.\\nThere is still one loose end we must tie up. Equation 5-4 shows how to go\\nfrom the dual solution to the primal solution in the case of a linear SVM\\nclassifier. But if you apply the kernel trick, you end up with equations that\\ninclude ϕ(x ). In fact, w ^ must have the same number of dimensions as\\nϕ(x ), which may be huge or even infinite, so you can’t compute it. But how\\ncan you make predictions without knowing w ^ ? Well, the good news is that\\nyou can plug the formula for w ^ from Equation 5-4 into the decision\\nfunction for a new instance x\\n, and you get an equation with only dot\\nproducts between input vectors. This makes it possible to use the kernel trick\\n(Equation 5-8).\\nEquation 5-8. Making predictions with a kernelized SVM\\n⊺\\n(i)\\n(i)\\n(n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 311}, page_content='h w ^,b ^ ϕ ( x (n) ) = w ^ ⊺ ϕ ( x (n) ) + b ^ = ∑ i=1 m α ^ (i) t (i) ϕ(x (i) ) ⊺ ϕ\\n( x (n) ) + b ^ = ∑ i=1 m α ^ (i) t (i) ϕ (x (i) ) ⊺ ϕ ( x (n) ) + b ^ = ∑ i=1 α ^ (i)\\n>0 m α ^ (i) t (i) K ( x (i) , x (n) ) + b ^\\nNote that since α  ≠ 0 only for support vectors, making predictions involves\\ncomputing the dot product of the new input vector x\\n with only the support\\nvectors, not all the training instances. Of course, you need to use the same\\ntrick to compute the bias term b^ (Equation 5-9).\\nEquation 5-9. Using the kernel trick to compute the bias term\\nb ^ = 1 n s ∑ i=1 α ^ (i) >0 m t (i) - w ^ ⊺ ϕ ( x (i) ) = 1 n s ∑ i=1 α ^ (i) >0 m\\nt (i) - ∑ j=1 m α ^ (j) t (j) ϕ(x (j) ) ⊺ ϕ ( x (i) ) = 1 n s ∑ i=1 α ^ (i) >0 m t (i) -\\n∑ j=1 α ^ (j) >0 m α ^ (j) t (j) K ( x (i) , x (j) )\\nIf you are starting to get a headache, that’s perfectly normal: it’s an\\nunfortunate side effect of the kernel trick.\\nNOTE\\nIt is also possible to implement online kernelized SVMs, capable of incremental learning,\\nas described in the papers “Incremental and Decremental Support Vector Machine\\nLearning”\\u2060  and “Fast Kernel Classifiers with Online and Active Learning”.\\u2060\\n These\\nkernelized SVMs are implemented in Matlab and C++. But for large-scale nonlinear\\nproblems, you may want to consider using random forests (see Chapter 7) or neural\\nnetworks (see Part II).\\n(i)\\n(n)\\n7\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 312}, page_content='Exercises\\n1. What is the fundamental idea behind support vector machines?\\n2. What is a support vector?\\n3. Why is it important to scale the inputs when using SVMs?\\n4. Can an SVM classifier output a confidence score when it classifies an\\ninstance? What about a probability?\\n5. How can you choose between LinearSVC, SVC, and SGDClassifier?\\n6. Say you’ve trained an SVM classifier with an RBF kernel, but it seems\\nto underfit the training set. Should you increase or decrease γ (gamma)?\\nWhat about C?\\n7. What does it mean for a model to be ϵ-insensitive?\\n8. What is the point of using the kernel trick?\\n9. Train a LinearSVC on a linearly separable dataset. Then train an SVC\\nand a SGDClassifier on the same dataset. See if you can get them to\\nproduce roughly the same model.\\n10. Train an SVM classifier on the wine dataset, which you can load using\\nsklearn.datasets.load_wine(). This dataset contains the chemical\\nanalyses of 178 wine samples produced by 3 different cultivators: the\\ngoal is to train a classification model capable of predicting the cultivator\\nbased on the wine’s chemical analysis. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all three\\nclasses. What accuracy can you reach?\\n11. Train and fine-tune an SVM regressor on the California housing dataset.\\nYou can use the original dataset rather than the tweaked version we used\\nin Chapter 2, which you can load using\\nsklearn.datasets.fetch_california_housing(). The targets represent'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 313}, page_content='hundreds of thousands of dollars. Since there are over 20,000 instances,\\nSVMs can be slow, so for hyperparameter tuning you should use far\\nfewer instances (e.g., 2,000) to test many more hyperparameter\\ncombinations. What is your best model’s RMSE?\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  Chih-Jen Lin et al., “A Dual Coordinate Descent Method for Large-Scale Linear SVM”,\\nProceedings of the 25th International Conference on Machine Learning (2008): 408–415.\\n2  John Platt, “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector\\nMachines” (Microsoft Research technical report, April 21, 1998).\\n3  Zeta (ζ) is the sixth letter of the Greek alphabet.\\n4  To learn more about quadratic programming, you can start by reading Stephen Boyd and Lieven\\nVandenberghe’s book Convex Optimization (Cambridge University Press) or watching Richard\\nBrown’s series of video lectures.\\n5  The objective function is convex, and the inequality constraints are continuously differentiable\\nand convex functions.\\n6  As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b.\\nHowever, in machine learning, vectors are frequently represented as column vectors (i.e., single-\\ncolumn matrices), so the dot product is achieved by computing a b. To remain consistent with\\nthe rest of the book, we will use this notation here, ignoring the fact that this technically results in\\na single-cell matrix rather than a scalar value.\\n7  Gert Cauwenberghs and Tomaso Poggio, “Incremental and Decremental Support Vector\\nMachine Learning”, Proceedings of the 13th International Conference on Neural Information\\nProcessing Systems (2000): 388–394.\\n8  Antoine Bordes et al., “Fast Kernel Classifiers with Online and Active Learning”, Journal of\\nMachine Learning Research 6 (2005): 1579–1619.\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 314}, page_content='Chapter 6. Decision Trees\\nDecision trees are versatile machine learning algorithms that can perform\\nboth classification and regression tasks, and even multioutput tasks. They are\\npowerful algorithms, capable of fitting complex datasets. For example, in\\nChapter 2 you trained a DecisionTreeRegressor model on the California\\nhousing dataset, fitting it perfectly (actually, overfitting it).\\nDecision trees are also the fundamental components of random forests (see\\nChapter 7), which are among the most powerful machine learning algorithms\\navailable today.\\nIn this chapter we will start by discussing how to train, visualize, and make\\npredictions with decision trees. Then we will go through the CART training\\nalgorithm used by Scikit-Learn, and we will explore how to regularize trees\\nand use them for regression tasks. Finally, we will discuss some of the\\nlimitations of decision trees.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 315}, page_content='Training and Visualizing a Decision Tree\\nTo understand decision trees, let’s build one and take a look at how it makes\\npredictions. The following code trains a DecisionTreeClassifier on the iris\\ndataset (see Chapter 4):\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\niris = load_iris(as_frame=True)\\nX_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\\ny_iris = iris.target\\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\\ntree_clf.fit(X_iris, y_iris)\\nYou can visualize the trained decision tree by first using the\\nexport_graphviz() function to output a graph definition file called\\niris_tree.dot:\\nfrom sklearn.tree import export_graphviz\\nexport_graphviz(\\n        tree_clf,\\n        out_file=\"iris_tree.dot\",\\n        feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\\n        class_names=iris.target_names,\\n        rounded=True,\\n        filled=True\\n    )\\nThen you can use graphviz.Source.from_file() to load and display the file in a\\nJupyter notebook:\\nfrom graphviz import Source\\nSource.from_file(\"iris_tree.dot\")\\nGraphviz is an open source graph visualization software package. It also'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 316}, page_content='includes a dot command-line tool to convert .dot files to a variety of formats,\\nsuch as PDF or PNG.\\nYour first decision tree looks like Figure 6-1.\\nFigure 6-1. Iris decision tree'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 317}, page_content='Making Predictions\\nLet’s see how the tree represented in Figure 6-1 makes predictions. Suppose\\nyou find an iris flower and you want to classify it based on its petals. You\\nstart at the root node (depth 0, at the top): this node asks whether the flower’s\\npetal length is smaller than 2.45 cm. If it is, then you move down to the root’s\\nleft child node (depth 1, left). In this case, it is a leaf node (i.e., it does not\\nhave any child nodes), so it does not ask any questions: simply look at the\\npredicted class for that node, and the decision tree predicts that your flower is\\nan Iris setosa (class=setosa).\\nNow suppose you find another flower, and this time the petal length is greater\\nthan 2.45 cm. You again start at the root but now move down to its right child\\nnode (depth 1, right). This is not a leaf node, it’s a split node, so it asks\\nanother question: is the petal width smaller than 1.75 cm? If it is, then your\\nflower is most likely an Iris versicolor (depth 2, left). If not, it is likely an Iris\\nvirginica (depth 2, right). It’s really that simple.\\nNOTE\\nOne of the many qualities of decision trees is that they require very little data preparation.\\nIn fact, they don’t require feature scaling or centering at all.\\nA node’s samples attribute counts how many training instances it applies to.\\nFor example, 100 training instances have a petal length greater than 2.45 cm\\n(depth 1, right), and of those 100, 54 have a petal width smaller than 1.75 cm\\n(depth 2, left). A node’s value attribute tells you how many training instances\\nof each class this node applies to: for example, the bottom-right node applies\\nto 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica. Finally, a node’s gini\\nattribute measures its Gini impurity: a node is “pure” (gini=0) if all training\\ninstances it applies to belong to the same class. For example, since the depth-\\n1 left node applies only to Iris setosa training instances, it is pure and its Gini\\nimpurity is 0. Equation 6-1 shows how the training algorithm computes the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 318}, page_content='Gini impurity G  of the i  node. The depth-2 left node has a Gini impurity\\nequal to 1 – (0/54)  – (49/54)  – (5/54)  ≈ 0.168.\\nEquation 6-1. Gini impurity\\nG i = 1 - ∑ k=1 n p i,k 2\\nIn this equation:\\nG  is the Gini impurity of the i  node.\\np  is the ratio of class k instances among the training instances in the i\\nnode.\\nNOTE\\nScikit-Learn uses the CART algorithm, which produces only binary trees, meaning trees\\nwhere split nodes always have exactly two children (i.e., questions only have yes/no\\nanswers). However, other algorithms, such as ID3, can produce decision trees with nodes\\nthat have more than two children.\\nFigure 6-2 shows this decision tree’s decision boundaries. The thick vertical\\nline represents the decision boundary of the root node (depth 0): petal length\\n= 2.45 cm. Since the lefthand area is pure (only Iris setosa), it cannot be split\\nany further. However, the righthand area is impure, so the depth-1 right node\\nsplits it at petal width = 1.75 cm (represented by the dashed line). Since\\nmax_depth was set to 2, the decision tree stops right there. If you set\\nmax_depth to 3, then the two depth-2 nodes would each add another decision\\nboundary (represented by the two vertical dotted lines).\\ni\\nth\\n2\\n2\\n2\\ni\\nth\\ni,k\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 319}, page_content='Figure 6-2. Decision tree decision boundaries\\nTIP\\nThe tree structure, including all the information shown in Figure 6-1, is available via the\\nclassifier’s tree_ attribute. Type help(tree_clf.tree_) for details, and see the this chapter’s\\nnotebook for an example.\\nMODEL INTERPRETATION: WHITE BOX VERSUS BLACK\\nBOX\\nDecision trees are intuitive, and their decisions are easy to interpret. Such\\nmodels are often called white box models. In contrast, as you will see,\\nrandom forests and neural networks are generally considered black box\\nmodels. They make great predictions, and you can easily check the\\ncalculations that they performed to make these predictions; nevertheless,\\nit is usually hard to explain in simple terms why the predictions were\\nmade. For example, if a neural network says that a particular person\\nappears in a picture, it is hard to know what contributed to this prediction:\\nDid the model recognize that person’s eyes? Their mouth? Their nose?\\nTheir shoes? Or even the couch that they were sitting on? Conversely,\\ndecision trees provide nice, simple classification rules that can even be'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 320}, page_content='applied manually if need be (e.g., for flower classification). The field of\\ninterpretable ML aims at creating ML systems that can explain their\\ndecisions in a way humans can understand. This is important in many\\ndomains—for example, to ensure the system does not make unfair\\ndecisions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 321}, page_content='Estimating Class Probabilities\\nA decision tree can also estimate the probability that an instance belongs to a\\nparticular class k. First it traverses the tree to find the leaf node for this\\ninstance, and then it returns the ratio of training instances of class k in this\\nnode. For example, suppose you have found a flower whose petals are 5 cm\\nlong and 1.5 cm wide. The corresponding leaf node is the depth-2 left node,\\nso the decision tree outputs the following probabilities: 0% for Iris setosa\\n(0/54), 90.7% for Iris versicolor (49/54), and 9.3% for Iris virginica (5/54).\\nAnd if you ask it to predict the class, it outputs Iris versicolor (class 1)\\nbecause it has the highest probability. Let’s check this:\\n>>> tree_clf.predict_proba([[5, 1.5]]).round(3)\\narray([[0.   , 0.907, 0.093]])\\n>>> tree_clf.predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere\\nelse in the bottom-right rectangle of Figure 6-2—for example, if the petals\\nwere 6 cm long and 1.5 cm wide (even though it seems obvious that it would\\nmost likely be an Iris virginica in this case).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 322}, page_content='The CART Training Algorithm\\nScikit-Learn uses the Classification and Regression Tree (CART) algorithm\\nto train decision trees (also called “growing” trees). The algorithm works by\\nfirst splitting the training set into two subsets using a single feature k and a\\nthreshold t  (e.g., “petal length ≤ 2.45 cm”). How does it choose k and t ? It\\nsearches for the pair (k, t ) that produces the purest subsets, weighted by their\\nsize. Equation 6-2 gives the cost function that the algorithm tries to minimize.\\nEquation 6-2. CART cost function for classification\\nJ ( k , t k ) = m left m G left + m right m G right where G left/right measures\\nthe impurity of the left/right subset m left/right is the number of instances in\\nthe left/right subset\\nOnce the CART algorithm has successfully split the training set in two, it\\nsplits the subsets using the same logic, then the sub-subsets, and so on,\\nrecursively. It stops recursing once it reaches the maximum depth (defined by\\nthe max_depth hyperparameter), or if it cannot find a split that will reduce\\nimpurity. A few other hyperparameters (described in a moment) control\\nadditional stopping conditions: min_samples_split, min_samples_leaf,\\nmin_weight_fraction_leaf, and max_leaf_nodes.\\nWARNING\\nAs you can see, the CART algorithm is a greedy algorithm: it greedily searches for an\\noptimum split at the top level, then repeats the process at each subsequent level. It does\\nnot check whether or not the split will lead to the lowest possible impurity several levels\\ndown. A greedy algorithm often produces a solution that’s reasonably good but not\\nguaranteed to be optimal.\\nUnfortunately, finding the optimal tree is known to be an NP-complete problem.\\u2060\\n It\\nrequires O(exp(m)) time, making the problem intractable even for small training sets. This\\nis why we must settle for a “reasonably good” solution when training decision trees.\\nk\\nk\\nk\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 323}, page_content='Computational Complexity\\nMaking predictions requires traversing the decision tree from the root to a\\nleaf. Decision trees generally are approximately balanced, so traversing the\\ndecision tree requires going through roughly O(log (m)) nodes, where\\nlog (m) is the binary logarithm of m, equal to log(m) / log(2). Since each\\nnode only requires checking the value of one feature, the overall prediction\\ncomplexity is O(log (m)), independent of the number of features. So\\npredictions are very fast, even when dealing with large training sets.\\nThe training algorithm compares all features (or less if max_features is set)\\non all samples at each node. Comparing all features on all samples at each\\nnode results in a training complexity of O(n × m log (m)).\\n2\\n2\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 324}, page_content='Gini Impurity or Entropy?\\nBy default, the DecisionTreeClassifier class uses the Gini impurity measure,\\nbut you can select the entropy impurity measure instead by setting the\\ncriterion hyperparameter to \"entropy\". The concept of entropy originated in\\nthermodynamics as a measure of molecular disorder: entropy approaches zero\\nwhen molecules are still and well ordered. Entropy later spread to a wide\\nvariety of domains, including in Shannon’s information theory, where it\\nmeasures the average information content of a message, as we saw in\\nChapter 4. Entropy is zero when all messages are identical. In machine\\nlearning, entropy is frequently used as an impurity measure: a set’s entropy is\\nzero when it contains instances of only one class. Equation 6-3 shows the\\ndefinition of the entropy of the i  node. For example, the depth-2 left node in\\nFigure 6-1 has an entropy equal to –(49/54) log  (49/54) – (5/54) log  (5/54)\\n≈ 0.445.\\nEquation 6-3. Entropy\\nH i = - ∑ k=1 p i,k ≠0 n p i,k log 2 ( p i,k )\\nSo, should you use Gini impurity or entropy? The truth is, most of the time it\\ndoes not make a big difference: they lead to similar trees. Gini impurity is\\nslightly faster to compute, so it is a good default. However, when they differ,\\nGini impurity tends to isolate the most frequent class in its own branch of the\\ntree, while entropy tends to produce slightly more balanced trees.\\u2060\\nth\\n2\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 325}, page_content='Regularization Hyperparameters\\nDecision trees make very few assumptions about the training data (as\\nopposed to linear models, which assume that the data is linear, for example).\\nIf left unconstrained, the tree structure will adapt itself to the training data,\\nfitting it very closely—indeed, most likely overfitting it. Such a model is\\noften called a nonparametric model, not because it does not have any\\nparameters (it often has a lot) but because the number of parameters is not\\ndetermined prior to training, so the model structure is free to stick closely to\\nthe data. In contrast, a parametric model, such as a linear model, has a\\npredetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the decision tree’s\\nfreedom during training. As you know by now, this is called regularization.\\nThe regularization hyperparameters depend on the algorithm used, but\\ngenerally you can at least restrict the maximum depth of the decision tree. In\\nScikit-Learn, this is controlled by the max_depth hyperparameter. The default\\nvalue is None, which means unlimited. Reducing max_depth will regularize\\nthe model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier class has a few other parameters that similarly\\nrestrict the shape of the decision tree:\\nmax_features\\nMaximum number of features that are evaluated for splitting at each node\\nmax_leaf_nodes\\nMaximum number of leaf nodes\\nmin_samples_split\\nMinimum number of samples a node must have before it can be split\\nmin_samples_leaf'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 326}, page_content='Minimum number of samples a leaf node must have to be created\\nmin_weight_fraction_leaf\\nSame as min_samples_leaf but expressed as a fraction of the total number\\nof weighted instances\\nIncreasing min_* hyperparameters or reducing max_* hyperparameters will\\nregularize the model.\\nNOTE\\nOther algorithms work by first training the decision tree without restrictions, then pruning\\n(deleting) unnecessary nodes. A node whose children are all leaf nodes is considered\\nunnecessary if the purity improvement it provides is not statistically significant. Standard\\nstatistical tests, such as the χ  test (chi-squared test), are used to estimate the probability\\nthat the improvement is purely the result of chance (which is called the null hypothesis). If\\nthis probability, called the p-value, is higher than a given threshold (typically 5%,\\ncontrolled by a hyperparameter), then the node is considered unnecessary and its children\\nare deleted. The pruning continues until all unnecessary nodes have been pruned.\\nLet’s test regularization on the moons dataset, introduced in Chapter 5. We’ll\\ntrain one decision tree without regularization, and another with\\nmin_samples_leaf=5. Here’s the code; Figure 6-3 shows the decision\\nboundaries of each tree:\\nfrom sklearn.datasets import make_moons\\nX_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\\ntree_clf1 = DecisionTreeClassifier(random_state=42)\\ntree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\\ntree_clf1.fit(X_moons, y_moons)\\ntree_clf2.fit(X_moons, y_moons)\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 327}, page_content='Figure 6-3. Decision boundaries of an unregularized tree (left) and a regularized tree (right)\\nThe unregularized model on the left is clearly overfitting, and the regularized\\nmodel on the right will probably generalize better. We can verify this by\\nevaluating both trees on a test set generated using a different random seed:\\n>>> X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2,\\n...                                         random_state=43)\\n...\\n>>> tree_clf1.score(X_moons_test, y_moons_test)\\n0.898\\n>>> tree_clf2.score(X_moons_test, y_moons_test)\\n0.92\\nIndeed, the second tree has a better accuracy on the test set.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 328}, page_content='Regression\\nDecision trees are also capable of performing regression tasks. Let’s build a\\nregression tree using Scikit-Learn’s DecisionTreeRegressor class, training it\\non a noisy quadratic dataset with max_depth=2:\\nimport numpy as np\\nfrom sklearn.tree import DecisionTreeRegressor\\nnp.random.seed(42)\\nX_quad = np.random.rand(200, 1) - 0.5  # a single random input feature\\ny_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)\\ntree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\\ntree_reg.fit(X_quad, y_quad)\\nThe resulting tree is represented in Figure 6-4.\\nFigure 6-4. A decision tree for regression\\nThis tree looks very similar to the classification tree you built earlier. The\\nmain difference is that instead of predicting a class in each node, it predicts a\\nvalue. For example, suppose you want to make a prediction for a new'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 329}, page_content='instance with x  = 0.2. The root node asks whether x  ≤ 0.197. Since it is not,\\nthe algorithm goes to the right child node, which asks whether x  ≤ 0.772.\\nSince it is, the algorithm goes to the left child node. This is a leaf node, and it\\npredicts value=0.111. This prediction is the average target value of the 110\\ntraining instances associated with this leaf node, and it results in a mean\\nsquared error equal to 0.015 over these 110 instances.\\nThis model’s predictions are represented on the left in Figure 6-5. If you set\\nmax_depth=3, you get the predictions represented on the right. Notice how\\nthe predicted value for each region is always the average target value of the\\ninstances in that region. The algorithm splits each region in a way that makes\\nmost training instances as close as possible to that predicted value.\\nFigure 6-5. Predictions of two decision tree regression models\\nThe CART algorithm works as described earlier, except that instead of trying\\nto split the training set in a way that minimizes impurity, it now tries to split\\nthe training set in a way that minimizes the MSE. Equation 6-4 shows the\\ncost function that the algorithm tries to minimize.\\nEquation 6-4. CART cost function for regression\\nJ(k,tk)=mleftmMSEleft+mrightmMSErightwhereMSEnode=∑i∈node(y^node\\ny(i))2mnodey^node=∑i∈nodey(i)mnode\\nJust like for classification tasks, decision trees are prone to overfitting when\\ndealing with regression tasks. Without any regularization (i.e., using the\\ndefault hyperparameters), you get the predictions on the left in Figure 6-6.\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 330}, page_content='These predictions are obviously overfitting the training set very badly. Just\\nsetting min_samples_leaf=10 results in a much more reasonable model,\\nrepresented on the right in Figure 6-6.\\nFigure 6-6. Predictions of an unregularized regression tree (left) and a regularized tree (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 331}, page_content='Sensitivity to Axis Orientation\\nHopefully by now you are convinced that decision trees have a lot going for\\nthem: they are relatively easy to understand and interpret, simple to use,\\nversatile, and powerful. However, they do have a few limitations. First, as\\nyou may have noticed, decision trees love orthogonal decision boundaries (all\\nsplits are perpendicular to an axis), which makes them sensitive to the data’s\\norientation. For example, Figure 6-7 shows a simple linearly separable\\ndataset: on the left, a decision tree can split it easily, while on the right, after\\nthe dataset is rotated by 45°, the decision boundary looks unnecessarily\\nconvoluted. Although both decision trees fit the training set perfectly, it is\\nvery likely that the model on the right will not generalize well.\\nFigure 6-7. Sensitivity to training set rotation\\nOne way to limit this problem is to scale the data, then apply a principal\\ncomponent analysis transformation. We will look at PCA in detail in\\nChapter 8, but for now you only need to know that it rotates the data in a way\\nthat reduces the correlation between the features, which often (not always)\\nmakes things easier for trees.\\nLet’s create a small pipeline that scales the data and rotates it using PCA,\\nthen train a DecisionTreeClassifier on that data. Figure 6-8 shows the\\ndecision boundaries of that tree: as you can see, the rotation makes it possible\\nto fit the dataset pretty well using only one feature, z , which is a linear\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 332}, page_content='function of the original petal length and width. Here’s the code:\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\npca_pipeline = make_pipeline(StandardScaler(), PCA())\\nX_iris_rotated = pca_pipeline.fit_transform(X_iris)\\ntree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)\\ntree_clf_pca.fit(X_iris_rotated, y_iris)\\nFigure 6-8. A tree’s decision boundaries on the scaled and PCA-rotated iris dataset'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 333}, page_content='Decision Trees Have a High Variance\\nMore generally, the main issue with decision trees is that they have quite a\\nhigh variance: small changes to the hyperparameters or to the data may\\nproduce very different models. In fact, since the training algorithm used by\\nScikit-Learn is stochastic—it randomly selects the set of features to evaluate\\nat each node—even retraining the same decision tree on the exact same data\\nmay produce a very different model, such as the one represented in Figure 6-\\n9 (unless you set the random_state hyperparameter). As you can see, it looks\\nvery different from the previous decision tree (Figure 6-2).\\nFigure 6-9. Retraining the same model on the same data may produce a very different model\\nLuckily, by averaging predictions over many trees, it’s possible to reduce\\nvariance significantly. Such an ensemble of trees is called a random forest,\\nand it’s one of the most powerful types of models available today, as you will\\nsee in the next chapter.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 334}, page_content='Exercises\\n1. What is the approximate depth of a decision tree trained (without\\nrestrictions) on a training set with one million instances?\\n2. Is a node’s Gini impurity generally lower or higher than its parent’s? Is\\nit generally lower/higher, or always lower/higher?\\n3. If a decision tree is overfitting the training set, is it a good idea to try\\ndecreasing max_depth?\\n4. If a decision tree is underfitting the training set, is it a good idea to try\\nscaling the input features?\\n5. If it takes one hour to train a decision tree on a training set containing\\none million instances, roughly how much time will it take to train\\nanother decision tree on a training set containing ten million instances?\\nHint: consider the CART algorithm’s computational complexity.\\n6. If it takes one hour to train a decision tree on a given training set,\\nroughly how much time will it take if you double the number of\\nfeatures?\\n7. Train and fine-tune a decision tree for the moons dataset by following\\nthese steps:\\na. Use make_moons(n_samples=10000, noise=0.4) to generate a\\nmoons dataset.\\nb. Use train_test_split() to split the dataset into a training set and a test\\nset.\\nc. Use grid search with cross-validation (with the help of the\\nGridSearchCV class) to find good hyperparameter values for a\\nDecisionTreeClassifier. Hint: try various values for\\nmax_leaf_nodes.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 335}, page_content='d. Train it on the full training set using these hyperparameters, and\\nmeasure your model’s performance on the test set. You should get\\nroughly 85% to 87% accuracy.\\n8. Grow a forest by following these steps:\\na. Continuing the previous exercise, generate 1,000 subsets of the\\ntraining set, each containing 100 instances selected randomly. Hint:\\nyou can use Scikit-Learn’s ShuffleSplit class for this.\\nb. Train one decision tree on each subset, using the best\\nhyperparameter values found in the previous exercise. Evaluate\\nthese 1,000 decision trees on the test set. Since they were trained on\\nsmaller sets, these decision trees will likely perform worse than the\\nfirst decision tree, achieving only about 80% accuracy.\\nc. Now comes the magic. For each test set instance, generate the\\npredictions of the 1,000 decision trees, and keep only the most\\nfrequent prediction (you can use SciPy’s mode() function for this).\\nThis approach gives you majority-vote predictions over the test set.\\nd. Evaluate these predictions on the test set: you should obtain a\\nslightly higher accuracy than your first model (about 0.5 to 1.5%\\nhigher). Congratulations, you have trained a random forest\\nclassifier!\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  P is the set of problems that can be solved in polynomial time (i.e., a polynomial of the dataset\\nsize). NP is the set of problems whose solutions can be verified in polynomial time. An NP-hard\\nproblem is a problem that can be reduced to a known NP-hard problem in polynomial time. An\\nNP-complete problem is both NP and NP-hard. A major open mathematical question is whether\\nor not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be found\\nfor any NP-complete problem (except perhaps one day on a quantum computer).\\n2  See Sebastian Raschka’s interesting analysis for more details.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 336}, page_content='Chapter 7. Ensemble Learning and\\nRandom Forests\\nSuppose you pose a complex question to thousands of random people, then\\naggregate their answers. In many cases you will find that this aggregated\\nanswer is better than an expert’s answer. This is called the wisdom of the\\ncrowd. Similarly, if you aggregate the predictions of a group of predictors\\n(such as classifiers or regressors), you will often get better predictions than\\nwith the best individual predictor. A group of predictors is called an\\nensemble; thus, this technique is called ensemble learning, and an ensemble\\nlearning algorithm is called an ensemble method.\\nAs an example of an ensemble method, you can train a group of decision tree\\nclassifiers, each on a different random subset of the training set. You can then\\nobtain the predictions of all the individual trees, and the class that gets the\\nmost votes is the ensemble’s prediction (see the last exercise in Chapter 6).\\nSuch an ensemble of decision trees is called a random forest, and despite its\\nsimplicity, this is one of the most powerful machine learning algorithms\\navailable today.\\nAs discussed in Chapter 2, you will often use ensemble methods near the end\\nof a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in machine\\nlearning competitions often involve several ensemble methods—most\\nfamously in the Netflix Prize competition.\\nIn this chapter we will examine the most popular ensemble methods,\\nincluding voting classifiers, bagging and pasting ensembles, random forests,\\nand boosting, and stacking ensembles.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 337}, page_content='Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80%\\naccuracy. You may have a logistic regression classifier, an SVM classifier, a\\nrandom forest classifier, a k-nearest neighbors classifier, and perhaps a few\\nmore (see Figure 7-1).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the\\npredictions of each classifier: the class that gets the most votes is the\\nensemble’s prediction. This majority-vote classifier is called a hard voting\\nclassifier (see Figure 7-2).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 338}, page_content='Figure 7-2. Hard voting classifier predictions\\nSomewhat surprisingly, this voting classifier often achieves a higher accuracy\\nthan the best classifier in the ensemble. In fact, even if each classifier is a\\nweak learner (meaning it does only slightly better than random guessing), the\\nensemble can still be a strong learner (achieving high accuracy), provided\\nthere are a sufficient number of weak learners in the ensemble and they are\\nsufficiently diverse.\\nHow is this possible? The following analogy can help shed some light on this\\nmystery. Suppose you have a slightly biased coin that has a 51% chance of\\ncoming up heads and 49% chance of coming up tails. If you toss it 1,000\\ntimes, you will generally get more or less 510 heads and 490 tails, and hence\\na majority of heads. If you do the math, you will find that the probability of\\nobtaining a majority of heads after 1,000 tosses is close to 75%. The more\\nyou toss the coin, the higher the probability (e.g., with 10,000 tosses, the\\nprobability climbs over 97%). This is due to the law of large numbers: as you\\nkeep tossing the coin, the ratio of heads gets closer and closer to the\\nprobability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses.\\nYou can see that as the number of tosses increases, the ratio of heads'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 339}, page_content='approaches 51%. Eventually all 10 series end up so close to 51% that they are\\nconsistently above 50%.\\nFigure 7-3. The law of large numbers\\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are\\nindividually correct only 51% of the time (barely better than random\\nguessing). If you predict the majority voted class, you can hope for up to 75%\\naccuracy! However, this is only true if all classifiers are perfectly\\nindependent, making uncorrelated errors, which is clearly not the case\\nbecause they are trained on the same data. They are likely to make the same\\ntypes of errors, so there will be many majority votes for the wrong class,\\nreducing the ensemble’s accuracy.\\nTIP\\nEnsemble methods work best when the predictors are as independent from one another as\\npossible. One way to get diverse classifiers is to train them using very different\\nalgorithms. This increases the chance that they will make very different types of errors,\\nimproving the ensemble’s accuracy.\\nScikit-Learn provides a VotingClassifier class that’s quite easy to use: just\\ngive it a list of name/predictor pairs, and use it like a normal classifier. Let’s\\ntry it on the moons dataset (introduced in Chapter 5). We will load and split\\nthe moons dataset into a training set and a test set, then we’ll create and train'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 340}, page_content='a voting classifier composed of three diverse classifiers:\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nvoting_clf = VotingClassifier(\\n    estimators=[\\n        (\\'lr\\', LogisticRegression(random_state=42)),\\n        (\\'rf\\', RandomForestClassifier(random_state=42)),\\n        (\\'svc\\', SVC(random_state=42))\\n    ]\\n)\\nvoting_clf.fit(X_train, y_train)\\nWhen you fit a VotingClassifier, it clones every estimator and fits the clones.\\nThe original estimators are available via the estimators attribute, while the\\nfitted clones are available via the estimators_ attribute. If you prefer a dict\\nrather than a list, you can use named_estimators or named_estimators_\\ninstead. To begin, let’s look at each fitted classifier’s accuracy on the test set:\\n>>> for name, clf in voting_clf.named_estimators_.items():\\n...     print(name, \"=\", clf.score(X_test, y_test))\\n...\\nlr = 0.864\\nrf = 0.896\\nsvc = 0.896\\nWhen you call the voting classifier’s predict() method, it performs hard\\nvoting. For example, the voting classifier predicts class 1 for the first instance\\nof the test set, because two out of three classifiers predict that class:\\n>>> voting_clf.predict(X_test[:1])\\narray([1])\\n>>> [clf.predict(X_test[:1]) for clf in voting_clf.estimators_]\\n[array([1]), array([1]), array([0])]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 341}, page_content='Now let’s look at the performance of the voting classifier on the test set:\\n>>> voting_clf.score(X_test, y_test)\\n0.912\\nThere you have it! The voting classifier outperforms all the individual\\nclassifiers.\\nIf all classifiers are able to estimate class probabilities (i.e., if they all have a\\npredict_proba() method), then you can tell Scikit-Learn to predict the class\\nwith the highest class probability, averaged over all the individual classifiers.\\nThis is called soft voting. It often achieves higher performance than hard\\nvoting because it gives more weight to highly confident votes. All you need\\nto do is set the voting classifier’s voting hyperparameter to \"soft\", and ensure\\nthat all classifiers can estimate class probabilities. This is not the case for the\\nSVC class by default, so you need to set its probability hyperparameter to\\nTrue (this will make the SVC class use cross-validation to estimate class\\nprobabilities, slowing down training, and it will add a predict_proba()\\nmethod). Let’s try that:\\n>>> voting_clf.voting = \"soft\"\\n>>> voting_clf.named_estimators[\"svc\"].probability = True\\n>>> voting_clf.fit(X_train, y_train)\\n>>> voting_clf.score(X_test, y_test)\\n0.92\\nWe reach 92% accuracy simply by using soft voting—not bad!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 342}, page_content='Bagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training\\nalgorithms, as just discussed. Another approach is to use the same training\\nalgorithm for every predictor but train them on different random subsets of\\nthe training set. When sampling is performed with replacement,\\u2060\\n this\\nmethod is called bagging\\u2060  (short for bootstrap aggregating\\u2060 ). When sampling\\nis performed without replacement, it is called pasting.\\u2060\\nIn other words, both bagging and pasting allow training instances to be\\nsampled several times across multiple predictors, but only bagging allows\\ntraining instances to be sampled several times for the same predictor. This\\nsampling and training process is represented in Figure 7-4.\\nFigure 7-4. Bagging and pasting involve training several predictors on different random samples of the\\ntraining set\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The\\n1\\n2\\n3\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 343}, page_content='aggregation function is typically the statistical mode for classification (i.e.,\\nthe most frequent prediction, just like with a hard voting classifier), or the\\naverage for regression. Each individual predictor has a higher bias than if it\\nwere trained on the original training set, but aggregation reduces both bias\\nand variance.\\u2060\\n Generally, the net result is that the ensemble has a similar\\nbias but a lower variance than a single predictor trained on the original\\ntraining set.\\nAs you can see in Figure 7-4, predictors can all be trained in parallel, via\\ndifferent CPU cores or even different servers. Similarly, predictions can be\\nmade in parallel. This is one of the reasons bagging and pasting are such\\npopular methods: they scale very well.\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 344}, page_content='Bagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting:\\nBaggingClassifier class (or BaggingRegressor for regression). The following\\ncode trains an ensemble of 500 decision tree classifiers:\\u2060\\n each is trained on\\n100 training instances randomly sampled from the training set with\\nreplacement (this is an example of bagging, but if you want to use pasting\\ninstead, just set bootstrap=False). The n_jobs parameter tells Scikit-Learn the\\nnumber of CPU cores to use for training and predictions, and –1 tells Scikit-\\nLearn to use all available cores:\\nfrom sklearn.ensemble import BaggingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\\n                            max_samples=100, n_jobs=-1, random_state=42)\\nbag_clf.fit(X_train, y_train)\\nNOTE\\nA BaggingClassifier automatically performs soft voting instead of hard voting if the base\\nclassifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is\\nthe case with decision tree classifiers.\\nFigure 7-5 compares the decision boundary of a single decision tree with the\\ndecision boundary of a bagging ensemble of 500 trees (from the preceding\\ncode), both trained on the moons dataset. As you can see, the ensemble’s\\npredictions will likely generalize much better than the single decision tree’s\\npredictions: the ensemble has a comparable bias but a smaller variance (it\\nmakes roughly the same number of errors on the training set, but the decision\\nboundary is less irregular).\\nBagging introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting; but\\nthe extra diversity also means that the predictors end up being less correlated,\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 345}, page_content='so the ensemble’s variance is reduced. Overall, bagging often results in better\\nmodels, which explains why it’s generally preferred. But if you have spare\\ntime and CPU power, you can use cross-validation to evaluate both bagging\\nand pasting and select the one that works best.\\nFigure 7-5. A single decision tree (left) versus a bagging ensemble of 500 trees (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 346}, page_content='Out-of-Bag Evaluation\\nWith bagging, some training instances may be sampled several times for any\\ngiven predictor, while others may not be sampled at all. By default a\\nBaggingClassifier samples m training instances with replacement\\n(bootstrap=True), where m is the size of the training set. With this process, it\\ncan be shown mathematically that only about 63% of the training instances\\nare sampled on average for each predictor.\\u2060\\n The remaining 37% of the\\ntraining instances that are not sampled are called out-of-bag (OOB) instances.\\nNote that they are not the same 37% for all predictors.\\nA bagging ensemble can be evaluated using OOB instances, without the need\\nfor a separate validation set: indeed, if there are enough estimators, then each\\ninstance in the training set will likely be an OOB instance of several\\nestimators, so these estimators can be used to make a fair ensemble prediction\\nfor that instance. Once you have a prediction for each instance, you can\\ncompute the ensemble’s prediction accuracy (or any other metric).\\nIn Scikit-Learn, you can set oob_score=True when creating a\\nBaggingClassifier to request an automatic OOB evaluation after training. The\\nfollowing code demonstrates this. The resulting evaluation score is available\\nin the oob_score_ attribute:\\n>>> bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\\n...                             oob_score=True, n_jobs=-1, random_state=42)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\n>>> bag_clf.oob_score_\\n0.896\\nAccording to this OOB evaluation, this BaggingClassifier is likely to achieve\\nabout 89.6% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score(y_test, y_pred)\\n0.92\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 347}, page_content='We get 92% accuracy on the test. The OOB evaluation was a bit too\\npessimistic, just over 2% too low.\\nThe OOB decision function for each training instance is also available\\nthrough the oob_decision_function_ attribute. Since the base estimator has a\\npredict_proba() method, the decision function returns the class probabilities\\nfor each training instance. For example, the OOB evaluation estimates that\\nthe first training instance has a 67.6% probability of belonging to the positive\\nclass and a 32.4% probability of belonging to the negative class:\\n>>> bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances\\narray([[0.32352941, 0.67647059],\\n       [0.3375    , 0.6625    ],\\n       [1.        , 0.        ]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 348}, page_content='Random Patches and Random Subspaces\\nThe BaggingClassifier class supports sampling the features as well. Sampling\\nis controlled by two hyperparameters: max_features and bootstrap_features.\\nThey work the same way as max_samples and bootstrap, but for feature\\nsampling instead of instance sampling. Thus, each predictor will be trained\\non a random subset of the input features.\\nThis technique is particularly useful when you are dealing with high-\\ndimensional inputs (such as images), as it can considerably speed up training.\\nSampling both training instances and features is called the random patches\\nmethod.\\u2060\\n Keeping all training instances (by setting bootstrap=False and\\nmax_samples=1.0) but sampling features (by setting bootstrap_features to\\nTrue and/or max_features to a value smaller than 1.0) is called the random\\nsubspaces method.\\u2060\\nSampling features results in even more predictor diversity, trading a bit more\\nbias for a lower variance.\\n8\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 349}, page_content='Random Forests\\nAs we have discussed, a random forest\\u2060  is an ensemble of decision trees,\\ngenerally trained via the bagging method (or sometimes pasting), typically\\nwith max_samples set to the size of the training set. Instead of building a\\nBaggingClassifier and passing it a DecisionTreeClassifier, you can use the\\nRandomForestClassifier class, which is more convenient and optimized for\\ndecision trees\\u2060\\n (similarly, there is a RandomForestRegressor class for\\nregression tasks). The following code trains a random forest classifier with\\n500 trees, each limited to maximum 16 leaf nodes, using all available CPU\\ncores:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\\n                                 n_jobs=-1, random_state=42)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier has all the hyperparameters\\nof a DecisionTreeClassifier (to control how trees are grown), plus all the\\nhyperparameters of a BaggingClassifier to control the ensemble itself.\\nThe random forest algorithm introduces extra randomness when growing\\ntrees; instead of searching for the very best feature when splitting a node (see\\nChapter 6), it searches for the best feature among a random subset of features.\\nBy default, it samples n features (where n is the total number of features).\\nThe algorithm results in greater tree diversity, which (again) trades a higher\\nbias for a lower variance, generally yielding an overall better model. So, the\\nfollowing BaggingClassifier is equivalent to the previous\\nRandomForestClassifier:\\nbag_clf = BaggingClassifier(\\n    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\\n    n_estimators=500, n_jobs=-1, random_state=42)\\n10\\n11'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 350}, page_content='Extra-Trees\\nWhen you are growing a tree in a random forest, at each node only a random\\nsubset of the features is considered for splitting (as discussed earlier). It is\\npossible to make trees even more random by also using random thresholds\\nfor each feature rather than searching for the best possible thresholds (like\\nregular decision trees do). For this, simply set splitter=\"random\" when\\ncreating a DecisionTreeClassifier.\\nA forest of such extremely random trees is called an extremely randomized\\ntrees\\u2060  (or extra-trees for short) ensemble. Once again, this technique trades\\nmore bias for a lower variance. It also makes extra-trees classifiers much\\nfaster to train than regular random forests, because finding the best possible\\nthreshold for each feature at every node is one of the most time-consuming\\ntasks of growing a tree.\\nYou can create an extra-trees classifier using Scikit-Learn’s\\nExtraTreesClassifier class. Its API is identical to the RandomForestClassifier\\nclass, except bootstrap defaults to False. Similarly, the ExtraTreesRegressor\\nclass has the same API as the RandomForestRegressor class, except bootstrap\\ndefaults to False.\\nTIP\\nIt is hard to tell in advance whether a RandomForestClassifier will perform better or worse\\nthan an ExtraTreesClassifier. Generally, the only way to know is to try both and compare\\nthem using cross-validation.\\n12'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 351}, page_content='Feature Importance\\nYet another great quality of random forests is that they make it easy to\\nmeasure the relative importance of each feature. Scikit-Learn measures a\\nfeature’s importance by looking at how much the tree nodes that use that\\nfeature reduce impurity on average, across all trees in the forest. More\\nprecisely, it is a weighted average, where each node’s weight is equal to the\\nnumber of training samples that are associated with it (see Chapter 6).\\nScikit-Learn computes this score automatically for each feature after training,\\nthen it scales the results so that the sum of all importances is equal to 1. You\\ncan access the result using the feature_importances_ variable. For example,\\nthe following code trains a RandomForestClassifier on the iris dataset\\n(introduced in Chapter 4) and outputs each feature’s importance. It seems that\\nthe most important features are the petal length (44%) and width (42%),\\nwhile sepal length and width are rather unimportant in comparison (11% and\\n2%, respectively):\\n>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris(as_frame=True)\\n>>> rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\\n>>> rnd_clf.fit(iris.data, iris.target)\\n>>> for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\\n...     print(round(score, 2), name)\\n...\\n0.11 sepal length (cm)\\n0.02 sepal width (cm)\\n0.44 petal length (cm)\\n0.42 petal width (cm)\\nSimilarly, if you train a random forest classifier on the MNIST dataset\\n(introduced in Chapter 3) and plot each pixel’s importance, you get the image\\nrepresented in Figure 7-6.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 352}, page_content='Figure 7-6. MNIST pixel importance (according to a random forest classifier)\\nRandom forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 353}, page_content='Boosting\\nBoosting (originally called hypothesis boosting) refers to any ensemble\\nmethod that can combine several weak learners into a strong learner. The\\ngeneral idea of most boosting methods is to train predictors sequentially, each\\ntrying to correct its predecessor. There are many boosting methods available,\\nbut by far the most popular are AdaBoost\\u2060  (short for adaptive boosting) and\\ngradient boosting. Let’s start with AdaBoost.\\n13'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 354}, page_content='AdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more\\nattention to the training instances that the predecessor underfit. This results in\\nnew predictors focusing more and more on the hard cases. This is the\\ntechnique used by AdaBoost.\\nFor example, when training an AdaBoost classifier, the algorithm first trains\\na base classifier (such as a decision tree) and uses it to make predictions on\\nthe training set. The algorithm then increases the relative weight of\\nmisclassified training instances. Then it trains a second classifier, using the\\nupdated weights, and again makes predictions on the training set, updates the\\ninstance weights, and so on (see Figure 7-7).\\nFigure 7-8 shows the decision boundaries of five consecutive predictors on\\nthe moons dataset (in this example, each predictor is a highly regularized\\nSVM classifier with an RBF kernel).\\u2060\\n The first classifier gets many\\ninstances wrong, so their weights get boosted. The second classifier therefore\\ndoes a better job on these instances, and so on. The plot on the right\\nrepresents the same sequence of predictors, except that the learning rate is\\nhalved (i.e., the misclassified instance weights are boosted much less at every\\niteration). As you can see, this sequential learning technique has some\\nsimilarities with gradient descent, except that instead of tweaking a single\\npredictor’s parameters to minimize a cost function, AdaBoost adds predictors\\nto the ensemble, gradually making it better.\\n14'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 355}, page_content='Figure 7-7. AdaBoost sequential training with instance weight updates\\nOnce all predictors are trained, the ensemble makes predictions very much\\nlike bagging or pasting, except that predictors have different weights\\ndepending on their overall accuracy on the weighted training set.\\nFigure 7-8. Decision boundaries of consecutive predictors\\nWARNING'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 356}, page_content='There is one important drawback to this sequential learning technique: training cannot be\\nparallelized since each predictor can only be trained after the previous predictor has been\\ntrained and evaluated. As a result, it does not scale as well as bagging or pasting.\\nLet’s take a closer look at the AdaBoost algorithm. Each instance weight w\\nis initially set to 1/m. A first predictor is trained, and its weighted error rate r\\nis computed on the training set; see Equation 7-1.\\nEquation 7-1. Weighted error rate of the j  predictor\\nrj = ∑ i=1 y^ j (i) ≠ y (i) m w(i) where y^ j(i) is the jth predictor’s\\npredictionfor the i th instance\\nThe predictor’s weight α  is then computed using Equation 7-2, where η is the\\nlearning rate hyperparameter (defaults to 1).\\u2060\\n The more accurate the\\npredictor is, the higher its weight will be. If it is just guessing randomly, then\\nits weight will be close to zero. However, if it is most often wrong (i.e., less\\naccurate than random guessing), then its weight will be negative.\\nEquation 7-2. Predictor weight\\nα j = η log 1-r j r j\\nNext, the AdaBoost algorithm updates the instance weights, using Equation\\n7-3, which boosts the weights of the misclassified instances.\\nEquation 7-3. Weight update rule\\nfor i = 1 , 2 , ⋯ , m w (i) ← w (i) if y j ^ (i) = y (i) w (i) exp ( α j ) if y j ^ (i) ≠\\ny (i)\\nThen all the instance weights are normalized (i.e., divided by ∑i=1mw(i)).\\nFinally, a new predictor is trained using the updated weights, and the whole\\nprocess is repeated: the new predictor’s weight is computed, the instance\\nweights are updated, then another predictor is trained, and so on. The\\nalgorithm stops when the desired number of predictors is reached, or when a\\nperfect predictor is found.\\nTo make predictions, AdaBoost simply computes the predictions of all the\\n(i)\\n1\\nth\\nj\\n15'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 357}, page_content='predictors and weighs them using the predictor weights α . The predicted\\nclass is the one that receives the majority of weighted votes (see Equation 7-\\n4).\\nEquation 7-4. AdaBoost predictions\\ny ^ ( x ) = argmax k ∑ j=1 y ^ j (x)=k N α j where N is the number of\\npredictors\\nScikit-Learn uses a multiclass version of AdaBoost called SAMME\\u2060  (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss\\nfunction). When there are just two classes, SAMME is equivalent to\\nAdaBoost. If the predictors can estimate class probabilities (i.e., if they have\\na predict_proba() method), Scikit-Learn can use a variant of SAMME called\\nSAMME.R (the R stands for “Real”), which relies on class probabilities rather\\nthan predictions and generally performs better.\\nThe following code trains an AdaBoost classifier based on 30 decision\\nstumps using Scikit-Learn’s AdaBoostClassifier class (as you might expect,\\nthere is also an AdaBoostRegressor class). A decision stump is a decision tree\\nwith max_depth=1—in other words, a tree composed of a single decision\\nnode plus two leaf nodes. This is the default base estimator for the\\nAdaBoostClassifier class:\\nfrom sklearn.ensemble import AdaBoostClassifier\\nada_clf = AdaBoostClassifier(\\n    DecisionTreeClassifier(max_depth=1), n_estimators=30,\\n    learning_rate=0.5, random_state=42)\\nada_clf.fit(X_train, y_train)\\nTIP\\nIf your AdaBoost ensemble is overfitting the training set, you can try reducing the number\\nof estimators or more strongly regularizing the base estimator.\\nj\\n16'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 358}, page_content='Gradient Boosting\\nAnother very popular boosting algorithm is gradient boosting.\\u2060\\n Just like\\nAdaBoost, gradient boosting works by sequentially adding predictors to an\\nensemble, each one correcting its predecessor. However, instead of tweaking\\nthe instance weights at every iteration like AdaBoost does, this method tries\\nto fit the new predictor to the residual errors made by the previous predictor.\\nLet’s go through a simple regression example, using decision trees as the\\nbase predictors; this is called gradient tree boosting, or gradient boosted\\nregression trees (GBRT). First, let’s generate a noisy quadratic dataset and fit\\na DecisionTreeRegressor to it:\\nimport numpy as np\\nfrom sklearn.tree import DecisionTreeRegressor\\nnp.random.seed(42)\\nX = np.random.rand(100, 1) - 0.5\\ny = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\\ntree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\\ntree_reg1.fit(X, y)\\nNext, we’ll train a second DecisionTreeRegressor on the residual errors made\\nby the first predictor:\\ny2 = y - tree_reg1.predict(X)\\ntree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\\ntree_reg2.fit(X, y2)\\nAnd then we’ll train a third regressor on the residual errors made by the\\nsecond predictor:\\ny3 = y2 - tree_reg2.predict(X)\\ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\\ntree_reg3.fit(X, y3)\\nNow we have an ensemble containing three trees. It can make predictions on\\n17'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 359}, page_content='a new instance simply by adding up the predictions of all the trees:\\n>>> X_new = np.array([[-0.4], [0.], [0.5]])\\n>>> sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\\narray([0.49484029, 0.04021166, 0.75026781])\\nFigure 7-9 represents the predictions of these three trees in the left column,\\nand the ensemble’s predictions in the right column. In the first row, the\\nensemble has just one tree, so its predictions are exactly the same as the first\\ntree’s predictions. In the second row, a new tree is trained on the residual\\nerrors of the first tree. On the right you can see that the ensemble’s\\npredictions are equal to the sum of the predictions of the first two trees.\\nSimilarly, in the third row another tree is trained on the residual errors of the\\nsecond tree. You can see that the ensemble’s predictions gradually get better\\nas trees are added to the ensemble.\\nYou can use Scikit-Learn’s GradientBoostingRegressor class to train GBRT\\nensembles more easily (there’s also a GradientBoostingClassifier class for\\nclassification). Much like the RandomForestRegressor class, it has\\nhyperparameters to control the growth of decision trees (e.g., max_depth,\\nmin_samples_leaf), as well as hyperparameters to control the ensemble\\ntraining, such as the number of trees (n_estimators). The following code\\ncreates the same ensemble as the previous one:\\nfrom sklearn.ensemble import GradientBoostingRegressor\\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\\n                                 learning_rate=1.0, random_state=42)\\ngbrt.fit(X, y)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 360}, page_content='Figure 7-9. In this depiction of gradient boosting, the first predictor (top left) is trained normally, then\\neach consecutive predictor (middle left and lower left) is trained on the previous predictor’s residuals;\\nthe right column shows the resulting ensemble’s predictions\\nThe learning_rate hyperparameter scales the contribution of each tree. If you\\nset it to a low value, such as 0.05, you will need more trees in the ensemble to\\nfit the training set, but the predictions will usually generalize better. This is a\\nregularization technique called shrinkage. Figure 7-10 shows two GBRT\\nensembles trained with different hyperparameters: the one on the left does not\\nhave enough trees to fit the training set, while the one on the right has about\\nthe right amount. If we added more trees, the GBRT would start to overfit the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 361}, page_content='training set.\\nFigure 7-10. GBRT ensembles with not enough predictors (left) and just enough (right)\\nTo find the optimal number of trees, you could perform cross-validation\\nusing GridSearchCV or RandomizedSearchCV, as usual, but there’s a\\nsimpler way: if you set the n_iter_no_change hyperparameter to an integer\\nvalue, say 10, then the GradientBoostingRegressor will automatically stop\\nadding more trees during training if it sees that the last 10 trees didn’t help.\\nThis is simply early stopping (introduced in Chapter 4), but with a little bit of\\npatience: it tolerates having no progress for a few iterations before it stops.\\nLet’s train the ensemble using early stopping:\\ngbrt_best = GradientBoostingRegressor(\\n    max_depth=2, learning_rate=0.05, n_estimators=500,\\n    n_iter_no_change=10, random_state=42)\\ngbrt_best.fit(X, y)\\nIf you set n_iter_no_change too low, training may stop too early and the\\nmodel will underfit. But if you set it too high, it will overfit instead. We also\\nset a fairly small learning rate and a high number of estimators, but the actual\\nnumber of estimators in the trained ensemble is much lower, thanks to early\\nstopping:\\n>>> gbrt_best.n_estimators_\\n92\\nWhen n_iter_no_change is set, the fit() method automatically splits the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 362}, page_content='training set into a smaller training set and a validation set: this allows it to\\nevaluate the model’s performance each time it adds a new tree. The size of\\nthe validation set is controlled by the validation_fraction hyperparameter,\\nwhich is 10% by default. The tol hyperparameter determines the maximum\\nperformance improvement that still counts as negligible. It defaults to 0.0001.\\nThe GradientBoostingRegressor class also supports a subsample\\nhyperparameter, which specifies the fraction of training instances to be used\\nfor training each tree. For example, if subsample=0.25, then each tree is\\ntrained on 25% of the training instances, selected randomly. As you can\\nprobably guess by now, this technique trades a higher bias for a lower\\nvariance. It also speeds up training considerably. This is called stochastic\\ngradient boosting.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 363}, page_content='Histogram-Based Gradient Boosting\\nScikit-Learn also provides another GBRT implementation, optimized for\\nlarge datasets: histogram-based gradient boosting (HGB). It works by\\nbinning the input features, replacing them with integers. The number of bins\\nis controlled by the max_bins hyperparameter, which defaults to 255 and\\ncannot be set any higher than this. Binning can greatly reduce the number of\\npossible thresholds that the training algorithm needs to evaluate. Moreover,\\nworking with integers makes it possible to use faster and more memory-\\nefficient data structures. And the way the bins are built removes the need for\\nsorting the features when training each tree.\\nAs a result, this implementation has a computational complexity of O(b×m)\\ninstead of O(n×m×log(m)), where b is the number of bins, m is the number of\\ntraining instances, and n is the number of features. In practice, this means that\\nHGB can train hundreds of times faster than regular GBRT on large datasets.\\nHowever, binning causes a precision loss, which acts as a regularizer:\\ndepending on the dataset, this may help reduce overfitting, or it may cause\\nunderfitting.\\nScikit-Learn provides two classes for HGB: HistGradientBoostingRegressor\\nand HistGradientBoostingClassifier. They’re similar to\\nGradientBoostingRegressor and GradientBoostingClassifier, with a few\\nnotable differences:\\nEarly stopping is automatically activated if the number of instances is\\ngreater than 10,000. You can turn early stopping always on or always\\noff by setting the early_stopping hyperparameter to True or False.\\nSubsampling is not supported.\\nn_estimators is renamed to max_iter.\\nThe only decision tree hyperparameters that can be tweaked are\\nmax_leaf_nodes, min_samples_leaf, and max_depth.\\nThe HGB classes also have two nice features: they support both categorical'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 364}, page_content='features and missing values. This simplifies preprocessing quite a bit.\\nHowever, the categorical features must be represented as integers ranging\\nfrom 0 to a number lower than max_bins. You can use an OrdinalEncoder for\\nthis. For example, here’s how to build and train a complete pipeline for the\\nCalifornia housing dataset introduced in Chapter 2:\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.preprocessing import OrdinalEncoder\\nhgb_reg = make_pipeline(\\n    make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\\n                            remainder=\"passthrough\"),\\n    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\\n)\\nhgb_reg.fit(housing, housing_labels)\\nThe whole pipeline is just as short as the imports! No need for an imputer,\\nscaler, or a one-hot encoder, so it’s really convenient. Note that\\ncategorical_features must be set to the categorical column indices (or a\\nBoolean array). Without any hyperparameter tuning, this model yields an\\nRMSE of about 47,600, which is not too bad.\\nTIP\\nSeveral other optimized implementations of gradient boosting are available in the Python\\nML ecosystem: in particular, XGBoost, CatBoost, and LightGBM. These libraries have\\nbeen around for several years. They are all specialized for gradient boosting, their APIs\\nare very similar to Scikit-Learn’s, and they provide many additional features, including\\nGPU acceleration; you should definitely check them out! Moreover, the TensorFlow\\nRandom Forests library provides optimized implementations of a variety of random forest\\nalgorithms, including plain random forests, extra-trees, GBRT, and several more.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 365}, page_content='Stacking\\nThe last ensemble method we will discuss in this chapter is called stacking\\n(short for stacked generalization).\\u2060\\n It is based on a simple idea: instead of\\nusing trivial functions (such as hard voting) to aggregate the predictions of all\\npredictors in an ensemble, why don’t we train a model to perform this\\naggregation? Figure 7-11 shows such an ensemble performing a regression\\ntask on a new instance. Each of the bottom three predictors predicts a\\ndifferent value (3.1, 2.7, and 2.9), and then the final predictor (called a\\nblender, or a meta learner) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\nFigure 7-11. Aggregating predictions using a blending predictor\\n18'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 366}, page_content='To train the blender, you first need to build the blending training set. You can\\nuse cross_val_predict() on every predictor in the ensemble to get out-of-\\nsample predictions for each instance in the original training set (Figure 7-12),\\nand use these can be used as the input features to train the blender; and the\\ntargets can simply be copied from the original training set. Note that\\nregardless of the number of features in the original training set (just one in\\nthis example), the blending training set will contain one input feature per\\npredictor (three in this example). Once the blender is trained, the base\\npredictors are retrained one last time on the full original training set.\\nFigure 7-12. Training the blender in a stacking ensemble\\nIt is actually possible to train several different blenders this way (e.g., one\\nusing linear regression, another using random forest regression) to get a'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 367}, page_content=\"whole layer of blenders, and then add another blender on top of that to\\nproduce the final prediction, as shown in Figure 7-13. You may be able to\\nsqueeze out a few more drops of performance by doing this, but it will cost\\nyou in both training time and system complexity.\\nFigure 7-13. Predictions in a multilayer stacking ensemble\\nScikit-Learn provides two classes for stacking ensembles: StackingClassifier\\nand StackingRegressor. For example, we can replace the VotingClassifier we\\nused at the beginning of this chapter on the moons dataset with a\\nStackingClassifier:\\nfrom sklearn.ensemble import StackingClassifier\\nstacking_clf = StackingClassifier(\\n    estimators=[\\n        ('lr', LogisticRegression(random_state=42)),\\n        ('rf', RandomForestClassifier(random_state=42)),\\n        ('svc', SVC(probability=True, random_state=42))\\n    ],\\n    final_estimator=RandomForestClassifier(random_state=43),\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 368}, page_content='cv=5  # number of cross-validation folds\\n)\\nstacking_clf.fit(X_train, y_train)\\nFor each predictor, the stacking classifier will call predict_proba() if\\navailable; if not it will fall back to decision_function() or, as a last resort, call\\npredict(). If you don’t provide a final estimator, StackingClassifier will use\\nLogisticRegression and StackingRegressor will use RidgeCV.\\nIf you evaluate this stacking model on the test set, you will find 92.8%\\naccuracy, which is a bit better than the voting classifier using soft voting,\\nwhich got 92%.\\nIn conclusion, ensemble methods are versatile, powerful, and fairly simple to\\nuse. Random forests, AdaBoost, and GBRT are among the first models you\\nshould test for most machine learning tasks, and they particularly shine with\\nheterogeneous tabular data. Moreover, as they require very little\\npreprocessing, they’re great for getting a prototype up and running quickly.\\nLastly, ensemble methods like voting classifiers and stacking classifiers can\\nhelp push your system’s performance to its limits.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 369}, page_content='Exercises\\n1. If you have trained five different models on the exact same training data,\\nand they all achieve 95% precision, is there any chance that you can\\ncombine these models to get better results? If so, how? If not, why?\\n2. What is the difference between hard and soft voting classifiers?\\n3. Is it possible to speed up training of a bagging ensemble by distributing\\nit across multiple servers? What about pasting ensembles, boosting\\nensembles, random forests, or stacking ensembles?\\n4. What is the benefit of out-of-bag evaluation?\\n5. What makes extra-trees ensembles more random than regular random\\nforests? How can this extra randomness help? Are extra-trees classifiers\\nslower or faster than regular random forests?\\n6. If your AdaBoost ensemble underfits the training data, which\\nhyperparameters should you tweak, and how?\\n7. If your gradient boosting ensemble overfits the training set, should you\\nincrease or decrease the learning rate?\\n8. Load the MNIST dataset (introduced in Chapter 3), and split it into a\\ntraining set, a validation set, and a test set (e.g., use 50,000 instances for\\ntraining, 10,000 for validation, and 10,000 for testing). Then train\\nvarious classifiers, such as a random forest classifier, an extra-trees\\nclassifier, and an SVM classifier. Next, try to combine them into an\\nensemble that outperforms each individual classifier on the validation\\nset, using soft or hard voting. Once you have found one, try it on the test\\nset. How much better does it perform compared to the individual\\nclassifiers?\\n9. Run the individual classifiers from the previous exercise to make\\npredictions on the validation set, and create a new training set with the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 370}, page_content='resulting predictions: each training instance is a vector containing the set\\nof predictions from all your classifiers for an image, and the target is the\\nimage’s class. Train a classifier on this new training set. Congratulations\\n—you have just trained a blender, and together with the classifiers it\\nforms a stacking ensemble! Now evaluate the ensemble on the test set.\\nFor each image in the test set, make predictions with all your classifiers,\\nthen feed the predictions to the blender to get the ensemble’s\\npredictions. How does it compare to the voting classifier you trained\\nearlier? Now try again using a StackingClassifier instead. Do you get\\nbetter performance? If so, why?\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  Imagine picking a card randomly from a deck of cards, writing it down, then placing it back in\\nthe deck before picking the next card: the same card could be sampled multiple times.\\n2  Leo Breiman, “Bagging Predictors”, Machine Learning 24, no. 2 (1996): 123–140.\\n3  In statistics, resampling with replacement is called bootstrapping.\\n4  Leo Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line”,\\nMachine Learning 36, no. 1–2 (1999): 85–103.\\n5  Bias and variance were introduced in Chapter 4.\\n6  max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max\\nnumber of sampled instances is equal to the size of the training set times max_samples.\\n7  As m grows, this ratio approaches 1 – exp(–1) ≈ 63%.\\n8  Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches”, Lecture Notes in Computer\\nScience 7523 (2012): 346–361.\\n9  Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests”, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence 20, no. 8 (1998): 832–844.\\n10  Tin Kam Ho, “Random Decision Forests”, Proceedings of the Third International Conference\\non Document Analysis and Recognition 1 (1995): 278.\\n11  The BaggingClassifier class remains useful if you want a bag of something other than decision\\ntrees.\\n12  Pierre Geurts et al., “Extremely Randomized Trees”, Machine Learning 63, no. 1 (2006): 3–42.\\n13  Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line\\nLearning and an Application to Boosting”, Journal of Computer and System Sciences 55, no. 1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 371}, page_content='(1997): 119–139.\\n14  This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost;\\nthey are slow and tend to be unstable with it.\\n15  The original AdaBoost algorithm does not use a learning rate hyperparameter.\\n16  For more details, see Ji Zhu et al., “Multi-Class AdaBoost”, Statistics and Its Interface 2, no. 3\\n(2009): 349–360.\\n17  Gradient boosting was first introduced in Leo Breiman’s 1997 paper “Arcing the Edge” and was\\nfurther developed in the 1999 paper “Greedy Function Approximation: A Gradient Boosting\\nMachine” by Jerome H. Friedman.\\n18  David H. Wolpert, “Stacked Generalization”, Neural Networks 5, no. 2 (1992): 241–259.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 372}, page_content='Chapter 8. Dimensionality\\nReduction\\nMany machine learning problems involve thousands or even millions of\\nfeatures for each training instance. Not only do all these features make\\ntraining extremely slow, but they can also make it much harder to find a good\\nsolution, as you will see. This problem is often referred to as the curse of\\ndimensionality.\\nFortunately, in real-world problems, it is often possible to reduce the number\\nof features considerably, turning an intractable problem into a tractable one.\\nFor example, consider the MNIST images (introduced in Chapter 3): the\\npixels on the image borders are almost always white, so you could\\ncompletely drop these pixels from the training set without losing much\\ninformation. As we saw in the previous chapter, (Figure 7-6) confirms that\\nthese pixels are utterly unimportant for the classification task. Additionally,\\ntwo neighboring pixels are often highly correlated: if you merge them into a\\nsingle pixel (e.g., by taking the mean of the two pixel intensities), you will\\nnot lose much information.\\nWARNING\\nReducing dimensionality does cause some information loss, just like compressing an\\nimage to JPEG can degrade its quality, so even though it will speed up training, it may\\nmake your system perform slightly worse. It also makes your pipelines a bit more complex\\nand thus harder to maintain. Therefore, I recommend you first try to train your system with\\nthe original data before considering using dimensionality reduction. In some cases,\\nreducing the dimensionality of the training data may filter out some noise and unnecessary\\ndetails and thus result in higher performance, but in general it won’t; it will just speed up\\ntraining.\\nApart from speeding up training, dimensionality reduction is also extremely'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 373}, page_content='useful for data visualization. Reducing the number of dimensions down to\\ntwo (or three) makes it possible to plot a condensed view of a high-\\ndimensional training set on a graph and often gain some important insights by\\nvisually detecting patterns, such as clusters. Moreover, data visualization is\\nessential to communicate your conclusions to people who are not data\\nscientists—in particular, decision makers who will use your results.\\nIn this chapter we will first discuss the curse of dimensionality and get a\\nsense of what goes on in high-dimensional space. Then we will consider the\\ntwo main approaches to dimensionality reduction (projection and manifold\\nlearning), and we will go through three of the most popular dimensionality\\nreduction techniques: PCA, random projection, and locally linear embedding\\n(LLE).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 374}, page_content='The Curse of Dimensionality\\nWe are so used to living in three dimensions\\u2060\\n that our intuition fails us\\nwhen we try to imagine a high-dimensional space. Even a basic 4D\\nhypercube is incredibly hard to picture in our minds (see Figure 8-1), let\\nalone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.\\nFigure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)\\u2060\\nIt turns out that many things behave very differently in high-dimensional\\nspace. For example, if you pick a random point in a unit square (a 1 × 1\\nsquare), it will have only about a 0.4% chance of being located less than\\n0.001 from a border (in other words, it is very unlikely that a random point\\nwill be “extreme” along any dimension). But in a 10,000-dimensional unit\\nhypercube, this probability is greater than 99.999999%. Most points in a\\nhigh-dimensional hypercube are very close to the border.\\u2060\\nHere is a more troublesome difference: if you pick two points randomly in a\\nunit square, the distance between these two points will be, on average,\\nroughly 0.52. If you pick two random points in a 3D unit cube, the average\\ndistance will be roughly 0.66. But what about two points picked randomly in\\na 1,000,000-dimensional unit hypercube? The average distance, believe it or\\nnot, will be about 408.25 (roughly 1,000,0006)! This is counterintuitive: how\\ncan two points be so far apart when they both lie within the same unit\\nhypercube? Well, there’s just plenty of space in high dimensions. As a result,\\nhigh-dimensional datasets are at risk of being very sparse: most training\\n1\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 375}, page_content='instances are likely to be far away from each other. This also means that a\\nnew instance will likely be far away from any training instance, making\\npredictions much less reliable than in lower dimensions, since they will be\\nbased on much larger extrapolations. In short, the more dimensions the\\ntraining set has, the greater the risk of overfitting it.\\nIn theory, one solution to the curse of dimensionality could be to increase the\\nsize of the training set to reach a sufficient density of training instances.\\nUnfortunately, in practice, the number of training instances required to reach\\na given density grows exponentially with the number of dimensions. With\\njust 100 features—significantly fewer than in the MNIST problem—all\\nranging from 0 to 1, you would need more training instances than atoms in\\nthe observable universe in order for training instances to be within 0.1 of\\neach other on average, assuming they were spread out uniformly across all\\ndimensions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 376}, page_content='Main Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a\\nlook at the two main approaches to reducing dimensionality: projection and\\nmanifold learning.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 377}, page_content='Projection\\nIn most real-world problems, training instances are not spread out uniformly\\nacross all dimensions. Many features are almost constant, while others are\\nhighly correlated (as discussed earlier for MNIST). As a result, all training\\ninstances lie within (or close to) a much lower-dimensional subspace of the\\nhigh-dimensional space. This sounds very abstract, so let’s look at an\\nexample. In Figure 8-2 you can see a 3D dataset represented by small\\nspheres.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 378}, page_content='dimensional (2D) subspace of the higher-dimensional (3D) space. If we\\nproject every training instance perpendicularly onto this subspace (as\\nrepresented by the short dashed lines connecting the instances to the plane),\\nwe get the new 2D dataset shown in Figure 8-3. Ta-da! We have just reduced\\nthe dataset’s dimensionality from 3D to 2D. Note that the axes correspond to\\nnew features z  and z : they are the coordinates of the projections on the\\nplane.\\nFigure 8-3. The new 2D dataset after projection\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 379}, page_content='Manifold Learning\\nHowever, projection is not always the best approach to dimensionality\\nreduction. In many cases the subspace may twist and turn, such as in the\\nfamous Swiss roll toy dataset represented in Figure 8-4.\\nFigure 8-4. Swiss roll dataset\\nSimply projecting onto a plane (e.g., by dropping x ) would squash different\\nlayers of the Swiss roll together, as shown on the left side of Figure 8-5.\\nWhat you probably want instead is to unroll the Swiss roll to obtain the 2D\\ndataset on the right side of Figure 8-5.\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 380}, page_content='Figure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)\\nThe Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is\\na 2D shape that can be bent and twisted in a higher-dimensional space. More\\ngenerally, a d-dimensional manifold is a part of an n-dimensional space\\n(where d < n) that locally resembles a d-dimensional hyperplane. In the case\\nof the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is\\nrolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold\\non which the training instances lie; this is called manifold learning. It relies\\non the manifold assumption, also called the manifold hypothesis, which holds\\nthat most real-world high-dimensional datasets lie close to a much lower-\\ndimensional manifold. This assumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images\\nhave some similarities. They are made of connected lines, the borders are\\nwhite, and they are more or less centered. If you randomly generated images,\\nonly a ridiculously tiny fraction of them would look like handwritten digits.\\nIn other words, the degrees of freedom available to you if you try to create a\\ndigit image are dramatically lower than the degrees of freedom you have if\\nyou are allowed to generate any image you want. These constraints tend to\\nsqueeze the dataset into a lower-dimensional manifold.\\nThe manifold assumption is often accompanied by another implicit\\nassumption: that the task at hand (e.g., classification or regression) will be\\nsimpler if expressed in the lower-dimensional space of the manifold. For'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 381}, page_content='example, in the top row of Figure 8-6 the Swiss roll is split into two classes:\\nin the 3D space (on the left) the decision boundary would be fairly complex,\\nbut in the 2D unrolled manifold space (on the right) the decision boundary is\\na straight line.\\nHowever, this implicit assumption does not always hold. For example, in the\\nbottom row of Figure 8-6, the decision boundary is located at x  = 5. This\\ndecision boundary looks very simple in the original 3D space (a vertical\\nplane), but it looks more complex in the unrolled manifold (a collection of\\nfour independent line segments).\\nIn short, reducing the dimensionality of your training set before training a\\nmodel will usually speed up training, but it may not always lead to a better or\\nsimpler solution; it all depends on the dataset.\\nHopefully you now have a good sense of what the curse of dimensionality is\\nand how dimensionality reduction algorithms can fight it, especially when the\\nmanifold assumption holds. The rest of this chapter will go through some of\\nthe most popular algorithms for dimensionality reduction.\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 382}, page_content='Figure 8-6. The decision boundary may not always be simpler with lower dimensions'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 383}, page_content='PCA\\nPrincipal component analysis (PCA) is by far the most popular\\ndimensionality reduction algorithm. First it identifies the hyperplane that lies\\nclosest to the data, and then it projects the data onto it, just like in Figure 8-2.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 384}, page_content='Preserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane,\\nyou first need to choose the right hyperplane. For example, a simple 2D\\ndataset is represented on the left in Figure 8-7, along with three different axes\\n(i.e., 1D hyperplanes). On the right is the result of the projection of the\\ndataset onto each of these axes. As you can see, the projection onto the solid\\nline preserves the maximum variance (top), while the projection onto the\\ndotted line preserves very little variance (bottom) and the projection onto the\\ndashed line preserves an intermediate amount of variance (middle).\\nFigure 8-7. Selecting the subspace on which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of\\nvariance, as it will most likely lose less information than the other\\nprojections. Another way to justify this choice is that it is the axis that\\nminimizes the mean squared distance between the original dataset and its\\nprojection onto that axis. This is the rather simple idea behind PCA.\\u2060\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 385}, page_content='Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the\\ntraining set. In Figure 8-7, it is the solid line. It also finds a second axis,\\northogonal to the first one, that accounts for the largest amount of the\\nremaining variance. In this 2D example there is no choice: it is the dotted\\nline. If it were a higher-dimensional dataset, PCA would also find a third\\naxis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as\\nmany axes as the number of dimensions in the dataset.\\nThe i  axis is called the i  principal component (PC) of the data. In Figure 8-\\n7, the first PC is the axis on which vector c  lies, and the second PC is the\\naxis on which vector c  lies. In Figure 8-2 the first two PCs are on the\\nprojection plane, and the third PC is the axis orthogonal to that plane. After\\nthe projection, in Figure 8-3, the first PC corresponds to the z  axis, and the\\nsecond PC corresponds to the z  axis.\\nNOTE\\nFor each principal component, PCA finds a zero-centered unit vector pointing in the\\ndirection of the PC. Since two opposing unit vectors lie on the same axis, the direction of\\nthe unit vectors returned by PCA is not stable: if you perturb the training set slightly and\\nrun PCA again, the unit vectors may point in the opposite direction as the original vectors.\\nHowever, they will generally still lie on the same axes. In some cases, a pair of unit\\nvectors may even rotate or swap (if the variances along these two axes are very close), but\\nthe plane they define will generally remain the same.\\nSo how can you find the principal components of a training set? Luckily,\\nthere is a standard matrix factorization technique called singular value\\ndecomposition (SVD) that can decompose the training set matrix X into the\\nmatrix multiplication of three matrices U Σ V , where V contains the unit\\nvectors that define all the principal components that you are looking for, as\\nshown in Equation 8-1.\\nEquation 8-1. Principal components matrix\\nth\\nth\\n1\\n2\\n1\\n2\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 386}, page_content='V = ∣ ∣ ∣ c 1 c 2 ⋯ c n ∣ ∣ ∣\\nThe following Python code uses NumPy’s svd() function to obtain all the\\nprincipal components of the 3D training set represented in Figure 8-2, then it\\nextracts the two unit vectors that define the first two PCs:\\nimport numpy as np\\nX = [...]  # create a small 3D dataset\\nX_centered = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered)\\nc1 = Vt[0]\\nc2 = Vt[1]\\nWARNING\\nPCA assumes that the dataset is centered around the origin. As you will see, Scikit-Learn’s\\nPCA classes take care of centering the data for you. If you implement PCA yourself (as in\\nthe preceding example), or if you use other libraries, don’t forget to center the data first.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 387}, page_content='Projecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the\\ndimensionality of the dataset down to d dimensions by projecting it onto the\\nhyperplane defined by the first d principal components. Selecting this\\nhyperplane ensures that the projection will preserve as much variance as\\npossible. For example, in Figure 8-2 the 3D dataset is projected down to the\\n2D plane defined by the first two principal components, preserving a large\\npart of the dataset’s variance. As a result, the 2D projection looks very much\\nlike the original 3D dataset.\\nTo project the training set onto the hyperplane and obtain a reduced dataset\\nX\\n of dimensionality d, compute the matrix multiplication of the training\\nset matrix X by the matrix W , defined as the matrix containing the first d\\ncolumns of V, as shown in Equation 8-2.\\nEquation 8-2. Projecting the training set down to d dimensions\\nX d-proj = X W d\\nThe following Python code projects the training set onto the plane defined by\\nthe first two principal components:\\nW2 = Vt[:2].T\\nX2D = X_centered @ W2\\nThere you have it! You now know how to reduce the dimensionality of any\\ndataset by projecting it down to any number of dimensions, while preserving\\nas much variance as possible.\\nd-proj\\nd'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 388}, page_content='Using Scikit-Learn\\nScikit-Learn’s PCA class uses SVD to implement PCA, just like we did\\nearlier in this chapter. The following code applies PCA to reduce the\\ndimensionality of the dataset down to two dimensions (note that it\\nautomatically takes care of centering the data):\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=2)\\nX2D = pca.fit_transform(X)\\nAfter fitting the PCA transformer to the dataset, its components_ attribute\\nholds the transpose of W : it contains one row for each of the first d principal\\ncomponents.\\nd'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 389}, page_content='Explained Variance Ratio\\nAnother useful piece of information is the explained variance ratio of each\\nprincipal component, available via the explained_variance_ratio_ variable.\\nThe ratio indicates the proportion of the dataset’s variance that lies along\\neach principal component. For example, let’s look at the explained variance\\nratios of the first two components of the 3D dataset represented in Figure 8-2:\\n>>> pca.explained_variance_ratio_\\narray([0.7578477 , 0.15186921])\\nThis output tells us that about 76% of the dataset’s variance lies along the\\nfirst PC, and about 15% lies along the second PC. This leaves about 9% for\\nthe third PC, so it is reasonable to assume that the third PC probably carries\\nlittle information.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 390}, page_content=\"Choosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to,\\nit is simpler to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance—say, 95% (An exception to this rule, of course,\\nis if you are reducing dimensionality for data visualization, in which case you\\nwill want to reduce the dimensionality down to 2 or 3).\\nThe following code loads and splits the MNIST dataset (introduced in\\nChapter 3) and performs PCA without reducing dimensionality, then\\ncomputes the minimum number of dimensions required to preserve 95% of\\nthe training set’s variance:\\nfrom sklearn.datasets import fetch_openml\\nmnist = fetch_openml('mnist_784', as_frame=False)\\nX_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\\nX_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_)\\nd = np.argmax(cumsum >= 0.95) + 1  # d equals 154\\nYou could then set n_components=d and run PCA again, but there’s a better\\noption. Instead of specifying the number of principal components you want to\\npreserve, you can set n_components to be a float between 0.0 and 1.0,\\nindicating the ratio of variance you wish to preserve:\\npca = PCA(n_components=0.95)\\nX_reduced = pca.fit_transform(X_train)\\nThe actual number of components is determined during training, and it is\\nstored in the n_components_ attribute:\\n>>> pca.n_components_\\n154\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 391}, page_content='Yet another option is to plot the explained variance as a function of the\\nnumber of dimensions (simply plot cumsum; see Figure 8-8). There will\\nusually be an elbow in the curve, where the explained variance stops growing\\nfast. In this case, you can see that reducing the dimensionality down to about\\n100 dimensions wouldn’t lose too much explained variance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nLastly, if you are using dimensionality reduction as a preprocessing step for a\\nsupervised learning task (e.g., classification), then you can tune the number\\nof dimensions as you would any other hyperparameter (see Chapter 2). For\\nexample, the following code example creates a two-step pipeline, first\\nreducing dimensionality using PCA, then classifying using a random forest.\\nNext, it uses RandomizedSearchCV to find a good combination of\\nhyperparameters for both PCA and the random forest classifier. This example\\ndoes a quick search, tuning only 2 hyperparameters, training on just 1,000\\ninstances, and running for just 10 iterations, but feel free to do a more\\nthorough search if you have the time:\\nfrom sklearn.ensemble import RandomForestClassifier'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 392}, page_content='from sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.pipeline import make_pipeline\\nclf = make_pipeline(PCA(random_state=42),\\n                    RandomForestClassifier(random_state=42))\\nparam_distrib = {\\n    \"pca__n_components\": np.arange(10, 80),\\n    \"randomforestclassifier__n_estimators\": np.arange(50, 500)\\n}\\nrnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3,\\n                                random_state=42)\\nrnd_search.fit(X_train[:1000], y_train[:1000])\\nLet’s look at the best hyperparameters found:\\n>>> print(rnd_search.best_params_)\\n{\\'randomforestclassifier__n_estimators\\': 465, \\'pca__n_components\\': 23}\\nIt’s interesting to note how low the optimal number of components is: we\\nreduced a 784-dimensional dataset to just 23 dimensions! This is tied to the\\nfact that we used a random forest, which is a pretty powerful model. If we\\nused a linear model instead, such as an SGDClassifier, the search would find\\nthat we need to preserve more dimensions (about 70).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 393}, page_content='PCA for Compression\\nAfter dimensionality reduction, the training set takes up much less space. For\\nexample, after applying PCA to the MNIST dataset while preserving 95% of\\nits variance, we are left with 154 features, instead of the original 784 features.\\nSo the dataset is now less than 20% of its original size, and we only lost 5%\\nof its variance! This is a reasonable compression ratio, and it’s easy to see\\nhow such a size reduction would speed up a classification algorithm\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions\\nby applying the inverse transformation of the PCA projection. This won’t\\ngive you back the original data, since the projection lost a bit of information\\n(within the 5% variance that was dropped), but it will likely be close to the\\noriginal data. The mean squared distance between the original data and the\\nreconstructed data (compressed and then decompressed) is called the\\nreconstruction error.\\nThe inverse_transform() method lets us decompress the reduced MNIST\\ndataset back to 784 dimensions:\\nX_recovered = pca.inverse_transform(X_reduced)\\nFigure 8-9 shows a few digits from the original training set (on the left), and\\nthe corresponding digits after compression and decompression. You can see\\nthat there is a slight image quality loss, but the digits are still mostly intact.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 394}, page_content='Figure 8-9. MNIST compression that preserves 95% of the variance\\nThe equation for the inverse transformation is shown in Equation 8-3.\\nEquation 8-3. PCA inverse transformation, back to the original number of dimensions\\nX recovered = X d-proj W d ⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 395}, page_content='Randomized PCA\\nIf you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses\\na stochastic algorithm called randomized PCA that quickly finds an\\napproximation of the first d principal components. Its computational\\ncomplexity is O(m × d ) + O(d ), instead of O(m × n ) + O(n ) for the full\\nSVD approach, so it is dramatically faster than full SVD when d is much\\nsmaller than n:\\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\\nX_reduced = rnd_pca.fit_transform(X_train)\\nTIP\\nBy default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the\\nrandomized PCA algorithm if max(m, n) > 500 and n_components is an integer smaller\\nthan 80% of min(m, n), or else it uses the full SVD approach. So the preceding code would\\nuse the randomized PCA algorithm even if you removed the svd_solver=\"randomized\"\\nargument, since 154 < 0.8 × 784. If you want to force Scikit-Learn to use full SVD for a\\nslightly more precise result, you can set the svd_solver hyperparameter to \"full\".\\n2\\n3\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 396}, page_content='Incremental PCA\\nOne problem with the preceding implementations of PCA is that they require\\nthe whole training set to fit in memory in order for the algorithm to run.\\nFortunately, incremental PCA (IPCA) algorithms have been developed that\\nallow you to split the training set into mini-batches and feed these in one\\nmini-batch at a time. This is useful for large training sets and for applying\\nPCA online (i.e., on the fly, as new instances arrive).\\nThe following code splits the MNIST training set into 100 mini-batches\\n(using NumPy’s array_split() function) and feeds them to Scikit-Learn’s\\nIncrementalPCA class\\u2060\\n to reduce the dimensionality of the MNIST dataset\\ndown to 154 dimensions, just like before. Note that you must call the\\npartial_fit() method with each mini-batch, rather than the fit() method with\\nthe whole training set:\\nfrom sklearn.decomposition import IncrementalPCA\\nn_batches = 100\\ninc_pca = IncrementalPCA(n_components=154)\\nfor X_batch in np.array_split(X_train, n_batches):\\n    inc_pca.partial_fit(X_batch)\\nX_reduced = inc_pca.transform(X_train)\\nAlternatively, you can use NumPy’s memmap class, which allows you to\\nmanipulate a large array stored in a binary file on disk as if it were entirely in\\nmemory; the class loads only the data it needs in memory, when it needs it.\\nTo demonstrate this, let’s first create a memory-mapped (memmap) file and\\ncopy the MNIST training set to it, then call flush() to ensure that any data still\\nin the cache gets saved to disk. In real life, X_train would typically not fit in\\nmemory, so you would load it chunk by chunk and save each chunk to the\\nright part of the memmap array:\\nfilename = \"my_mnist.mmap\"\\nX_mmap = np.memmap(filename, dtype=\\'float32\\', mode=\\'write\\', shape=X_train.shape)\\nX_mmap[:] = X_train  # could be a loop instead, saving the data chunk by chunk\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 397}, page_content='X_mmap.flush()\\nNext, we can load the memmap file and use it like a regular NumPy array.\\nLet’s use the IncrementalPCA class to reduce its dimensionality. Since this\\nalgorithm uses only a small part of the array at any given time, memory usage\\nremains under control. This makes it possible to call the usual fit() method\\ninstead of partial_fit(), which is quite convenient:\\nX_mmap = np.memmap(filename, dtype=\"float32\", mode=\"readonly\").reshape(-1, 784)\\nbatch_size = X_mmap.shape[0] // n_batches\\ninc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\\ninc_pca.fit(X_mmap)\\nWARNING\\nOnly the raw binary data is saved to disk, so you need to specify the data type and shape\\nof the array when you load it. If you omit the shape, np.memmap() returns a 1D array.\\nFor very high-dimensional datasets, PCA can be too slow. As you saw\\nearlier, even if you use randomized PCA its computational complexity is still\\nO(m × d ) + O(d ), so the target number of dimensions d must not be too\\nlarge. If you are dealing with a dataset with tens of thousands of features or\\nmore (e.g., images), then training may become much too slow: in this case,\\nyou should consider using random projection instead.\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 398}, page_content='Random Projection\\nAs its name suggests, the random projection algorithm projects the data to a\\nlower-dimensional space using a random linear projection. This may sound\\ncrazy, but it turns out that such a random projection is actually very likely to\\npreserve distances fairly well, as was demonstrated mathematically by\\nWilliam B. Johnson and Joram Lindenstrauss in a famous lemma. So, two\\nsimilar instances will remain similar after the projection, and two very\\ndifferent instances will remain very different.\\nObviously, the more dimensions you drop, the more information is lost, and\\nthe more distances get distorted. So how can you choose the optimal number\\nof dimensions? Well, Johnson and Lindenstrauss came up with an equation\\nthat determines the minimum number of dimensions to preserve in order to\\nensure—with high probability—that distances won’t change by more than a\\ngiven tolerance. For example, if you have a dataset containing m = 5,000\\ninstances with n = 20,000 features each, and you don’t want the squared\\ndistance between any two instances to change by more than ε = 10%,  then\\nyou should project the data down to d dimensions, with d ≥ 4 log(m) / (½ ε² -\\n⅓ ε³), which is 7,300 dimensions. That’s quite a significant dimensionality\\nreduction! Notice that the equation does not use n, it only relies on m and ε.\\nThis equation is implemented by the johnson_lindenstrauss_min_dim()\\nfunction:\\n>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\\n>>> m, ε = 5_000, 0.1\\n>>> d = johnson_lindenstrauss_min_dim(m, eps=ε)\\n>>> d\\n7300\\nNow we can just generate a random matrix P of shape [d, n], where each item\\nis sampled randomly from a Gaussian distribution with mean 0 and variance\\n1 / d, and use it to project a dataset from n dimensions down to d:\\nn = 20_000\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 399}, page_content='np.random.seed(42)\\nP = np.random.randn(d, n) / np.sqrt(d)  # std dev = square root of variance\\nX = np.random.randn(m, n)  # generate a fake dataset\\nX_reduced = X @ P.T\\nThat’s all there is to it! It’s simple and efficient, and no training is required:\\nthe only thing the algorithm needs to create the random matrix is the dataset’s\\nshape. The data itself is not used at all.\\nScikit-Learn offers a GaussianRandomProjection class to do exactly what we\\njust did: when you call its fit() method, it uses\\njohnson_lindenstrauss_min_dim() to determine the output dimensionality,\\nthen it generates a random matrix, which it stores in the components_\\nattribute. Then when you call transform(), it uses this matrix to perform the\\nprojection. When creating the transformer, you can set eps if you want to\\ntweak ε (it defaults to 0.1), and n_components if you want to force a specific\\ntarget dimensionality d. The following code example gives the same result as\\nthe preceding code (you can also verify that gaussian_rnd_proj.components_\\nis equal to P):\\nfrom sklearn.random_projection import GaussianRandomProjection\\ngaussian_rnd_proj = GaussianRandomProjection(eps=ε, random_state=42)\\nX_reduced = gaussian_rnd_proj.fit_transform(X)  # same result as above\\nScikit-Learn also provides a second random projection transformer, known as\\nSparseRandomProjection. It determines the target dimensionality in the same\\nway, generates a random matrix of the same shape, and performs the\\nprojection identically. The main difference is that the random matrix is\\nsparse. This means it uses much less memory: about 25 MB instead of almost\\n1.2 GB in the preceding example! And it’s also much faster, both to generate\\nthe random matrix and to reduce dimensionality: about 50% faster in this\\ncase. Moreover, if the input is sparse, the transformation keeps it sparse\\n(unless you set dense_output=True). Lastly, it enjoys the same distance-\\npreserving property as the previous approach, and the quality of the\\ndimensionality reduction is comparable. In short, it’s usually preferable to'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 400}, page_content='use this transformer instead of the first one, especially for large or sparse\\ndatasets.\\nThe ratio r of nonzero items in the sparse random matrix is called its density.\\nBy default, it is equal to 1/n. With 20,000 features, this means that only 1 in\\n~141 cells in the random matrix is nonzero: that’s quite sparse! You can set\\nthe density hyperparameter to another value if you prefer. Each cell in the\\nsparse random matrix has a probability r of being nonzero, and each nonzero\\nvalue is either –v or +v (both equally likely), where v = 1/dr.\\nIf you want to perform the inverse transform, you first need to compute the\\npseudo-inverse of the components matrix using SciPy’s pinv() function, then\\nmultiply the reduced data by the transpose of the pseudo-inverse:\\ncomponents_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\\nX_recovered = X_reduced @ components_pinv.T\\nWARNING\\nComputing the pseudo-inverse may take a very long time if the components matrix is\\nlarge, as the computational complexity of pinv() is O(dn²) if d < n, or O(nd²) otherwise.\\nIn summary, random projection is a simple, fast, memory-efficient, and\\nsurprisingly powerful dimensionality reduction algorithm that you should\\nkeep in mind, especially when you deal with high-dimensional datasets.\\nNOTE\\nRandom projection is not always used to reduce the dimensionality of large datasets. For\\nexample, a 2017 paper\\u2060  by Sanjoy Dasgupta et al. showed that the brain of a fruit fly\\nimplements an analog of random projection to map dense low-dimensional olfactory\\ninputs to sparse high-dimensional binary outputs: for each odor, only a small fraction of\\nthe output neurons get activated, but similar odors activate many of the same neurons.\\nThis is similar to a well-known algorithm called locality sensitive hashing (LSH), which is\\ntypically used in search engines to group similar documents.\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 401}, page_content='LLE\\nLocally linear embedding (LLE)\\u2060  is a nonlinear dimensionality reduction\\n(NLDR) technique. It is a manifold learning technique that does not rely on\\nprojections, unlike PCA and random projection. In a nutshell, LLE works by\\nfirst measuring how each training instance linearly relates to its nearest\\nneighbors, and then looking for a low-dimensional representation of the\\ntraining set where these local relationships are best preserved (more details\\nshortly). This approach makes it particularly good at unrolling twisted\\nmanifolds, especially when there is not too much noise.\\nThe following code makes a Swiss roll, then uses Scikit-Learn’s\\nLocallyLinearEmbedding class to unroll it:\\nfrom sklearn.datasets import make_swiss_roll\\nfrom sklearn.manifold import LocallyLinearEmbedding\\nX_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\\nX_unrolled = lle.fit_transform(X_swiss)\\nThe variable t is a 1D NumPy array containing the position of each instance\\nalong the rolled axis of the Swiss roll. We don’t use it in this example, but it\\ncan be used as a target for a nonlinear regression task.\\nThe resulting 2D dataset is shown in Figure 8-10. As you can see, the Swiss\\nroll is completely unrolled, and the distances between instances are locally\\nwell preserved. However, distances are not preserved on a larger scale: the\\nunrolled Swiss roll should be a rectangle, not this kind of stretched and\\ntwisted band. Nevertheless, LLE did a pretty good job of modeling the\\nmanifold.\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 402}, page_content='Figure 8-10. Unrolled Swiss roll using LLE\\nHere’s how LLE works: for each training instance x , the algorithm\\nidentifies its k-nearest neighbors (in the preceding code k = 10), then tries to\\nreconstruct x  as a linear function of these neighbors. More specifically, it\\ntries to find the weights w  such that the squared distance between x  and ∑\\nj=1 m w i,j x (j) is as small as possible, assuming w  = 0 if x  is not one of\\nthe k-nearest neighbors of x . Thus the first step of LLE is the constrained\\noptimization problem described in Equation 8-4, where W is the weight\\nmatrix containing all the weights w . The second constraint simply\\nnormalizes the weights for each training instance x .\\nEquation 8-4. LLE step 1: linearly modeling local relationships\\nW ^ = argmin W ∑ i=1 m x (i) -∑ j=1 m w i,j x (j) 2 subject to w i,j = 0 if x\\n(j) is not one of the k n.n. of x (i) ∑ j=1 m w i,j = 1 for i = 1 , 2 , ⋯ , m\\nAfter this step, the weight matrix W ^ (containing the weights w^i,j) encodes\\nthe local linear relationships between the training instances. The second step\\nis to map the training instances into a d-dimensional space (where d < n)\\nwhile preserving these local relationships as much as possible. If z  is the\\nimage of x  in this d-dimensional space, then we want the squared distance\\nbetween z  and ∑ j=1 m w ^ i,j z (j) to be as small as possible. This idea\\n(i)\\n(i)\\ni,j\\n(i)\\ni,j\\n(j)\\n(i)\\ni,j\\n(i)\\n(i)\\n(i)\\n(i)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 403}, page_content='leads to the unconstrained optimization problem described in Equation 8-5. It\\nlooks very similar to the first step, but instead of keeping the instances fixed\\nand finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the\\nlow-dimensional space. Note that Z is the matrix containing all z .\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ ^ = argmin Z ∑ i=1 m z (i) -∑ j=1 m w ^ i,j z (j) 2\\nScikit-Learn’s LLE implementation has the following computational\\ncomplexity: O(m log(m)n log(k)) for finding the k-nearest neighbors, O(mnk )\\nfor optimizing the weights, and O(dm ) for constructing the low-dimensional\\nrepresentations. Unfortunately, the m  in the last term makes this algorithm\\nscale poorly to very large datasets.\\nAs you can see, LLE is quite different from the projection techniques, and it’s\\nsignificantly more complex, but it can also construct much better low-\\ndimensional representations, especially if the data is nonlinear.\\n(i)\\n3\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 404}, page_content='Other Dimensionality Reduction Techniques\\nBefore we conclude this chapter, let’s take a quick look at a few other\\npopular dimensionality reduction techniques available in Scikit-Learn:\\nsklearn.manifold.MDS\\nMultidimensional scaling (MDS) reduces dimensionality while trying to\\npreserve the distances between the instances. Random projection does\\nthat for high-dimensional data, but it doesn’t work well on low-\\ndimensional data.\\nsklearn.manifold.Isomap\\nIsomap creates a graph by connecting each instance to its nearest\\nneighbors, then reduces dimensionality while trying to preserve the\\ngeodesic distances between the instances. The geodesic distance between\\ntwo nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.\\nsklearn.manifold.TSNE\\nt-distributed stochastic neighbor embedding (t-SNE) reduces\\ndimensionality while trying to keep similar instances close and dissimilar\\ninstances apart. It is mostly used for visualization, in particular to\\nvisualize clusters of instances in high-dimensional space. For example, in\\nthe exercises at the end of this chapter you will use t-SNE to visualize a\\n2D map of the MNIST images.\\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis\\nLinear discriminant analysis (LDA) is a linear classification algorithm\\nthat, during training, learns the most discriminative axes between the\\nclasses. These axes can then be used to define a hyperplane onto which to\\nproject the data. The benefit of this approach is that the projection will\\nkeep classes as far apart as possible, so LDA is a good technique to'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 405}, page_content='reduce dimensionality before running another classification algorithm\\n(unless LDA alone is sufficient).\\nFigure 8-11 shows the results of MDS, Isomap, and t-SNE on the Swiss roll.\\nMDS manages to flatten the Swiss roll without losing its global curvature,\\nwhile Isomap drops it entirely. Depending on the downstream task,\\npreserving the large-scale structure may be good or bad. t-SNE does a\\nreasonable job of flattening the Swiss roll, preserving a bit of curvature, and\\nit also amplifies clusters, tearing the roll apart. Again, this might be good or\\nbad, depending on the downstream task.\\nFigure 8-11. Using various techniques to reduce the Swiss roll to 2D'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 406}, page_content='Exercises\\n1. What are the main motivations for reducing a dataset’s dimensionality?\\nWhat are the main drawbacks?\\n2. What is the curse of dimensionality?\\n3. Once a dataset’s dimensionality has been reduced, is it possible to\\nreverse the operation? If so, how? If not, why?\\n4. Can PCA be used to reduce the dimensionality of a highly nonlinear\\ndataset?\\n5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the\\nexplained variance ratio to 95%. How many dimensions will the\\nresulting dataset have?\\n6. In what cases would you use regular PCA, incremental PCA,\\nrandomized PCA, or random projection?\\n7. How can you evaluate the performance of a dimensionality reduction\\nalgorithm on your dataset?\\n8. Does it make any sense to chain two different dimensionality reduction\\nalgorithms?\\n9. Load the MNIST dataset (introduced in Chapter 3) and split it into a\\ntraining set and a test set (take the first 60,000 instances for training, and\\nthe remaining 10,000 for testing). Train a random forest classifier on the\\ndataset and time how long it takes, then evaluate the resulting model on\\nthe test set. Next, use PCA to reduce the dataset’s dimensionality, with\\nan explained variance ratio of 95%. Train a new random forest classifier\\non the reduced dataset and see how long it takes. Was training much\\nfaster? Next, evaluate the classifier on the test set. How does it compare\\nto the previous classifier? Try again with an SGDClassifier. How much\\ndoes PCA help now?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 407}, page_content='10. Use t-SNE to reduce the first 5,000 images of the MNIST dataset down\\nto 2 dimensions and plot the result using Matplotlib. You can use a\\nscatterplot using 10 different colors to represent each image’s target\\nclass. Alternatively, you can replace each dot in the scatterplot with the\\ncorresponding instance’s class (a digit from 0 to 9), or even plot scaled-\\ndown versions of the digit images themselves (if you plot all digits the\\nvisualization will be too cluttered, so you should either draw a random\\nsample or plot an instance only if no other instance has already been\\nplotted at a close distance). You should get a nice visualization with\\nwell-separated clusters of digits. Try using other dimensionality\\nreduction algorithms, such as PCA, LLE, or MDS, and compare the\\nresulting visualizations.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  Well, four dimensions if you count time, and a few more if you are a string theorist.\\n2  Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia\\nuser NerdBoy1392 (Creative Commons BY-SA 3.0). Reproduced from\\nhttps://en.wikipedia.org/wiki/Tesseract.\\n3  Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much\\nsugar they put in their coffee), if you consider enough dimensions.\\n4  Karl Pearson, “On Lines and Planes of Closest Fit to Systems of Points in Space”, The London,\\nEdinburgh, and Dublin Philosophical Magazine and Journal of Science 2, no. 11 (1901): 559–\\n572.\\n5  Scikit-Learn uses the algorithm described in David A. Ross et al., “Incremental Learning for\\nRobust Visual Tracking”, International Journal of Computer Vision 77, no. 1–3 (2008): 125–\\n141.\\n6  ε is the Greek letter epsilon, often used for tiny values.\\n7  Sanjoy Dasgupta et al., “A neural algorithm for a fundamental computing problem”, Science\\n358, no. 6364 (2017): 793–796.\\n8  Sam T. Roweis and Lawrence K. Saul, “Nonlinear Dimensionality Reduction by Locally Linear\\nEmbedding”, Science 290, no. 5500 (2000): 2323–2326.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 408}, page_content='Chapter 9. Unsupervised Learning\\nTechniques\\nAlthough most of the applications of machine learning today are based on\\nsupervised learning (and as a result, this is where most of the investments go\\nto), the vast majority of the available data is unlabeled: we have the input\\nfeatures X, but we do not have the labels y. The computer scientist Yann\\nLeCun famously said that “if intelligence was a cake, unsupervised learning\\nwould be the cake, supervised learning would be the icing on the cake, and\\nreinforcement learning would be the cherry on the cake.” In other words,\\nthere is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nSay you want to create a system that will take a few pictures of each item on\\na manufacturing production line and detect which items are defective. You\\ncan fairly easily create a system that will take pictures automatically, and this\\nmight give you thousands of pictures every day. You can then build a\\nreasonably large dataset in just a few weeks. But wait, there are no labels! If\\nyou want to train a regular binary classifier that will predict whether an item\\nis defective or not, you will need to label every single picture as “defective”\\nor “normal”. This will generally require human experts to sit down and\\nmanually go through all the pictures. This is a long, costly, and tedious task,\\nso it will usually only be done on a small subset of the available pictures. As\\na result, the labeled dataset will be quite small, and the classifier’s\\nperformance will be disappointing. Moreover, every time the company makes\\nany change to its products, the whole process will need to be started over\\nfrom scratch. Wouldn’t it be great if the algorithm could just exploit the\\nunlabeled data without needing humans to label every picture? Enter\\nunsupervised learning.\\nIn Chapter 8 we looked at the most common unsupervised learning task:\\ndimensionality reduction. In this chapter we will look at a few more'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 409}, page_content='unsupervised tasks:\\nClustering\\nThe goal is to group similar instances together into clusters. Clustering is\\na great tool for data analysis, customer segmentation, recommender\\nsystems, search engines, image segmentation, semi-supervised learning,\\ndimensionality reduction, and more.\\nAnomaly detection (also called outlier detection)\\nThe objective is to learn what “normal” data looks like, and then use that\\nto detect abnormal instances. These instances are called anomalies, or\\noutliers, while the normal instances are called inliers. Anomaly detection\\nis useful in a wide variety of applications, such as fraud detection,\\ndetecting defective products in manufacturing, identifying new trends in\\ntime series, or removing outliers from a dataset before training another\\nmodel, which can significantly improve the performance of the resulting\\nmodel.\\nDensity estimation\\nThis is the task of estimating the probability density function (PDF) of the\\nrandom process that generated the dataset. Density estimation is\\ncommonly used for anomaly detection: instances located in very low-\\ndensity regions are likely to be anomalies. It is also useful for data\\nanalysis and visualization.\\nReady for some cake? We will start with two clustering algorithms, k-means\\nand DBSCAN, then we’ll discuss Gaussian mixture models and see how they\\ncan be used for density estimation, clustering, and anomaly detection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 410}, page_content='Clustering Algorithms: k-means and DBSCAN\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have\\nnever seen before. You look around and you notice a few more. They are not\\nidentical, yet they are sufficiently similar for you to know that they most\\nlikely belong to the same species (or at least the same genus). You may need\\na botanist to tell you what species that is, but you certainly don’t need an\\nexpert to identify groups of similar-looking objects. This is called clustering:\\nit is the task of identifying similar instances and assigning them to clusters, or\\ngroups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However,\\nunlike classification, clustering is an unsupervised task. Consider Figure 9-1:\\non the left is the iris dataset (introduced in Chapter 4), where each instance’s\\nspecies (i.e., its class) is represented with a different marker. It is a labeled\\ndataset, for which classification algorithms such as logistic regression,\\nSVMs, or random forest classifiers are well suited. On the right is the same\\ndataset, but without the labels, so you cannot use a classification algorithm\\nanymore. This is where clustering algorithms step in: many of them can\\neasily detect the lower-left cluster. It is also quite easy to see with our own\\neyes, but it is not so obvious that the upper-right cluster is composed of two\\ndistinct subclusters. That said, the dataset has two additional features (sepal\\nlength and width) that are not represented here, and clustering algorithms can\\nmake good use of all features, so in fact they identify the three clusters fairly\\nwell (e.g., using a Gaussian mixture model, only 5 instances out of 150 are\\nassigned to the wrong cluster).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 411}, page_content='Figure 9-1. Classification (left) versus clustering (right)\\nClustering is used in a wide variety of applications, including:\\nCustomer segmentation\\nYou can cluster your customers based on their purchases and their\\nactivity on your website. This is useful to understand who your customers\\nare and what they need, so you can adapt your products and marketing\\ncampaigns to each segment. For example, customer segmentation can be\\nuseful in recommender systems to suggest content that other users in the\\nsame cluster enjoyed.\\nData analysis\\nWhen you analyze a new dataset, it can be helpful to run a clustering\\nalgorithm, and then analyze each cluster separately.\\nDimensionality reduction\\nOnce a dataset has been clustered, it is usually possible to measure each\\ninstance’s affinity with each cluster; affinity is any measure of how well\\nan instance fits into a cluster. Each instance’s feature vector x can then be\\nreplaced with the vector of its cluster affinities. If there are k clusters,\\nthen this vector is k-dimensional. The new vector is typically much\\nlower-dimensional than the original feature vector, but it can preserve\\nenough information for further processing.\\nFeature engineering'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 412}, page_content='The cluster affinities can often be useful as extra features. For example,\\nwe used k-means in Chapter 2 to add geographic cluster affinity features\\nto the California housing dataset, and they helped us get better\\nperformance.\\nAnomaly detection (also called outlier detection)\\nAny instance that has a low affinity to all the clusters is likely to be an\\nanomaly. For example, if you have clustered the users of your website\\nbased on their behavior, you can detect users with unusual behavior, such\\nas an unusual number of requests per second.\\nSemi-supervised learning\\nIf you only have a few labels, you could perform clustering and propagate\\nthe labels to all the instances in the same cluster. This technique can\\ngreatly increase the number of labels available for a subsequent\\nsupervised learning algorithm, and thus improve its performance.\\nSearch engines\\nSome search engines let you search for images that are similar to a\\nreference image. To build such a system, you would first apply a\\nclustering algorithm to all the images in your database; similar images\\nwould end up in the same cluster. Then when a user provides a reference\\nimage, all you’d need to do is use the trained clustering model to find this\\nimage’s cluster, and you could then simply return all the images from this\\ncluster.\\nImage segmentation\\nBy clustering pixels according to their color, then replacing each pixel’s\\ncolor with the mean color of its cluster, it is possible to considerably\\nreduce the number of different colors in an image. Image segmentation is\\nused in many object detection and tracking systems, as it makes it easier\\nto detect the contour of each object.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 413}, page_content='There is no universal definition of what a cluster is: it really depends on the\\ncontext, and different algorithms will capture different kinds of clusters.\\nSome algorithms look for instances centered around a particular point, called\\na centroid. Others look for continuous regions of densely packed instances:\\nthese clusters can take on any shape. Some algorithms are hierarchical,\\nlooking for clusters of clusters. And the list goes on.\\nIn this section, we will look at two popular clustering algorithms, k-means\\nand DBSCAN, and explore some of their applications, such as nonlinear\\ndimensionality reduction, semi-supervised learning, and anomaly detection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 414}, page_content=\"k-means\\nConsider the unlabeled dataset represented in Figure 9-2: you can clearly see\\nfive blobs of instances. The k-means algorithm is a simple algorithm capable\\nof clustering this kind of dataset very quickly and efficiently, often in just a\\nfew iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957 as a\\ntechnique for pulse-code modulation, but it was only published outside of the\\ncompany in 1982.\\u2060\\n In 1965, Edward W. Forgy had published virtually the\\nsame algorithm, so k-means is sometimes referred to as the Lloyd–Forgy\\nalgorithm.\\nFigure 9-2. An unlabeled dataset composed of five blobs of instances\\nLet’s train a k-means clusterer on this dataset. It will try to find each blob’s\\ncenter and assign each instance to the closest blob:\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import make_blobs\\nX, y = make_blobs([...])  # make the blobs: y contains the cluster IDs, but we\\n                          # will not use them; that's what we want to predict\\nk = 5\\nkmeans = KMeans(n_clusters=k, random_state=42)\\ny_pred = kmeans.fit_predict(X)\\n1\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 415}, page_content='Note that you have to specify the number of clusters k that the algorithm must\\nfind. In this example, it is pretty obvious from looking at the data that k\\nshould be set to 5, but in general it is not that easy. We will discuss this\\nshortly.\\nEach instance will be assigned to one of the five clusters. In the context of\\nclustering, an instance’s label is the index of the cluster to which the\\nalgorithm assigns this instance; this is not to be confused with the class labels\\nin classification, which are used as targets (remember that clustering is an\\nunsupervised learning task). The KMeans instance preserves the predicted\\nlabels of the instances it was trained on, available via the labels_ instance\\nvariable:\\n>>> y_pred\\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\\n>>> y_pred is kmeans.labels_\\nTrue\\nWe can also take a look at the five centroids that the algorithm found:\\n>>> kmeans.cluster_centers_\\narray([[-2.80389616,  1.80117999],\\n       [ 0.20876306,  2.25551336],\\n       [-2.79290307,  2.79641063],\\n       [-1.46679593,  2.28585348],\\n       [-2.80037642,  1.30082566]])\\nYou can easily assign new instances to the cluster whose centroid is closest:\\n>>> import numpy as np\\n>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\\n>>> kmeans.predict(X_new)\\narray([1, 1, 2, 2], dtype=int32)\\nIf you plot the cluster’s decision boundaries, you get a Voronoi tessellation:\\nsee Figure 9-3, where each centroid is represented with an X.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 416}, page_content='Figure 9-3. k-means decision boundaries (Voronoi tessellation)\\nThe vast majority of the instances were clearly assigned to the appropriate\\ncluster, but a few instances were probably mislabeled, especially near the\\nboundary between the top-left cluster and the central cluster. Indeed, the k-\\nmeans algorithm does not behave very well when the blobs have very\\ndifferent diameters because all it cares about when assigning an instance to a\\ncluster is the distance to the centroid.\\nInstead of assigning each instance to a single cluster, which is called hard\\nclustering, it can be useful to give each instance a score per cluster, which is\\ncalled soft clustering. The score can be the distance between the instance and\\nthe centroid or a similarity score (or affinity), such as the Gaussian radial\\nbasis function we used in Chapter 2. In the KMeans class, the transform()\\nmethod measures the distance from each instance to every centroid:\\n>>> kmeans.transform(X_new).round(2)\\narray([[2.81, 0.33, 2.9 , 1.49, 2.89],\\n       [5.81, 2.8 , 5.85, 4.48, 5.84],\\n       [1.21, 3.29, 0.29, 1.69, 1.71],\\n       [0.73, 3.22, 0.36, 1.55, 1.22]])\\nIn this example, the first instance in X_new is located at a distance of about\\n2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the\\nthird centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 417}, page_content='If you have a high-dimensional dataset and you transform it this way, you\\nend up with a k-dimensional dataset: this transformation can be a very\\nefficient nonlinear dimensionality reduction technique. Alternatively, you can\\nuse these distances as extra features to train another model, as in Chapter 2.\\nThe k-means algorithm\\nSo, how does the algorithm work? Well, suppose you were given the\\ncentroids. You could easily label all the instances in the dataset by assigning\\neach of them to the cluster whose centroid is closest. Conversely, if you were\\ngiven all the instance labels, you could easily locate each cluster’s centroid\\nby computing the mean of the instances in that cluster. But you are given\\nneither the labels nor the centroids, so how can you proceed? Start by placing\\nthe centroids randomly (e.g., by picking k instances at random from the\\ndataset and using their locations as centroids). Then label the instances,\\nupdate the centroids, label the instances, update the centroids, and so on until\\nthe centroids stop moving. The algorithm is guaranteed to converge in a finite\\nnumber of steps (usually quite small). That’s because the mean squared\\ndistance between the instances and their closest centroids can only go down\\nat each step, and since it cannot be negative, it’s guaranteed to converge.\\nYou can see the algorithm in action in Figure 9-4: the centroids are initialized\\nrandomly (top left), then the instances are labeled (top right), then the\\ncentroids are updated (center left), the instances are relabeled (center right),\\nand so on. As you can see, in just three iterations the algorithm has reached a\\nclustering that seems close to optimal.\\nNOTE\\nThe computational complexity of the algorithm is generally linear with regard to the\\nnumber of instances m, the number of clusters k, and the number of dimensions n.\\nHowever, this is only true when the data has a clustering structure. If it does not, then in\\nthe worst-case scenario the complexity can increase exponentially with the number of\\ninstances. In practice, this rarely happens, and k-means is generally one of the fastest\\nclustering algorithms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 418}, page_content='Figure 9-4. The k-means algorithm\\nAlthough the algorithm is guaranteed to converge, it may not converge to the\\nright solution (i.e., it may converge to a local optimum): whether it does or\\nnot depends on the centroid initialization. Figure 9-5 shows two suboptimal\\nsolutions that the algorithm can converge to if you are not lucky with the\\nrandom initialization step.\\nFigure 9-5. Suboptimal solutions due to unlucky centroid initializations'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 419}, page_content='Let’s take a look at a few ways you can mitigate this risk by improving the\\ncentroid initialization.\\nCentroid initialization methods\\nIf you happen to know approximately where the centroids should be (e.g., if\\nyou ran another clustering algorithm earlier), then you can set the init\\nhyperparameter to a NumPy array containing the list of centroids, and set\\nn_init to 1:\\ngood_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\\nkmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\\nkmeans.fit(X)\\nAnother solution is to run the algorithm multiple times with different random\\ninitializations and keep the best solution. The number of random\\ninitializations is controlled by the n_init hyperparameter: by default it is equal\\nto 10, which means that the whole algorithm described earlier runs 10 times\\nwhen you call fit(), and Scikit-Learn keeps the best solution. But how exactly\\ndoes it know which solution is the best? It uses a performance metric! That\\nmetric is called the model’s inertia, which is the sum of the squared distances\\nbetween the instances and their closest centroids. It is roughly equal to 219.4\\nfor the model on the left in Figure 9-5, 258.6 for the model on the right in\\nFigure 9-5, and only 211.6 for the model in Figure 9-3. The KMeans class\\nruns the algorithm n_init times and keeps the model with the lowest inertia.\\nIn this example, the model in Figure 9-3 will be selected (unless we are very\\nunlucky with n_init consecutive random initializations). If you are curious, a\\nmodel’s inertia is accessible via the inertia_ instance variable:\\n>>> kmeans.inertia_\\n211.59853725816836\\nThe score() method returns the negative inertia (it’s negative because a\\npredictor’s score() method must always respect Scikit-Learn’s “greater is\\nbetter” rule: if a predictor is better than another, its score() method should\\nreturn a greater score):'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 420}, page_content='>>> kmeans.score(X)\\n-211.5985372581684\\nAn important improvement to the k-means algorithm, k-means++, was\\nproposed in a 2006 paper by David Arthur and Sergei Vassilvitskii.\\u2060\\n They\\nintroduced a smarter initialization step that tends to select centroids that are\\ndistant from one another, and this improvement makes the k-means algorithm\\nmuch less likely to converge to a suboptimal solution. The paper showed that\\nthe additional computation required for the smarter initialization step is well\\nworth it because it makes it possible to drastically reduce the number of times\\nthe algorithm needs to be run to find the optimal solution. The k-means++\\ninitialization algorithm works like this:\\n1. Take one centroid c\\n, chosen uniformly at random from the dataset.\\n2. Take a new centroid c , choosing an instance x  with probability\\nDx(i)2 / ∑j=1mDx(j)2, where D(x ) is the distance between the\\ninstance x  and the closest centroid that was already chosen. This\\nprobability distribution ensures that instances farther away from already\\nchosen centroids are much more likely to be selected as centroids.\\n3. Repeat the previous step until all k centroids have been chosen.\\nThe KMeans class uses this initialization method by default.\\nAccelerated k-means and mini-batch k-means\\nAnother improvement to the k-means algorithm was proposed in a 2003\\npaper by Charles Elkan.\\u2060\\n On some large datasets with many clusters, the\\nalgorithm can be accelerated by avoiding many unnecessary distance\\ncalculations. Elkan achieved this by exploiting the triangle inequality (i.e.,\\nthat a straight line is always the shortest distance between two points\\u2060\\n) and\\nby keeping track of lower and upper bounds for distances between instances\\nand centroids. However, Elkan’s algorithm does not always accelerate\\ntraining, and sometimes it can even slow down training significantly; it\\ndepends on the dataset. Still, if you want to give it a try, set\\nalgorithm=\"elkan\".\\n2\\n(1)\\n(i)\\n(i)\\n(i)\\n(i)\\n3\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 421}, page_content='Yet another important variant of the k-means algorithm was proposed in a\\n2010 paper by David Sculley.\\u2060\\n Instead of using the full dataset at each\\niteration, the algorithm is capable of using mini-batches, moving the\\ncentroids just slightly at each iteration. This speeds up the algorithm\\n(typically by a factor of three to four) and makes it possible to cluster huge\\ndatasets that do not fit in memory. Scikit-Learn implements this algorithm in\\nthe MiniBatchKMeans class, which you can use just like the KMeans class:\\nfrom sklearn.cluster import MiniBatchKMeans\\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\\nminibatch_kmeans.fit(X)\\nIf the dataset does not fit in memory, the simplest option is to use the\\nmemmap class, as we did for incremental PCA in Chapter 8. Alternatively,\\nyou can pass one mini-batch at a time to the partial_fit() method, but this will\\nrequire much more work, since you will need to perform multiple\\ninitializations and select the best one yourself.\\nAlthough the mini-batch k-means algorithm is much faster than the regular k-\\nmeans algorithm, its inertia is generally slightly worse. You can see this in\\nFigure 9-6: the plot on the left compares the inertias of mini-batch k-means\\nand regular k-means models trained on the previous five-blobs dataset using\\nvarious numbers of clusters k. The difference between the two curves is\\nsmall, but visible. In the plot on the right, you can see that mini-batch k-\\nmeans is roughly 3.5 times faster than regular k-means on this dataset.\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 422}, page_content='Figure 9-6. Mini-batch k-means has a higher inertia than k-means (left) but it is much faster (right),\\nespecially as k increases\\nFinding the optimal number of clusters\\nSo far, we’ve set the number of clusters k to 5 because it was obvious by\\nlooking at the data that this was the correct number of clusters. But in\\ngeneral, it won’t be so easy to know how to set k, and the result might be\\nquite bad if you set it to the wrong value. As you can see in Figure 9-7, for\\nthis dataset setting k to 3 or 8 results in fairly bad models.\\nYou might be thinking that you could just pick the model with the lowest\\ninertia. Unfortunately, it is not that simple. The inertia for k=3 is about 653.2,\\nwhich is much higher than for k=5 (211.6). But with k=8, the inertia is just\\n119.1. The inertia is not a good performance metric when trying to choose k\\nbecause it keeps getting lower as we increase k. Indeed, the more clusters\\nthere are, the closer each instance will be to its closest centroid, and therefore\\nthe lower the inertia will be. Let’s plot the inertia as a function of k. When we\\ndo this, the curve often contains an inflexion point called the elbow (see\\nFigure 9-8).\\nFigure 9-7. Bad choices for the number of clusters: when k is too small, separate clusters get merged\\n(left), and when k is too large, some clusters get chopped into multiple pieces (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 423}, page_content='Figure 9-8. Plotting the inertia as a function of the number of clusters k\\nAs you can see, the inertia drops very quickly as we increase k up to 4, but\\nthen it decreases much more slowly as we keep increasing k. This curve has\\nroughly the shape of an arm, and there is an elbow at k = 4. So, if we did not\\nknow better, we might think 4 was a good choice: any lower value would be\\ndramatic, while any higher value would not help much, and we might just be\\nsplitting perfectly good clusters in half for no good reason.\\nThis technique for choosing the best value for the number of clusters is rather\\ncoarse. A more precise (but also more computationally expensive) approach\\nis to use the silhouette score, which is the mean silhouette coefficient over all\\nthe instances. An instance’s silhouette coefficient is equal to (b – a) / max(a,\\nb), where a is the mean distance to the other instances in the same cluster\\n(i.e., the mean intra-cluster distance) and b is the mean nearest-cluster\\ndistance (i.e., the mean distance to the instances of the next closest cluster,\\ndefined as the one that minimizes b, excluding the instance’s own cluster).\\nThe silhouette coefficient can vary between –1 and +1. A coefficient close to\\n+1 means that the instance is well inside its own cluster and far from other\\nclusters, while a coefficient close to 0 means that it is close to a cluster\\nboundary; finally, a coefficient close to –1 means that the instance may have\\nbeen assigned to the wrong cluster.\\nTo compute the silhouette score, you can use Scikit-Learn’s\\nsilhouette_score() function, giving it all the instances in the dataset and the\\nlabels they were assigned:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 424}, page_content='>>> from sklearn.metrics import silhouette_score\\n>>> silhouette_score(X, kmeans.labels_)\\n0.655517642572828\\nLet’s compare the silhouette scores for different numbers of clusters (see\\nFigure 9-9).\\nFigure 9-9. Selecting the number of clusters k using the silhouette score\\nAs you can see, this visualization is much richer than the previous one:\\nalthough it confirms that k = 4 is a very good choice, it also highlights the\\nfact that k = 5 is quite good as well, and much better than k = 6 or 7. This was\\nnot visible when comparing inertias.\\nAn even more informative visualization is obtained when we plot every\\ninstance’s silhouette coefficient, sorted by the clusters they are assigned to\\nand by the value of the coefficient. This is called a silhouette diagram (see\\nFigure 9-10). Each diagram contains one knife shape per cluster. The shape’s\\nheight indicates the number of instances in the cluster, and its width\\nrepresents the sorted silhouette coefficients of the instances in the cluster\\n(wider is better).\\nThe vertical dashed lines represent the mean silhouette score for each number\\nof clusters. When most of the instances in a cluster have a lower coefficient\\nthan this score (i.e., if many of the instances stop short of the dashed line,\\nending to the left of it), then the cluster is rather bad since this means its\\ninstances are much too close to other clusters. Here we can see that when k =\\n3 or 6, we get bad clusters. But when k = 4 or 5, the clusters look pretty good:\\nmost instances extend beyond the dashed line, to the right and closer to 1.0.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 425}, page_content='When k = 4, the cluster at index 1 (the second from the bottom) is rather big.\\nWhen k = 5, all clusters have similar sizes. So, even though the overall\\nsilhouette score from k = 4 is slightly greater than for k = 5, it seems like a\\ngood idea to use k = 5 to get clusters of similar sizes.\\nFigure 9-10. Analyzing the silhouette diagrams for various values of k'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 426}, page_content='Limits of k-means\\nDespite its many merits, most notably being fast and scalable, k-means is not\\nperfect. As we saw, it is necessary to run the algorithm several times to avoid\\nsuboptimal solutions, plus you need to specify the number of clusters, which\\ncan be quite a hassle. Moreover, k-means does not behave very well when the\\nclusters have varying sizes, different densities, or nonspherical shapes. For\\nexample, Figure 9-11 shows how k-means clusters a dataset containing three\\nellipsoidal clusters of different dimensions, densities, and orientations.\\nAs you can see, neither of these solutions is any good. The solution on the\\nleft is better, but it still chops off 25% of the middle cluster and assigns it to\\nthe cluster on the right. The solution on the right is just terrible, even though\\nits inertia is lower. So, depending on the data, different clustering algorithms\\nmay perform better. On these types of elliptical clusters, Gaussian mixture\\nmodels work great.\\nFigure 9-11. k-means fails to cluster these ellipsoidal blobs properly\\nTIP\\nIt is important to scale the input features (see Chapter 2) before you run k-means, or the\\nclusters may be very stretched and k-means will perform poorly. Scaling the features does\\nnot guarantee that all the clusters will be nice and spherical, but it generally helps k-means.\\nNow let’s look at a few ways we can benefit from clustering. We will use k-\\nmeans, but feel free to experiment with other clustering algorithms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 427}, page_content='Using Clustering for Image Segmentation\\nImage segmentation is the task of partitioning an image into multiple\\nsegments. There are several variants:\\nIn color segmentation, pixels with a similar color get assigned to the\\nsame segment. This is sufficient in many applications. For example, if\\nyou want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nIn semantic segmentation, all pixels that are part of the same object type\\nget assigned to the same segment. For example, in a self-driving car’s\\nvision system, all pixels that are part of a pedestrian’s image might be\\nassigned to the “pedestrian” segment (there would be one segment\\ncontaining all the pedestrians).\\nIn instance segmentation, all pixels that are part of the same individual\\nobject are assigned to the same segment. In this case there would be a\\ndifferent segment for each pedestrian.\\nThe state of the art in semantic or instance segmentation today is achieved\\nusing complex architectures based on convolutional neural networks (see\\nChapter 14). In this chapter we are going to focus on the (much simpler)\\ncolor segmentation task, using k-means.\\nWe’ll start by importing the Pillow package (successor to the Python Imaging\\nLibrary, PIL), which we’ll then use to load the ladybug.png image (see the\\nupper-left image in Figure 9-12), assuming it’s located at filepath:\\n>>> import PIL\\n>>> image = np.asarray(PIL.Image.open(filepath))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array. The first dimension’s size is the\\nheight; the second is the width; and the third is the number of color channels,\\nin this case red, green, and blue (RGB). In other words, for each pixel there is'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 428}, page_content='a 3D vector containing the intensities of red, green, and blue as unsigned 8-\\nbit integers between 0 and 255. Some images may have fewer channels (such\\nas grayscale images, which only have one), and some images may have more\\nchannels (such as images with an additional alpha channel for transparency,\\nor satellite images, which often contain channels for additional light\\nfrequencies (like infrared).\\nThe following code reshapes the array to get a long list of RGB colors, then it\\nclusters these colors using k-means with eight clusters. It creates a\\nsegmented_img array containing the nearest cluster center for each pixel (i.e.,\\nthe mean color of each pixel’s cluster), and lastly it reshapes this array to the\\noriginal image shape. The third line uses advanced NumPy indexing; for\\nexample, if the first 10 labels in kmeans_.labels_ are equal to 1, then the first\\n10 colors in segmented_img are equal to kmeans.cluster_centers_[1]:\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters=8, random_state=42).fit(X)\\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\\nsegmented_img = segmented_img.reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12. You can\\nexperiment with various numbers of clusters, as shown in the figure. When\\nyou use fewer than eight clusters, notice that the ladybug’s flashy red color\\nfails to get a cluster of its own: it gets merged with colors from the\\nenvironment. This is because k-means prefers clusters of similar sizes. The\\nladybug is small—much smaller than the rest of the image—so even though\\nits color is flashy, k-means fails to dedicate a cluster to it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 429}, page_content='Figure 9-12. Image segmentation using k-means with various numbers of color clusters\\nThat wasn’t too hard, was it? Now let’s look at another application of\\nclustering.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 430}, page_content='Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have\\nplenty of unlabeled instances and very few labeled instances. In this section,\\nwe’ll use the digits dataset, which is a simple MNIST-like dataset containing\\n1,797 grayscale 8 × 8 images representing the digits 0 to 9. First, let’s load\\nand split the dataset (it’s already shuffled):\\nfrom sklearn.datasets import load_digits\\nX_digits, y_digits = load_digits(return_X_y=True)\\nX_train, y_train = X_digits[:1400], y_digits[:1400]\\nX_test, y_test = X_digits[1400:], y_digits[1400:]\\nWe will pretend we only have labels for 50 instances. To get a baseline\\nperformance, let’s train a logistic regression model on these 50 labeled\\ninstances:\\nfrom sklearn.linear_model import LogisticRegression\\nn_labeled = 50\\nlog_reg = LogisticRegression(max_iter=10_000)\\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\\nWe can then measure the accuracy of this model on the test set (note that the\\ntest set must be labeled):\\n>>> log_reg.score(X_test, y_test)\\n0.7481108312342569\\nThe model’s accuracy is just 74.8%. That’s not great: indeed, if you try\\ntraining the model on the full training set, you will find that it will reach\\nabout 90.7% accuracy. Let’s see how we can do better. First, let’s cluster the\\ntraining set into 50 clusters. Then, for each cluster, we’ll find the image\\nclosest to the centroid. We’ll call these images the representative images:\\nk = 50'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 431}, page_content='kmeans = KMeans(n_clusters=k, random_state=42)\\nX_digits_dist = kmeans.fit_transform(X_train)\\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\\nX_representative_digits = X_train[representative_digit_idx]\\nFigure 9-13 shows the 50 representative images.\\nFigure 9-13. Fifty representative digit images (one per cluster)\\nLet’s look at each image and manually label them:\\ny_representative_digits = np.array([1, 3, 6, 0, 7, 9, 2, ..., 5, 1, 9, 9, 3, 7])\\nNow we have a dataset with just 50 labeled instances, but instead of being\\nrandom instances, each of them is a representative image of its cluster. Let’s\\nsee if the performance is any better:\\n>>> log_reg = LogisticRegression(max_iter=10_000)\\n>>> log_reg.fit(X_representative_digits, y_representative_digits)\\n>>> log_reg.score(X_test, y_test)\\n0.8488664987405542\\nWow! We jumped from 74.8% accuracy to 84.9%, although we are still only\\ntraining the model on 50 instances. Since it is often costly and painful to label\\ninstances, especially when it has to be done manually by experts, it is a good\\nidea to label representative instances rather than just random instances.\\nBut perhaps we can go one step further: what if we propagated the labels to\\nall the other instances in the same cluster? This is called label propagation:\\ny_train_propagated = np.empty(len(X_train), dtype=np.int64)\\nfor i in range(k):\\n    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 432}, page_content='Now let’s train the model again and look at its performance:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_train, y_train_propagated)\\n>>> log_reg.score(X_test, y_test)\\n0.8942065491183879\\nWe got another significant accuracy boost! Let’s see if we can do even better\\nby ignoring the 1% of instances that are farthest from their cluster center: this\\nshould eliminate some outliers. The following code first computes the\\ndistance from each instance to its closest cluster center, then for each cluster\\nit sets the 1% largest distances to –1. Lastly, it creates a set without these\\ninstances marked with a –1 distance:\\npercentile_closest = 99\\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\\nfor i in range(k):\\n    in_cluster = (kmeans.labels_ == i)\\n    cluster_dist = X_cluster_dist[in_cluster]\\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\\n    above_cutoff = (X_cluster_dist > cutoff_distance)\\n    X_cluster_dist[in_cluster & above_cutoff] = -1\\npartially_propagated = (X_cluster_dist != -1)\\nX_train_partially_propagated = X_train[partially_propagated]\\ny_train_partially_propagated = y_train_propagated[partially_propagated]\\nNow let’s train the model again on this partially propagated dataset and see\\nwhat accuracy we get:\\n>>> log_reg = LogisticRegression(max_iter=10_000)\\n>>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\\n>>> log_reg.score(X_test, y_test)\\n0.9093198992443325\\nNice! With just 50 labeled instances (only 5 examples per class on average!)\\nwe got 90.9% accuracy, which is actually slightly higher than the\\nperformance we got on the fully labeled digits dataset (90.7%). This is partly\\nthanks to the fact that we dropped some outliers, and partly because the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 433}, page_content='propagated labels are actually pretty good—their accuracy is about 97.5%, as\\nthe following code shows:\\n>>> (y_train_partially_propagated == y_train[partially_propagated]).mean()\\n0.9755555555555555\\nTIP\\nScikit-Learn also offers two classes that can propagate labels automatically:\\nLabelSpreading and LabelPropagation in the sklearn.semi_supervised package. Both\\nclasses construct a similarity matrix between all the instances, and iteratively propagate\\nlabels from labeled instances to similar unlabeled instances. There’s also a very different\\nclass called SelfTrainingClassifier in the same package: you give it a base classifier (such\\nas a RandomForestClassifier) and it trains it on the labeled instances, then uses it to\\npredict labels for the unlabeled samples. It then updates the training set with the labels it is\\nmost confident about, and repeats this process of training and labeling until it cannot add\\nlabels anymore. These techniques are not magic bullets, but they can occasionally give\\nyour model a little boost.\\nACTIVE LEARNING\\nTo continue improving your model and your training set, the next step\\ncould be to do a few rounds of active learning, which is when a human\\nexpert interacts with the learning algorithm, providing labels for specific\\ninstances when the algorithm requests them. There are many different\\nstrategies for active learning, but one of the most common ones is called\\nuncertainty sampling. Here is how it works:\\n1. The model is trained on the labeled instances gathered so far, and\\nthis model is used to make predictions on all the unlabeled instances.\\n2. The instances for which the model is most uncertain (i.e., where its\\nestimated probability is lowest) are given to the expert for labeling.\\n3. You iterate this process until the performance improvement stops\\nbeing worth the labeling effort.\\nOther active learning strategies include labeling the instances that would'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 434}, page_content='result in the largest model change or the largest drop in the model’s\\nvalidation error, or the instances that different models disagree on (e.g.,\\nan SVM and a random forest).\\nBefore we move on to Gaussian mixture models, let’s take a look at\\nDBSCAN, another popular clustering algorithm that illustrates a very\\ndifferent approach based on local density estimation. This approach allows\\nthe algorithm to identify clusters of arbitrary shapes.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 435}, page_content='DBSCAN\\nThe density-based spatial clustering of applications with noise (DBSCAN)\\nalgorithm defines clusters as continuous regions of high density. Here is how\\nit works:\\nFor each instance, the algorithm counts how many instances are located\\nwithin a small distance ε (epsilon) from it. This region is called the\\ninstance’s ε-neighborhood.\\nIf an instance has at least min_samples instances in its ε-neighborhood\\n(including itself), then it is considered a core instance. In other words,\\ncore instances are those that are located in dense regions.\\nAll instances in the neighborhood of a core instance belong to the same\\ncluster. This neighborhood may include other core instances; therefore, a\\nlong sequence of neighboring core instances forms a single cluster.\\nAny instance that is not a core instance and does not have one in its\\nneighborhood is considered an anomaly.\\nThis algorithm works well if all the clusters are well separated by low-density\\nregions. The DBSCAN class in Scikit-Learn is as simple to use as you might\\nexpect. Let’s test it on the moons dataset, introduced in Chapter 5:\\nfrom sklearn.cluster import DBSCAN\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=1000, noise=0.05)\\ndbscan = DBSCAN(eps=0.05, min_samples=5)\\ndbscan.fit(X)\\nThe labels of all the instances are now available in the labels_ instance\\nvariable:\\n>>> dbscan.labels_\\narray([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5, [...], 3,  3,  4,  2,  6,  3])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 436}, page_content='Notice that some instances have a cluster index equal to –1, which means that\\nthey are considered as anomalies by the algorithm. The indices of the core\\ninstances are available in the core_sample_indices_ instance variable, and the\\ncore instances themselves are available in the components_ instance variable:\\n>>> dbscan.core_sample_indices_\\narray([  0,   4,   5,   6,   7,   8,  10,  11, [...], 993, 995, 997, 998, 999])\\n>>> dbscan.components_\\narray([[-0.02137124,  0.40618608],\\n       [-0.84192557,  0.53058695],\\n       [...],\\n       [ 0.79419406,  0.60777171]])\\nThis clustering is represented in the lefthand plot of Figure 9-14. As you can\\nsee, it identified quite a lot of anomalies, plus seven different clusters. How\\ndisappointing! Fortunately, if we widen each instance’s neighborhood by\\nincreasing eps to 0.2, we get the clustering on the right, which looks perfect.\\nLet’s continue with this model.\\nFigure 9-14. DBSCAN clustering using two different neighborhood radiuses\\nSurprisingly, the DBSCAN class does not have a predict() method, although\\nit has a fit_predict() method. In other words, it cannot predict which cluster a\\nnew instance belongs to. This decision was made because different\\nclassification algorithms can be better for different tasks, so the authors\\ndecided to let the user choose which one to use. Moreover, it’s not hard to\\nimplement. For example, let’s train a KNeighborsClassifier:\\nfrom sklearn.neighbors import KNeighborsClassifier'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 437}, page_content='knn = KNeighborsClassifier(n_neighbors=50)\\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\\nNow, given a few new instances, we can predict which clusters they most\\nlikely belong to and even estimate a probability for each cluster:\\n>>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\\n>>> knn.predict(X_new)\\narray([1, 0, 1, 0])\\n>>> knn.predict_proba(X_new)\\narray([[0.18, 0.82],\\n       [1.  , 0.  ],\\n       [0.12, 0.88],\\n       [1.  , 0.  ]])\\nNote that we only trained the classifier on the core instances, but we could\\nalso have chosen to train it on all the instances, or all but the anomalies: this\\nchoice depends on the final task.\\nThe decision boundary is represented in Figure 9-15 (the crosses represent\\nthe four instances in X_new). Notice that since there is no anomaly in the\\ntraining set, the classifier always chooses a cluster, even when that cluster is\\nfar away. It is fairly straightforward to introduce a maximum distance, in\\nwhich case the two instances that are far away from both clusters are\\nclassified as anomalies. To do this, use the kneighbors() method of the\\nKNeighborsClassifier. Given a set of instances, it returns the distances and\\nthe indices of the k-nearest neighbors in the training set (two matrices, each\\nwith k columns):\\n>>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\\n>>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\\n>>> y_pred[y_dist > 0.2] = -1\\n>>> y_pred.ravel()\\narray([-1,  0,  1, -1])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 438}, page_content='Figure 9-15. Decision boundary between two clusters\\nIn short, DBSCAN is a very simple yet powerful algorithm capable of\\nidentifying any number of clusters of any shape. It is robust to outliers, and it\\nhas just two hyperparameters (eps and min_samples). If the density varies\\nsignificantly across the clusters, however, or if there’s no sufficiently low-\\ndensity region around some clusters, DBSCAN can struggle to capture all the\\nclusters properly. Moreover, its computational complexity is roughly O(m n),\\nso it does not scale well to large datasets.\\nTIP\\nYou may also want to try hierarchical DBSCAN (HDBSCAN), which is implemented in\\nthe scikit-learn-contrib project, as it is usually better than DBSCAN at finding clusters of\\nvarying densities.\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 439}, page_content='Other Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should\\ntake a look at. I cannot cover them all in detail here, but here is a brief\\noverview:\\nAgglomerative clustering\\nA hierarchy of clusters is built from the bottom up. Think of many tiny\\nbubbles floating on water and gradually attaching to each other until\\nthere’s one big group of bubbles. Similarly, at each iteration,\\nagglomerative clustering connects the nearest pair of clusters (starting\\nwith individual instances). If you drew a tree with a branch for every pair\\nof clusters that merged, you would get a binary tree of clusters, where the\\nleaves are the individual instances. This approach can capture clusters of\\nvarious shapes; it also produces a flexible and informative cluster tree\\ninstead of forcing you to choose a particular cluster scale, and it can be\\nused with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix, which is a sparse m × m\\nmatrix that indicates which pairs of instances are neighbors (e.g., returned\\nby sklearn.neighbors.kneighbors_graph()). Without a connectivity matrix,\\nthe algorithm does not scale well to large datasets.\\nBIRCH\\nThe balanced iterative reducing and clustering using hierarchies (BIRCH)\\nalgorithm was designed specifically for very large datasets, and it can be\\nfaster than batch k-means, with similar results, as long as the number of\\nfeatures is not too large (<20). During training, it builds a tree structure\\ncontaining just enough information to quickly assign each new instance to\\na cluster, without having to store all the instances in the tree: this\\napproach allows it to use limited memory while handling huge datasets.\\nMean-shift\\nThis algorithm starts by placing a circle centered on each instance; then'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 440}, page_content='for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates\\nthis mean-shifting step until all the circles stop moving (i.e., until each of\\nthem is centered on the mean of the instances it contains). Mean-shift\\nshifts the circles in the direction of higher density, until each of them has\\nfound a local density maximum. Finally, all the instances whose circles\\nhave settled in the same place (or close enough) are assigned to the same\\ncluster. Mean-shift has some of the same features as DBSCAN, like how\\nit can find any number of clusters of any shape, it has very few\\nhyperparameters (just one—the radius of the circles, called the\\nbandwidth), and it relies on local density estimation. But unlike\\nDBSCAN, mean-shift tends to chop clusters into pieces when they have\\ninternal density variations. Unfortunately, its computational complexity is\\nO(m n), so it is not suited for large datasets.\\nAffinity propagation\\nIn this algorithm, instances repeatedly exchange messages between one\\nanother until every instance has elected another instance (or itself) to\\nrepresent it. These elected instances are called exemplars. Each exemplar\\nand all the instances that elected it form one cluster. In real-life politics,\\nyou typically want to vote for a candidate whose opinions are similar to\\nyours, but you also want them to win the election, so you might choose a\\ncandidate you don’t fully agree with, but who is more popular. You\\ntypically evaluate popularity through polls. Affinity propagation works in\\na similar way, and it tends to choose exemplars located near the center of\\nclusters, similar to k-means. But unlike with k-means, you don’t have to\\npick a number of clusters ahead of time: it is determined during training.\\nMoreover, affinity propagation can deal nicely with clusters of different\\nsizes. Sadly, this algorithm has a computational complexity of O(m ), so\\nit is not suited for large datasets.\\nSpectral clustering\\nThis algorithm takes a similarity matrix between the instances and creates\\na low-dimensional embedding from it (i.e., it reduces the matrix’s\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 441}, page_content='dimensionality), then it uses another clustering algorithm in this low-\\ndimensional space (Scikit-Learn’s implementation uses k-means).\\nSpectral clustering can capture complex cluster structures, and it can also\\nbe used to cut graphs (e.g., to identify clusters of friends on a social\\nnetwork). It does not scale well to large numbers of instances, and it does\\nnot behave well when the clusters have very different sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density\\nestimation, clustering, and anomaly detection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 442}, page_content='Gaussian Mixtures\\nA Gaussian mixture model (GMM) is a probabilistic model that assumes that\\nthe instances were generated from a mixture of several Gaussian distributions\\nwhose parameters are unknown. All the instances generated from a single\\nGaussian distribution form a cluster that typically looks like an ellipsoid.\\nEach cluster can have a different ellipsoidal shape, size, density, and\\norientation, just like in Figure 9-11. When you observe an instance, you know\\nit was generated from one of the Gaussian distributions, but you are not told\\nwhich one, and you do not know what the parameters of these distributions\\nare.\\nThere are several GMM variants. In the simplest variant, implemented in the\\nGaussianMixture class, you must know in advance the number k of Gaussian\\ndistributions. The dataset X is assumed to have been generated through the\\nfollowing probabilistic process:\\nFor each instance, a cluster is picked randomly from among k clusters.\\nThe probability of choosing the j  cluster is the cluster’s weight ϕ .\\u2060\\nThe index of the cluster chosen for the i  instance is noted z .\\nIf the i  instance was assigned to the j  cluster (i.e., z  = j), then the\\nlocation x  of this instance is sampled randomly from the Gaussian\\ndistribution with mean μ  and covariance matrix Σ . This is noted x\\n~ ᷒(μ , Σ ).\\nSo what can you do with such a model? Well, given the dataset X, you\\ntypically want to start by estimating the weights ϕ and all the distribution\\nparameters μ\\n to μ\\n and Σ\\n to Σ\\n. Scikit-Learn’s GaussianMixture class\\nmakes this super easy:\\nfrom sklearn.mixture import GaussianMixture\\ngm = GaussianMixture(n_components=3, n_init=10)\\ngm.fit(X)\\nth\\n(j)\\n6\\nth\\n(i)\\nth\\nth\\n(i)\\n(i)\\n(j)\\n(j)\\n(i)\\n(j)\\n(j)\\n(1)\\n(k)\\n(1)\\n(k)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 443}, page_content='Let’s look at the parameters that the algorithm estimated:\\n>>> gm.weights_\\narray([0.39025715, 0.40007391, 0.20966893])\\n>>> gm.means_\\narray([[ 0.05131611,  0.07521837],\\n       [-1.40763156,  1.42708225],\\n       [ 3.39893794,  1.05928897]])\\n>>> gm.covariances_\\narray([[[ 0.68799922,  0.79606357],\\n        [ 0.79606357,  1.21236106]],\\n       [[ 0.63479409,  0.72970799],\\n        [ 0.72970799,  1.1610351 ]],\\n       [[ 1.14833585, -0.03256179],\\n        [-0.03256179,  0.95490931]]])\\nGreat, it worked fine! Indeed, two of the three clusters were generated with\\n500 instances each, while the third cluster only contains 250 instances. So the\\ntrue cluster weights are 0.4, 0.4, and 0.2, respectively, and that’s roughly\\nwhat the algorithm found. Similarly, the true means and covariance matrices\\nare quite close to those found by the algorithm. But how? This class relies on\\nthe expectation-maximization (EM) algorithm, which has many similarities\\nwith the k-means algorithm: it also initializes the cluster parameters\\nrandomly, then it repeats two steps until convergence, first assigning\\ninstances to clusters (this is called the expectation step) and then updating the\\nclusters (this is called the maximization step). Sounds familiar, right? In the\\ncontext of clustering, you can think of EM as a generalization of k-means that\\nnot only finds the cluster centers (μ\\n to μ\\n), but also their size, shape, and\\norientation (Σ\\n to Σ\\n), as well as their relative weights (ϕ\\n to ϕ\\n). Unlike\\nk-means, though, EM uses soft cluster assignments, not hard assignments.\\nFor each instance, during the expectation step, the algorithm estimates the\\nprobability that it belongs to each cluster (based on the current cluster\\nparameters). Then, during the maximization step, each cluster is updated\\nusing all the instances in the dataset, with each instance weighted by the\\nestimated probability that it belongs to that cluster. These probabilities are\\ncalled the responsibilities of the clusters for the instances. During the\\n(1)\\n(k)\\n(1)\\n(k)\\n(1)\\n(k)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 444}, page_content='maximization step, each cluster’s update will mostly be impacted by the\\ninstances it is most responsible for.\\nWARNING\\nUnfortunately, just like k-means, EM can end up converging to poor solutions, so it needs\\nto be run several times, keeping only the best solution. This is why we set n_init to 10. Be\\ncareful: by default n_init is set to 1.\\nYou can check whether or not the algorithm converged and how many\\niterations it took:\\n>>> gm.converged_\\nTrue\\n>>> gm.n_iter_\\n4\\nNow that you have an estimate of the location, size, shape, orientation, and\\nrelative weight of each cluster, the model can easily assign each instance to\\nthe most likely cluster (hard clustering) or estimate the probability that it\\nbelongs to a particular cluster (soft clustering). Just use the predict() method\\nfor hard clustering, or the predict_proba() method for soft clustering:\\n>>> gm.predict(X)\\narray([0, 0, 1, ..., 2, 2, 2])\\n>>> gm.predict_proba(X).round(3)\\narray([[0.977, 0.   , 0.023],\\n       [0.983, 0.001, 0.016],\\n       [0.   , 1.   , 0.   ],\\n       ...,\\n       [0.   , 0.   , 1.   ],\\n       [0.   , 0.   , 1.   ],\\n       [0.   , 0.   , 1.   ]])\\nA Gaussian mixture model is a generative model, meaning you can sample\\nnew instances from it (note that they are ordered by cluster index):\\n>>> X_new, y_new = gm.sample(6)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 445}, page_content='>>> X_new\\narray([[-0.86944074, -0.32767626],\\n       [ 0.29836051,  0.28297011],\\n       [-2.8014927 , -0.09047309],\\n       [ 3.98203732,  1.49951491],\\n       [ 3.81677148,  0.53095244],\\n       [ 2.84104923, -0.73858639]])\\n>>> y_new\\narray([0, 0, 1, 2, 2, 2])\\nIt is also possible to estimate the density of the model at any given location.\\nThis is achieved using the score_samples() method: for each instance it is\\ngiven, this method estimates the log of the probability density function (PDF)\\nat that location. The greater the score, the higher the density:\\n>>> gm.score_samples(X).round(2)\\narray([-2.61, -3.57, -3.33, ..., -3.51, -4.4 , -3.81])\\nIf you compute the exponential of these scores, you get the value of the PDF\\nat the location of the given instances. These are not probabilities, but\\nprobability densities: they can take on any positive value, not just a value\\nbetween 0 and 1. To estimate the probability that an instance will fall within a\\nparticular region, you would have to integrate the PDF over that region (if\\nyou do so over the entire space of possible instance locations, the result will\\nbe 1).\\nFigure 9-16 shows the cluster means, the decision boundaries (dashed lines),\\nand the density contours of this model.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 446}, page_content='Figure 9-16. Cluster means, decision boundaries, and density contours of a trained Gaussian mixture\\nmodel\\nNice! The algorithm clearly found an excellent solution. Of course, we made\\nits task easy by generating the data using a set of 2D Gaussian distributions\\n(unfortunately, real-life data is not always so Gaussian and low-dimensional).\\nWe also gave the algorithm the correct number of clusters. When there are\\nmany dimensions, or many clusters, or few instances, EM can struggle to\\nconverge to the optimal solution. You might need to reduce the difficulty of\\nthe task by limiting the number of parameters that the algorithm has to learn.\\nOne way to do this is to limit the range of shapes and orientations that the\\nclusters can have. This can be achieved by imposing constraints on the\\ncovariance matrices. To do this, set the covariance_type hyperparameter to\\none of the following values:\\n\"spherical\"\\nAll clusters must be spherical, but they can have different diameters (i.e.,\\ndifferent variances).\\n\"diag\"\\nClusters can take on any ellipsoidal shape of any size, but the ellipsoid’s\\naxes must be parallel to the coordinate axes (i.e., the covariance matrices\\nmust be diagonal).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 447}, page_content='\"tied\"\\nAll clusters must have the same ellipsoidal shape, size, and orientation\\n(i.e., all clusters share the same covariance matrix).\\nBy default, covariance_type is equal to \"full\", which means that each cluster\\ncan take on any shape, size, and orientation (it has its own unconstrained\\ncovariance matrix). Figure 9-17 plots the solutions found by the EM\\nalgorithm when covariance_type is set to \"tied\" or \"spherical\".\\nFigure 9-17. Gaussian mixtures for tied clusters (left) and spherical clusters (right)\\nNOTE\\nThe computational complexity of training a GaussianMixture model depends on the\\nnumber of instances m, the number of dimensions n, the number of clusters k, and the\\nconstraints on the covariance matrices. If covariance_type is \"spherical\" or \"diag\", it is\\nO(kmn), assuming the data has a clustering structure. If covariance_type is \"tied\" or \"full\",\\nit is O(kmn  + kn ), so it will not scale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. We’ll see\\nhow in the next section.\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 448}, page_content='Using Gaussian Mixtures for Anomaly Detection\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any\\ninstance located in a low-density region can be considered an anomaly. You\\nmust define what density threshold you want to use. For example, in a\\nmanufacturing company that tries to detect defective products, the ratio of\\ndefective products is usually well known. Say it is equal to 2%. You then set\\nthe density threshold to be the value that results in having 2% of the instances\\nlocated in areas below that threshold density. If you notice that you get too\\nmany false positives (i.e., perfectly good products that are flagged as\\ndefective), you can lower the threshold. Conversely, if you have too many\\nfalse negatives (i.e., defective products that the system does not flag as\\ndefective), you can increase the threshold. This is the usual precision/recall\\ntrade-off (see Chapter 3). Here is how you would identify the outliers using\\nthe fourth percentile lowest density as the threshold (i.e., approximately 4%\\nof the instances will be flagged as anomalies):\\ndensities = gm.score_samples(X)\\ndensity_threshold = np.percentile(densities, 2)\\nanomalies = X[densities < density_threshold]\\nFigure 9-18 represents these anomalies as stars.\\nA closely related task is novelty detection: it differs from anomaly detection\\nin that the algorithm is assumed to be trained on a “clean” dataset,\\nuncontaminated by outliers, whereas anomaly detection does not make this\\nassumption. Indeed, outlier detection is often used to clean up a dataset.\\nTIP\\nGaussian mixture models try to fit all the data, including the outliers; if you have too many\\nof them this will bias the model’s view of “normality”, and some outliers may wrongly be\\nconsidered as normal. If this happens, you can try to fit the model once, use it to detect and\\nremove the most extreme outliers, then fit the model again on the cleaned-up dataset.\\nAnother approach is to use robust covariance estimation methods (see the EllipticEnvelope\\nclass).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 449}, page_content='Figure 9-18. Anomaly detection using a Gaussian mixture model\\nJust like k-means, the GaussianMixture algorithm requires you to specify the\\nnumber of clusters. So how can you find that number?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 450}, page_content='Selecting the Number of Clusters\\nWith k-means, you can use the inertia or the silhouette score to select the\\nappropriate number of clusters. But with Gaussian mixtures, it is not possible\\nto use these metrics because they are not reliable when the clusters are not\\nspherical or have different sizes. Instead, you can try to find the model that\\nminimizes a theoretical information criterion, such as the Bayesian\\ninformation criterion (BIC) or the Akaike information criterion (AIC),\\ndefined in Equation 9-1.\\nEquation 9-1. Bayesian information criterion (BIC) and Akaike information criterion (AIC)\\nB I C = log ( m ) p - 2 log ( L ^ ) A I C = 2 p - 2 log ( L ^ )\\nIn these equations:\\nm is the number of instances, as always.\\np is the number of parameters learned by the model.\\nL^ is the maximized value of the likelihood function of the model.\\nBoth the BIC and the AIC penalize models that have more parameters to\\nlearn (e.g., more clusters) and reward models that fit the data well. They often\\nend up selecting the same model. When they differ, the model selected by the\\nBIC tends to be simpler (fewer parameters) than the one selected by the AIC,\\nbut tends to not fit the data quite as well (this is especially true for larger\\ndatasets).\\nLIKELIHOOD FUNCTION\\nThe terms “probability” and “likelihood” are often used interchangeably\\nin everyday language, but they have very different meanings in statistics.\\nGiven a statistical model with some parameters θ, the word “probability”\\nis used to describe how plausible a future outcome x is (knowing the\\nparameter values θ), while the word “likelihood” is used to describe how\\nplausible a particular set of parameter values θ are, after the outcome x is'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 451}, page_content='known.\\nConsider a 1D mixture model of two Gaussian distributions centered at –\\n4 and +1. For simplicity, this toy model has a single parameter θ that\\ncontrols the standard deviations of both distributions. The top-left contour\\nplot in Figure 9-19 shows the entire model f(x; θ) as a function of both x\\nand θ. To estimate the probability distribution of a future outcome x, you\\nneed to set the model parameter θ. For example, if you set θ to 1.3 (the\\nhorizontal line), you get the probability density function f(x; θ=1.3)\\nshown in the lower-left plot. Say you want to estimate the probability that\\nx will fall between –2 and +2. You must calculate the integral of the PDF\\non this range (i.e., the surface of the shaded region). But what if you\\ndon’t know θ, and instead if you have observed a single instance x=2.5\\n(the vertical line in the upper-left plot)? In this case, you get the\\nlikelihood function ℒ(θ|x=2.5)=f(x=2.5; θ), represented in the upper-right\\nplot.\\nFigure 9-19. A model’s parametric function (top left), and some derived functions: a PDF (lower\\nleft), a likelihood function (top right), and a log likelihood function (lower right)\\nIn short, the PDF is a function of x (with θ fixed), while the likelihood\\nfunction is a function of θ (with x fixed). It is important to understand\\nthat the likelihood function is not a probability distribution: if you'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 452}, page_content='integrate a probability distribution over all possible values of x, you\\nalways get 1, but if you integrate the likelihood function over all possible\\nvalues of θ the result can be any positive value.\\nGiven a dataset X, a common task is to try to estimate the most likely\\nvalues for the model parameters. To do this, you must find the values that\\nmaximize the likelihood function, given X. In this example, if you have\\nobserved a single instance x=2.5, the maximum likelihood estimate\\n(MLE) of θ is θ^=1.5. If a prior probability distribution g over θ exists, it\\nis possible to take it into account by maximizing ℒ(θ|x)g(θ) rather than\\njust maximizing ℒ(θ|x). This is called maximum a-posteriori (MAP)\\nestimation. Since MAP constrains the parameter values, you can think of\\nit as a regularized version of MLE.\\nNotice that maximizing the likelihood function is equivalent to\\nmaximizing its logarithm (represented in the lower-right plot in Figure 9-\\n19). Indeed, the logarithm is a strictly increasing function, so if θ\\nmaximizes the log likelihood, it also maximizes the likelihood. It turns\\nout that it is generally easier to maximize the log likelihood. For example,\\nif you observed several independent instances x\\n to x\\n, you would need\\nto find the value of θ that maximizes the product of the individual\\nlikelihood functions. But it is equivalent, and much simpler, to maximize\\nthe sum (not the product) of the log likelihood functions, thanks to the\\nmagic of the logarithm which converts products into sums: log(ab) =\\nlog(a) + log(b).\\nOnce you have estimated θ^, the value of θ that maximizes the likelihood\\nfunction, then you are ready to compute L^=L(θ^,X), which is the value\\nused to compute the AIC and BIC; you can think of it as a measure of\\nhow well the model fits the data.\\nTo compute the BIC and AIC, call the bic() and aic() methods:\\n>>> gm.bic(X)\\n8189.747000497186\\n>>> gm.aic(X)\\n8102.521720382148\\n(1)\\n(m)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 453}, page_content='Figure 9-20 shows the BIC for different numbers of clusters k. As you can\\nsee, both the BIC and the AIC are lowest when k=3, so it is most likely the\\nbest choice.\\nFigure 9-20. AIC and BIC for different numbers of clusters k'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 454}, page_content='Bayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, you can\\nuse the BayesianGaussianMixture class, which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Set the number of clusters\\nn_components to a value that you have good reason to believe is greater than\\nthe optimal number of clusters (this assumes some minimal knowledge about\\nthe problem at hand), and the algorithm will eliminate the unnecessary\\nclusters automatically. For example, let’s set the number of clusters to 10 and\\nsee what happens:\\n>>> from sklearn.mixture import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\\n>>> bgm.fit(X)\\n>>> bgm.weights_.round(2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only three clusters are\\nneeded, and the resulting clusters are almost identical to the ones in Figure 9-\\n16.\\nA final note about Gaussian mixture models: although they work great on\\nclusters with ellipsoidal shapes, they don’t do so well with clusters of very\\ndifferent shapes. For example, let’s see what happens if we use a Bayesian\\nGaussian mixture model to cluster the moons dataset (see Figure 9-21).\\nOops! The algorithm desperately searched for ellipsoids, so it found eight\\ndifferent clusters instead of two. The density estimation is not too bad, so this\\nmodel could perhaps be used for anomaly detection, but it failed to identify\\nthe two moons. To conclude this chapter, let’s take a quick look at a few\\nalgorithms capable of dealing with arbitrarily shaped clusters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 455}, page_content='Figure 9-21. Fitting a Gaussian mixture to nonellipsoidal clusters'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 456}, page_content='Other Algorithms for Anomaly and Novelty Detection\\nScikit-Learn implements other algorithms dedicated to anomaly detection or\\nnovelty detection:\\nFast-MCD (minimum covariance determinant)\\nImplemented by the EllipticEnvelope class, this algorithm is useful for\\noutlier detection, in particular to clean up a dataset. It assumes that the\\nnormal instances (inliers) are generated from a single Gaussian\\ndistribution (not a mixture). It also assumes that the dataset is\\ncontaminated with outliers that were not generated from this Gaussian\\ndistribution. When the algorithm estimates the parameters of the Gaussian\\ndistribution (i.e., the shape of the elliptic envelope around the inliers), it is\\ncareful to ignore the instances that are most likely outliers. This technique\\ngives a better estimation of the elliptic envelope and thus makes the\\nalgorithm better at identifying the outliers.\\nIsolation forest\\nThis is an efficient algorithm for outlier detection, especially in high-\\ndimensional datasets. The algorithm builds a random forest in which each\\ndecision tree is grown randomly: at each node, it picks a feature\\nrandomly, then it picks a random threshold value (between the min and\\nmax values) to split the dataset in two. The dataset gradually gets\\nchopped into pieces this way, until all instances end up isolated from the\\nother instances. Anomalies are usually far from other instances, so on\\naverage (across all the decision trees) they tend to get isolated in fewer\\nsteps than normal instances.\\nLocal outlier factor (LOF)\\nThis algorithm is also good for outlier detection. It compares the density\\nof instances around a given instance to the density around its neighbors.\\nAn anomaly is often more isolated than its k-nearest neighbors.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 457}, page_content='One-class SVM\\nThis algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly)\\nmapping all the instances to a high-dimensional space, then separating the\\ntwo classes using a linear SVM classifier within this high-dimensional\\nspace (see Chapter 5). Since we just have one class of instances, the one-\\nclass SVM algorithm instead tries to separate the instances in high-\\ndimensional space from the origin. In the original space, this will\\ncorrespond to finding a small region that encompasses all the instances. If\\na new instance does not fall within this region, it is an anomaly. There are\\na few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel when it is in fact normal. It\\nworks great, especially with high-dimensional datasets, but like all SVMs\\nit does not scale to large datasets.\\nPCA and other dimensionality reduction techniques with an\\ninverse_transform() method\\nIf you compare the reconstruction error of a normal instance with the\\nreconstruction error of an anomaly, the latter will usually be much larger.\\nThis is a simple and often quite efficient anomaly detection approach (see\\nthis chapter’s exercises for an example).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 458}, page_content='Exercises\\n1. How would you define clustering? Can you name a few clustering\\nalgorithms?\\n2. What are some of the main applications of clustering algorithms?\\n3. Describe two techniques to select the right number of clusters when\\nusing k-means.\\n4. What is label propagation? Why would you implement it, and how?\\n5. Can you name two clustering algorithms that can scale to large datasets?\\nAnd two that look for regions of high density?\\n6. Can you think of a use case where active learning would be useful? How\\nwould you implement it?\\n7. What is the difference between anomaly detection and novelty\\ndetection?\\n8. What is a Gaussian mixture? What tasks can you use it for?\\n9. Can you name two techniques to find the right number of clusters when\\nusing a Gaussian mixture model?\\n10. The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel\\nimages of faces. Each image is flattened to a 1D vector of size 4,096.\\nForty different people were photographed (10 times each), and the usual\\ntask is to train a model that can predict which person is represented in\\neach picture. Load the dataset using the\\nsklearn.datasets.fetch_olivetti_faces() function, then split it into a\\ntraining set, a validation set, and a test set (note that the dataset is\\nalready scaled between 0 and 1). Since the dataset is quite small, you\\nwill probably want to use stratified sampling to ensure that there are the\\nsame number of images per person in each set. Next, cluster the images\\nusing k-means, and ensure that you have a good number of clusters'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 459}, page_content='(using one of the techniques discussed in this chapter). Visualize the\\nclusters: do you see similar faces in each cluster?\\n11. Continuing with the Olivetti faces dataset, train a classifier to predict\\nwhich person is represented in each picture, and evaluate it on the\\nvalidation set. Next, use k-means as a dimensionality reduction tool, and\\ntrain a classifier on the reduced set. Search for the number of clusters\\nthat allows the classifier to get the best performance: what performance\\ncan you reach? What if you append the features from the reduced set to\\nthe original features (again, searching for the best number of clusters)?\\n12. Train a Gaussian mixture model on the Olivetti faces dataset. To speed\\nup the algorithm, you should probably reduce the dataset’s\\ndimensionality (e.g., use PCA, preserving 99% of the variance). Use the\\nmodel to generate some new faces (using the sample() method), and\\nvisualize them (if you used PCA, you will need to use its\\ninverse_transform() method). Try to modify some images (e.g., rotate,\\nflip, darken) and see if the model can detect the anomalies (i.e., compare\\nthe output of the score_samples() method for normal images and for\\nanomalies).\\n13. Some dimensionality reduction techniques can also be used for anomaly\\ndetection. For example, take the Olivetti faces dataset and reduce it with\\nPCA, preserving 99% of the variance. Then compute the reconstruction\\nerror for each image. Next, take some of the modified images you built\\nin the previous exercise and look at their reconstruction error: notice\\nhow much larger it is. If you plot a reconstructed image, you will see\\nwhy: it tries to reconstruct a normal face.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at https://homl.info/colab3.\\n1  Stuart P. Lloyd, “Least Squares Quantization in PCM”, IEEE Transactions on Information\\nTheory 28, no. 2 (1982): 129–137.\\n2  David Arthur and Sergei Vassilvitskii, “k-Means++: The Advantages of Careful Seeding”,\\nProceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (2007): 1027–'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 460}, page_content='1035.\\n3  Charles Elkan, “Using the Triangle Inequality to Accelerate k-Means”, Proceedings of the 20th\\nInternational Conference on Machine Learning (2003): 147–153.\\n4  The triangle inequality is AC ≤ AB + BC, where A, B and C are three points and AB, AC, and\\nBC are the distances between these points.\\n5  David Sculley, “Web-Scale K-Means Clustering”, Proceedings of the 19th International\\nConference on World Wide Web (2010): 1177–1178.\\n6  Phi (ϕ or φ) is the 21st letter of the Greek alphabet.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 461}, page_content='Part II. Neural Networks and Deep\\nLearning'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 462}, page_content='Chapter 10. Introduction to\\nArtificial Neural Networks with\\nKeras\\nBirds inspired us to fly, burdock plants inspired Velcro, and nature has\\ninspired countless more inventions. It seems only logical, then, to look at the\\nbrain’s architecture for inspiration on how to build an intelligent machine.\\nThis is the logic that sparked artificial neural networks (ANNs), machine\\nlearning models inspired by the networks of biological neurons found in our\\nbrains. However, although planes were inspired by birds, they don’t have to\\nflap their wings to fly. Similarly, ANNs have gradually become quite\\ndifferent from their biological cousins. Some researchers even argue that we\\nshould drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible\\nsystems.\\u2060\\nANNs are at the very core of deep learning. They are versatile, powerful, and\\nscalable, making them ideal to tackle large and highly complex machine\\nlearning tasks such as classifying billions of images (e.g., Google Images),\\npowering speech recognition services (e.g., Apple’s Siri), recommending the\\nbest videos to watch to hundreds of millions of users every day (e.g.,\\nYouTube), or learning to beat the world champion at the game of Go\\n(DeepMind’s AlphaGo).\\nThe first part of this chapter introduces artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures and leading up to\\nmultilayer perceptrons, which are heavily used today (other architectures will\\nbe explored in the next chapters). In the second part, we will look at how to\\nimplement neural networks using TensorFlow’s Keras API. This is a\\nbeautifully designed and simple high-level API for building, training,\\nevaluating, and running neural networks. But don’t be fooled by its\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 463}, page_content='simplicity: it is expressive and flexible enough to let you build a wide variety\\nof neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. And should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, or even use\\nTensorFlow directly, as you will see in Chapter 12.\\nBut first, let’s go back in time to see how artificial neural networks came to\\nbe!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 464}, page_content='From Biological to Artificial Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first\\nintroduced back in 1943 by the neurophysiologist Warren McCulloch and the\\nmathematician Walter Pitts. In their landmark paper\\u2060  “A Logical Calculus of\\nIdeas Immanent in Nervous Activity”, McCulloch and Pitts presented a\\nsimplified computational model of how biological neurons might work\\ntogether in animal brains to perform complex computations using\\npropositional logic. This was the first artificial neural network architecture.\\nSince then many other architectures have been invented, as you will see.\\nThe early successes of ANNs led to the widespread belief that we would soon\\nbe conversing with truly intelligent machines. When it became clear in the\\n1960s that this promise would go unfulfilled (at least for quite a while),\\nfunding flew elsewhere, and ANNs entered a long winter. In the early 1980s,\\nnew architectures were invented and better training techniques were\\ndeveloped, sparking a revival of interest in connectionism, the study of neural\\nnetworks. But progress was slow, and by the 1990s other powerful machine\\nlearning techniques had been invented, such as support vector machines (see\\nChapter 5). These techniques seemed to offer better results and stronger\\ntheoretical foundations than ANNs, so once again the study of neural\\nnetworks was put on hold.\\nWe are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, here are a few good reasons to\\nbelieve that this time is different and that the renewed interest in ANNs will\\nhave a much more profound impact on our lives:\\nThere is now a huge quantity of data available to train neural networks,\\nand ANNs frequently outperform other ML techniques on very large and\\ncomplex problems.\\nThe tremendous increase in computing power since the 1990s now\\nmakes it possible to train large neural networks in a reasonable amount\\nof time. This is in part due to Moore’s law (the number of components\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 465}, page_content='in integrated circuits has doubled about every 2 years over the last 50\\nyears), but also thanks to the gaming industry, which has stimulated the\\nproduction of powerful GPU cards by the millions. Moreover, cloud\\nplatforms have made this power accessible to everyone.\\nThe training algorithms have been improved. To be fair they are only\\nslightly different from the ones used in the 1990s, but these relatively\\nsmall tweaks have had a huge positive impact.\\nSome theoretical limitations of ANNs have turned out to be benign in\\npractice. For example, many people thought that ANN training\\nalgorithms were doomed because they were likely to get stuck in local\\noptima, but it turns out that this is not a big problem in practice,\\nespecially for larger neural networks: the local optima often perform\\nalmost as well as the global optimum.\\nANNs seem to have entered a virtuous circle of funding and progress.\\nAmazing products based on ANNs regularly make the headline news,\\nwhich pulls more and more attention and funding toward them, resulting\\nin more and more progress and even more amazing products.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 466}, page_content='Biological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological\\nneuron (represented in Figure 10-1). It is an unusual-looking cell mostly\\nfound in animal brains. It’s composed of a cell body containing the nucleus\\nand most of the cell’s complex components, many branching extensions\\ncalled dendrites, plus one very long extension called the axon. The axon’s\\nlength may be just a few times longer than the cell body, or up to tens of\\nthousands of times longer. Near its extremity the axon splits off into many\\nbranches called telodendria, and at the tip of these branches are minuscule\\nstructures called synaptic terminals (or simply synapses), which are\\nconnected to the dendrites or cell bodies of other neurons.\\u2060\\n Biological\\nneurons produce short electrical impulses called action potentials (APs, or\\njust signals), which travel along the axons and make the synapses release\\nchemical signals called neurotransmitters. When a neuron receives a\\nsufficient amount of these neurotransmitters within a few milliseconds, it\\nfires its own electrical impulses (actually, it depends on the neurotransmitters,\\nas some of them inhibit the neuron from firing).\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 467}, page_content='Figure 10-1. A biological neuron\\u2060\\nThus, individual biological neurons seem to behave in a simple way, but\\nthey’re organized in a vast network of billions, with each neuron typically\\nconnected to thousands of other neurons. Highly complex computations can\\nbe performed by a network of fairly simple neurons, much like a complex\\nanthill can emerge from the combined efforts of simple ants. The architecture\\nof biological neural networks (BNNs)\\u2060\\n is the subject of active research, but\\nsome parts of the brain have been mapped. These efforts show that neurons\\nare often organized in consecutive layers, especially in the cerebral cortex\\n(the outer layer of the brain), as shown in Figure 10-2.\\n4\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'format': 'PDF 1.4', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Aurélien Géron', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20221212164216+00'00'\", 'page': 468}, page_content='Figure 10-2. Multiple layers in a biological neural network (human cortex)\\u2060\\n6'),\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f91e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
