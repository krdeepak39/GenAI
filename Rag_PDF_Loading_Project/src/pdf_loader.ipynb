{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da620e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515124bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f861d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: Daily_Dose_Of_Data_Science_Full_Archive.pdf\n",
      "  ✓ Loaded 531 pages\n",
      "\n",
      "Processing: Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf\n",
      "  ✓ Loaded 1351 pages\n",
      "\n",
      "Total documents loaded: 1882\n"
     ]
    }
   ],
   "source": [
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede2020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 0, 'page_label': '1', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DATASCIENCE\\nFREE2024 EDITION\\nFULL ARCHIVE\\n Avi ChawlaDailyDoseofDS.comDaily Dose ofData Science'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 1, 'page_label': '2', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00o\\x00a\\x00\\x00\\x00h\\x00\\x00o\\x00\\x00\\x00u\\x00\\x00f\\x00h\\x00\\x00\\x00o\\x00\\x00\\n\\x00n\\x00\\x00o\\x00\\x00\\x00i\\x00\\x00?\\nReadingtimeofthisbookisabout9-10hours.Butnotallchapterswillbeof\\nrelevancetoyou.This2-minuteassessmentwilltestyourcurrentexpertiseand\\nrecommendchaptersthatwillbemostusefultoyou.\\nScantheQRcodebeloworopenthislinktostarttheassessment.Itwillonlytake\\n2minutestocomplete.\\nhttps://bit.ly/DS-assessment\\n1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 2, 'page_label': '3', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00e\\x00f\\x00o\\x00\\x00e\\x00\\x00\\x00\\n\\x00e\\x00\\x00i\\x00\\x00\\x001\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\n\\x00.\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00a\\x00\\x00d\\x00\\x00m\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\n\\x00r\\x00\\x00s\\x00\\x00r\\x00e\\x00\\x00n\\x00\\x00g\\x00\\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\x00\\x00u\\x00\\x00i\\x00\\x00s\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00n\\x00\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x00\\n\\x00n\\x00\\x00o\\x00\\x00c\\x00\\x00o\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x002\\n\\x00u\\x00\\x00d\\x00\\x00g\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00M\\x00\\x00)\\x00o\\x00\\x00l\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x005\\n\\x00c\\x00\\x00v\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\n\\x00.\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00e\\x00n\\x00\\x00e\\x00\\x00r\\x00\\x00p\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\n\\x00o\\x00\\x00n\\x00\\x00m\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x007\\n\\x00i\\x00\\x00d\\x00r\\x00\\x00i\\x00\\x00o\\x00\\x00r\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x003\\n  \\x00r\\x00\\x00i\\x00\\x00t\\x00h\\x00\\x00k\\x00\\x00i\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\n\\x00r\\x00\\x00i\\x00\\x00t\\x00c\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x006\\n\\x00\\x00t\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00u\\x00\\x00i\\x00\\x00P\\x00\\x00r\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\n\\x00.\\x00\\x00\\x00i\\x00\\x00e\\x00\\x00a\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\n\\x00a\\x00\\x00l\\x00m\\x00\\x00t\\x00\\x00n\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x006\\n\\x00o\\x00\\x00l\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\n\\x00o\\x00\\x00r\\x00\\x00o\\x00\\x00\\x00c\\x00\\x00a\\x00\\x00y\\x00o\\x00\\x00s\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\n\\x00s\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00o\\x00\\x00\\x00n\\x00N\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x000\\n\\x00h\\x00\\x00\\x00i\\x00\\x00e\\x00\\x00a\\x00\\x00r\\x00\\x00n\\x00\\x00c\\x00\\x00v\\x00\\x00i\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00\\x00c\\x00\\x00a\\x00\\x00y\\x00o\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x005\\n\\x00h\\x00ﬄ\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00r\\x00\\x00r\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\n\\x00.\\x00\\x00\\x00o\\x00\\x00l\\x00o\\x00\\x00r\\x00\\x00s\\x00\\x00n\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x002\\n\\x00n\\x00\\x00l\\x00\\x00g\\x00\\x00i\\x00\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00o\\x00\\x00l\\x00o\\x00\\x00r\\x00\\x00s\\x00\\x00n\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x002\\n\\x00c\\x00\\x00v\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\n\\x00.\\x00\\x00\\x00e\\x00\\x00o\\x00\\x00e\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x001\\n\\x00e\\x00\\x00o\\x00\\x00L\\x00o\\x00\\x00l\\x00\\x00r\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00\\x00o\\x00\\x00b\\x00\\x00k\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x001\\n\\x00\\x00a\\x00\\x00\\x00o\\x00e\\x00\\x00\\x00L\\x00o\\x00\\x00l\\x00\\x00n\\x00r\\x00\\x00u\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x006\\n\\x00e\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00r\\x00\\x00l\\x00\\x00g\\x00n\\x00\\x00o\\x00\\x00l\\x00e\\x00\\x00s\\x00\\x00y\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\n\\x00.\\x00\\x00\\x00L\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x000\\x00\\n\\x00h\\x00\\x00e\\x00i\\x00\\x00h\\x00\\x00P\\x00\\x00e\\x00\\x00r\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x000\\x00\\n\\x00u\\x00\\x00-\\x00\\x00d\\x00\\x00\\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\x00s\\x00\\x00o\\x00\\x00\\x00s\\x00\\x00A\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x000\\x00\\n\\x00\\x00L\\x00\\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\x00\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 3, 'page_label': '4', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00i\\x00\\x00\\x002\\x00\\x00l\\x00\\x00s\\x00\\x00a\\x00\\x00L\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x001\\x00\\n\\x00\\x001\\x00\\x00L\\x00u\\x00\\x00a\\x00\\x00n\\x00\\x00l\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x001\\x00\\n\\x00r\\x00\\x00n\\x00\\x00g\\x00n\\x00\\x00n\\x00\\x00r\\x00\\x00c\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00l\\x00\\x00i\\x00\\x00\\x00f\\x000\\x00L\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x001\\x00\\n\\x005\\x00o\\x00\\x00\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00a\\x00\\x00e\\x00\\x00t\\x00\\x00a\\x00\\x00eﬁ\\x00i\\x00\\x00o\\x00\\x00\\x00n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\x00\\n\\x00o\\x00\\x00o\\x00e\\x00\\x00a\\x00\\x00y\\x00m\\x00\\x00o\\x00\\x00\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00t\\x00\\x00\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00-\\x00\\x00a\\x00\\x00iﬁ\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00l\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x002\\x00\\n\\x00o\\x00\\x00\\x00n\\x00\\x00r\\x00\\x00o\\x00\\x00l\\x00m\\x00\\x00o\\x00\\x00m\\x00\\x00t\\x00ﬀ\\x00r\\x00\\x00\\x00i\\x00\\x00t\\x00e\\x00o\\x00\\x00g\\x00n\\x00a\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00o\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00f\\x006\\x00L\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x003\\x00\\n\\x000\\x00o\\x00\\x00\\x00o\\x00\\x00o\\x00\\x00o\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x003\\x00\\n\\x00o\\x00\\x00o\\x00c\\x00\\x00a\\x00\\x00y\\x00s\\x00\\x00r\\x00\\x00n\\x00\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x003\\x00\\n\\x00\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x003\\x00\\n\\x00h\\x00\\x00\\x00o\\x00o\\x00f\\x00\\x00r\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00?\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x004\\x00\\n\\x00o\\x00\\x00l\\x00\\x00e\\x00\\x00e\\x00\\x00\\x00s\\x00\\x00i\\x00\\x00-\\x00\\x00r\\x00\\x00\\x00c\\x00\\x00r\\x00\\x00e\\x00\\x00ﬀ\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x004\\x00\\n\\x00\\x002\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00c\\x00\\x00\\x00o\\x00\\x00d\\x00\\x00i\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x004\\x00\\n\\x00L\\x00\\x00s\\x00\\x00M—\\x00h\\x00\\x00’\\x00\\x00h\\x00\\x00iﬀ\\x00r\\x00\\x00c\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x004\\x00\\n\\x00o\\x00ﬁ\\x00e\\x00\\x00e\\x00n\\x00\\x00r\\x00\\x00l\\x00n\\x00\\x00r\\x00\\x00i\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00r\\x00\\x00l\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x005\\x00\\n\\x00h\\x00\\x00s\\x00L\\x00\\x00a\\x00\\x00e\\x00\\x00n\\x00n\\x00\\x00a\\x00\\x00d\\x00s\\x00\\x00m\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x005\\x00\\n\\x00h\\x00\\x00t\\x00\\x00h\\x00\\x00y\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x006\\x00\\n\\x00h\\x00\\x00r\\x00\\x00e\\x00\\x00a\\x00\\x00l\\x00\\x00o\\x00\\x00s\\x00i\\x00\\x00a\\x00\\x00e\\x00v\\x00\\x00\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x001\\x00a\\x00\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00a\\x00\\x00\\x00o\\x00\\x00a\\x00\\x00t\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x007\\x00\\n\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00s\\x00\\x00i\\x00\\x00l\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x007\\x00\\n\\x001\\x00e\\x00\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\x00\\x00n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x008\\x00\\n\\x00\\x00o\\x00\\x00o\\x00\\x00i\\x00\\x00n\\x00\\x00r\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00f\\x00o\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x008\\x00\\n\\x00\\x003\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00eﬁ\\x00i\\x00\\x00o\\x00\\x00\\x00n\\x00\\x00n\\x00\\x00r\\x00\\x00g\\x00n\\x00\\x00e\\x00\\x00c\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\x00\\n\\x001\\x00y\\x00\\x00s\\x00f\\x00a\\x00\\x00a\\x00\\x00e\\x00\\x00n\\x00\\x00a\\x00\\x00s\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\x00\\n\\x00y\\x00\\x00i\\x00\\x00l\\x00e\\x00\\x00u\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x000\\x00\\n\\x00e\\x00\\x00u\\x00\\x00\\x00i\\x00\\x00r\\x00\\x00i\\x00\\x00t\\x00\\x00n\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x000\\x00\\n\\x00\\x00a\\x00\\x00g\\x00\\x00i\\x00\\x00l\\x00a\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x001\\x00\\n\\x00h\\x00ﬄ\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x001\\x00\\n\\x00h\\x00\\x00r\\x00\\x00e\\x00e\\x00\\x00o\\x00\\x00o\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00e\\x00\\x00c\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x001\\x00\\n\\x00\\x004\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x002\\x00\\n\\x00h\\x00\\x00e\\x00\\x00\\x00q\\x00\\x00r\\x00\\x00\\x00r\\x00\\x00r\\x00M\\x00\\x00)\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00k\\x00\\x00a\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00o\\x00y\\x00\\x00r\\x00\\x00r\\x00\\x00e\\x00\\x00r\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x002\\x00\\n\\x00o\\x00\\x00s\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00s\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x002\\x00\\n\\x00o\\x00\\x00o\\x00u\\x00\\x00d\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00l\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x003\\x00\\n3'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 4, 'page_label': '5', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\n\\x00u\\x00\\x00y\\x00a\\x00\\x00a\\x00\\x00e\\x00r\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x003\\x00\\n\\x00i\\x00\\x00a\\x00\\x00y\\x00s\\x00\\x00s\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00o\\x00\\x00a\\x00\\x00e\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x003\\x00\\n\\x00t\\x00\\x00s\\x00\\x00d\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00u\\x00\\x00a\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x003\\x00\\n  \\x00e\\x00\\x00r\\x00\\x00i\\x00\\x00d\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00l\\x00\\x00G\\x00\\x00s\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n\\x00e\\x00\\x00-\\x00\\x00ﬂ\\x00t\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n\\x00u\\x00\\x00r\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x005\\x00\\n\\x00\\x005\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\x00n\\x00\\x00n\\x00\\x00m\\x00\\x00e\\x00e\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x005\\x00\\n\\x00o\\x00\\x00e\\x00\\x00e\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00s\\x00\\x00n\\x00\\x00\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x005\\x00\\n\\x00r\\x00\\x00s\\x00\\x00r\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00\\x00n\\x00\\x00\\x00a\\x00\\x00i\\x00\\x00p\\x00\\x00a\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00h\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\x00u\\x00\\x00\\x00e\\x00h\\x00\\x00o\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00e\\x00\\x00f\\x00\\x00r\\x00r\\x00\\x00n\\x00\\x00g\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\x00L\\x00\\x00\\x00S\\x00v\\x00\\x00ﬁ\\x00!\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x007\\x00\\n\\x00O\\x00\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00n\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00s\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x008\\x00\\n\\x00r\\x00\\x00n\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00s\\x00\\x00n\\x00a\\x00\\x00e\\x00a\\x00\\x00s\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x008\\x00\\n\\x00\\x00i\\x00\\x00a\\x00\\x00u\\x00\\x00e\\x00o\\x00d\\x00\\x00o\\x00\\x00t\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x008\\x00\\n\\x00\\x006\\x00\\x00i\\x00\\x00n\\x00\\x00o\\x00\\x00l\\x00\\x00y\\x00e\\x00\\x00c\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\x00\\n\\x00h\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00f\\x00V\\x00\\x00i\\x00\\x00c\\x00\\x00\\x00n\\x00C\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\x00\\n\\x00e\\x00\\x00e\\x00\\x00C\\x00\\x00s\\x00\\x00C\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\x00\\n\\x00C\\x00\\x00s\\x00o\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00z\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x000\\x00\\n\\x00-\\x00\\x00E\\x00s\\x00\\x00N\\x00—\\x00h\\x00\\x00'\\x00\\x00h\\x00\\x00iﬀ\\x00r\\x00\\x00c\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x000\\x00\\n\\x00o\\x00\\x00o\\x00v\\x00\\x00d\\x00e\\x00\\x00i\\x00\\x00\\x00i\\x00\\x00e\\x00\\x00y\\x00-\\x00\\x00E\\x00r\\x00\\x00e\\x00\\x00i\\x00\\x00s\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\x00\\n\\x00c\\x00\\x00l\\x00\\x00a\\x00\\x00\\x00S\\x00\\x00\\x00i\\x00\\x00\\x00P\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x001\\x00\\n\\x00c\\x00\\x00e\\x00S\\x00\\x00\\x00o\\x00i\\x00\\x00i\\x00\\x00s\\x00f\\x00a\\x00\\x00\\x00o\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00p\\x00\\x00T\\x00\\x00E\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x001\\x00\\n\\x00C\\x00\\x00s\\x00\\x00-\\x00\\x00E\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x002\\x00\\n\\x00\\x007\\x00\\x00l\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00e\\x00\\x00n\\x00\\x00M\\x00\\x00n\\x00\\x00\\x00\\x00u\\x00\\x00-\\x00\\x00o\\x00\\x00y\\x00\\x00s\\x00f\\x00l\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x002\\x00\\n\\x00n\\x00\\x00i\\x00\\x00i\\x00\\x00e\\x00\\x00u\\x00\\x00s\\x00o\\x00\\x00l\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00v\\x00\\x00u\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00r\\x00\\x00t\\x00\\x00n\\x00\\x00M\\x00\\x00n\\x00\\x00s\\x00M\\x00\\x00n\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x003\\x00\\n\\x00o\\x00\\x00o\\x00\\x00\\x00i\\x00\\x00B\\x00\\x00c\\x00\\x00M\\x00\\x00n\\x00\\x00o\\x00\\x00s\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x003\\x00\\n\\x00N\\x00\\x00d\\x00\\x00v\\x00\\x00\\x00M\\x00\\x00n\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n\\x00M\\x00\\x00n\\x00\\x00s\\x00\\x00a\\x00\\x00s\\x00\\x00n\\x00i\\x00\\x00u\\x00\\x00\\x00o\\x00\\x00l\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n\\x00B\\x00\\x00A\\x00\\x00+\\x00\\x00\\x00a\\x00\\x00e\\x00\\x00n\\x00\\x00c\\x00\\x00a\\x00\\x00\\x00\\x00l\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00o\\x00B\\x00\\x00A\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n\\x00D\\x00\\x00C\\x00\\x00\\x00s\\x00\\x00B\\x00\\x00A\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n4\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 5, 'page_label': '6', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x008\\x00\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00n\\x00\\x00y\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x005\\x00\\n\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00=\\x00r\\x00\\x00i\\x00\\x00i\\x00\\x00n\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x005\\x00\\n\\x00e\\x00\\x00r\\x00\\x00f\\x00u\\x00\\x00a\\x00\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00c\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x005\\x00\\n\\x00e\\x00\\x00s\\x00\\x00\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00a\\x00\\x00n\\x00\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00s\\x00\\x00c\\x00\\x00t\\x00\\x00n\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x005\\x00\\n\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00i\\x00\\x00\\x00r\\x00\\x00n\\x00\\x00\\x00a\\x00\\x00g\\x00\\x00i\\x00\\x00l\\x00a\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x005\\x00\\n\\x00\\x009\\x00\\x00r\\x00\\x00t\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x00u\\x00\\x00i\\x00\\x00r\\x00\\x00\\x00e\\x00o\\x00\\x00r\\x00\\x00\\x00e\\x00h\\x00\\x00t\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x00s\\x00\\x00g\\x00r\\x00\\x00y\\x00\\x00a\\x00\\x00l\\x00\\x00n\\x00\\x00o\\x00d\\x00\\x00t\\x00\\x00y\\x00r\\x00\\x00t\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00\\x001\\x00\\x00\\x00N\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x007\\x00\\n\\x00s\\x00\\x00g\\x00N\\x00\\x00\\x00n\\x00m\\x00\\x00l\\x00\\x00c\\x00\\x00\\x00a\\x00\\x00s\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00p\\x00\\x00o\\x00\\x00m\\x00\\x00e\\x00e\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00h\\x00\\x00r\\x00e\\x00\\x00c\\x00\\x00s\\x00\\x00g\\x00n\\x00\\x00r\\x00\\x00\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00x\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x008\\x00\\n\\x00\\x001\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x008\\x00\\n\\x00h\\x00\\x00s\\x00e\\x00\\x00e\\x00\\x00r\\x00\\x00k\\x00a\\x00\\x00e\\x00\\x00\\x00T\\x00\\x00c\\x00\\x00?\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x008\\x00\\n\\x00h\\x00\\x00a\\x00\\x00e\\x00\\x00t\\x00\\x00s\\x00e\\x00\\x00n\\x00\\x00B\\x00\\x00e\\x00\\x00e\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x008\\x00\\n\\x00\\x001\\x00\\x00\\x00i\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\x00\\n\\x00\\x00y\\x00\\x00s\\x00f\\x00i\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00e\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x009\\x00\\n\\x00i\\x00\\x00F\\x00\\x00e\\x00\\x00\\x00n\\x00\\x00N\\x00\\x00m\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x009\\x00\\n\\x00\\x001\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00s\\x00n\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x000\\x00\\n\\x00h\\x00\\x00\\x00s\\x00a\\x00\\x00o\\x00\\x00p\\x00\\x00t\\x00\\x00\\x00g\\x00a\\x00\\x00l\\x00o\\x00\\x00L\\x00o\\x00\\x00l\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x000\\x00\\n\\x00e\\x00\\x00u\\x00\\x00\\x00c\\x00\\x00i\\x00\\x00\\x00s\\x00O\\x00\\x00l\\x00\\x00y\\x00\\x00e\\x00\\x00s\\x00\\x00r\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x000\\x00\\n\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00b\\x00\\x00t\\x00o\\x00\\x00r\\x00\\x00s\\x00\\x00r\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x000\\x00\\n\\x00h\\x00\\x00r\\x00\\x00\\x00u\\x00\\x00o\\x00\\x00\\x00f\\x00e\\x00\\x00u\\x00\\x00\\x00c\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00t\\x00\\x00d\\x00\\x00d\\x00\\x00a\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\x00\\n\\x002\\x00e\\x00\\x00l\\x00\\x00i\\x00\\x00t\\x00\\x00n\\x00s\\x00o\\x00\\x00u\\x00\\x00\\x00s\\x00\\x00\\x00o\\x00\\x00e\\x00\\x00l\\x00\\x00i\\x00\\x00t\\x00\\x00n\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\x00\\n\\x00\\x001\\x00\\x00\\x00i\\x00\\x00e\\x00\\x00a\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00s\\x00o\\x00\\x00\\x00o\\x00\\x00l\\x00a\\x00\\x00\\x00eﬁ\\x00i\\x00\\x00t\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x002\\x00\\n\\x00a\\x00\\x00s\\x00\\x00n\\x00p\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00r\\x00\\x00n\\x00n\\x00\\x00e\\x00\\x00-\\x00\\x00m\\x00\\x00a\\x00\\x00\\x00u\\x00\\x00e\\x00\\x00a\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00\\x001\\x00\\x00\\x00a\\x00\\x00\\x00n\\x00\\x00y\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x003\\x00\\n\\x005\\x00a\\x00\\x00a\\x00↔\\x00o\\x00\\x00r\\x00↔\\x00Q\\x00↔\\x00y\\x00\\x00a\\x00\\x00\\x00r\\x00\\x00s\\x00\\x00t\\x00\\x00n\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x003\\x00\\n\\x00\\x00l\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00\\x00o\\x00a\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00r\\x00\\x00e\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x003\\x00\\n\\x00c\\x00\\x00l\\x00\\x00a\\x00\\x00\\x00a\\x00\\x00a\\x00\\x00i\\x00\\x00\\x00P\\x00\\x00s\\x00\\x00g\\x00A\\x00\\x00D\\x00\\x00u\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x003\\x00\\n\\x00i\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00\\x00n\\x00\\x00y\\x00\\x00s\\x00i\\x00\\x00\\x00e\\x00\\x00m\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x003\\x00\\n\\x00a\\x00\\x00F\\x00\\x00m\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x004\\x00\\n5'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 6, 'page_label': '7', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00u\\x00\\x00m\\x00\\x00e\\x00\\x00D\\x00\\x00o\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x004\\x00\\n\\x00\\x001\\x00\\x00\\x00a\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00s\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x004\\x00\\n\\x00o\\x00\\x00\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00l\\x00\\x00s\\x00n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x004\\x00\\n\\x00o\\x00\\x00r\\x00\\x00Q\\x00l\\x00\\x00s\\x00r\\x00\\x00t\\x00\\x00?\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x005\\x00\\n\\x00\\x00l\\x00\\x00a\\x00\\x00\\x00l\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00\\x00o\\x00r\\x00\\x00i\\x00\\x00o\\x00\\x00l\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x005\\x00\\n\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00o\\x00\\x00r\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x00o\\x00\\x00i\\x00\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x00n\\x00\\x00c\\x00\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00s\\x00i\\x00\\x00\\x00n\\x00\\x00t\\x00x\\x00\\x00\\x00n\\x00\\x00n\\x00\\x00t\\x00\\x00i\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x006\\x00\\n\\x00r\\x00\\x00e\\x00\\x00i\\x00\\x00a\\x00\\x00z\\x00\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x006\\x00\\n\\x00a\\x00\\x00e\\x00\\x00i\\x00\\x00r\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x007\\x00\\n\\x00i\\x00\\x00e\\x00\\x00n\\x00\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00p\\x00\\x00k\\x00\\x00n\\x00\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00\\x001\\x00\\x00\\x00Q\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x007\\x00\\n\\x00r\\x00\\x00p\\x00\\x00g\\x00e\\x00\\x00,\\x00o\\x00\\x00u\\x00\\x00n\\x00\\x00u\\x00\\x00\\x00n\\x00Q\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x007\\x00\\n\\x00e\\x00\\x00,\\x00n\\x00\\x00,\\x00n\\x00\\x00a\\x00\\x00r\\x00\\x00\\x00o\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x008\\x00\\n\\x00s\\x00\\x00Q\\x00\\x00N\\x00\\x00\\x00N\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00i\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\x00\\n\\x00\\x001\\x00\\x00\\x00y\\x00\\x00o\\x00\\x00O\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x009\\x00\\n\\x00e\\x00\\x00e\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00e\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\x00\\n\\x00e\\x00\\x00r\\x00\\x00t\\x00\\x00s\\x00n\\x00y\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x009\\x00\\n\\x000\\x00o\\x00\\x00\\x00o\\x00\\x00o\\x00\\x00a\\x00\\x00c\\x00e\\x00\\x00o\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x000\\x00\\n\\x00e\\x00\\x00r\\x00\\x00ﬃ\\x00i\\x00\\x00t\\x00l\\x00\\x00s\\x00b\\x00\\x00c\\x00\\x00\\x00s\\x00\\x00g\\x00l\\x00\\x00s\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x001\\x00\\n\\x00h\\x00\\x00o\\x00\\x00t\\x00e\\x00n\\x00\\x00k\\x00\\x00o\\x00\\x00l\\x00\\x00o\\x00\\x00a\\x00\\x00(\\x00\\x00n\\x00y\\x00\\x00r\\x00\\x00?\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00\\x001\\x00\\n\\x00r\\x00\\x00\\x00O\\x00\\x00n\\x00\\x00p\\x00\\x00l\\x00\\x00i\\x00\\x00\\x00s\\x00i\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00\\x00y\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x002\\x00\\n\\x00\\x00o\\x00\\x00o\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00b\\x00\\x00t\\x00_\\x00\\x00i\\x00\\x00_\\x00\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n\\x00u\\x00\\x00t\\x00\\x00n\\x00v\\x00\\x00l\\x00\\x00d\\x00\\x00g\\x00n\\x00y\\x00\\x00o\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x00.\\x00\\x002\\x00\\n6'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 7, 'page_label': '8', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00\\n\\x00e\\x00\\x00n\\x00\\x00g\\n7'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 8, 'page_label': '9', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00n\\x00\\x00g\\x00a\\x00\\x00d\\x00\\x00m\\x00\\n\\x00r\\x00\\x00s\\x00\\x00r \\x00e\\x00\\x00n\\x00\\x00g\\x00 \\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\x00 \\x00u\\x00\\x00i\\x00\\x00s\\x00 \\x00e\\x00\\x00n\\x00\\x00g\\n\\x00n\\x00\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00n\\x00\\x00g\\nMostMLmodelsaretrainedindependentlywithoutanyinteractionwithother\\nmodels.However,intherealmofreal-worldML,therearemanypowerful\\nlearningtechniquesthatrelyonmodelinteractionstoimproveperformance.\\nThefollowing\\nimage\\nsummarizes\\nfoursuch\\nwell-adopted\\nandmust-know\\ntraining\\nmethodologies:\\n8'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 9, 'page_label': '10', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00r\\x00\\x00s\\x00\\x00r\\x00e\\x00\\x00n\\x00\\x00g\\nThisisextremelyusefulwhen:\\n● Thetaskofinteresthaslessdata.\\n● Butarelatedtaskhasabundantdata.\\nThisishowitworks:\\n● Trainaneuralnetworkmodel(basemodel)ontherelatedtask.\\n● Replacethelastfewlayersonthebasemodelwithnewlayers.\\n● Trainthenetworkonthetaskofinterest,butdon’tupdatetheweightsof\\ntheunreplacedlayersduringbackpropagation.\\nBytrainingamodelontherelatedtaskﬁrst,wecancapturethecorepatternsof\\nthetaskofinterest.Later,wecanadjustthelastfewlayerstocapture\\ntask-speciﬁcbehavior.\\nAnotherideawhichissomewhatalongtheselinesisknowledgedistillation,\\nwhichinvolvesthe“transfer”ofknowledge.Wewilldiscussitintheupcoming\\nchapters.\\nTransferlearningiscommonlyusedinmanycomputervisiontasks.\\n9'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 10, 'page_label': '11', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x002\\x00\\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\nFine-tuninginvolvesupdatingtheweightsofsomeoralllayersofthepre-trained\\nmodeltoadaptittothenewtask.\\nTheideamayappearsimilartotransferlearning,butinﬁne-tuning,wetypically\\ndonotreplacethelastfewlayersofthepre-trainednetwork.\\nInstead,thepretrainedmodelitselfisadjustedtothenewdata.\\n\\x003\\x00\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\nAsthenamesuggests,amodelistrainedtoperformmultipletasks\\nsimultaneously.\\nThemodelsharesknowledgeacrosstasks,aimingtoimprovegeneralizationand\\nperformanceoneachtask.\\n10'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 11, 'page_label': '12', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItcanhelpinscenarioswheretasksarerelated,ortheycanbeneﬁtfromshared\\nrepresentations.\\nInfact,themotiveformulti-tasklearningisnotjusttoimprovegeneralization.\\nWecanalsosavecomputepowerduringtrainingbyhavingasharedlayerand\\ntask-speciﬁcsegments.\\n● Imaginetrainingtwomodelsindependentlyonrelatedtasks.\\n● Nowcompareittohavinganetworkwithsharedlayersandthen\\ntask-speciﬁcbranches.\\nOption2willtypicallyresultin:\\n● Bettergeneralizationacrossalltasks.\\n● Lessmemoryutilizationtostoremodelweights.\\n● Lessresourceutilizationduringtraining.\\n\\x004\\x00\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00n\\x00\\x00g\\nLet’sdiscussitinthenextchapter.\\n11'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 12, 'page_label': '13', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00o\\x00\\x00c\\x00\\x00o\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00n\\x00\\x00g\\nInmyopinion,federatedlearningisamongthoseverypowerfulMLtechniques\\nthatisnotgiventhetrueattentionitdeserves.\\nHere’savisualthatdepictshowitworks:\\nLet’sunderstandthistopicinthischapterandwhyIconsiderthistobean\\nimmenselyvaluableskilltohave.\\n\\x00h\\x00\\x00r\\x00\\x00l\\x00\\x00\\nModerndevices(likesmartphones)haveaccesstoawealthofdatathatcanbe\\nsuitableforMLmodels.\\nTogetsomeperspective,considerthenumberofimagesyouhaveonyourphone\\nrightnow,thenumberofkeystrokesyoupressdaily,etc.\\nThat’splentyofdata,isn’tit?\\nAndthisisjustaboutoneuser—you.\\nButapplicationscanhavemillionsofusers.TheamountofdatawecantrainML\\nmodelsonisunfathomable.\\nSowhatistheproblemhere?\\nTheproblemisthatalmostalldataavailableonmoderndevicesisprivate.\\n12'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 13, 'page_label': '14', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Imagesareprivate.\\n● Messagesyousendareprivate.\\n● Voicenotesareprivate.\\nBeingprivate,itislikelythatitcannotbeaggregatedinacentrallocation,as\\ntraditionally,MLmodelsarealwaystrainedoncentrallylocateddatasets.\\nButthisdataisstillvaluabletous,isn’tit?\\nWewanttoutilizeitinsomeway.\\n\\x00h\\x00\\x00o\\x00\\x00t\\x00\\x00n\\nFederatedlearningsmartlyaddressesthischallengeoftrainingMLmodelson\\nprivatedata.\\nHere’sthecoreidea:\\n13'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 14, 'page_label': '15', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Insteadofaggregatingdataonacentralserver,dispatchamodeltoanend\\ndevice.\\n● Trainthemodelontheuser’sprivatedataontheirdevice.\\n● Fetchthetrainedmodelbacktothecentralserver.\\n● Aggregateallmodelsobtainedfromallenddevicestoformacomplete\\nmodel.\\nThat’saninnovativesolutionbecauseeachclientpossessesalocaltraining\\ndatasetthatremainsexclusivelyontheirdeviceandisneveruploadedtothe\\nserver.\\nYet,westillgettotrainamodelonthisprivatedata.\\nSendaglobalmodeltotheuser’sdevice,trainamodelonprivatedata,and\\nretrieveitback.\\nFurthermore,federatedlearningdistributesmostcomputationtoauser’sdevice.\\nAsaresult,thecentralserverdoesnotneedtheenormouscomputingthatit\\nwouldhavedemandedotherwise.\\nThisisthecoreideabehindfederatedlearning.\\n14'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 15, 'page_label': '16', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00d\\x00\\x00g\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00M\\x00\\x00)\\x00o\\x00\\x00l\\x00\\nMostMLmodelsaretrainedononetask.Asaresult,manystruggletointuitively\\nunderstandhowamodelcanbetrainedonmultipletaskssimultaneously.\\nSolet’sdiscussitinthischapter.\\nToreiterate,inMTL,thenetworkhasafewsharedlayersandtask-speciﬁc\\nsegments.Duringbackpropagation,gradientsareaccumulatedfromallbranches,\\nasdepictedintheanimationbelow:\\nLet’stakeasimpleexampletounderstanditsimplementation.\\nConsiderwewantourmodeltotakearealvalue(x)asinputandgeneratetwo\\noutputs:\\n● sin(x)\\n● cos(x)\\nThiscanbeformulatedasanMTLproblem.\\n15'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 16, 'page_label': '17', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFirst,wedeﬁneourmodelclassusingPyTorch.\\n● Wehavesomefullyconnectedlayersinself.model→Thesearetheshared\\nlayers.\\n● Furthermore,wehavetheoutput-speciﬁclayerstopredictsin(x)andcos(x).\\nNext,let’sdeﬁnetheforward\\npassintheclassabove:\\n● First,wepassthe\\ninputthroughthe\\nsharedlayers\\n(self.model).\\n● Theoutputofthe\\nsharedlayersispassed\\nthroughthesinand\\ncosbranches.\\n● Wereturntheoutput\\nfrombothbranches.\\n16'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 17, 'page_label': '18', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWearealmostdone.Theﬁnalpartofthisimplementationistotrainthemodel.\\nLet’susemeansquarederrorasthelossfunction.Thetrainingloopis\\nimplementedbelow:\\n● Wepasstheinputdatathroughthemodel.\\n● Itreturnstwooutputs,onefromeachsegmentofthenetwork.\\n● Wecomputethebranch-speciﬁclossvalues(loss1andloss2)usingtrue\\npredictions.\\n● Weaddthetwolossvaluestogetthetotallossforthenetwork.\\n● Finally,werunthebackwardpass.\\nWiththis,wehavetrainedourMTLmodel.Also,wegetadecreasingloss,which\\ndepictsthatthemodelisbeingtrained.\\nAndthat’showwetrainanMTLmodel.Youcanextendthesameideatobuild\\nanyMTLmodelofyourchoice.\\n17'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 18, 'page_label': '19', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nDorememberthatbuildinganMTLmodelonunrelatedtaskswillnotproduce\\ngoodresults.\\nThus,“task-relatedness”isacriticalcomponentofallMTLmodelsbecauseof\\nthesharedlayers.Also,itisNOTnecessarythateverytaskmustequally\\ncontributetotheentirenetwork’sloss.\\nWemayassignweightstoeachtaskaswell,asdepictedbelow:\\nTheweightscouldbebasedontaskimportance.\\nOr…\\nAttimes,Ialsousedynamictaskweights,whichcouldbeinverselyproportional\\ntothevalidationaccuracyachievedonthattask.\\nMyrationalebehindthistechniqueisthatinanMTLsetting,sometaskscanbe\\neasywhileotherscanbediﬃcult.\\nIfthemodelachieveshighaccuracyononetaskduringtraining,wecansafely\\nreduceitslosscontributionsothatthemodelfocusesmoreonthesecondtask.\\nYoucandownloadthenotebookforthischapterhere:https://bit.ly/3ztY5hy.\\n18'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 19, 'page_label': '20', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00c\\x00\\x00v\\x00\\x00e\\x00\\x00n\\x00\\x00g\\nThere’snotmuchwecandotobuildasupervisedsystemwhenthedatawebegin\\nwithisunlabeled.\\nUsingunsupervisedtechniques(iftheyﬁtthetask)canbeasolution,but\\nsupervisedsystemsaretypicallyonparwithunsupervisedones.\\nAnotherway,iffeasible,istorelyonself-supervisedlearning.\\nSelf-supervisedlearningiswhenwehaveanunlabeleddataset(saytextdata),but\\nwesomehowﬁgureoutawaytobuildasupervisedlearningmodeloutofit.\\nThisbecomespossibleduetotheinherentnatureofthetask.\\nConsideranLLM,forinstance.\\nInanutshell,itscoreobjectiveistopredictthenexttokenbasedonpreviously\\npredictedtokens(orthegivencontext).\\n19'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 20, 'page_label': '21', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisaclassiﬁcationtask,andthelabelsaretokens.\\nButtextdataisraw.Ithasnolabels.\\nThenhowdidwetrainthisclassiﬁcationtask?\\nSelf-supervisedtechniquessolvethisproblem.\\nDuetotheinherentnatureofthetask(next-tokenprediction,tobespeciﬁc),\\neverypieceofrawtextdataisalreadyself-labeled.\\n20'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 21, 'page_label': '22', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThemodelisonlysupposedtolearnthemappingfromprevioustokenstothe\\nnexttoken.\\nThisiscalledself-supervisedlearning,whichisquitepromising,butithas\\nlimitedapplicability,largelydependingonthetask.\\nAtthisstage,theonlypossibilityonenoticesisannotatingthedataset.However,\\ndataannotationisdiﬃcult,expensive,time-consuming,andtedious.\\nActivelearningisarelativelyeasy,inexpensive,quick,andinterestingwayto\\naddressthis.\\nAsthenamesuggests,theideaistobuildthemodelwithactivehumanfeedback\\nonexamplesitisstrugglingwith.Thevisualbelowsummarizesthis:\\n21'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 22, 'page_label': '23', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’sgetintothedetails.\\nWebeginbymanuallylabelingatinypercentageofthedataset.\\nWhilethere’snoruleonhowmuchdatashouldbelabeled,Ihaveusedactive\\nlearning(successfully)whilelabelingaslowas~1%ofthedataset,sotry\\nsomethinginthatrange.\\nNext,buildamodelonthissmalllabeleddataset.\\nOfcourse,thiswon’tbeaperfectmodel,butthat’sokay.Next,generate\\npredictionsonthedatasetwedidnotlabel:\\n22'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 23, 'page_label': '24', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIt’sobviousthatwecannotdetermineifthesepredictionsarecorrectaswedo\\nnothaveanylabels.\\nThat’swhyweneedtobeabitselectivewiththetypeofmodelwechoose.\\nMorespeciﬁcally,weneedamodelthat,eitherimplicitlyorexplicitly,canalso\\nprovideaconﬁdencelevelwithitspredictions.\\nAsthenamesuggests,aconﬁdencelevelreﬂectsthemodel’sconﬁdencein\\ngeneratingaprediction.\\nIfamodelcouldspeak,itwouldbelike:\\n● Iampredictinga“cat”andam95%conﬁdentaboutmyprediction.\\n● Iampredictinga“cat”andam5%conﬁdentaboutmyprediction.\\n● Andsoon…\\nProbabilisticmodels(onesthatprovideaprobabilisticestimateforeachclass)are\\ntypicallyagoodﬁthere.\\n23'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 24, 'page_label': '25', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisbecauseonecandetermineaproxyforconﬁdencelevelfromprobabilistic\\noutputs.\\nIntheabovetwoexamples,considerthegapbetween1stand2ndhighest\\nprobabilities:\\n● Inexample#1,thegapislarge.Thiscanindicatethatthemodelisquite\\nconﬁdentinitsprediction.\\n● Inexample#2,thegapissmall.ThiscanindicatethatthemodelisNOT\\nquiteconﬁdentinitsprediction.\\nNow,gobacktothepredictionsgeneratedaboveandranktheminorderof\\nconﬁdence:\\n24'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 25, 'page_label': '26', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIntheaboveimage:\\n● Themodelisalreadyquiteconﬁdentwiththeﬁrsttwoinstances.There’s\\nnopointcheckingthose.\\n● Instead,itwouldbebestifwe(thehuman)annotatetheinstanceswith\\nwhichitisleastconﬁdent.\\nTogetsomemoreperspective,considertheimagebelow.Logicallyspeaking,\\nwhichdatapoint’shumanlabelwillprovidemoreinformationtothemodel?I\\nknowyoualreadyknowtheanswer.\\nThus,inthenextstep,weprovideourhumanlabeltothelow-conﬁdence\\npredictionsandfeeditbacktothemodelwiththepreviouslylabeleddataset:\\nRepeatthisafewtimesandstopwhenyouaresatisﬁedwiththeperformance.\\nInmyexperience,activelearninghasalwaysbeenanimmenselytime-saving\\napproachtobuildingsupervisedmodelsonunlabeleddatasets.\\nTheonlythingthatyouhavetobecarefulaboutisgeneratingconﬁdence\\nmeasures.\\n25'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 26, 'page_label': '27', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfyoumessthisup,itwillaﬀecteverysubsequenttrainingstep.\\nThere’sonemorethingIliketodowhenusingactivelearning.\\nWhilecombiningthelow-conﬁdencedatawiththeseeddata,wecanalsousethe\\nhigh-conﬁdencedata.Thelabelswouldbethemodel’spredictions.\\nThisvariantofactivelearningiscalledcooperativelearning.\\n26'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 27, 'page_label': '28', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00t\\x00\\x00e\\x00n\\x00\\x00e\\x00\\x00r\\x00\\n\\x00p\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\n\\x00o\\x00\\x00n\\x00\\x00m\\nAsweprogresstowardsbuildinglarger\\nandlargermodels,everybitofpossible\\noptimizationbecomescrucial.\\nAnd,ofcourse,therearevariouswaystospeedupmodeltraining,like:\\n● Batchprocessing\\n● LeveragedistributedtrainingusingframeworkslikePySparkMLLib.\\n● UsebetterHyperparameterOptimization,likeBayesianOptimization,\\nwhichwewilldiscussinthischapter.\\n● andmanyothertechniques.\\nMomentumisanotherreliableandeﬀectivetechniquetospeedupmodel\\ntraining.WhileMomentumisprettypopular,manypeoplestruggletointuitively\\nunderstandhowitworksandwhyitiseﬀective.Let’sunderstandinthischapter.\\n\\x00s\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00i\\x00\\x00t\\x00e\\x00\\x00e\\x00\\x00\\nIngradientdescent,everyparameterupdatesolelydependsonthecurrent\\ngradient.Thisisclearfromthegradientweightupdateruleshownbelow:\\n27'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 28, 'page_label': '29', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsaresult,weenduphavingmanyunwantedoscillationsduringthe\\noptimizationprocess.\\nLet’sunderstandthismorevisually.\\nImaginethisisthelossfunctioncontourplot,andtheoptimallocation\\n(parameterconﬁgurationwherethelossfunctionisminimum)ismarkedhere:\\nSimplyput,thisplotillustrateshowgradientdescentmovestowardstheoptimal\\nsolution.Ateachiteration,thealgorithmcalculatesthegradientoftheloss\\nfunctionatthecurrentparametervaluesandupdatestheweights.\\nThisisdepictedbelow:\\nNoticetwothingshere:\\n● Itunnecessarilyoscillatesvertically.\\n● Itendsupatthenon-optimalsolutiona\\x00ersomeepochs.\\n28'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 29, 'page_label': '30', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIdeally,wewouldhaveexpectedourweightupdatestolookthis:\\n● Itmusthavetakenlongerstepsinthehorizontaldirection…\\n● …andsmallerverticalstepsbecauseamovementinthisdirectionis\\nunnecessary.\\nThisideaisalsodepictedbelow:\\n\\x00o\\x00\\x00o\\x00\\x00n\\x00\\x00m\\x00o\\x00\\x00e\\x00\\x00h\\x00\\x00\\x00r\\x00\\x00l\\x00\\x00?\\nMomentum-basedoptimizationslightlymodiﬁestheupdateruleofgradient\\ndescent.Morespeciﬁcally,italsoconsidersamovingaverageofpastgradients:\\n29'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 30, 'page_label': '31', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThishelpsushandletheunnecessaryverticaloscillationswesawearlier.\\nHow?\\nAsMomentumconsidersamovingaverageofpastgradients,soiftherecent\\ngradientupdatetrajectorylooksasshowninthefollowingimage,thenitisclear\\nthatitsaverageintheverticaldirectionwillbeverylowwhilethatinthe\\nhorizontaldirectionwillbelarge(whichispreciselywhatwewant):\\nAsthismovingaveragegetsaddedtothegradientupdates,ithelpsthe\\noptimizationalgorithmtakelargerstepsinthedesireddirection.\\nThisway,wecan:\\n● Smoothentheoptimizationtrajectory.\\n30'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 31, 'page_label': '32', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Reduceunnecessaryoscillationsinparameterupdates,whichalsospeeds\\nuptraining.\\nThisisalsoevidentfromtheimagebelow:\\nThistime,thegradientupdatetrajectoryshowsmuchsmalleroscillationsinthe\\nverticaldirection,anditalsomanagestoreachanoptimumunderthesame\\nnumberofepochsasearlier.\\nThisisthecoreideabehindMomentumandhowitworks.\\nOfcourse,Momentumdoesintroduceanotherhyperparameter(Momentumrate)\\ninthemodel,whichshouldbetunedappropriatelylikeanyother\\nhyperparameter:\\nForinstance,consideringthe2Dcontourswediscussedabove:\\n31'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 32, 'page_label': '33', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● SettinganextremelylargevalueofMomentumratewillsigniﬁcantly\\nexpeditegradientupdateinthehorizontaldirection.Thismayleadto\\novershootingtheminima,asdepictedbelow:\\n● What’smore,settinganextremelysmallvalueofMomentumwillslow\\ndowntheoptimalgradientupdate,defeatingthewholepurposeof\\nMomentum.\\nIfyouwanttohaveamorehands-onexperience,checkoutthistool:\\nhttps://bit.ly/4cOrJN1.\\n32'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 33, 'page_label': '34', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00d\\x00r\\x00\\x00i\\x00\\x00o\\x00\\x00r\\x00\\x00n\\x00\\x00g\\n\\x00o\\x00\\x00e\\x00\\x00\\nTypicaldeeplearninglibrariesarereallyconservativewhenitcomestoassigning\\ndatatypes.\\nThedatatypeassignedbydefaultisusually64-bitor32-bit,whenthereisalso\\nscopefor16-bit,forinstance.Thisisalsoevidentfromthecodebelow:\\nAsaresult,wearenotentirelyoptimalateﬃcientlyallocatingmemory.Of\\ncourse,thisisdonetoensurebetterprecisioninrepresentinginformation.\\n33'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 34, 'page_label': '35', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHowever,thisprecisionalwayscomesatthecostofadditionalmemory\\nutilization,whichmaynotbedesiredinallsituations.\\nInfact,itisalsoobservedthatmanytensoroperations,especiallymatrix\\nmultiplication,aremuchfasterwhenweoperateundersmallerprecisiondata\\ntypesthanlargerones,asdemonstratedbelow:\\n34'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 35, 'page_label': '36', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nMoreover,sinceﬂ\\x00a\\x00\\x006isonlyhalfthesizeofﬂ\\x00a\\x00\\x002,itsusagereducesthe\\nmemoryrequiredtotrainthenetwork.Thisalsoallowsustotrainlargermodels,\\ntrainonlargermini-batches(resultinginevenmorespeedup),etc.\\nMixedprecisiontrainingisaprettyreliableandwidelyadoptedtechniqueinthe\\nindustrytoachievethis.\\nAsthenamesuggests,theideaistoemploylowerprecisionﬂ\\x00a\\x00\\x006(wherever\\nfeasible,likeinconvolutionsandmatrixmultiplications)alongwithﬂ\\x00a\\x00\\x002—\\nthatiswhythename“mixedprecision.”\\nThisisalistofsomemodelsIfoundthatweretrainedusingmixedprecision:\\nIt’sprettyclearthatmixedprecisiontrainingismuchmorepopularlyused,but\\nwedon’tgettohearaboutito\\x00en.\\n\\x00e\\x00\\x00r\\x00\\x00e\\x00e\\x00\\x00n\\x00\\x00\\x00h\\x00\\x00e\\x00\\x00n\\x00\\x00a\\x00\\x00e\\x00\\x00i\\x00\\x00…\\nFromtheabovediscussion,itmustbeclearthatasweusealow-precisiondata\\ntype(ﬂ\\x00a\\x00\\x006),wemightunknowinglyintroducesomenumericalinconsistencies\\nandinaccuracies.\\n35'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 36, 'page_label': '37', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nToavoidthem,therearesomebestpracticesformixedprecisiontrainingthatI\\nwanttotalkaboutnext,alongwiththecode.\\n\\x00i\\x00\\x00d\\x00r\\x00\\x00i\\x00\\x00o\\x00\\x00r\\x00\\x00n\\x00\\x00g\\x00n\\x00y\\x00\\x00r\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00t\\x00\\x00\\x00s\\nLeveragingmixedprecisiontraininginPyTorchrequiresafewmodiﬁcationsin\\ntheexistingnetworktrainingimplementation.Considerthisisourcurrent\\nPyTorchmodeltrainingimplementation:\\nTheﬁrstthingweintroducehereisascalerobjectthatwillscalethelossvalue:\\n36'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 37, 'page_label': '38', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWedothisbecause,attimes,theoriginallossvaluecanbesolow,thatwemight\\nnotbeabletocomputegradientsinﬂ\\x00a\\x00\\x006withfullprecision.Suchsituations\\nmaynotproduceanyupdatetothemodel’sweights.\\nScalingthelosstoahighernumericalrangeensuresthatevensmallgradients\\ncancontributetotheweightupdates.\\nButtheseminutegradientscanonlybeaccommodatedintotheweightmatrix\\nwhentheweightmatrixitselfisrepresentedinhighprecision,i.e.,ﬂoat32.Thus,\\nasaconservativemeasure,wetendtokeeptheweightsinﬂ\\x00a\\x00\\x002.\\nThatsaid,thelossscalingstepisnotentirelynecessarybecause,inmy\\nexperience,theselittleupdatestypicallyappeartowardstheendstagesofthe\\nmodeltraining.Thus,itcanbefairtoassumethatsmallupdatesmaynot\\ndrasticallyimpactthemodelperformance.Butdon’ttakethisasadeﬁnite\\nconclusion,soit’ssomethingthatIwantyoutovalidatewhenyouusemixed\\nprecisiontraining.\\nMovingon,astheweights\\n(whicharematrices)are\\nrepresentedinﬂ\\x00a\\x00\\x002,\\nwecannotexpectthe\\nspeedupfrom\\nrepresentingthemin\\nﬂ\\x00a\\x00\\x006,iftheyremain\\nthisway:\\n37'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 38, 'page_label': '39', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nToleveragetheseﬂaot16-basedspeedups,herearethestepswefollow:\\n1. Wemakeaﬂ\\x00a\\x00\\x006copyofweightsduringtheforwardpass.\\n2. Next,wecomputethelossvalueinﬂ\\x00a\\x00\\x002andscaleittohavemore\\nprecisioningradients,whichworksinﬂ\\x00a\\x00\\x006.\\na. Thereasonwecomputegradientsinﬂ\\x00a\\x00\\x006isbecause,likeforward\\npass,gradientcomputationsalsoinvolvematrixmultiplications.\\nb. Thus,keepingtheminﬂ\\x00a\\x00\\x006canprovideadditionalspeedup.\\n3. Oncewehavecomputedthegradientsinﬂ\\x00a\\x00\\x006,theheavymatrix\\nmultiplicationoperationshavebeencompleted.Now,allweneedtodois\\nupdatetheoriginalweightmatrix,whichisinﬂ\\x00a\\x00\\x002.\\n4. Thus,wemakeaﬂ\\x00a\\x00\\x002copyoftheabovegradients,removethescalewe\\nappliedinStep2,andupdatetheﬂ\\x00a\\x00\\x002weights.\\n5. Done!\\n38'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 39, 'page_label': '40', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThemixed-precisionsettingsintheforwardpassarecarriedoutbythe\\n\\x00o\\x00\\x00h\\x00\\x00u\\x00\\x00c\\x00\\x00t\\x00\\x00contextmanager:\\nNow,it’stimetohandlethebackwardpass.\\n39'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 40, 'page_label': '41', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Line13→\\x00c\\x00\\x00e\\x00\\x00s\\x00\\x00l\\x00\\x00l\\x00\\x00s\\x00\\x00b\\x00\\x00k\\x00\\x00r\\x00\\x00):Thescalerobjectscalestheloss\\nvalueand\\x00a\\x00\\x00w\\x00\\x00d\\x00\\x00iscalledtocomputethegradients.\\n● Line14→\\x00c\\x00\\x00e\\x00\\x00s\\x00\\x00p\\x00\\x00p\\x00\\x00:Unscalegradientsandupdateweights.\\n● Line15→\\x00c\\x00\\x00e\\x00\\x00u\\x00\\x00a\\x00\\x00(\\x00:Updatethescaleforthenextiteration.\\n● Line16→\\x00p\\x00\\x00z\\x00\\x00o\\x00\\x00r\\x00\\x00(\\x00:Zerogradients.\\nDone!\\nTheeﬃcacyofmixedprecisionscalingovertraditionaltrainingisevidentfrom\\ntheimagebelow:\\nMixedprecisiontrainingisover\\x00.\\x00\\x00fasterthanconventionaltraining.\\n40'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 41, 'page_label': '42', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n  \\x00r\\x00\\x00i\\x00\\x00t\\x00h\\x00\\x00k\\x00\\x00i\\x00\\x00i\\x00\\x00\\nNeuralnetworksprimarilyutilizememoryintwoways:\\n1. Whentheystoremodelweights(thisisﬁxedmemoryutilization).\\n2. Whentheyaretrained(thisisdynamic).Ithappensintwoways:\\na. Duringforwardpasswhilecomputingandstoringactivationsofall\\nlayers.\\nb. Duringbackwardpasswhilecomputinggradientsateachlayer.\\nThelatter,i.e.,dynamicmemoryutilization,o\\x00enrestrictsusfromtraining\\nlargermodelswithbiggerbatchsizes.\\nThisisbecausememoryutilizationscalesproportionatelywiththebatchsize.\\nThatsaid,there’saprettyincredibletechniquethatletsusincreasethebatchsize\\nwhilemaintainingtheoverallmemoryutilization.\\nItiscalledGradientcheckpointing,andinmyexperience,it’sahighlyunderrated\\ntechniquetoreducethememoryoverheadsofneuralnetworks.\\nLet’sunderstandthisinmoredetail.\\n41'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 42, 'page_label': '43', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00r\\x00\\x00i\\x00\\x00t\\x00h\\x00\\x00\\x00p\\x00\\x00n\\x00\\x00\\x00g\\x00o\\x00\\x00s\\x00\\nGradientcheckpointingisbasedontwokeyobservationsonhowneural\\nnetworkstypicallywork:\\n1)Theactivationsofaspeciﬁclayer\\ncanbesolelycomputedusingthe\\nactivationsofthepreviouslayer.For\\ninstance,intheimagebelow,“Layer\\nB”activationscanbecomputedfrom\\n“LayerA”activationsonly.\\n2)Updatingtheweightsofalayeronly\\ndependsontwothings:\\na. Theactivationsofthatlayer.\\nb. Thegradientscomputedinthe\\nnext(right)layer(orrather,the\\nrunninggradients).\\nGradientcheckpointingexploitsthesetwoobservationstooptimizememory\\nutilization.Here’showitworks:\\n● Step1)Dividethenetworkintosegmentsbeforetheforwardpass:\\n42'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 43, 'page_label': '44', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Step2)Duringtheforwardpass,onlystoretheactivationsoftheﬁrstlayer\\nineachsegment.Discardtherestwhentheyhavebeenusedtocompute\\ntheactivationsofthenextlayer.\\n● Step3)Nowcomesbackpropagation.Toupdatetheweightsofalayer,we\\nneeditsactivations.Thus,werecomputethoseactivationsusingtheﬁrst\\nlayerinthatsegment.Forinstance,asshownintheimagebelow,toupdate\\ntheweightsoftheredlayers,werecomputetheiractivationsusingthe\\nactivationsofthecyanlayer,whicharealreadyavailableinmemory.\\nDone!\\nThisishowgradientcheckpointingworks.\\n43'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 44, 'page_label': '45', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTosummarize,theideaisthatwedon’tneedtostorealltheintermediate\\nactivationsinmemory.Instead,storingafewofthemandrecomputingtherest\\nonlywhentheyareneededcansigniﬁcantlyreducethememoryrequirement.The\\nwholeideamakesintuitivesenseaswell.\\nThisalsoallowsustotrainthe\\nnetworkonlargerbatchesofdata.\\nTypically,myobservationhasbeen\\nthatgradientcheckpointing(GCP)can\\nreducememoryusagebyatleast\\n50-60%,whichismassive.\\nOfcourse,aswecomputesomeactivationstwice,thisdoescomeatthecostof\\nincreasedrun-time,whichcantypicallyrangebetween15-25%.Sothere’salways\\natradeoﬀbetweenmemoryandrun-time.\\nThatsaid,anotheradvantageisthatitallowsustousealargerbatchsize,which\\ncanslightly(notentirelythough)countertheincreasedrun-time.\\nNonetheless,gradientcheckpointingisanextremelypowerfultechniquetotrain\\nlargermodels,whichIhavefoundtobeprettyhelpfulattimes,withoutresorting\\ntomoreintensivetechniqueslikedistributedtraining,forinstance.\\nThankfully,gradientcheckpointingisalsoimplementedbymanyopen-source\\ndeeplearningframeworkslikePytorch,etc.\\nHere’sademo.\\n44'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 45, 'page_label': '46', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00i\\x00\\x00t\\x00h\\x00\\x00\\x00p\\x00\\x00n\\x00\\x00\\x00g\\x00n\\x00y\\x00\\x00r\\x00\\x00\\nToutilizethis,webeginbyimportingthenecessarylibrariesandfunctions:\\nNext,wedeﬁneourneuralnetwork:\\nAsdemonstratedabove,intheforwardmethod,weusethe\\ncheckpoint_sequentialmethodtousegradientcheckpointinganddividethe\\nnetworkintotwosegments.\\nNext,wecanproceedwithnetworktrainingasweusuallywould.\\n45'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 46, 'page_label': '47', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00i\\x00\\x00t\\x00c\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\nUndermemory\\nconstraints,itis\\nalways\\nrecommendedto\\ntraintheneural\\nnetworkwitha\\nsmallbatchsize.\\nDespitethat,there’satechniquecalledgradientaccumulation,whichletsus\\n(logically)increasebatchsizewithoutexplicitlyincreasingthebatchsize.\\nConfused?\\nLet’sunderstandinthischapter.Butbeforethat,wemustunderstand…\\n\\x00h\\x00\\x00o\\x00e\\x00\\x00a\\x00\\x00e\\x00\\x00o\\x00\\x00s\\x00y\\x00\\x00c\\x00\\x00l\\x00\\x00x\\x00\\x00o\\x00\\x00\\x00u\\x00\\x00n\\x00\\x00r\\x00\\x00n\\x00\\x00g\\x00\\nTheprimarymemory\\noverheadinaneural\\nnetworkcomesfrom\\nbackpropagation.Thisis\\nbecause,during\\nbackpropagation,wemust\\nstorethelayeractivations\\ninmemory.A\\x00erall,they\\nareusedtocomputethe\\ngradients.\\nThebiggerthenetwork,themoreactivationsanetworkmuststoreinmemory.\\nAlso,undermemoryconstraints,havingalargebatchsizewillresultin:\\n● storingmanyactivations\\n● usingthosemanyactivationstocomputethegradients\\n46'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 47, 'page_label': '48', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThismayleadtomoreresourceconsumptionthanavailable—resultingin\\ntrainingfailure.Butbyreducingthebatchsize,wecanlimitthememoryusage\\nandtrainthenetwork.\\n\\x00h\\x00\\x00\\x00s\\x00r\\x00\\x00i\\x00\\x00t\\x00c\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\x00n\\x00\\x00o\\x00\\x00o\\x00\\x00\\x00t\\x00e\\x00\\x00\\x00n\\n\\x00n\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00h\\x00i\\x00\\x00\\x00n\\x00e\\x00\\x00r\\x00\\x00o\\x00\\x00t\\x00\\x00i\\x00\\x00s\\x00\\nConsiderwearetraininganeuralnetworkonmini-batches.\\nWetrainthenetworkasfollows:\\n● Oneverymini-batch:\\n○ Runtheforwardpasswhilestoringtheactivations.\\n○ Duringbackwardpass:\\n■ Computetheloss\\n■ Computethegradients\\n■ Updatetheweights\\nGradientaccumulationmodiﬁesthelaststepofthebackwardpass,i.e.,weight\\nupdates.Morespeciﬁcally,insteadofupdatingtheweightsoneverymini-batch,\\nwecandothis:\\n47'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 48, 'page_label': '49', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n1. Runtheforwardpassonamini-batch.\\n2. Computethegradientvaluesforweightsinthenetwork.\\n3. Don’tupdatetheweightsyet.\\n4. Runtheforwardpassonthenextmini-batch.\\n5. Computethegradientvaluesforweightsandaddthemtothegradients\\nobtainedinstep2.\\n6. Repeatsteps3-5forafewmoremini-batches.\\n7. Updatetheweightsonlya\\x00erprocessingafewmini-batches.\\nThistechniqueworks\\nbecauseaccumulatingthe\\ngradientsacrossmultiple\\nmini-batchesresultsinthe\\nsamesumofgradientsasif\\nwewereprocessingthem\\ntogether.Thus,logically\\nspeaking,usinggradient\\naccumulation,wecan\\nmimicalargerbatchsize\\nwithouthavingtoexplicitly\\nincreasethebatchsize.\\nForinstance,saywewanttouseabatchsizeof64.However,currentmemorycan\\nonlysupportabatchsizeof16.\\nNoworries!\\n● Wecanuseabatchsizeofsize16.\\n● Wecanaccumulatethegradientsfromeverymini-batch.\\n● Wecanupdatetheweightsonlyonceevery8mini-batches.\\nThus,eﬀectively,weusedabatchsizeof16*8(=128)insteadofwhatweoriginally\\nintended—64.\\n48'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 49, 'page_label': '50', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00m\\x00\\x00e\\x00\\x00n\\x00\\x00t\\x00\\x00n\\nLet’slookathowwecanimplementthis.InPyTorch,atypicaltrainingloopis\\nimplementedasfollows:\\n● Weclearthegradients\\n● Runtheforwardpass\\n● Computetheloss\\n● Computethegradients\\n● Updatetheweights\\nHowever,asdiscussedearlier,ifneeded,wecanonlyupdatetheweightsa\\x00era\\nfewiterations.Thus,wemustcontinuetoaccumulatethegradients,whichis\\npreciselywhat\\x00o\\x00\\x00.\\x00\\x00c\\x00\\x00a\\x00\\x00(\\x00does.\\n49'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 50, 'page_label': '51', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAlso,as\\x00p\\x00\\x00m\\x00\\x00e\\x00\\x00z\\x00\\x00o\\x00\\x00r\\x00\\x00(\\x00clearsthegradients,wemustonlyexecuteit\\na\\x00erupdatingtheweights.Thisideaisimplementedbelow:\\n● First,wedeﬁneacc_steps—thenumberofmini-batchesa\\x00erwhichwe\\nwanttoupdatetheweights.\\n● Next,weruntheforwardpass.\\n● Movingon,wecomputethelossandthegradients.\\n● Asdiscussedearlier,wewillnotupdatetheweightsyetandinsteadletthe\\ngradientsaccumulateforafewmoremini-batches.\\n● Weonlyupdatetheweightswhentheifconditionistrue.\\n● A\\x00erupdating,wecleartheaccumulatedgradients.\\nThisway,wecanoptimizeneuralnetworktraininginmemory-constrained\\nsettings.\\n50'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 51, 'page_label': '52', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00o\\x00\\x00\\nBeforeweend,itisessentialtonotethatgradientaccumulationisNOTaremedy\\ntoimproverun-timeinmemory-constrainedsituations.Infact,wecanalsoverify\\nthisfrommyexperiment:\\nInstead,itsobjectiveistoreduceoverallmemoryusage.\\nOfcourse,it’struethatweareupdatingtheweightsonlya\\x00erafewiterations.\\nSo,itwillbeabitfasterthanupdatingoneveryiteration.Yet,wearestill\\nprocessingandcomputinggradientsonsmallmini-batches,whichisthecore\\noperationhere.\\nNonetheless,thegoodthingisthatevenifyouarenotundermemoryconstraints,\\nyoucanstillusegradientaccumulation.\\n● Specifyyourtypicalbatchsize.\\n● Runforwardpass.\\n● Computelossandgradients.\\n● Updateonlya\\x00erafewiterations.\\nYoucandownloadthenotebookhere:https://bit.ly/3xNCfFt.\\n51'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 52, 'page_label': '53', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00t\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00u\\x00\\x00i\\x00\\x00P\\x00\\x00r\\x00\\x00n\\x00\\x00g\\nBydefault,deeplearningmodelsonlyutilizeasingleGPUfortraining,evenif\\nmultipleGPUsareavailable.\\nAnidealwaytoproceed(especiallyinbig-datasettings)istodistributethe\\ntrainingworkloadacrossmultipleGPUs.Thegraphicbelowdepictsfour\\ncommonstrategiesformulti-GPUtraining:\\n52'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 53, 'page_label': '54', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00o\\x00\\x00l\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00m\\n● Diﬀerentparts(orlayers)ofthemodelareplacedondiﬀerentGPUs.\\n● UsefulforhugemodelsthatdonotﬁtonasingleGPU.\\n● However,modelparallelismalsointroducesseverebottlenecksasit\\nrequiresdataﬂowbetweenGPUswhenactivationsfromoneGPUare\\ntransferredtoanotherGPU.\\n\\x002\\x00\\x00e\\x00\\x00o\\x00\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00m\\n● Distributesandprocessesindividualtensoroperationsacrossmultiple\\ndevicesorprocessors.\\n● Itisbasedontheideathatalargetensoroperation,suchasmatrix\\nmultiplication,canbedividedintosmallertensoroperations,andeach\\nsmalleroperationcanbeexecutedonaseparatedeviceorprocessor.\\n53'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 54, 'page_label': '55', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Suchparallelizationstrategiesareinherentlybuiltintostandard\\nimplementationsofPyTorchandotherdeeplearningframeworks,butthey\\nbecomemuchmorepronouncedinadistributedsetting.\\n\\x003\\x00\\x00a\\x00\\x00\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00m\\n● ReplicatethemodelacrossallGPUs.\\n● Dividetheavailabledataintosmallerbatches,andeachbatchisprocessed\\nbyaseparateGPU.\\n● Theupdates(orgradients)fromeachGPUarethenaggregatedandusedto\\nupdatethemodelparametersoneveryGPU.\\n\\x004\\x00\\x00i\\x00\\x00l\\x00\\x00e\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00m\\n● Thisiso\\x00enconsideredacombinationofdataparallelismandmodel\\nparallelism.\\n54'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 55, 'page_label': '56', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Sotheissuewithstandardmodelparallelismisthat1stGPUremainsidle\\nwhendataisbeingpropagatedthroughlayersavailablein2ndGPU:\\n● Pipelineparallelismaddressesthisbyloadingthenextmicro-batchofdata\\noncethe1stGPUhasﬁnishedthecomputationsonthe1stmicro-batch\\nandtransferredactivationstolayersavailableinthe2ndGPU.Theprocess\\nlookslikethis:\\n○ 1stmicro-batchpassesthroughthelayerson1stGPU.\\n○ 2ndGPUreceivesactivationson1stmicro-batchfrom1stGPU.\\n○ Whilethe2ndGPUpassesthedatathroughthelayers,another\\nmicro-batchisloadedonthe1stGPU.\\n○ Andtheprocesscontinues.\\n● GPUutilizationdrasticallyimprovesthisway.Thisisevidentfromthe\\nanimationbelowwheremulti-GPUsarebeingutilizedatthesame\\ntimestamp(lookat\\x00=\\x00\\x00\\x00=\\x00\\x00\\x00=\\x00\\x00andt=6):\\nThosewerefourcommonstrategiesformulti-GPUtraining.\\n55'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 56, 'page_label': '57', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00e\\x00\\x00a\\x00\\x00o\\x00\\x00\\n\\x00a\\x00\\x00l\\x00m\\x00\\x00t\\x00\\x00n\\x00\\nForeveryinstanceinsingle-labelclassiﬁcationdatasets,theentireprobability\\nmassbelongstoasingleclass,andtherestarezero.Thisisdepictedbelow:\\nTheissueisthat,attimes,suchlabeldistributionsexcessivelymotivatethemodel\\ntolearnthetrueclassforeverysamplewithprettyhighconﬁdence.Thiscan\\nimpactitsgeneralizationcapabilities.\\nLabelsmoothingisalesser-talkedregularisationtechniquethatelegantly\\naddressesthisissue.\\n56'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 57, 'page_label': '58', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove,withlabelsmoothing:\\n● Weintentionallyreducetheprobabilitymassofthetrueclassslightly.\\n● Thereducedprobabilitymassisuniformlydistributedtoallotherclasses.\\nSimplyput,thiscanbethoughtofasaskingthemodeltobe“lessoverconﬁdent”\\nduringtrainingandpredictionwhilestillattemptingtomakeaccurate\\npredictions.\\nTheeﬃcacyofthistechniqueisevidentfromtheimagebelow:\\nInthisexperiment,ItrainedtwoneuralnetworksontheFashionMNISTdataset\\nwiththeexactsameweightinitialization.\\n● Onewithoutlabelsmoothing.\\n● Anotherwithlabelsmoothing.\\nThemodelwithlabelsmoothingresultedinabettertestaccuracy,i.e.,better\\ngeneralization.\\n57'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 58, 'page_label': '59', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00\\x00o\\x00\\x00o\\x00s\\x00\\x00a\\x00\\x00l\\x00m\\x00\\x00t\\x00\\x00n\\x00\\x00\\nA\\x00erusinglabelsmoothingformanyofmyprojects,Ihavealsorealizedthatitis\\nnotwellsuitedforallusecases.Soit’simportanttoknowwhenyoushouldnot\\nuseit.\\nSee,ifyouonlycareaboutgettingtheﬁnalpredictioncorrectandimproving\\ngeneralization,labelsmoothingwillbeaprettyhandytechnique.However,I\\nwouldn’trecommendutilizingitifyoucareabout:\\n● Gettingthepredictioncorrect.\\n● Andunderstandingthemodel’sconﬁdenceingeneratingaprediction.\\nThisisbecauseaswediscussedabove,labelsmoothingguidesthemodelto\\nbecome“lessoverconﬁdent”aboutitspredictions.Thus,wetypicallynoticea\\ndropintheconﬁdencevaluesforeveryprediction,asdepictedbelow:\\nOnaspeciﬁctestinstance:\\n● Themodelwithoutlabelsmoothingoutputs99%probabilityforclass3.\\n● Withlabelsmoothing,althoughthepredictionisstillcorrect,the\\nconﬁdencedropsto74%.\\nThisissomethingtokeepinmindwhenusinglabelsmoothing.Nonetheless,the\\ntechniqueisindeedprettypromisingforregularizingdeeplearningmodels.You\\ncandownloadthecodenotebookforthischapterhere:https://bit.ly/4ePt08d.\\n58'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 59, 'page_label': '60', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00l\\x00o\\x00\\x00\\nBinaryclassiﬁcation\\ntasksaretypically\\ntrainedusingthe\\nbinarycrossentropy\\n(BCE)lossfunction:\\nFornotationalconvenience,ifwedeﬁnep\\x00asthefollowing:\\n…thenwecanalsowritethecross-entropylossfunctionas:\\nThatsaid,onelimitationofBCElossisthatitweighsprobabilitypredictionsfor\\nbothclassesequally,whichisevidentfromitssymmetry:\\nFormoreclarity,considerthetablebelow,whichdepictstwoinstances,onefrom\\ntheminorityclassandanotherfromthemajorityclass,bothwiththesameloss:\\n59'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 60, 'page_label': '61', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThiscausesproblemswhenweuseBCEforimbalanceddatasets,whereinmost\\ninstancesfromthedominatingclassare“easilyclassiﬁable.”Thus,alossvalueof,\\nsay,\\x00l\\x00\\x00(\\x00\\x003\\x00fromthemajorityclassinstanceshould(ideally)beweighedLESS\\nthanthesamelossvaluefromtheminorityclass.\\nFocallossisaprettyhandyandusefulalternativetoaddressthisissue.Itis\\ndeﬁnedasfollows:\\nAsdepictedabove,itintroducesanadditionalmultiplicativefactorcalled\\ndownweighing,andtheparameterγ\\x00G\\x00\\x00m\\x00\\x00isahyperparameter.\\nPlottingBCE(classy=1)andFocalloss(forclassy=1andγ=3),wegetthe\\nfollowingcurve:\\n60'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 61, 'page_label': '62', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsshownintheﬁgureabove,focallossreducesthecontributionofthe\\npredictionsthemodelisprettyconﬁdentabout.Also,thehigherthevalueofγ\\n(Gamma),themoredownweighingtakesplace,asshowninthisplotbelow:\\nMovingon,whiletheFocallossfunctionreducesthecontributionofconﬁdent\\npredictions,wearen’tdoneyet.\\nThefocallossfunctionnowisstillsymmetriclikeBCE:\\n61'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 62, 'page_label': '63', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nToaddressthis,wemustaddanotherweighingparameter(α),whichisthe\\ninverseoftheclassfrequency,asdepictedbelow:\\nTheαparameteristheinverseoftheclassfrequencyThus,theﬁnallossfunction\\ncomesouttobethefollowing:\\nByusingbothdownweighingandinverseweighing,themodelgraduallylearns\\npatternsspeciﬁctothehardexamplesinsteadofalwaysbeingoverlyconﬁdentin\\npredictingeasyinstances.\\n62'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 63, 'page_label': '64', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTotesttheeﬃcacyoffocalloss\\ninaclassimbalancesetting,I\\ncreatedadummyclassiﬁcation\\ndatasetwitha90:10imbalance\\nratio:\\nNext,Itrainedtwoneuralnetworkmodels(withthesamearchitectureof2\\nhiddenlayers):\\n● OnewithBCEloss\\n● AnotherwithFocalloss\\nThedecisionregionplotandtestaccuracyforthesetwomodelsisdepicted\\nbelow:\\nItisclearthat:\\n● ThemodeltrainedwithBCEloss(le\\x00)alwayspredictsthemajorityclass.\\n● Themodeltrainedwithfocalloss(right)focusesrelativelymoreon\\nminorityclasspatterns.Asaresult,itperformsbetter.\\nDownloadthisJupyternotebooktogetstartedwithFocalloss:\\nhttps://bit.ly/45XzNZC.\\n63'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 64, 'page_label': '65', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00r\\x00\\x00o\\x00\\x00\\x00c\\x00\\x00a\\x00\\x00y\\x00o\\x00\\x00s\\x00\\nSometimeback,IwasinvitedbyatechstartuptoconducttheirMLinterviews.I\\ninterviewed12candidatesandmostlyaskedpracticalMLquestions.\\nHowever,thereweresomeconceptualquestionsaswell,liketheonebelow,\\nwhichIintentionallyaskedeverycandidate:\\n\\x00o\\x00\\x00o\\x00\\x00\\x00r\\x00\\x00o\\x00\\x00\\x00o\\x00\\x00?\\nPrettysimple,right?Apparently,everycandidategavemeanincompleteanswer,\\nwhichIhavementionedbelow:\\n\\x00a\\x00\\x00i\\x00\\x00t\\x00\\x00’\\x00n\\x00\\x00e\\x00\\nInagist,theideaistozerooutneuronsrandomlyinaneuralnetwork.Thisis\\ndonetoregularizethenetwork.\\nDropoutisonlyappliedduringtraining,andwhichneuronactivationstozeroout\\n(ordrop)isdecidedusingaBernoullidistribution:\\n“p”isthedropoutprobabilityspeciﬁedin,say,PyTorch→nn.Dropout(p).\\n64'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 65, 'page_label': '66', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00y\\x00o\\x00\\x00o\\x00\\x00u\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00\\x00s\\x00h\\x00\\x00e\\x00n\\x00\\x00h\\x00\\x00g\\x00l\\x00\\x00\\x00h\\x00\\x00\\x00e\\x00o\\x00n\\x00r\\x00\\x00o\\x00\\x00?\\n\\x00a\\x00\\x00i\\x00\\x00t\\x00\\x00:\\x00o\\x00\\x00h\\x00\\x00\\x00s\\x00t\\x00\\x00e\\x00n\\x00\\x00\\x00e\\x00\\x00\\x00u\\x00\\x00e\\x00\\x00o\\x00\\x00\\x00n\\x00\\x00r\\x00\\x00n\\x00h\\x00\\n\\x00e\\x00\\x00o\\x00\\x00\\x00s\\x00e\\x00s\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00d\\x00\\n\\x00o\\x00\\x00\\x00o\\x00\\x00n\\x00\\x00a\\x00\\x00\\x00o\\x00h\\x00\\x00o\\x00\\x00c…\\nOfcourse,Iamnotsayingthattheabovedetailsareincorrect.Theyarecorrect.\\nHowever,thisisjust50%ofhowDropoutworks,anddisappointingly,most\\nresourcesdon’tcovertheremaining50%.Ifyoutooareonlyawareofthe50%\\ndetailsImentionedabove,continuereading.\\n\\x00o\\x00\\x00r\\x00\\x00o\\x00\\x00\\x00c\\x00\\x00a\\x00\\x00y\\x00o\\x00\\x00s\\x00\\nTobegin,wemustnotethatDropoutisonlyappliedduringtraining,butnot\\nduringtheinference/evaluationstage:\\nNow,considerthataneuron’sinputiscomputedusing100neuronsinthe\\nprevioushiddenlayer:\\n65'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 66, 'page_label': '67', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForsimplicity,let’sassumeacoupleofthingshere:\\n● Theactivationofeveryyellowneuronis1.\\n● Theedgeweightfromtheyellowneuronstotheblueneuronisalso1.\\nAsaresult,theinputreceivedbytheblueneuronwillbe100,asdepictedbelow:\\nNow,duringtraining,ifwewereusingDropoutwith,say,a40%dropoutrate,\\nthenroughly40%oftheyellowneuronactivationswouldhavebeenzeroedout.\\nAsaresult,theinputreceivedbytheblueneuronwouldhavebeenaround60:\\n66'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 67, 'page_label': '68', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHowever,theabovepointisonlyvalidforthetrainingstage.\\nIfthesamescenariohadexistedduringtheinferencestageinstead,thenthe\\ninputreceivedbytheblueneuronwouldhavebeen100.\\nThus,undersimilarconditions:\\n● Theinputreceivedduringtraining→60.\\n● Theinputreceivedduringinference→100.\\nDoyouseeanyproblemhere?\\nDuringtraining,theaverageneuroninputsaresigniﬁcantlylowerthanthose\\nreceivedduringinference.\\nMoreformally,usingDropoutsigniﬁcantlyaﬀectsthescaleoftheactivations.\\nHowever,itisdesiredthattheneuronsthroughoutthemodelmustreceivethe\\nroughlysamemean(orexpectedvalue)ofactivationsduringtrainingand\\ninference.Toaddressthis,Dropoutperformsoneadditionalstep.\\nThisideaistoscaletheremainingactiveinputsduringtraining.Thesimplest\\nwaytodothisisbyscalingallactivationsduringtrainingbyafactorof\\x00/\\x00\\x00-\\x00\\x00,\\nwherepisthedropoutrate.Forinstance,usingthistechniqueontheneuron\\ninputof60,wegetthefollowing(recallthatwesetp=40%):\\n67'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 68, 'page_label': '69', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove,scalingtheneuroninputbringsittothedesiredrange,which\\nmakestrainingandinferencestagescoherentforthenetwork.\\n\\x00e\\x00\\x00f\\x00\\x00\\x00g\\x00x\\x00\\x00\\x00i\\x00\\x00n\\x00\\x00l\\x00\\x00\\nInfact,wecanverifythattypicalimplementationsofDropout,fromPyTorch,for\\ninstance,docarryoutthisstep.Let’sdeﬁneadropoutlayerasfollows:\\nNow,let’sconsiderarandomtensorandapplythisdropoutlayertoit:\\nAsdepictedabove,theretainedvalueshaveincreased.\\n● Thesecondvaluegoesfrom0.13→0.16.\\n● Thethirdvaluegoesfrom0.93→1.16.\\n● andsoon…\\nWhat’smore,theretainedvaluesarepreciselythesameaswewouldhave\\nobtainedbyexplicitlyscalingtheinputtensorwith1/(1-p):\\n68'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 69, 'page_label': '70', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfweweretodothesamethinginevaluationmodeinstead,wenoticethatno\\nvalueisdroppedandnoscalingtakesplaceeither,whichmakessenseasDropout\\nisonlyusedduringtraining:\\nThisistheremaining50%details,which,inmyexperience,mostresourcesdo\\nnotcover,andasaresult,mostpeoplearen’tawareof.\\nButitisahighlyimportantstepinDropout,whichmaintainsnumerical\\ncoherencebetweentrainingandinferencestages.\\nWiththat,nowyouknow100%ofhowDropoutworks.\\nNext,let’sdiscussanissuewithDropoutincaseofCNNs.\\n69'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 70, 'page_label': '71', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00s\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00o\\x00\\x00\\x00n\\x00N\\x00\\x00\\nWhenitcomestotrainingneuralnetworks,itisalwaysrecommendedtouse\\nDropouttoimproveitsgeneralizationpower.\\nThisappliesnotjusttoCNNsbuttoallotherneuralnetworks.AndIamsureyou\\nalreadyknowtheabovedetails,solet’sgetintotheinterestingpart.\\n\\x00h\\x00\\x00r\\x00\\x00l\\x00\\x00\\x00f\\x00s\\x00\\x00g\\x00r\\x00\\x00o\\x00\\x00\\x00n\\x00N\\x00\\x00\\nThecoreoperationthatmakesCNNssopowerfulisconvolution,whichallows\\nthemtocapturelocalpatterns,suchasedgesandtextures,andhelpsextract\\nrelevantinformationfromtheinput.\\nFromapurelymathematicalperspective,weslideaﬁlter(showninyellowbelow)\\novertheinput(showningreenbelow)andtaketheelement-wisesumbetween\\ntheﬁlterandtheoverlappedinputtogettheconvolutionoutput:\\nHere,ifweretoapplythetraditionalDropout,theinputfeatureswouldlook\\nsomethinglikethis:\\n70'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 71, 'page_label': '72', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInfullyconnectedlayers,wezerooutneurons.InCNNs,however,werandomly\\nzerooutthepixelvaluesbeforeconvolution,asdepictedabove.\\nButthisisn’tfoundtobethateﬀectivespeciﬁcallyforconvolutionlayers.To\\nunderstandthis,considerwehavesomeimagedata.Ineveryimage,wewould\\nﬁndthatnearbyfeatures(orpixels)arehighlycorrelatedspatially.\\nForinstance,imaginezoominginonthepixellevelofthedigit‘9’.Here,we\\nwouldnoticethattheredpixel(orfeature)ishighlycorrelatedwithotherfeatures\\ninitsvicinity:\\nThus,droppingtheredfeatureusingDropoutwilllikelyhavenoeﬀectandits\\ninformationcanstillbesenttothenextlayer.\\nSimplyput,thenatureoftheconvolutionoperationdefeatstheentirepurposeof\\nthetraditionalDropoutprocedure.\\n71'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 72, 'page_label': '73', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00o\\x00\\x00t\\x00\\x00n\\nDropBlockisamuchbetter,eﬀective,andintuitivewaytoregularizeCNNs.The\\ncoreideainDropBlockistodropacontiguousregionoffeatures(orpixels)\\nratherthanindividualpixels.Thisisdepictedbelow:\\nSimilartoDropoutinfullyconnectedlayers,whereinthenetworktriesto\\ngeneratemorerobustwaystoﬁtthedataintheabsenceofsomeactivations,in\\nthecaseofDropBlock,theconvolutionlayersgetmorerobusttoﬁtthedata\\ndespitetheabsenceofablockoffeatures.\\nMoreover,theideaofDropBlockalsomakesintuitivesense—ifacontiguous\\nregionofafeatureisdropped,theproblemofusingDropoutwithconvolution\\noperationcanbeavoided.\\n\\x00r\\x00\\x00B\\x00\\x00c\\x00\\x00a\\x00\\x00m\\x00\\x00e\\x00\\x00\\nDropBlockhastwomainparameters:\\n● \\x00l\\x00\\x00k\\x00\\x00i\\x00\\x00:Thesizeoftheboxtobedropped.\\n● \\x00r\\x00\\x00_\\x00\\x00t\\x00:Thedropprobabilityofthecentralpixel.\\n72'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 73, 'page_label': '74', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nToapplyDropBlock,ﬁrst,wecreateabinarymaskontheinputsampledfromthe\\nBernoullidistribution:\\nNext,wecreateablockofsize\\x00l\\x00\\x00k\\x00\\x00i\\x00\\x00*\\x00\\x00o\\x00\\x00_\\x00\\x00z\\x00whichhasthesampled\\npixelsatthecenter:\\nTheeﬃcacyofDropBlockoverDropoutisevidentfromtheresultstablebelow:\\nOntheImageNetclassiﬁcationdataset:\\n● DropBlockprovidesa1.33%gainoverDropout.\\n● DropBlockwithLabelsmoothing(discussedinthelastchapter)providesa\\n1.55%gainoverDropout.\\n73'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 74, 'page_label': '75', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThankfully,DropBlockisalsointegratedwithPyTorch.\\nThere’salsoalibraryforDropBlock,called“\\x00r\\x00\\x00b\\x00\\x00c\\x00,”whichalsoprovidesthe\\nlinearschedulerfordrop_rate.\\nSothethingisthattheresearcherswhoproposedDropBlockfoundthe\\ntechniquetobemoreeﬀectivewhenthedrop_ratewasincreasedgradually.\\nTheDropBlocklibraryimplementsthescheduler.Butofcourse,therearewaysto\\ndothisinPyTorchaswell.Soit’sentirelyuptoyouwhichimplementationyou\\nwanttouse:\\n● DropBlockPyTorch:https://bit.ly/3xZfT3E.\\n● DropBlocklibrary:https://github.com/miguelvr/dropblock.\\n74'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 75, 'page_label': '76', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00 \\x00i\\x00\\x00e\\x00 \\x00a\\x00\\x00r\\x00 \\x00n\\x00 \\x00c\\x00\\x00v\\x00\\x00i\\x00\\x00 \\x00u\\x00\\x00t\\x00\\x00n\\x00\\n\\x00c\\x00\\x00a\\x00\\x00y\\x00o\\nEveryoneknowstheobjectiveofanactivationfunctioninaneuralnetwork.They\\nletthenetworklearnnon-linearpatterns.Thereisnothingnewhere,andIam\\nsureyouareawareofthattoo.\\nHowever,onethingIhaveo\\x00en\\nrealizedisthatmostpeoplestruggleto\\nbuildanintuitiveunderstandingof\\nwhatexactlyaneuralnetwork\\nconsistentlytriestoachieveduringits\\nlayer-a\\x00er-layertransformations.\\nInthischapter,letmeshareauniqueperspectiveonthis,whichwouldreallyhelp\\nyouunderstandtheinternalworkingsofaneuralnetwork.\\nIhavesupportedthischapterwithplentyofvisualsforbetterunderstanding.\\nAlso,forsimplicity,weshallconsiderabinaryclassiﬁcationusecase.\\n\\x00a\\x00\\x00g\\x00\\x00u\\x00\\x00\\nThedataundergoesaseriesoftransformationsateachhiddenlayer:\\n● Lineartransformationofthedataobtainedfromthepreviouslayer\\n● …followedbyanon-linearityusinganactivationfunction—ReLU,\\nSigmoid,Tanh,etc.\\n75'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 76, 'page_label': '77', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheabovetransformationsareperformedoneveryhiddenlayerofaneural\\nnetwork.Now,noticesomethinghere.\\nAssumethatwejustappliedthe\\nabovedatatransformationonthe\\nverylasthiddenlayeroftheneural\\nnetwork.Oncewedothat,the\\nactivationsprogresstowardthe\\noutputlayerofthenetworkforone\\nﬁnaltransformation,whichis\\nentirelylinear.\\nTheabovetransformationisentirelylinearbecauseallsourcesofnon-linearity\\n(activationsfunctions)existonorbeforethelasthiddenlayer.Andduringthe\\nforwardpass,oncethedataleavesthelasthiddenlayer,thereisnofurtherscope\\nfornon-linearity.\\nThus,tomakeaccuratepredictions,thedatareceivedbytheoutputlayerfrom\\nthelasthiddenlayerMUSTBElinearlyseparable.\\n76'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 77, 'page_label': '78', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTosummarize….\\nWhiletransformingthedatathroughallitshiddenlayersandjustbefore\\nreachingtheoutputlayer,aneuralnetworkisconstantlyhustlingtoprojectthe\\ndatatoaspacewhereitsomehowbecomeslinearlyseparable.Ifitdoes,the\\noutputlayerbecomesanalogoustoalogisticregressionmodel,whichcaneasily\\nhandlethislinearlyseparabledata.\\nInfact,wecanalsoverifythisexperimentally.\\nTovisualizetheinputtransformation,wecanaddadummyhiddenlayerwith\\njusttwoneuronsrightbeforetheoutputlayerandtraintheneuralnetworkagain.\\nWhydoweaddalayerwithjusttwoneurons?\\nThisway,wecaneasilyvisualizethetransformation.Weexpectthatifweplot\\ntheactivationsofthis2Ddummyhiddenlayer,theymustbelinearlyseparable.\\nThebelowvisualpreciselydepictsthis.\\n77'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 78, 'page_label': '79', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAswenoticeabove,whiletheinputdatawaslinearlyinseparable,theinput\\nreceivedbytheoutputlayerisindeedlinearlyseparable.\\nThistransformeddatacanbeeasilyhandledbytheoutputclassiﬁcationlayer.\\nAndthisshowsthatallaneuralnetworkistryingtodoistransformthedatainto\\nalinearlyseparableformbeforereachingtheoutputlayer.\\n78'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 79, 'page_label': '80', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00ﬄ\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00r\\x00\\x00r\\x00\\x00n\\x00\\x00g\\nDeeplearningmodelsmayfail\\ntoconvergeduetovarious\\nreasons.Somecausesare\\nobviousandcommon,and\\ntherefore,quicklyrectiﬁable,\\nliketoohigh/lowlearning\\nrate,nodatanormalization,\\nnobatchnormalization,etc.\\nButtheproblemariseswhenthecauseisn’tthatapparent.Therefore,itmaytake\\nsomeserioustimetodebugifyouareunawareofthem.Inthischapter,Iwantto\\ntalkaboutonesuchdata-relatedmistake,whichIoncecommittedduringmy\\nearlydaysinmachinelearning.Admittedly,ittookmequitesometimetoﬁgure\\nitoutbackthenbecauseIhadnoideaabouttheissue.\\n\\x00x\\x00\\x00r\\x00\\x00e\\x00\\x00\\nConsideraclassiﬁcationneuralnetworktrainedusingmini-batchgradient\\ndescent.\\n\\x00i\\x00\\x00-\\x00\\x00t\\x00\\x00\\x00r\\x00\\x00i\\x00\\x00t\\x00e\\x00\\x00e\\x00\\x00:\\x00p\\x00\\x00t\\x00\\x00e\\x00\\x00o\\x00\\x00\\x00e\\x00\\x00h\\x00\\x00\\x00s\\x00\\x00g\\x00\\x00e\\x00\\x00a\\x00\\x00\\n\\x00o\\x00\\x00t\\x00\\x00t\\x00\\x00i\\x00\\x00.\\n79'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 80, 'page_label': '81', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHere,wetraintwodiﬀerentneural\\nnetworks:\\n● Version1:Thedatasetis\\norderedbylabels.\\n● Version2:Thedatasetis\\nproperlyshuﬄedbylabels.\\nAnd,ofcourse,beforetraining,weensurethatbothnetworkshadthesame\\ninitialweights,learningrate,andothersettings.\\nTheimagedepictsthe\\nepoch-by-epoch\\nperformanceofthetwo\\nmodels.Onthele\\x00,we\\nhavethemodeltrained\\nonlabel-ordereddata,\\nandtheoneontheright\\nwastrainedonthe\\nshuﬄeddataset.\\nItisclearthatthemodelreceivingalabel-ordereddatasetmiserablyfailsto\\nconvergewhiletheothermodel,althoughoverﬁts,showsthatmodelhasbeen\\nlearneﬀectively.\\n\\x00h\\x00\\x00o\\x00\\x00\\x00h\\x00\\x00\\x00a\\x00\\x00e\\x00\\x00\\nNow,ifyouthinkaboutitforasecond,overall,bothmodelsreceivedthesame\\ndata,didn’tthey?Yet,theorderinwhichthedatawasfedtothesemodelstotally\\ndeterminedtheirperformance.IvividlyrememberthatwhenIfacedthisissue,I\\nknewthatmydatawasorderedbylabels.\\n80'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 81, 'page_label': '82', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nYet,itneveroccurredtomethatorderingmayinﬂuencethemodelperformance\\nbecausethedatawillalwaysbethesameregardlessoftheordering.\\nButlater,Irealizedthatthispoint\\nwillonlybevalidwhenthemodel\\nseestheentiredataandupdates\\nthemodelweightsinonego,i.e.,\\ninbatchgradientdescent,as\\ndepictedinthisimage.\\nButinthecaseofmini-batchgradientdescent,theweightsareupdateda\\x00er\\neverymini-batch.Thus,thepredictionandweightupdateonasubsequent\\nmini-batchisinﬂuencedbythepreviousmini-batches.\\nInthecontextoflabel-ordereddata,wheresamplesofthesameclassaregrouped\\ntogether,mini-batchgradientdescentwillleadthemodeltolearnpatterns\\nspeciﬁctotheclassitexcessivelysawearlyonintraining.Incontrast,randomly\\nordereddataensuresthateachmini-batchcontainsabalancedrepresentationof\\nclasses.Thisallowsthemodeltolearnamorecomprehensivesetoffeatures\\nthroughoutthetrainingprocess.\\nOfcourse,theideaofshuﬄingisnotvalidfortime-seriesdatasetsastheir\\ntemporalstructureisimportant.Thegoodthingisthatifyouhappentouse,say,\\nPyTorchDataLoader,youaresafe.Thisisbecauseitalreadyimplements\\nshuﬄing.Butifyouhaveacustomimplementation,ensurethatyouarenot\\nmakinganysucherror.\\nBeforeIend,onethingthatyoumustALWAYSrememberwhentrainingneural\\nnetworksisthatthesemodelscanproﬁcientlylearnentirelynon-existing\\npatternsaboutyourdataset.Sonevergivethemanychancetodoso.\\n81'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 82, 'page_label': '83', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00l\\x00o\\x00\\x00r\\x00\\x00s\\x00\\x00n\\n\\x00n\\x00\\x00l\\x00\\x00g\\x00\\x00i\\x00\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00o\\x00\\x00l\\x00o\\x00\\x00r\\x00\\x00s\\x00\\x00n\\nModelaccuracyalone(oranequivalentperformancemetric)rarelydetermines\\nwhichmodelwillbedeployed.\\nThisisbecausewealsoconsiderseveraloperationalmetrics,suchas:\\n● InferenceLatency:Timetakenbythemodeltoreturnaprediction.\\n● Modelsize:Thememoryoccupiedbythemodel.\\n● Easeofscalability,etc.\\nInthischapter,letmeshareatechnique(withademo)calledknowledge\\ndistillation,whichiscommonlyusedtocompressMLmodelsandcontributeto\\ntheaboveoperationalmetrics.\\n\\x00h\\x00\\x00\\x00s\\x00n\\x00\\x00l\\x00\\x00g\\x00\\x00i\\x00\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00\\nInagist,theideaistotrainasmaller/simplermodel(calledthe“student”model)\\nthatmimicsthebehaviorofalarger/complexmodel(calledthe“teacher”model).\\n82'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 83, 'page_label': '84', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisinvolvestwosteps:\\n● Traintheteachermodelaswetypicallywould.\\n● Trainastudentmodelthatmatchestheoutputoftheteachermodel.\\nIfwecompareittoanacademicteacher-studentscenario,thestudentmaynotbe\\nasperformantastheteacher.\\nButwithconsistenttraining,asmallermodelmayget(almost)asgoodasthe\\nlargerone.\\nAclassicexampleofamodeldevelopedinthiswayisDistillBERT.Itisastudent\\nmodelofBERT.\\n● DistilBERTisapproximately40%smallerthanBERT,whichisamassive\\ndiﬀerenceinsize.\\n● Still,itretainsapproximately97%oftheBERT’scapabilities.\\nNext,let’slookatademo.\\n83'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 84, 'page_label': '85', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00l\\x00\\x00g\\x00\\x00i\\x00\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00e\\x00\\x00\\nIntheinterestoftime,let’ssaywehavealreadytrainedthefollowingCNNmodel\\nontheMNISTdataset(IhaveprovidedthefullJupyternotebooktowardstheend,\\ndon’tworry):\\nTheepoch-by-epochtraininglossandvalidationaccuracyisdepictedbelow:\\nNext,let’sdeﬁneasimplermodelwithoutanyconvolutionallayers:\\n84'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 85, 'page_label': '86', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nBeingaclassiﬁcationmodel,theoutputwillbeaprobabilitydistributionoverthe\\n<N>classes:\\nThus,wecantrainthestudentmodelsuchthatitsprobabilitydistribution\\nmatchesthatoftheteachermodel.\\nOnewaytodothisistouseKLdivergenceasalossfunction.\\n85'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 86, 'page_label': '87', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItmeasureshowmuchinformationislostwhenweusedistributionQto\\napproximatedistributionP.\\n\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00o\\x00\\x00o\\x00\\x00\\x00h\\x00\\x00\\x00i\\x00\\x00\\x00e\\x00h\\x00\\x00L\\x00i\\x00\\x00r\\x00\\x00\\x00c\\x00\\x00f\\x00=\\x00\\x00\\nThus,inourcase:\\n● Pwillbetheprobabilitydistributionfromtheteachermodel.\\n● Qwillbetheprobabilitydistributionfromthestudentmodel.\\nThelossfunctionisimplementedbelow:\\nFinally,wetrainthestudentmodelasfollows:\\n86'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 87, 'page_label': '88', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nDone!\\nThefollowingimagecomparesthetraininglossandvalidationaccuracyofthe\\ntwomodels:\\nOfcourse,asshowninthehighlightedlinesabove,theperformanceofthe\\nstudentmodelisnotasgoodastheteachermodel,whichisexpected.\\nHowever,itisstillprettypromising,giventhatitwasonlycomposedofsimple\\nfeed-forwardlayers.\\nAlso,asdepictedbelow,thestudentmodelisapproximately35%fasterthanthe\\nteachermodel,whichisasigniﬁcantincreaseintheinferencerun-timeofthe\\nmodelforabouta\\x00-\\x00\\x00dropinthetestaccuracy.\\n87'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 88, 'page_label': '89', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThatsaid,oneofthebiggestdownsidesofknowledgedistillationisthatonemust\\nstilltrainalargerteachermodelﬁrsttotrainthestudentmodel.\\nButinaresource-constrainedenvironment,itmaynotbefeasibletotrainalarge\\nteachermodel.\\nSothistechniqueassumesthatwearenotresource-constrainedatleastinthe\\ndevelopmentenvironment.\\nInthenextchapter,let’sdiscussonemoretechniquetocompressMLmodelsand\\nreducetheirmemoryfootprint.\\n88'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 89, 'page_label': '90', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00c\\x00\\x00v\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00i\\x00\\x00\\nOncewecompletenetworktraining,wearealmostalwaysle\\x00withplentyof\\nuselessneurons—onesthatmakenearlyzerocontributiontothenetwork’s\\nperformance,buttheystillconsumememory.\\nInotherwords,thereisahighpercentageofneurons,which,ifremovedfromthe\\ntrainednetwork,willnotaﬀecttheperformanceremarkably:\\nAnd,ofcourse,Iamnotsayingthisasarandomanduninformedthought.Ihave\\nexperimentallyveriﬁedthisoverandoveracrossmyprojects.\\nHere’sthecoreidea.\\nA\\x00ertrainingiscomplete,werunthe\\ndatasetthroughthemodel(no\\nbackpropagationthistime)and\\nanalyzetheaverageactivationof\\nindividualneurons.Here,weo\\x00en\\nobservethatmanyneuronactivations\\narealwaysclosetonear-zerovalues.\\nThus,theycanbeprunedfromthenetwork,astheywillhaveverylittleimpacton\\nthemodel’soutput.\\nForpruning,wecandecideonapruningthreshold(λ)andpruneallneurons\\nwhoseactivationsarelessthanthisthreshold.\\n89'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 90, 'page_label': '91', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThismakesintuitivesenseaswell.\\nMorespeciﬁcally,ifaneuronrarelypossessesahighactivationvalue,thenitis\\nfairtoassumethatitisn’tcontributingtothemodel’soutput,andwecansafely\\npruneit.\\nThefollowingtable\\ncomparesthe\\naccuracyofthe\\nprunedmodelwith\\ntheoriginal(full)\\nmodelacrossa\\nrangeofpruning\\nthresholds(λ):\\nAtapruningthresholdλ=0.4,thevalidationaccuracyofthemodeldropsbyjust\\n0.62%,butthenumberofparametersdropsby72%.\\nThatisahugereduction,whilebothmodelsbeingalmostequallygood!Of\\ncourse,thereisatrade-oﬀbecausewearenotdoingaswellastheoriginalmodel.\\nButinmanycases,especiallywhendeployingMLmodels,accuracyisnotthe\\nonlyprimarymetricthatdecidesthese.\\nInstead,severaloperational\\nmetricslikeeﬃciency,speed,\\nmemoryconsumption,etc.,are\\nalsoakeydecidingfactor.\\nThatiswhymodel\\ncompressiontechniquesareso\\ncrucialinsuchcases.\\n90'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 91, 'page_label': '92', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00o\\x00\\x00e\\x00\\x00\\n\\x00e\\x00\\x00o\\x00\\x00L\\x00o\\x00\\x00l\\x00\\x00r\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00\\x00o\\x00\\x00b\\x00\\x00k\\nThecoreobjectiveofmodeldeploymentistoobtainanAPIendpointthatcanbe\\nusedforinferencepurposes:\\nWhilethissoundssimple,deploymentistypicallyquiteatediousand\\ntime-consumingprocess.Onemustmaintainenvironmentﬁles,conﬁgure\\nvarioussettings,ensurealldependenciesarecorrectlyinstalled,andmanymore.\\nSo,inthischapter,Iwanttohelpyousimplifythisprocess.Morespeciﬁcally,we\\nshalllearnhowtodeployanyMLmodelrightfromaJupyterNotebookinjust\\nthreesimplestepsusingtheModelbitAPI.\\nModelbitletsusseamlesslydeployMLmodelsdirectlyfromourPython\\nnotebooks(orgit)toSnowﬂake,Redshi\\x00,andREST.\\n91'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 92, 'page_label': '93', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00o\\x00\\x00e\\x00\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00l\\x00\\x00t\\nAssumewehavealreadytrainedourmodel.\\nForsimplicity,let’sassumeittobealinearregressionmodeltrainedusing\\nsklearn,butitcanbeanyothermodelaswell:\\nLet’sseehowwecandeploythismodelwithModelbit!\\n● First,weinstalltheModelbitpackageviapip:\\n● Next,welogintoModelbitfromourJupyterNotebook(makesureyou\\nhavecreatedanaccounthere:Modelbit)\\n92'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 93, 'page_label': '94', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Finally,wedeployit,buthere’sanimportantpointtonote:\\nTodeployamodelusingModelbit,wemustdeﬁneaninferencefunction.\\nSimplyput,thisfunctioncontainsthecodethatwillbeexecutedatinference.\\nThus,itwillberesponsibleforreturningtheprediction.\\nWemustspecifytheinputparametersrequiredbythemodelinthismethod.\\nAlso,wecannameitanythingwewant.\\nForourlinearregressioncase,theinferencefunctioncanbeasfollows:\\n● Wedeﬁneafunction\\x00y\\x00\\x00r\\x00\\x00e\\x00\\x00o\\x00\\x00e\\x00\\x00(\\x00.\\n● Next,wespecifytheinputofthemodelasaparameterofthismethod.\\n● Wevalidatetheinputforitsdatatype.\\n● Finally,wereturntheprediction.\\nOnegoodthingaboutModelbitisthateverydependencyofthefunction(the\\nmodelobjectinthiscase)ispickledandsenttoproductionautomaticallyalong\\n93'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 94, 'page_label': '95', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nwiththefunction.Thus,wecanreferenceanyobjectinthismethod.Oncewe\\nhavedeﬁnedthefunction,wecanproceedwithdeploymentasfollows:\\nWehavesuccessfully\\ndeployedthemodelin\\nthreesimplesteps,\\nthattoo,rightfromthe\\nJupyterNotebook!\\nOnceourmodelhas\\nbeensuccessfully\\ndeployed,itwill\\nappearinourModelbit\\ndashboard.\\nAsshownabove,ModelbitprovidesanAPIendpoint.Wecanuseitforinference\\npurposesasfollows:\\n94'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 95, 'page_label': '96', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIntheaboverequest,datapassedtotheendpointisalistoflists.\\nTheﬁrstnumberinthelististheinputID.AllentriesfollowingtheIDinalist\\narethefunctionparameters.\\nLastly,wecanalsospecifyspeciﬁcversionsofthelibrariesorPythonusedwhile\\ndeployingourmodel.Thisisdepictedbelow:\\nIsn’tthatcool,simple,andelegantovertraditionaldeploymentapproaches?\\n95'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 96, 'page_label': '97', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00a\\x00\\x00\\x00o\\x00e\\x00\\x00\\x00L\\x00o\\x00\\x00l\\x00\\x00n\\x00r\\x00\\x00u\\x00\\x00i\\x00\\x00\\nDespiterigorouslytestinganMLmodellocally(onvalidationandtestsets),it\\ncouldbeaterribleideatoinstantlyreplacethepreviousmodelwithanewmodel.\\nAmorereliablestrategyistotestthemodelinproduction(yes,onreal-world\\nincomingdata).Whilethismightsoundrisky,MLteamsdoitallthetime,andit\\nisn’tthatcomplicated.Thefollowingvisualdepicts4commonstrategiestodoso:\\n96'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 97, 'page_label': '98', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Thecurrentmodeliscalledthelegacymodel.\\n● Thenewmodeliscalledthecandidatemodel.\\n\\x001\\x00\\x00/\\x00\\x00e\\x00\\x00\\x00n\\x00\\n● Distributetheincomingrequestsnon-uniformlybetweenthelegacymodel\\nandthecandidatemodel.\\n● Intentionallylimittheexposureofthecandidatemodeltoavoidany\\npotentialrisks.Thus,thenumberofrequestssenttothecandidatemodel\\nmustbelow.\\n\\x002\\x00\\x00a\\x00\\x00r\\x00\\x00e\\x00\\x00\\x00n\\x00\\n● InA/Btesting,sincetraﬃcisrandomlyredirectedtoeithermodel\\nirrespectiveoftheuser,itcanpotentiallyaﬀectallusers.\\n● Incanarytesting,thecandidatemodelisreleasedtoasmallsubsetofusers\\ninproductionandgraduallyrolledouttomoreusers.\\n97'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 98, 'page_label': '99', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x003\\x00\\x00n\\x00\\x00r\\x00\\x00\\x00v\\x00\\x00\\x00e\\x00\\x00\\x00n\\x00\\n● Thisinvolvesmixingthepredictionsofmultiplemodelsintheresponse.\\n● ConsiderAmazon’srecommendationengine.Ininterleaveddeployments,\\nsomeproductrecommendationsdisplayedonthehomepagecancome\\nfromthelegacymodel,whilesomecancomefromthecandidatemodel.\\n\\x004\\x00\\x00h\\x00\\x00o\\x00\\x00e\\x00\\x00\\x00n\\x00\\n● Alloftheabovetechniquesaﬀectsome(orall)users.\\n● Shadowtesting(ordarklaunches)letsustestanewmodelinaproduction\\nenvironmentwithoutaﬀectingtheuserexperience.\\n● Thecandidatemodelisdeployedalongsidetheexistinglegacymodeland\\nservesrequestslikethelegacymodel.However,theoutputisnotsentback\\ntotheuser.Instead,theoutputisloggedforlaterusetobenchmarkits\\nperformanceagainstthelegacymodel.\\n● Weexplicitlydeploythecandidatemodelinsteadoftestingoﬄinebecause\\ntheproductionenvironmentisdiﬃculttoreplicateoﬄine.\\nShadowtestingoﬀersrisk-freetestingofthecandidatemodelinaproduction\\nenvironment.\\n98'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 99, 'page_label': '100', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00r\\x00\\x00l\\x00\\x00g\\x00n\\x00\\x00o\\x00\\x00l\\x00e\\x00\\x00s\\x00\\x00y\\nReal-worldMLdeploymentisneverjustabout“deployment”—hostthemodel\\nsomewhere,obtainanAPIendpoint,integrateitintotheapplication,andyouare\\ndone!\\nThisisbecause,inreality,plentyofthingsmustbedonepost-deploymentto\\nensurethemodel’sreliabilityandperformance.\\n\\x001\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00r\\x00\\x00\\nTobegin,itisimmenselycrucialto\\nversioncontrolMLdeployments.You\\nmayhavenoticedthiswhileusing\\nChatGPT,forinstance.\\nButupdatingdoesnotsimplymeanoverwritingthepreviousversion.\\nInstead,MLmodelsarealwaysversion-controlled(usinggittools).\\nTheadvantagesofversion-controllingMLdeploymentsareprettyobvious:\\n● Incaseofsuddenmishapspost-deployment,wecaninstantlyrollbackto\\nanolderversion.\\n● Wecanfacilitateparalleldevelopmentwithbranching,andmanymore.\\n99'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 100, 'page_label': '101', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x002\\x00\\x00o\\x00\\x00l\\x00e\\x00\\x00s\\x00\\x00y\\nAnotherpracticalideaistomaintainamodelregistryfordeployments.Let’s\\nunderstandwhatitis.\\nSimplyput,amodelregistrycanbeconsidered\\nrepositoryofmodels.See,typically,wemightbe\\ninclinedtoversionourcodeandtheMLmodel\\ntogether:\\nHowever,whenweuseamodelregistry,weversionmodelsseparatelyfromthe\\ncode.Letmegiveyouanintuitiveexampletounderstandthisbetter.Imagineour\\ndeployedmodeltakesthreeinputstogenerateaprediction:\\nWhilewritingtheinferencecode,weoverlookedthat,attimes,oneoftheinputs\\nmightbemissing.Werealizedthisbyanalyzingthemodel’slogs.\\n100'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 101, 'page_label': '102', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWemaywanttoﬁxthisquickly(atleastforawhile)beforewedecideonthenext\\nstepsmoreconcretely.Thus,wemaydecidetoupdatetheinferencecodeby\\nassigningadummyvalueforthemissinginput.\\nThiswillallowthemodeltostillprocesstheincomingrequest.\\nLetmeaskyouaquestion:“Didweupdatethemodel?”\\nNo,right?\\nHere,weonlyneedtoupdatetheinferencecode.Themodelwillremainthe\\nsame.\\nButifweweretoversionthemodelandcodetogether,itwouldleadtoa\\nredundantmodelandtakeupextraspace.\\nHowever,bymaintainingamodelregistry:\\n● Wecanonlyupdatetheinferencecode.\\n● Avoidpushinganew(yetunwanted)modeltodeployment.\\nThismakesintuitivesenseaswell,doesn’tit?\\n101'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 102, 'page_label': '103', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\n\\x00L\\x00\\x00\\n\\x00h\\x00\\x00e\\x00i\\x00\\x00h\\x00\\x00P\\x00\\x00e\\x00\\x00r\\x00\\x00o\\x00\\nGPT-2(XL)has1.5Billionparameters,anditsparametersconsume~3GBof\\nmemoryin16-bitprecision.\\nUnder16-bitprecision,oneparametertakesup2bytesofmemory,so1.5B\\nparameterswillconsume3GBofmemory.\\nWhat’syourestimatefortheminimummemoryneededtotrainGPT-2ona\\nsingleGPU?\\n● Optimizer→Adam\\n● Batchsize→32\\n● Numberoftransformerlayers→48\\n● Sequencelength→1000\\n\\x00h\\x00\\x00'\\x00\\x00o\\x00\\x00\\x00s\\x00\\x00m\\x00\\x00e\\x00o\\x00\\x00h\\x00\\x00i\\x00\\x00m\\x00\\x00\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00o\\x00r\\x00\\x00n\\x00\\x00P\\x00\\x002\\n\\x00o\\x00\\x00l\\x00s\\x00\\x00e\\x00G\\x00\\x00)\\x00n\\x00\\x00i\\x00\\x00l\\x00\\x00P\\x00\\x00\\n- \\x00-\\x00\\x00B\\n- \\x00-\\x00\\x00\\x00B\\n- \\x002\\x00\\x005\\x00B\\n- \\x002\\x00\\x005\\x00B\\n- \\x000\\x00\\x00B\\nTheanswermightsurpriseyou.\\n102\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 103, 'page_label': '104', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nOnecanbarelytraina3GBGPT-2modelonasingleGPUwith32GBof\\nmemory.\\nButhowcouldthatbeevenpossible?Wheredoesallthememorygo?\\nLet’sunderstand.\\nTherearesomanyfrontsonwhichthemodelconsistentlytakesupmemory\\nduringtraining.\\n\\x001\\x00\\x00p\\x00\\x00m\\x00\\x00e\\x00\\x00t\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00i\\x00\\x00t\\x00\\x00\\x00n\\x00\\x00a\\x00\\x00m\\x00\\x00e\\x00\\x00e\\x00\\x00r\\x00\\nMixedprecisiontrainingiswidelyusedtospeedupmodeltraining.\\nAsthenamesuggests,theideaistoutilizelower-precisionﬂ\\x00a\\x00\\x006(wherever\\nfeasible,likeinconvolutionsandmatrixmultiplications)alongwithﬂ\\x00a\\x00\\x002—\\nthatiswhythename“mixedprecision.”\\nBoththeforwardandbackwardpropagationareperformedusingthe16-bit\\nrepresentationsofweightsandgradients.\\nThus,ifthemodelhasΦparameters,then:\\n● Weightswillconsume2*Φbytes.\\n● Gradientswillconsume2*Φbytes.\\n103'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 104, 'page_label': '105', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHere,theﬁgure“2”representsamemoryconsumptionof2bytes/paramter\\n(16-bit).\\nMoreover,theupdatesattheendofthebackwardpropagationarestillperformed\\nunder32-bitforeﬀectivecomputation.Iamtalkingaboutthecircledstepinthe\\nimagebelow:\\nAdamisoneofthemostpopularoptimizersformodeltraining.\\nWhilemanypractitionersuseitjustbecauseitispopular,theydon’trealizethat\\nduringtraining,Adamstorestwooptimizerstatestocomputetheupdates—\\nmomentumandvarianceofthegradients:\\nThus,ifthemodelhasΦparameters,thenthesetwooptimizerstateswill\\nconsume:\\n● 4*Φbytesformomentum.\\n104'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 105, 'page_label': '106', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Another4*Φbytesforvariance.\\nHere,theﬁgure“4”representsamemoryconsumptionof4bytes/paramter\\n(32-bit).\\nLastly,asshownintheﬁgureabove,theﬁnalupdatesarealwaysadjustedinthe\\n32-bitrepresentationofthemodelweights.Thisleadsto:\\n● Another4*Φbytesformodelparameters.\\nLet’ssumthemup:\\nThat’s16*Φ,or24GBofmemory,whichisridiculouslyhigherthanthe3GB\\nmemoryutilizedby16-bitparameters.\\nAndwehaven’tconsideredeverythingyet.\\n105'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 106, 'page_label': '107', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x002\\x00\\x00c\\x00\\x00v\\x00\\x00i\\x00\\x00s\\nForbigdeeplearningmodels,likeLLMs,Activationstakeupsigniﬁcantmemory\\nduringtraining.\\nMoreformally,thetotalnumberofactivationscomputedinonetransformblock\\nofGPT-2are:\\nThus,acrossalltransformerblocks,thiscomesouttobe:\\nThisistheconﬁgurationforGPT2-XL:\\nThiscomesouttobe~30Bactivations.Aseachactivationisrepresentedin16-bit,\\nallactivationscollectivelyconsume60GBofmemory.\\n106'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 107, 'page_label': '108', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWithtechniqueslikegradientcheckpointing(discussedinthepreviouschapter),\\nthiscouldbebroughtdowntoabout8-9GBattheexpenseof25-30%more\\nrun-time.\\nThistechniquetakescompletememoryconsumptiontoabout32-35GBrange,\\nwhichImentionedearlier,forameager3GBmodel,andthattoowithapretty\\nsmallbatchsizeofjust32.Ontopofthis,therearealsosomemoreoverheads\\ninvolved,likememoryfragmentation.\\nItoccurswhentherearesmall,unusedgapsbetweenallocatedmemoryblocks,\\nleadingtoineﬃcientuseoftheavailablememory.\\nMemoryallocationrequestsfailbecauseoftheunavailabilityofcontiguous\\nmemoryblocks.\\n\\x00o\\x00\\x00l\\x00\\x00i\\x00\\x00\\nIntheabovediscussion,weconsideredarelativelysmallmodel—GPT-2(XL)\\nwith1.5Billionparameters,whichistinycomparedtothescaleofmodelsbeing\\ntrainedthesedays.\\nHowever,thediscussionmayhavehelpedyoureﬂectontheinherentchallenges\\nofbuildingLLMs.Manypeopleo\\x00ensaythatGPTsareonlyaboutstackingmore\\nandmorelayersinthemodelandmakingthenetworkbigger.\\nIfitwasthateasy,everybodywouldhavebeendoingit.Fromthisdiscussion,you\\nmayhaveunderstoodthatit’snotassimpleasappendingmorelayers.\\nEvenoneadditionallayercanleadtomultipleGBsofadditionalmemory\\nrequirement.Multi-GPUtrainingisattheforefrontofthesemodels,whichwe\\ncoveredinanearlierchapterinthisbook.\\n107'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 108, 'page_label': '109', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00-\\x00\\x00d\\x00\\x00\\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\x00s\\x00\\x00o\\x00\\x00\\x00s\\x00\\x00A\\x00\\nHere’savisualwhichillustrates“full-modelﬁne-tuning,”“ﬁne-tuningwith\\nLoRA,”and“retrievalaugmentedgeneration(RAG).”\\nAllthreetechniquesareusedtoaugmenttheknowledgeofanexistingmodel\\nwithadditionaldata.\\n108'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 109, 'page_label': '110', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00u\\x00\\x00ﬁ\\x00e\\x00\\x00u\\x00\\x00n\\x00\\nFine-tuningmeansadjustingtheweightsofapre-trainedmodelonanewdataset\\nforbetterperformance.\\nWhilethisﬁne-tuningtechniquehasbeensuccessfullyusedforalongtime,\\nproblemsarisewhenweuseitonmuchlargermodels—LLMs,forinstance,\\nprimarilybecauseof:\\n● Theirsize.\\n● Thecostinvolvedinﬁne-tuningallweights.\\n● Thecostinvolvedinmaintainingalllargeﬁne-tunedmodels.\\n\\x002\\x00\\x00o\\x00\\x00ﬁ\\x00e\\x00\\x00u\\x00\\x00n\\x00\\nLoRAﬁne-tuningaddressesthelimitationsoftraditionalﬁne-tuning.Thecore\\nideaistodecomposetheweightmatrices(someorall)oftheoriginalmodelinto\\nlow-rankmatricesandtraintheminstead.Forinstance,inthegraphicbelow,the\\nbottomnetworkrepresentsthelargepre-trainedmodel,andthetopnetwork\\nrepresentsthemodelwithLoRAlayers.\\n109'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 110, 'page_label': '111', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\nTheideaistotrainonlytheLoRAnetworkandfreezethelargemodel.\\nLookingattheabovevisual,youmightthink:\\nButtheLoRAmodelhasmoreneuronsthantheoriginalmodel.Howdoesthat\\nhelp?Tounderstandthis,youmustmakeitclearthatneuronsdon'thave\\nanythingtodowiththememoryofthenetwork.\\nTheyarejustusedtoillustratethedimensionalitytransformationfromonelayer\\ntoanother.\\nItistheweightmatrices(ortheconnectionsbetweentwolayers)thattakeup\\nmemory.Thus,wemustbecomparingtheseconnectionsinstead:\\nLookingattheabovevisual,itisprettyclearthattheLoRAnetworkhas\\nrelativelyveryfewconnections.\\n\\x003\\x00\\x00A\\x00\\nRetrievalaugmentedgeneration(RAG)isanotherprettycoolwaytoaugment\\nneuralnetworkswithadditionalinformation,withouthavingtoﬁne-tunethe\\nmodel.\\nThisisillustratedbelow:\\n110\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 111, 'page_label': '112', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThereare7steps,whicharealsomarkedintheabovevisual:\\n● Step1-2:Takeadditionaldata,anddumpitinavectordatabasea\\x00er\\nembedding.(Thisisonlydoneonce.Ifthedataisevolving,justkeep\\ndumpingtheembeddingsintothevectordatabase.There’snoneedto\\nrepeatthisagainfortheentiredata)\\n● Step3:Usethesameembeddingmodeltoembedtheuserquery.\\n● Step4-5:Findthenearestneighborsinthevectordatabasetothe\\nembeddedquery.\\n● Step6-7:Providetheoriginalqueryandtheretrieveddocuments(formore\\ncontext)totheLLMtogetaresponse.\\nInfact,evenitsnameentirelyjustiﬁeswhatwedowiththistechnique:\\n111'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 112, 'page_label': '113', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Retrieval:Accessingandretrievinginformationfromaknowledgesource,\\nsuchasadatabaseormemory.\\n● Augmented:Enhancingorenrichingsomething,inthiscase,thetext\\ngenerationprocess,withadditionalinformationorcontext.\\n● Generation:Theprocessofcreatingorproducingsomething,inthis\\ncontext,generatingtextorlanguage.\\nOfcourse,therearemanyproblemswithRAGtoo,suchas:\\n● RAGsinvolvesimilaritymatchingbetweenthequeryvectorandthevectors\\noftheadditionaldocuments.However,questionsarestructurallyvery\\ndiﬀerentfromanswers.\\n● TypicalRAGsystemsarewell-suitedonlyforlookup-based\\nquestion-answeringsystems.Forinstance,wecannotbuildaRAGpipeline\\ntosummarizetheadditionaldata.TheLLMnevergetsinfoaboutallthe\\ndocumentsinitspromptbecausethesimilaritymatchingsteponly\\nretrievestopmatches.\\nSo,it’sprettyclearthatRAGhasbothprosandcons.\\n● Weneverhavetoﬁne-tunethemodel,whichsavesalotofcomputing\\npower.\\n● Butthisalsolimitstheapplicabilitytospeciﬁctypesofsystems.\\nLet’scontinuethediscussiononLLMﬁne-tuninginthenextchapter.\\n112'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 113, 'page_label': '114', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00L\\x00\\x00i\\x00\\x00-\\x00\\x00n\\x00\\x00g\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\nTraditionalﬁne-tuning(depictedbelow)isinfeasiblewithLLMsbecausethese\\nmodelshavebillionsofparametersandarehundredsofGBsinsize,andnot\\neveryonehasaccesstosuchcomputinginfrastructure.\\nButtoday,wehavemanyoptimalwaystoﬁne-tuneLLMs,andﬁvepopular\\ntechniquesaredepictedbelow:\\n113'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 114, 'page_label': '115', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n- LoRA:Addtwolow-rankmatricesAandBalongsideweightmatrices,\\nwhichcontainthetrainableparameters.Insteadofﬁne-tuningW,adjust\\ntheupdatesintheselow-rankmatrices.\\n- LoRA-FA:WhileLoRAconsiderablydecreasesthetotaltrainable\\nparameters,itstillrequiressubstantialactivationmemorytoupdatethe\\nlow-rankweights.LoRA-FA(FAstandsforFrozen-A)freezesthematrixA\\nandonlyupdatesmatrixB.\\n- VeRA:InLoRA,everylayerhasadiﬀerentpairoflow-rankmatricesAand\\nB,andbothmatricesaretrained.InVeRA,however,matricesAandBare\\nfrozen,random,andsharedacrossallmodellayers.VeRAfocuseson\\nlearningsmall,layer-speciﬁcscalingvectors,denotedasbandd,whichare\\ntheonlytrainableparametersinthissetup.\\n114'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 115, 'page_label': '116', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n- Delta-LoRA:Here,inadditiontotraininglow-rankmatrices,thematrixW\\nisalsoadjustedbutnotinthetraditionalway.Instead,thediﬀerence(or\\ndelta)betweentheproductofthelow-rankmatricesAandBintwo\\nconsecutivetrainingstepsisaddedtoW:\\n- LoRA+:InLoRA,bothmatricesAandBareupdatedwiththesame\\nlearningrate.Authorsfoundthatsettingahigherlearningrateformatrix\\nBresultsinmoreoptimalconvergence.\\n115'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 116, 'page_label': '117', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00l\\x00\\x00s\\x00\\x00a\\x00\\x00L\\n116'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 117, 'page_label': '118', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00L\\x00u\\x00\\x00a\\x00\\x00n\\x00\\x00l\\x00\\n\\x00r\\x00\\x00n\\x00\\x00g \\x00n\\x00 \\x00n\\x00\\x00r\\x00\\x00c\\x00 \\x00i\\x00\\x00 \\x00o\\x00\\x00l\\x00\\x00i\\x00\\x00 \\x00f \\x000 \\x00L\\n\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00\\nHere’stherun-timecomplexityofthe10mostpopularMLalgorithms.\\nButwhyevencareaboutruntime?TherearemultiplereasonswhyIalwayscare\\naboutruntimeandwhyyoushouldtoo.\\n117'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 118, 'page_label': '119', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTobegin,weknowthateveryoneisabigfanofsklearnimplementations.It\\nliterallytakesjusttwo(maxthree)linesofcodetorunanyMLalgorithmwith\\nsklearn.Yet,inmyexperience,duetothissimplicity,mostuserso\\x00enoverlook:\\n● Thecoreunderstandingofanalgorithm.\\n● Thedata-speciﬁcconditionsthatallowustouseanalgorithm.\\nForinstance,you’llbeupforabigsurpriseifyouuseSVMort-SNEonadataset\\nwithplentyofsamples.\\n● SVM’srun-timegrowscubicallywiththetotalnumberofsamples.\\n● t-SNE’srun-timegrowsquadraticallywiththetotalnumberofsamples.\\nAnotheradvantageofﬁguringouttherun-timeisthatithelpsusunderstand\\nhowanalgorithmworksend-to-end.Ofcourse,intheabovetable,Ihavemade\\nsomeassumptionshereandthere.Forinstance:\\n● Inarandomforest,alldecisiontreesmayhavediﬀerentdepths.Buthere,I\\nhaveassumedthattheyareequal.\\n● DuringinferenceinkNN,weﬁrstﬁndthedistancetoalldatapoints.This\\ngivesalistofdistancesofsize\\x00(totalsamples).\\n○ Then,weﬁndthek-smallestdistancesfromthislist.\\n○ Therun-timetodeterminethek-smallestvaluesmaydependonthe\\nimplementation.\\n■ Sortingandselectingthek-smallestvalueswillbe\\x00(\\x00\\x00o\\x00\\x00).\\n■ Butifweuseapriorityqueue,itwilltake\\x00(\\x00\\x00o\\x00\\x00k\\x00\\x00.\\n● Int-SNE,there’salearningstep.Sincethemajorrun-timecomesfrom\\ncomputingthepairwisesimilaritiesinthehigh-dimensionalspace,we\\nhaveignoredthatstep.\\nNonetheless,thetablestillaccuratelyreﬂectsthegeneralrun-timeofeachof\\nthesealgorithms.\\nAsanexercise,Iwouldencourageyoutoderivetheserun-timecomplexities\\nyourself.Thisactivitywillprovideyousomuchconﬁdenceinalgorithmic\\nunderstanding.\\n118'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 119, 'page_label': '120', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x005 \\x00o\\x00\\x00 \\x00m\\x00\\x00r\\x00\\x00n\\x00 \\x00a\\x00\\x00e\\x00\\x00t\\x00\\x00a\\x00 \\x00eﬁ\\x00i\\x00\\x00o\\x00\\x00 \\x00n\\n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\n\\x00I\\x00\\x00a\\x00\\x00e\\x00\\x00t\\x00\\x00a\\x00\\x00n\\x00\\x00l\\x00\\x00g\\x00\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\x00n\\x00\\x00a\\x00\\x00i\\x00\\x00\\n\\x00e\\x00\\x00n\\x00\\x00g\\x00\\x00\\nThisisaquestionthatsomanypeoplehave,especiallythosewhoarejustgetting\\nstarted.\\nShortanswer:Yes,it’simportant,andhere’swhyIsayso.\\nSee…thesedays,onecando“ML”withoutunderstandinganymathematical\\ndetailsofanalgorithm.Forinstance(andthankstosklearn,bytheway):\\n● Onecanuseanyclusteringalgorithmin2-3linesofcode.\\n● Onecantrainclassiﬁcationmodelsin2-3linesofcode.\\n● Andmore.\\nThisisbothgoodandbad:\\n● It’sgoodbecauseitsavesustime.\\n● It’sbadbecausethistemptsustoignoretheunderlyingdetails.\\nInfact,Iknowmanydatascientists(mainlyontheappliedside)whodonot\\nentirelyunderstandthemathematicaldetailsbutcanstillbuildanddeploy\\nmodels.\\nNothingwrong.\\n119'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 120, 'page_label': '121', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHowever,whenItalktothem,Ialsoseesomedisconnectbetween“Whatthey\\nareusing”and“Whytheyareusingit.”\\nDuetoalackofunderstandingoftheunderlyingdetails:\\n● Theyﬁnditquitediﬃculttooptimizetheirmodels.\\n● Theystruggletoidentifypotentialareasofimprovement.\\n● Theytakealongertimetodebugwhenthingsdon’tworkwell.\\n● Theydonotfullyunderstandtheroleofspeciﬁchyperparameters.\\n● Theyuseanyalgorithmwithoutestimatingtheirtimecomplexityﬁrst.\\nIfitfeelslikeyouareoneofthem,it’sokay.Thisproblemcanbesolved.\\nThatsaid,ifyougenuinelyaspiretoexcelinthisﬁeld,buildingacuriosityforthe\\nunderlyingmathematicaldetailsholdsexponentialreturns.\\n● Algorithmicawarenesswillgiveyouconﬁdence.\\n● Itwilldecreaseyourtimetobuildanditerate.\\nGradually,youwillgofromahit-and-trialapproachto“Iknowwhatshould\\nwork.”\\nTohelpyoutakethatﬁrststep,Ipreparedthefollowingvisual,whichlistssome\\nofthemostimportantmathematicalformulationsusedinDataScienceand\\nStatistics(innospeciﬁcorder).\\nBeforereadingahead,lookatthemonebyoneandcalculatehowmanyofthem\\ndoyoualreadyknow:\\n120'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 121, 'page_label': '122', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSomeofthetermsareprettyself-explanatory,soIwon’tgothrougheachofthem,\\nlike:\\n● GradientDescent,NormalDistribution,Sigmoid,Correlation,Cosine\\nsimilarity,NaiveBayes,F1score,ReLU,So\\x00max,MSE,MSE+L2\\nregularization,KMeans,Linearregression,SVM,Logloss.\\nHerearetheremainingterms:\\n● MLE(MaximumLikelihoodEstimation):Amethodforestimatingthe\\nparametersofastatisticalmodelbymaximizingthelikelihoodofthe\\nobserveddata.Wecovereditinthepreviouschapter.\\n121'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 122, 'page_label': '123', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Z-score:Astandardizedvaluethatindicateshowmanystandarddeviations\\nadatapointisfromthemean.\\n● OrdinaryLeastSquares:Aclosed-formsolutionforlinearregression\\nobtainedusingtheMLEstepmentionedabove.\\n● Entropy:Ameasureoftheuncertaintyofarandomvariable.\\n● EigenVectors:Thenon-zerovectorsthatdonotchangetheirdirection\\nwhenalineartransformationisapplied.Itiswidelyusedindimensionality\\nreductiontechniqueslikePCA.\\n● R2(R-squared):Astatisticalmeasurethatrepresentstheproportionof\\nvarianceexplainedbyaregressionmodel.\\n● KLdivergence:Assesshowmuchinformationislostwhenonedistribution\\nisusedtoapproximateanotherdistribution.Itisusedasalossfunctionin\\nthet-SNEalgorithm.\\n● SVD:Afactorizationtechniquethatdecomposesamatrixintothreeother\\nmatrices,o\\x00ennotedasU,Σ,andV.Itisfundamentalinlinearalgebrafor\\napplicationslikedimensionalityreduction,noisereduction,anddata\\ncompression.\\n● Lagrangemultipliers:Theyarecommonlyusedmathematicaltechniques\\ntosolveconstrainedoptimizationproblems.Forinstance,consideran\\noptimizationproblemwithanobjectivefunction\\x00(\\x00\\x00andassumethatthe\\nconstraintsare\\x00(\\x00\\x00=\\x00and\\x00(\\x00\\x00=\\x00.Lagrangemultiplierssolvethis.\\n122'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 123, 'page_label': '124', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00 \\x00o \\x00e\\x00\\x00a\\x00\\x00y \\x00m\\x00\\x00o\\x00\\x00 \\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00t\\x00\\x00\\n\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00-\\x00\\x00a\\x00\\x00iﬁ\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00l\\x00\\nMLmodelbuildingistypicallyaniterativeprocess.Givensomedataset:\\n● Wetrainamodel.\\n● Weevaluateit.\\n● Andwecontinuetoimproveituntilwearesatisﬁedwiththeperformance.\\nHere,theeﬃcacyofanymodelimprovementstrategy(say,introducinganew\\nfeature)isdeterminedusingsomesortofperformancemetric.\\nHowever,Ihaveo\\x00enobservedthatwhenimprovingprobabilistic\\nmulticlass-classiﬁcationmodels,thistechniquecanbeabitdeceptivewhenthe\\neﬃcacyisdeterminedusing“Accuracy.”\\n123'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 124, 'page_label': '125', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00t\\x00\\x00\\x00u\\x00\\x00i\\x00\\x00a\\x00\\x00-\\x00\\x00a\\x00\\x00iﬁ\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00e\\x00o\\x00\\x00l\\x00\\x00h\\x00\\x00\\n\\x00u\\x00\\x00u\\x00\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00e\\x00\\x00o\\x00\\x00i\\x00\\x00\\x00o\\x00a\\x00\\x00\\x00l\\x00\\x00s\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00a\\x00\\x00e\\x00\\x00o\\x00\\x00s\\x00\\nInotherwords,itispossiblethatweareactuallymakinggoodprogressin\\nimprovingthemodel,but“Accuracy”isnotreﬂectingthat(yet).\\nLet’sunderstand.\\n\\x00i\\x00\\x00a\\x00\\x00\\x00f\\x00c\\x00\\x00r\\x00\\x00y\\nInprobabilisticmulticlass-classiﬁcationmodels,Accuracyisdeterminedusing\\ntheoutputlabelthathasthehighestprobability:\\nNow,it’spossiblethattheactuallabelisnotpredictedwiththehighest\\nprobabilitybythemodel,butit’sinthetop“k”outputlabels.\\n124'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 125, 'page_label': '126', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,intheimagebelow,theactuallabel(ClassC)isnotthehighest\\nprobabilitylabel,butit’satleastinthetop2predictedprobabilities(ClassBand\\nClassC):\\nAndwhatifinanearlierversionofourmodel,theoutputprobabilityofClassC\\nwasthelowest,asdepictedbelow:\\nNow,ofcourse,inbothcases,theﬁnalpredictionisincorrect.\\nHowever,whileiteratingfrom“Version1”to“Version2”usingsomemodel\\nimprovementtechniques,wegenuinelymadegoodprogress.\\n125'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 126, 'page_label': '127', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNonetheless,Accuracyentirelydiscardsthisasitonlycaresaboutthehighest\\nprobabilitylabel.\\nIhopeyouunderstandtheproblemhere.\\n\\x00o\\x00\\x00t\\x00\\x00n\\nWheneverIambuildinganditerativelyimprovinganyprobabilisticmulticlass\\nclassiﬁcationmodel,Ialwaysusethetop-kaccuracyscore.Asthenamesuggests,\\nitcomputeswhetherthecorrectlabelisamongthetopklabelspredicted\\nprobabilitiesornot.\\nAsyoumayhavealreadyguessed,top-1accuracyscoreisthetraditional\\nAccuracyscore.Thisisamuchbetterindicatortoassesswhethermymodel\\nimprovementeﬀortsaretranslatingintomeaningfulenhancementsinpredictive\\nperformanceornot.\\nForinstance,ifthetop-3accuracyscoregoesfrom75%to90%,thistotally\\nsuggeststhatwhateverwedidtoimprovethemodelwaseﬀective:\\n● Earlier,thecorrectpredictionwasinthetop3labelsonly75%ofthetime.\\n● Butnow,thecorrectpredictionisinthetop3labels90%ofthetime.\\n126'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 127, 'page_label': '128', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsaresult,onecaneﬀectivelyredirecttheirengineeringeﬀortsintheright\\ndirection.Ofcourse,whatIamsayingshouldonlybeusedtoassessthemodel\\nimprovementeﬀorts.\\nThisisbecausetruepredictivepowerwillinevitablybedeterminedusing\\ntraditionalmodelaccuracy.Somakesureyouaregraduallyprogressingonthe\\nAccuracyfronttoo.\\nIdeally,itisexpectedthat“Top-kAccuracy”maycontinuetoincreaseduring\\nmodeliterations,whichreﬂectsimprovementinperformance.Accuracy,\\nhowever,maystaythesameforawhile,asdepictedbelow:\\nTop-kaccuracyscoreisalsoavailableinSklearn:\\n127'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 128, 'page_label': '129', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00 \\x00n\\x00\\x00r\\x00 \\x00o\\x00\\x00l \\x00m\\x00\\x00o\\x00\\x00m\\x00\\x00t \\x00ﬀ\\x00r\\x00\\x00 \\x00i\\x00\\x00t \\x00e\\n\\x00o\\x00\\x00g\\x00n\\x00a\\x00\\x00\\nBackin2019,IwasworkingwithanMLresearchgroupinGermany.\\nOneday,aPh.D.student\\ncameuptome(and\\nothersinthelab),handed\\noverasmallsampleof\\nthedatasethewas\\nworkingwith,and\\nrequestedustolabelit,\\ndespitehavingtrue\\nlabels.\\nThismademecuriousaboutwhygatheringhumanlabelswasnecessaryforhim\\nwhenhealreadyhadgroundtruthlabelsavailable.SoIasked.\\nWhatIlearnedthatdaychangedmyapproachtoincrementalmodel\\nimprovement,andIamsureyouwillﬁndthisideafascinatingtoo.\\nLetmeexplainwhatIlearned.\\nConsiderwearebuildingamulticlassclassiﬁcationmodel.Sayit’samodelthat\\nclassiﬁesaninputimageasarock,paper,orscissors:\\n128'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 129, 'page_label': '130', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForsimplicity,let’sassumethere’snoclassimbalance.Calculatingtheclass-wise\\nvalidationaccuraciesgivesusthefollowingresults:\\n\\x00u\\x00\\x00t\\x00\\x00n\\x00\\x00h\\x00\\x00h\\x00l\\x00\\x00s\\x00o\\x00\\x00d\\x00o\\x00\\x00o\\x00\\x00\\x00n\\x00\\x00i\\x00\\x00v\\x00\\x00y\\x00r\\x00\\x00e\\x00\\x00\\x00o\\x00n\\x00\\x00e\\x00\\x00\\n\\x00u\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00m\\x00\\x00o\\x00\\x00\\x00h\\x00\\x00o\\x00\\x00l\\x00n\\x00\\nA\\x00erlookingattheseresults,\\nmostpeoplebelievethat\\n“Scissor”istheworst-performing\\nclassandshouldbeinspected\\nfurther.\\nButthismightnotbetrue.AndthisispreciselywhatthatPh.D.studentwanted\\ntoverifybycollectinghumanlabels.Let’ssaythatthehumanlabelsgiveusthe\\nfollowingresults:\\nBasedonthis,doyoustillthinkthemodelperformstheworstonthe“Scissor”\\nclass?\\nNo,right?\\n129'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 130, 'page_label': '131', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nImean,ofcourse,themodelhastheleastaccuracyonthe“Scissor”class,andI\\namnotdenyingit.However,withmorecontext,wenoticethatthemodelisdoing\\naprettygoodjobclassifyingthe“Scissor”class.Thisisbecauseanaverage\\nhumanisachievingjust2%higheraccuracyincomparisontowhatourmodelis\\nabletoachieve.\\nHowever,theaboveresultsastonishinglyrevealthatitisthe“Rock”classinstead\\nthatdemandsmoreattention.Theaccuracydiﬀerencebetweenanaverage\\nhumanandthemodeliswaytoohigh(13%).Hadwenotknownthis,wewould\\nhavecontinuedtoimprovethe“Scissor”class,wheninreality,“Rock”requires\\nmoreimprovement.\\nEversinceIlearnedthistechnique,Ihavefounditsuperhelpfultodeterminemy\\nnextstepsformodelimprovement,ifpossible.Isay“ifpossible”becauseI\\nunderstandthatmanydatasetsarehardforhumanstointerpretandlabel.\\nNonetheless,ifitisfeasibletosetupsucha“humanbaseline,”onecangetso\\nmuchclarityintohowthemodelisperforming.\\nAsaresult,onecaneﬀectivelyredirecttheirengineeringeﬀortsintheright\\ndirection.\\nOfcourse,Iamnotclaimingthatthiswillbeuniversallyusefulinallusecases.\\nForinstance,ifthemodelisalreadyperformingbetterthanthebaseline,the\\nmodelimprovementsfromthereonwillhavetobeguidedbasedonpastresults.\\nYet,insuchcases,surpassingahumanbaselineatleasthelpsusvalidatethatthe\\nmodelisdoingbetterthanwhatahumancando.\\n130'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 131, 'page_label': '132', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00f\\x006\\x00L\\x00l\\x00\\x00s\\nThebelowvisualdepictsthemostcommonlyusedlossfunctionsbyvariousML\\nalgorithms.\\n131'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 132, 'page_label': '133', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x000\\x00o\\x00\\x00\\x00o\\x00\\x00o\\x00\\x00o\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\nThebelowvisualdepictssomecommonlyusedlossfunctionsinregressionand\\nclassiﬁcationtasks.\\n132'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 133, 'page_label': '134', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00o\\x00c\\x00\\x00a\\x00\\x00y\\x00s\\x00\\x00r\\x00\\x00n\\x00\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00\\x00e\\x00\\nItisprettyconventionaltosplitthegivendataintotrain,test,andvalidationsets.\\nHowever,therearequiteafewmisconceptionsabouthowtheyaremeanttobe\\nused,especiallythevalidationandtestsets.\\nInthischapter,let’sclearthemupandseehowtotrulyusetrain,validation,and\\ntestsets.\\nWebeginbysplittingthedatainto:\\n● Train\\n● Validation\\n● Test\\nAtthispoint,justassumethatthetestdatadoesnotevenexist.Forgetaboutit\\ninstantly.\\n133'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 134, 'page_label': '135', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nBeginwiththetrainset.Thisisyourwholeworldnow.\\n● Youanalyzeit\\n● Youtransformit\\n● Youuseittodeterminefeatures\\n● Youﬁtamodelonit\\nA\\x00ermodeling,youwouldwanttomeasurethemodel’sperformanceonunseen\\ndata,wouldn’tyou?\\nBringinthevalidationsetnow.\\nBasedonvalidationperformance,improvethemodel.Here’showyouiteratively\\nbuildyourmodel:\\n134'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 135, 'page_label': '136', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Trainusingatrainset\\n● Evaluateitusingthevalidationset\\n● Improvethemodel\\n● Evaluateagainusingthevalidationset\\n● Improvethemodelagain\\n● andsoon.\\nUntil...\\nYoureachapointwhereyoustartoverﬁttingthevalidationset.\\nThisindicatesthatyouhaveexploited(orpolluted)thevalidationset.\\nNoworries.\\nMergeitwiththetrainsetandgenerateanewsplitoftrainandvalidation.\\nNote:Relyoncross-validationifneeded,especiallywhenyoudon’thavemuch\\ndata.Youmaystillusecross-validationifyouhaveenoughdata.Butitcanbe\\ncomputationallyintensive.\\nNow,ifyouarehappywiththemodel’sperformance,evaluateitontestdata.\\nWhatyouuseatestsetfor:\\n● Getaﬁnalandunbiasedreviewofthemodel.\\nWhatyouDON’Tuseatestsetfor:\\n● Analysis,decision-making,etc.\\n135'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 136, 'page_label': '137', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfthemodelisunderperformingonthetestset,noproblem.Gobacktothe\\nmodelingstageandimproveit.\\nBUT(andhere’swhatmostpeopledowrong)!\\nTheyusethesametestsetagain.Thisisnotallowed!\\nThinkofitthisway.\\nYourprofessortaughtyouintheclassroom.Allin-classlessonsandexamplesare\\nthetrainset.\\nTheprofessorgaveyoutake-homeassignments,whichactedlikevalidationsets.\\nYougotsomewrongandsomeright.Basedonthis,youadjustedyourtopic\\nfundamentals,i.e.,improvedthemodel.\\nNow,ifyoukeepsolvingthesametake-homeassignmentrepeatedly,youwill\\neventuallyoverﬁtit,won’tyou?\\nThatiswhywebringinanewvalidationseta\\x00ersomeiterations.\\nTheﬁnalexamdaypaperisyourtestset.Ifyoudowell,awesome!\\nButifyoufail,theprofessorcannotgiveyoutheexactexampapernexttime,can\\nthey?Thisisbecauseyouknowwhat’sinside.\\nOfcourse,byevaluatingamodelonthetestset,themodelnevergetsto“know”\\nthepreciseexamplesinsidethatset.Buttheissueisthatthetestsethasbeen\\nexposednow.\\nYourpreviousevaluationwillinevitablyinﬂuenceanyfurtherevaluationsonthat\\nspeciﬁctestset.ThatiswhyyoumustalwaysuseaspeciﬁctestsetonlyONCE.\\nOnceyoudo,mergeitwiththetrainandvalidationsetandgenerateanentirely\\nnewsplit.\\nRepeat.\\nAndthatishowyouusetrain,validation,andtestsetsinmachinelearning.\\n136'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 137, 'page_label': '138', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\nTuningandvalidatingmachinelearningmodelsonasinglevalidationsetcanbe\\nmisleadingandsometimesyieldoverlyoptimisticresults.\\nThiscanoccurduetoaluckyrandomsplitofdata,whichresultsinamodelthat\\nperformsexceptionallywellonthevalidationsetbutpoorlyonnew,unseendata.\\nThatiswhyweo\\x00enusecrossvalidationinsteadofsimplesingle-setvalidation.\\nCrossvalidationinvolvesrepeatedlypartitioningtheavailabledataintosubsets,\\ntrainingthemodelonafewsubsets,andvalidatingontheremainingsubsets.\\nThemainadvantageofcrossvalidationisthatitprovidesamorerobustand\\nunbiasedestimateofmodelperformancecomparedtothetraditionalvalidation\\nmethod.\\nBelowareﬁveofthemostcommonlyusedandmust-knowcrossvalidation\\ntechniques.\\n\\x00e\\x00\\x00e\\x00\\x00n\\x00\\x00O\\x00\\x00\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\n● Leaveonedatapointforvalidation.\\n● Trainthemodelontheremainingdatapoints.\\n● Repeatforallpoints.\\n● Ofcourse,asyoumayhaveguessed,thisispracticallyinfeasiblewhenyou\\nhavemanydatapoints.Thisisbecausenumberofmodelsisequalto\\nnumberofdatapoints.\\n● WecanextendthistoLeave-p-OutCrossValidation,where,ineach\\niteration,pobservationsarereservedforvalidation,andtherestareused\\nfortraining.\\n137'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 138, 'page_label': '139', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00-\\x00\\x00l\\x00\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\n● Splitdataintokequally-sizedsubsets.\\n● Selectonesubsetforvalidation.\\n● Trainthemodelontheremainingsubsets.\\n● Repeatforallsubsets.\\n\\x00o\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\n● Mostlyusedfordatawithtemporalstructure.\\n● Datasplittingrespectsthetemporalorder,usingaﬁxed-sizetraining\\nwindow.\\n● Themodelisevaluatedonthesubsequentwindow.\\n138'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 139, 'page_label': '140', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00l\\x00\\x00k\\x00\\x00\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\n● Anothercommontechniquefortime-seriesdata.\\n● Incontrasttorollingcrossvalidation,thesliceofdataisintentionallykept\\nshortifthevariancedoesnotchangeappreciablyfromonewindowtothe\\nnext.\\n● Thisalsosavescomputationoverrollingcrossvalidation.\\n\\x00t\\x00\\x00t\\x00ﬁ\\x00d\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\n● Theabove-discussedtechniquesmaynotworkforimbalanceddatasets.\\nStratiﬁedcrossvalidationismainlyusedforpreservingtheclass\\ndistribution.\\n● Thepartitioningensuresthattheclassdistributionispreserved.\\nLet’scontinueourdiscussiononcrossvalidationinthenextchapter.\\n139'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 140, 'page_label': '141', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00\\x00o\\x00o\\x00f\\x00\\x00r\\x00r\\x00\\x00s\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00?\\nLetmeaskyouaquestion.\\nButbeforeIdothat,Ineedtoborrowyourimaginationforjustamoment.\\n\\x00m\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00r\\x00\\x00u\\x00\\x00d\\x00\\x00g\\x00o\\x00\\x00\\x00u\\x00\\x00r\\x00\\x00s\\x00\\x00\\x00a\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00o\\x00\\x00l\\x00\\x00o\\x00\\n\\x00r\\x00\\x00s\\x00\\x00g\\x00r\\x00\\x00s\\x00\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00n\\x00p\\x00\\x00m\\x00\\x00\\x00e\\x00\\x00f\\n\\x00y\\x00\\x00r\\x00\\x00r\\x00\\x00e\\x00\\x00r\\x00\\x00\\nEssentially,everyhyperparameterconﬁgurationcorrespondstoacross-validation\\nperformance:\\nA\\x00erobtainingthebesthyperparameters,weneedtoﬁnalizeamodel(sayfor\\nproduction);otherwise,whatisthepointofallthishassle?Now,here’sthe\\nquestion:\\nA\\x00erobtainingtheoptimalhyperparameters,whatwouldyoubemoreinclined\\ntodo:\\n1)Retrainthemodelagainonthe\\nentiredata(train+validation+test)\\nwiththeoptimalhyperparameters?\\nIfwedothis,rememberthatwecan’t\\nreliablyvalidatethisnewmodelas\\nthereisnounseendatale\\x00.\\n140'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 141, 'page_label': '142', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n2)Justproceedwiththe\\nbest-performingmodelbasedon\\ncross-validationperformanceitself.If\\nwedothis,rememberthatweare\\nleavingoutimportantdata,whichwe\\ncouldhavetrainedourmodelwith.\\nWhatwouldyoudo?\\n\\x00e\\x00\\x00m\\x00\\x00n\\x00\\x00\\x00\\x00a\\x00\\x00\\nMystrongpreferencehasalmostalwaysbeen“retraininganewmodelwith\\nentiredata.”\\nThereare,ofcourse,someconsiderationstokeepinmind,whichIhavelearned\\nthroughthemodelsIhavebuiltanddeployed.Thatsaid,inmostcases,retraining\\nistheidealwaytoproceed.\\nLetmeexplain.\\n\\x00h\\x00\\x00e\\x00\\x00\\x00i\\x00\\x00h\\x00\\x00o\\x00\\x00l\\x00\\nWewouldwanttoretrainanewmodelbecause,inaway,wearealreadysatisﬁed\\nwiththecross-validationperformance,which,byitsverynature,isanout-of-fold\\nmetric.\\nAnout-of-folddataisdatathathasnotbeenseenbythemodelduringthe\\ntraining.Anout-of-foldmetricistheperformanceonthatdata.\\n141'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 142, 'page_label': '143', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInotherwords,wealreadybelievethatthemodelalignswithhowweexpectitto\\nperformonunseendata.\\nThus,incorporatingthisunseenvalidationsetinthetrainingdataandretraining\\nthemodelwillMOSTLIKELYhaveNOeﬀectonitsperformanceonunseendata\\na\\x00erdeployment(assumingasuddencovariateshi\\x00hasn’tkickedin,whichisa\\ndiﬀerentissuealtogether).\\nIf,however,wewerenotsatisﬁedwiththecross-validationperformanceitself,we\\nwouldn’tevenbethinkingaboutﬁnalizingamodelintheﬁrstplace.\\nInstead,wewouldbethinkingaboutwaystoimprovethemodelbyworkingon\\nfeatureengineering,tryingnewhyperparameters,experimentingwithdiﬀerent\\nmodels,andmore.\\nThereasoningmakesintuitivesenseaswell.\\nIt’shardformetorecallanyinstancewhereretrainingdidsomething\\ndisastrouslybadtotheoverallmodel.\\n142'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 143, 'page_label': '144', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInfact,Ivividlyrememberoneinstancewherein,whileIwasproductionizingthe\\nmodel(ittookmeacoupleofdaysa\\x00erretraining),theteamhadgatheredsome\\nmorelabeleddata.\\nThemodeldidn’tshowanyperformancedegradationwhenIevaluatedit(justto\\ndouble-check).Asanaddedbeneﬁt,thisalsohelpedensurethatIhadmadeno\\nerrorswhileproductionizingmymodel.\\n\\x00o\\x00\\x00\\x00o\\x00\\x00i\\x00\\x00r\\x00\\x00i\\x00\\x00s\\nHere,pleasenotethatit’snotarulethatyoumustalwaysretrainanewmodel.\\nTheﬁelditselfandthetasksonecansolveareprettydiverse,soonemustbe\\nopen-mindedwhilesolvingtheproblemathand.OneofthereasonsIwouldn’t\\nwanttoretrainanewmodelisthatittakesdaysorweekstotrainthemodel.\\nInfact,evenifweretrainanewmodel,thereareMANYbusinesssituationsin\\nwhichstakesarejusttoohigh.\\n143'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 144, 'page_label': '145', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,onecanneveraﬀordtobenegligentaboutdeployingamodelwithout\\nre-evaluatingit—transactionalfraud,forinstance.\\nInsuchcases,Ihaveseenthatwhileateamworksonproductionizingthemodel,\\ndataengineersgathersomemoredatainthemeantime.\\nBeforedeploying,theteamwoulddosomeﬁnalchecksonthatdataset.\\nThenewlygathereddataisthenconsideredinthesubsequentiterationsofmodel\\nimprovements.\\n144'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 145, 'page_label': '146', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00l\\x00\\x00e\\x00\\x00e\\x00\\x00\\x00s\\x00\\x00i\\x00\\x00-\\x00\\x00r\\x00\\x00\\x00c\\x00\\x00r\\x00\\x00e\\x00\\x00ﬀ\\nItiswell-knownthatasthenumberofmodelparametersincreases,wetypically\\noverﬁtthedatamoreandmore.Forinstance,considerﬁttingapolynomial\\nregressionmodeltrainedonthisdummydatasetbelow:\\n\\x00n\\x00a\\x00\\x00\\x00o\\x00\\x00o\\x00\\x00t\\x00n\\x00\\x00,\\x00h\\x00\\x00\\x00s\\x00a\\x00\\x00e\\x00\\x00\\x00o\\x00\\x00n\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00\\x00s\\x00\\x00o\\x00\\x00o\\x00\\x00l\\x00\\nItisexpectedthataswe’llincreasethedegree(m)andtrainthepolynomial\\nregressionmodel:\\n● Thetraininglosswillgetcloserandclosertozero.\\n● Thetest(orvalidation)losswillﬁrstreduceandthengetbiggerandbigger.\\nThisisbecause,withahigherdegree,themodelwillﬁnditeasiertocontortits\\nregressionﬁtthrougheachtrainingdatapoint,whichmakessense.\\n145'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 146, 'page_label': '147', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInfact,thisisalsoevidentfromthefollowinglossplot:\\nButnoticewhathappenswhenwecontinuetoincreasethedegree(\\x00):\\nThat’sstrange,right?\\nWhydoesthetestlossincreasetoacertainpointbutthendecrease?\\nThiswasnotexpected,wasit?\\n146'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 147, 'page_label': '148', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWell…whatyouareseeingiscalledthe“doubledescentphenomenon,”whichis\\nquitecommonlyobservedinmanyMLmodels,especiallydeeplearningmodels.\\nItshowsthat,counterintuitively,increasingthemodelcomplexitybeyondthe\\npointofinterpolationcanimprovegeneralizationperformance.\\nInfact,thiswholeideaisdeeplyrootedtowhyLLMs,althoughmassivelybig\\n(billionsoreventrillionsofparameters),canstillgeneralizeprettywell.\\nAndit’shardtoacceptitbecausethisphenomenondirectlychallengesthe\\ntraditionalbias-variancetrade-oﬀwelearninanyintroductoryMLclass:\\nPuttingitanotherway,trainingverylargemodels,evenwithmoreparameters\\nthantrainingdatapoints,canstillgeneralizewell.\\nTothebestofmyknowledge,thisisstillanopenquestion,anditisn’tentirely\\nclearwhyneuralnetworksexhibitthisbehavior.\\nTherearesometheoriesaroundregularization,however,suchasthisone:\\nItcouldbethatthemodelappliessomesortofimplicitregularization,with\\nwhich,itcanpreciselyfocusonanaptnumberofparametersforgeneralization.\\nButtobehonest,nothingisclearyet.\\n147'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 148, 'page_label': '149', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00t\\x00\\x00i\\x00\\x00\\x00c\\x00\\x00\\x00o\\x00\\x00d\\x00\\x00i\\x00\\x00s\\n\\x00L\\x00\\x00s\\x00\\x00M—\\x00h\\x00\\x00’\\x00\\x00h\\x00\\x00iﬀ\\x00r\\x00\\x00c\\x00\\x00\\nMaximumlikelihoodestimation(MLE)andexpectationmaximization(EM)are\\ntwopopulartechniquestodeterminetheparametersofstatisticalmodels.\\nDuetoitsapplicabilityinMANYstatisticalmodels,Ihaveseenitbeingaskedin\\nplentyofdatascienceinterviewsaswell,especiallythedistinctionbetweenthe\\ntwo.Thefollowingvisualsummarizeshowtheywork:\\n148'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 149, 'page_label': '150', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00m\\x00\\x00\\x00i\\x00\\x00l\\x00\\x00o\\x00\\x00\\x00s\\x00\\x00m\\x00\\x00i\\x00\\x00\\x00M\\x00\\x00)\\nMLEstartswithalabeleddatasetandaimstodeterminetheparametersofthe\\nstatisticalmodelwearetryingtoﬁt.\\nTheprocessisprettysimpleandstraightforward.InMLE,we:\\n● Startbyassumingadatagenerationprocess.Simplyput,thisdata\\ngenerationprocessreﬂectsourbeliefaboutthedistributionoftheoutput\\nlabel(\\x00),giventheinput(\\x00).\\n● Next,wedeﬁnethelikelihoodofobservingthedata.Aseachobservationis\\nindependent,thelikelihoodofobservingtheentiredataisthesameasthe\\nproductofobservingindividualobservations:\\n149'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 150, 'page_label': '151', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Thelikelihoodfunctionabovedependsonparametervalues(θ).Our\\nobjectiveistodeterminethosespeciﬁcparametervaluesthatmaximizethe\\nlikelihoodfunction.Wedothisasfollows:\\nThisgivesourparameterestimatesthatwouldhavemostlikelygeneratedthe\\ngivendata.\\nThatwasprettysimple,wasn’tit?\\nButwhatdowedoifwe\\ndon’thavetruelabels?\\nWestillwantto\\nestimatetheparameters,\\ndon’twe?\\nMLE,asyoumayhaveguessed,willnotbeapplicable.Thetruelabel(y),being\\nunobserved,makesitimpossibletodeﬁnealikelihoodfunctionlikewedid\\nearlier.\\nInsuchcases,advancedtechniqueslikeexpectationmaximizationarepretty\\nhelpful.\\n150'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 151, 'page_label': '152', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00x\\x00\\x00c\\x00\\x00t\\x00\\x00n\\x00a\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\x00E\\x00\\x00\\nEMisaniterativeoptimizationtechniquetoestimatetheparametersof\\nstatisticalmodels.Itisparticularlyusefulwhenwehaveanunobserved(or\\nhidden)label.Oneexamplesituationcouldbeasfollows:\\nAsdepictedabove,weassumethatthedatawasgeneratedfrommultiple\\ndistributions(amixture).However,theobserved/completedatadoesnotcontain\\nthatinformation.Inotherwords,theobserveddatasetdoesnothaveinformation\\naboutwhetheraspeciﬁcrowwasgeneratedfromdistribution1ordistribution2.\\nHaditcontainedthelabel(\\x00)information,wewouldhavealreadyusedMLE.\\nEMhelpsuswithparameterestimatesofsuchdatasets.ThecoreideabehindEM\\nisasfollows:\\n● Makeaguessabouttheinitialparameters(θ).\\n● Expectation(E)step:Computetheposteriorprobabilitiesofthe\\nunobservedlabel(let’scallit‘z’)usingtheaboveparameters.\\n151'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 152, 'page_label': '153', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n○ Here,‘z’isalsocalledalatentvariable,whichmeanshiddenor\\nunobserved.\\n○ Relatingittoourcase,weknowthatthetruelabelexistsinnature.\\nButwedon’tknowwhatitis.\\n○ Thus,wereplaceitwithalatentvariable‘z’andestimateits\\nposteriorprobabilitiesusingtheguessedparameters.\\n● Giventhatwenowhaveaproxy(notprecise,though)forthetruelabel,we\\ncandeﬁnean“expectedlikelihood”function.Thus,weusetheabove\\nposteriorprobabilitiestodoso:\\n● Maximization(M)step:Sonowwehavealikelihoodfunctiontoworkwith.\\nMaximizingitwithrespecttotheparameterswillgiveusanewestimate\\nfortheparameters(θ`).\\n● Next,weusetheupdatedparameters(θ`)torecomputetheposterior\\nprobabilitieswedeﬁnedintheexpectationstep.\\n● Wewillupdatethelikelihoodfunction(L)usingthenewposterior\\nprobabilities.\\n● Again,maximizingitwillgiveusanewestimatefortheparameters(θ).\\n● Andthisprocessgoesonandonuntilconvergence.\\nThepointisthatinexpectationmaximization,werepeatedlyiteratebetweenthe\\nEandtheMstepsuntiltheparametersconverge.AgoodthingaboutEMisthat\\nitalwaysconverges.Yet,attimes,itmightconvergetoalocalextrema.\\nMLEvs.EMisapopularquestionaskedinmanydatascienceinterviews.\\n152'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 153, 'page_label': '154', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00ﬁ\\x00e\\x00\\x00e\\x00n\\x00\\x00r\\x00\\x00l\\x00n\\x00\\x00r\\x00\\x00i\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00r\\x00\\x00l\\nStatisticalestimatesalwayshavesomeuncertainty.\\nForinstance,alinearregressionmodelneverpredictsanactualvalue.\\nConsiderasimpleexampleofmodelinghousepricesjustbasedonitsarea.A\\npredictionwouldn’ttellthetruevalueofahousebasedonitsarea.Thisis\\nbecausediﬀerenthousesofthesamesizecanhavediﬀerentprices.\\nInstead,whatitpredictsisthemeanvaluerelatedtotheoutcomeataparticular\\ninput.\\nThepointis…\\nThere’salwayssomeuncertaintyinvolvedinstatisticalestimates,anditis\\nimportanttocommunicateit.\\nInthisspeciﬁccase,therearetwotypesofuncertainties:\\n● Theuncertaintyinestimatingthetruemeanvalue.\\n● Theuncertaintyinestimatingthetruevalue.\\nConﬁdenceintervalandpredictionintervalhelpuscapturetheseuncertainties.\\nLet’sunderstand.\\n153'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 154, 'page_label': '155', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nConsiderthefollowingdummydataset:\\nLet’sﬁtalinearregressionmodelusingstatsmodelandprintapartofthe\\nregressionsummary:\\nNoticethatthecoeﬃcientofthepredictor“x1”is\\x00.\\x00\\x000\\x00witha\\x005\\x00interval\\nof \\x001\\x00\\x007\\x00\\x00\\x00.\\x00\\x008\\x00.\\n\\x00t\\x00s\\x00\\x005\\x00\\x00n\\x00\\x00r\\x00\\x00l\\x00e\\x00\\x00u\\x00\\x00\\x00.\\x00\\x005\\x00\\x00.\\x00\\x005\\x00\\x00.\\x00\\x00.\\nThisisknownastheconﬁdenceinterval,whichcomesfromsampling\\nuncertainty.\\n154'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 155, 'page_label': '156', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nMorespeciﬁcally,this\\nuncertaintyarisesbecause\\nthedataweusedabovefor\\nmodelingisjustasampleof\\nthepopulation.\\nSo,ifwegatheredmoresuchsamplesandﬁtanOLStoeachsample,thetrue\\ncoeﬃcient(whichwecanonlyknowifwehadthedatafortheentirepopulation)\\nwouldlie95%ofthetimeinthisconﬁdenceinterval.\\nNext,weusethismodeltomakeapredictionasfollows:\\n● Thepredictedvalueis\\x00.\\x00\\x00\\x00m\\x00\\x00n\\x00.\\n● The\\x005\\x00conﬁdenceintervalis\\x006\\x00\\x003\\x00\\x00\\x00.\\x00\\x009\\x00.\\n● The\\x005\\x00predictionintervalis\\x001\\x00\\x006\\x00\\x004\\x00\\x009\\x00.\\nTheconﬁdenceintervalwesawabovewasforthecoeﬃcient,sowhatdoesthe\\nconﬁdenceintervalrepresentinthiscase?\\nSimilartowhatwediscussedabove,thedataisjustasampleofthepopulation.\\nTheregressionﬁtobtainedbythissampleproducedaprediction(somemean\\nvalue)fortheinput\\x00=\\x00.\\nHowever,ifwegatheredmoresuchsamplesandﬁtanOLStoeachdataset,the\\ntruemeanvalue(whichwecanonlyknowifwehadthedatafortheentire\\n155'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 156, 'page_label': '157', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\npopulation)forthisspeciﬁcinput(\\x00=\\x00)wouldlie\\x005\\x00ofthetimeinthis\\nconﬁdenceinterval.\\nComingtothepredictioninterval…\\n…wenoticethatitiswiderthantheconﬁdenceinterval.Whyisit,andwhatdoes\\nthisintervaltell?\\nWhatwesawabovewithconﬁdenceintervalwasaboutestimatingthetrue\\npopulationmeanataspeciﬁcinput.\\nWhatwearetalkingaboutnowisobtaininganintervalwherethetruevaluefor\\naninputcanlie.\\nThus,thisadditionaluncertaintyappearsbecauseinourdataset,forthesame\\nvalueofinputx,therecanbemultiplediﬀerentvaluesoftheoutcome.Thisis\\ndepictedbelow:\\n156'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 157, 'page_label': '158', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,itiswiderthantheconﬁdenceinterval.Plottingitacrosstheentireinput\\nrange,wegetthefollowingplot:\\nGiventhatthemodelispredictingameanvalue(asdepictedbelow),wehaveto\\nrepresentthepredictionuncertaintythattheactualvaluecanlieanywhereinthe\\npredictioninterval:\\nA\\x005\\x00predictionintervaltellsusthatwecanbe\\x005\\x00surethattheactualvalue\\nofthisobservationwillfallwithinthisinterval.\\n157'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 158, 'page_label': '159', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSotosummarize:\\n● Aconﬁdenceintervalcapturesthesamplinguncertainty.Moredatameans\\nlesssamplinguncertainty,whichinturnleadstoasmallerinterval.\\n● Inadditiontothesamplinguncertainty,thepredictionintervalalso\\nrepresentstheuncertaintyinestimatingthetruevalueofaparticulardata\\npoint.Thus,itiswiderthantheconﬁdenceinterval.\\nCommunicatingtheseuncertaintiesisquitecrucialindecision-makingbecause\\nitprovidesaclearerunderstandingofthereliabilityandprecisionofpredictions.\\nThistransparencyallowsstakeholderstomakemoreinformeddecisionsby\\nconsideringtherangeofpossibleoutcomesandtheassociatedrisks.\\n158'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 159, 'page_label': '160', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00s\\x00L\\x00\\x00a\\x00\\x00e\\x00\\x00n\\x00n\\x00\\x00a\\x00\\x00d\\x00s\\x00\\x00m\\x00\\x00o\\x00\\x00\\nTheOLSestimatorforlinearregression(shownbelow)isknownasanunbiased\\nestimator.\\n● Whatdowemeanbythat?\\n● WhyisOLScalledsuch?\\n\\x00a\\x00\\x00g\\x00\\x00u\\x00\\x00\\nThegoalofstatisticalmodelingistomakeconclusionsaboutthewhole\\npopulation.\\nHowever,itisprettyobviousthatobservingtheentirepopulationisimpractical.\\nInotherwords,giventhatwecannotobserve(orcollectdataof)theentire\\npopulation,wecannotobtainthetrueparameter(β)forthepopulation:\\n159'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 160, 'page_label': '161', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,wemustobtainparameterestimates(B̂)onsamplesandinferthetrue\\nparameter(β)forthepopulationfromthoseestimates:\\nAnd,ofcourse,wewantthesesampleestimates(B̂)tobereliabletodeterminethe\\nactualparameter(β).\\nTheOLSestimatorensuresthat.\\nLet’sunderstandhow!\\n\\x00r\\x00\\x00\\x00o\\x00\\x00l\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00l\\nWhenusingalinearregressionmodel,weassumethattheresponsevariable(Y)\\nandfeatures(X)fortheentirepopulationarerelatedasfollows:\\n● βisthetrueparameterthatwearenotawareof.\\n● εistheerrorterm.\\n\\x00x\\x00\\x00c\\x00\\x00\\x00\\x00a\\x00\\x00e\\x00f\\x00L\\x00\\x00s\\x00\\x00m\\x00\\x00e\\x00\\nTheclosed-formsolutionofOLSisgivenby:\\n160'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 161, 'page_label': '162', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWhat’smore,asdiscussedabove,usingOLSondiﬀerentsampleswillresultin\\ndiﬀerentparameterestimates:\\nLet’sﬁndtheexpectedvalueofOLSestimates\\x00[\\x00̂\\x00.\\nSimplyput,theexpectedvalueistheaveragevalueoftheparametersifwerun\\nOLSonmanysamples.\\nThisisgivenby:\\nSubstitute\\x00̂ astheOLSsolution\\nHere,substitute\\x00\\x00β\\x00\\x00ε:\\nIfyouarewonderinghowwecansubstitute\\x00\\x00β\\x00\\x00εwhenwedon’tknow\\nwhatβis,thenhere’stheexplanation:\\nSee,wecandothatsubstitutionbecauseevenifwedon’tknowtheparameterβ\\nforthewholepopulation,weknowthatthesamplewasdrawnfromthe\\npopulation.\\n161'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 162, 'page_label': '163', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,theequationintermsofthetrueparameters(\\x00\\x00β\\x00\\x00ε)stillholdsfor\\nthesample.\\nLetmegiveyouanexample.\\nSaythepopulationdatawasdeﬁnedby \\x00\\x00\\x00i\\x00\\x00x\\x00\\x00ε.Ofcourse,wewouldn’t\\nknowthis,butjustkeepthatasideforasecond.\\nNow,evenifweweretodrawsamplesfromthispopulationdata,thetrue\\nequation\\x00\\x00\\x00i\\x00\\x00x\\x00\\x00εwouldstillbevalidonthesampleddatapoints,\\nwouldn’tit?\\nThesameideahasbeenextendedforexpectedvalue.\\nComingbacktothefollowing:\\nLet’sopentheinnerparenthesis:\\n162'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 163, 'page_label': '164', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSimplifying,weget:\\nAndﬁnally,whatdoweget?\\nTheexpectedvalueofparameterestimatesonthesamplesequalsthetrue\\nparametervalue β.\\nAndthisispreciselywhatthedeﬁnitionofanunbiasedestimatoris.\\nMoreformally,anestimatoriscalledunbiasediftheexpectedvalueofthe\\nparametersisequaltotheactualparametervalue.\\nAndthatiswhywecallOLSanunbiasedestimator.\\n163'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 164, 'page_label': '165', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00a\\x00\\x00a\\x00\\x00y\\nManypeoplemisinterpretunbiasednesswiththeideathattheparameter\\nestimatesfromasinglerunofOLSonasampleareequaltothetrueparameter\\nvalues.\\nDon’tmakethatmistake.\\nInstead,unbiasednessimpliesthatifweweretogenerateOLSestimatesonmany\\ndiﬀerentsamples(drawnfromthesamepopulation),thentheexpectedvalueof\\nobtainedestimateswillbeequaltothetruepopulationparameter.\\nAnd,ofcourse,allthisisbasedontheassumptionthatwehavegood\\nrepresentativesamplesandthattheassumptionsoflinearregressionarenot\\nviolated.\\n164'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 165, 'page_label': '166', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00t\\x00\\x00h\\x00\\x00y\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00e\\nAssessingthesimilaritybetweentwoprobabilitydistributionsisquitehelpfulat\\ntimes.Forinstance,imaginewehavealabeleddataset\\x00X\\x00\\x00).\\nByanalyzingthelabel(\\x00)distribution,wemayhypothesizeitsdistributionbefore\\nbuildingastatisticalmodel.\\nWealsolookedatthisinanearlierchapterongeneralizedlinearmodels(GLMs).\\nWhilevisualinspectioniso\\x00en\\nhelpful,thisapproachisquite\\nsubjectiveandmayleadto\\nmisleadingconclusions.\\nThus,itisessentialtobeawareofquantitativemeasuresaswell.Bhattacharyya\\ndistanceisonesuchreliablemeasure.\\nItquantiﬁesthesimilaritybetweentwoprobabilitydistributions.\\nThecoreideaistoapproximatetheoverlapbetweentwodistributions,which\\nmeasuresthe“closeness”betweenthetwodistributionsunderconsideration.\\n165'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 166, 'page_label': '167', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nBhattacharyyadistanceismeasuredasfollows:\\nFortwodiscreteprobability\\ndistributions\\nFortwocontinuousprobability\\ndistributions(replace\\nsummationwithanintegral):\\nItseﬀectivenessisevidentfromtheimagebelow.\\nHere,wehavean\\nobserveddistribution\\n(Blue).Next,we\\nmeasureitsdistance\\nfrom:\\n● Gaussian→0.19.\\n● Gamma→0.03.\\nAhighBhattacharyya\\ndistanceindicatesless\\noverlapormore\\ndissimilarity.Thislets\\nusconcludethatthe\\nobserveddistribution\\nresemblesaGamma\\ndistribution.\\nTheresultsalsoresonatewithvisualinspection.\\n166'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 167, 'page_label': '168', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00L\\x00i\\x00\\x00r\\x00\\x00\\x00c\\x00\\x00s\\x00\\x00h\\x00\\x00t\\x00\\x00h\\x00\\x00y\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00e\\nNow,manyo\\x00engetconfusedbetweenKLDivergenceandBhattacharyya\\ndistance.Eﬀectively,botharequantitativemeasurestodeterminethe“similarity”\\nbetweentwodistributions.\\nHowever,theirnotionof“similarity”isentirelydiﬀerent.\\nThecoreideabehindKLDivergenceistoassesshowmuchinformationislost\\nwhenonedistributionisusedtoapproximateanotherdistribution.\\nThemoreinformationislost,themoretheKLDivergenceand,consequently,the\\nlessthe“similarity”.Also,approximatingadistributionQusingPmaynotbethe\\nsameasdoingthereverse—PusingQ.ThismakesKLDivergenceasymmetric\\ninnature.\\nMovingon,\\nBhattacharyyadistance\\nmeasurestheoverlap\\nbetweentwo\\ndistributions.\\nThis“overlap”iso\\x00eninterpretedasameasureofcloseness(ordistance)\\nbetweenthetwodistributionsunderconsideration.Thus,Bhattacharyyadistance\\nprimarilyservesadistancemetric,likeEuclidean,forinstance.\\n167'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 168, 'page_label': '169', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nBeingadistancemetric,itissymmetric\\ninnature.Wecanalsoverifythisfrom\\nthedistanceformula:\\nJustlikeweuseEuclideandistanceto\\nﬁndthedistancebetweentwopoints,we\\ncanuseBhattacharyyadistancetoﬁnd\\nthedistancebetweentwodistributions.\\nButifweintendtomeasuretheamountofinformationlostwhenweapproximate\\nonedistributionusinganother,KLdivergenceismoreapt.Infact,KLdivergence\\nalsoservesasalossfunctioninmachinelearningalgorithmsattimes(int-SNE).\\n● Saywehaveanobserveddistribution(P)andwanttoapproximateitwith\\nanothersimplerdistributionQ.\\n● So,wecandeﬁneasimplerparametricdistributionQ.\\n● Next,wecanmeasuretheinformationlostbyapproximatingPusingQ\\nwithKLdivergence.\\n● Aswewanttominimizetheinformationlost,wecanuseKLdivergenceas\\nourobjectivefunction.\\n● Finally,wecanusegradientdescenttodeterminetheparametersofQsuch\\nthatweminimizetheKLdivergence.\\nBhattacharyyadistancehasmanyapplications,notjustinmachinelearningbut\\ninmanyotherdomains.Forinstance:\\n● Usingthisdistance,wecansimplifycomplexdistributionstosimpleones\\nifthedistanceislow.\\n● Inimageprocessing,Bhattacharyyadistanceiso\\x00enusedforimage\\nmatching.Bycomparingthecolorortexturedistributionsofimages,it\\nhelpsidentifysimilarobjectsorscenes,etc.\\nTheonlysmallcaveatisthatBhattacharyyadistancedoesnotsatisfythetriangle\\ninequality,sothat’ssomethingtokeepinmind.\\n168'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 169, 'page_label': '170', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00 \\x00r\\x00\\x00e\\x00 \\x00a\\x00\\x00l\\x00\\x00o\\x00\\x00s \\x00i\\x00\\x00a\\x00\\x00e \\x00v\\x00\\x00 \\x00u\\x00\\x00i\\x00\\x00a\\x00\\n\\x00i\\x00\\x00a\\x00\\x00e\\x00\\nDuringdistancecalculation,Euclideandistanceassumesindependentaxes.\\nThus,Euclideandistancewillproducemisleadingresultsifyourfeaturesare\\ncorrelated.Forinstance,considerthisdummydatasetbelow:\\nClearly,thefeaturesarecorrelated.Here,considerthreepointsmarkedP1,P2,\\nandP3inthisdataset.\\n169'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 170, 'page_label': '171', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nConsideringthedatadistribution,somethingtellsusthatP2isclosertoP1than\\nP3.ThisisbecauseP2liesmorewithinthedatadistributionthanP3.\\nYet,Euclideandistanceignoresthis,andP2andP3comeouttobeequidistantto\\nP1,asdepictedbelow:\\nMahalanobisdistanceaddressesthislimitation.Itisadistancemetricthattakes\\nintoaccountthedatadistribution.\\nAsaresult,itcanmeasurehowfarawayadatapointisfromthedistribution,\\nwhichEuclideancannot.\\nReferringtotheearlierdatasetagain,withMahalanobisdistance,P2comesout\\ntobeclosertoP1thanP3.\\n170'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 171, 'page_label': '172', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00o\\x00\\x00\\x00t\\x00o\\x00\\x00?\\nInagist,theobjectiveistoconstructanewcoordinatesystemwithindependent\\nandorthogonalaxes.Thestepsare:\\n● Step1:Transformthecolumnsintouncorrelatedvariables.\\n● Step2:Scalethenewvariablestomaketheirvarianceequalto1.\\n● Step3:FindtheEuclideandistanceinthisnewcoordinatesystem.\\nSo,eventually,wedouseEuclideandistance.However,weﬁrsttransformthe\\ndatatoensurethatitobeystheassumptionsofEuclideandistance.\\n\\x00s\\x00\\x00\\nOneofthemostcommon\\nusecasesofMahalanobis\\ndistanceisoutlier\\ndetection.Reconsidering\\nthedatasetwediscussed\\nearlierwhereP3isclearly\\nanoutlier.\\n171'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 172, 'page_label': '173', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfweconsiderP1asthedistribution’scentroidanduseEuclideandistance,we\\nwillinferthatP3isnotanoutlierasbothP2andP3areequidistanttoP1.\\nUsingMahalanobisdistance,however,providesaclearerpicture:\\nThisbecomesmoreusefulinahigh-dimensionalsettingwherevisualizationis\\ninfeasible.\\nAnotherusecasewetypicallydonothearofo\\x00en,butthatexistsisavariantof\\nkNNthatisimplementedwithMahalanobisdistanceinstead.\\nScipyimplementstheMahalanobisdistance,whichyoucancheckhere:\\nhttps://bit.ly/3LjAymm.\\n172'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 173, 'page_label': '174', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00a\\x00\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00a\\x00\\x00\\x00o\\x00\\x00a\\x00\\x00t\\x00\\nManyMLmodelsassume(orworkbetter)underthepresenceofnormal\\ndistribution.\\nForinstance:\\n● Linearregressionassumesresidualsarenormallydistributed.\\n● Attimes,transformingthedatatonormaldistributioncanbebeneﬁcial.\\n● Lineardiscriminantanalysis(LDA)isderivedundertheassumptionof\\nnormaldistribution,etc.\\nThus,beingawareofthewaystotestnormalityisextremelycrucialfordata\\nscientists.Thevisualbelowdepictsthe11essentialwaystotestnormality.\\nLet’sunderstandtheseinthischapter.\\n173'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 174, 'page_label': '175', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00l\\x00\\x00t\\x00\\x00g\\x00e\\x00\\x00o\\x00\\x00\\x00s\\x00\\x00f\\x00\\x00x\\x00\\x00a\\x00\\x00t\\x00\\x00y\\x00\\n● Histogram\\n● QQPlot(Weshallcoveritintheplottingsectionofthisbook).\\n● KDEPlot\\n● ViolinPlot\\nWhileplottingiso\\x00enreliable,itisasubjectiveapproachandpronetoerrors.\\nThus,wemustknowreliablequantitativemeasuresaswell.\\n\\x002\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00c\\x00\\x00\\x00e\\x00\\x00o\\x00\\x00:\\n1) Shapiro-Wilktest:\\n● Findsastatisticusingthecorrelationbetweentheobserveddataand\\ntheexpectedvaluesunderanormaldistribution.\\n● Thep-valueindicatesthelikelihoodofobservingsuchacorrelation\\nifthedatawerenormallydistributed.\\n● Ahighp-valueindicatesanormaldistribution.\\n2) KStest:\\n● Measuresthemaxdiﬀerencebetweenthecumulativedistribution\\nfunctions(CDF)ofobservedandnormaldistribution.\\n● Theoutputstatisticisbasedonthemaxdiﬀerencebetweenthetwo\\nCDFs.\\n● Ahighp-valueindicatesanormaldistribution.\\n3) Anderson-Darlingtest:\\n● Measuresthediﬀerencesbetweentheobserveddataandthe\\nexpectedvaluesunderanormaldistribution.\\n● Emphasizesthediﬀerencesinthetailofthedistribution.\\n● Thismakesitparticularlyeﬀectiveatdetectingdeviationsinthe\\nextremevalues.\\n174'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 175, 'page_label': '176', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n4)Lillieforstest:\\n● ItisamodiﬁcationoftheKStest.\\n● TheKStestisappropriateinsituationswheretheparametersofthe\\nreferencedistributionareknown.\\n● Iftheparametersareunknown,Lillieforsisrecommended.\\n● Getstarted:StatsmodelDocs.\\n\\x003\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00e\\x00\\x00u\\x00\\x00s\\nDistancemeasuresareanotherreliableandmoreintuitivewaytotestnormality.\\nButtheycanbeabittrickytouse.\\nSee,theproblemisthatasingledistancevalueneedsmorecontextfor\\ninterpretability.\\nForinstance,ifthedistancebetweentwodistributionsis5,isthislargeorsmall?\\nWeneedmorecontext.\\nIpreferusingthesemeasuresasfollows:\\n● Findthedistancebetweentheobserveddistributionandmultiplereference\\ndistributions.\\n● Selectthereferencedistributionwiththeminimumdistancetothe\\nobserveddistribution.\\n175'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 176, 'page_label': '177', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHereareafewdistancecommonandusefulmeasures:\\n1) Bhattacharyyadistance:\\n● Measuretheoverlapbetweentwodistributions.\\n● This“overlap”iso\\x00eninterpretedasclosenessbetweentwo\\ndistributions.\\n● ChoosethedistributionthathastheleastBhattacharyyadistanceto\\ntheobserveddistribution.\\n2) Hellingerdistance:\\n● ItisusedquitesimilartohowweusetheBhattacharyyadistance\\n● ThediﬀerenceisthatBhattacharyyadistancedoesnotsatisfy\\ntriangularinequality.\\n● ButHellingerdistancedoes.\\n176'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 177, 'page_label': '178', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n3) KLDivergence:\\n● Itisnotentirelya\"distancemetric\"perse,butcanbeusedinthis\\ncase.\\n● Measureinformationlostwhenonedistributionisapproximated\\nusinganotherdistribution.\\n● Themoreinformationislost,themoretheKLDivergence.\\n● ChoosethedistributionthathastheleastKLdivergencefromthe\\nobserveddistribution.\\nKLdivergenceisusedasalossfunctioninthet-SNEalgorithm.\\n177'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 178, 'page_label': '179', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00s\\x00\\x00i\\x00\\x00l\\x00\\x00o\\x00\\x00\\nIndatascienceandstatistics,manyfolkso\\x00enuse“probability”and“likelihood”\\ninterchangeably.\\nHowever,likelihoodandprobabilityDONOTconveythesamemeaning.\\nAndthemisunderstandingissomewhatunderstandable,giventhattheycarry\\nsimilarmeaningsinourregularlanguage.\\nWhilewritingthischapter,IsearchedfortheirmeaningintheCambridge\\nDictionary.Here’swhatitsays:\\n● Probability:thelevelofpossibilityofsomethinghappeningorbeingtrue.\\n● Likelihood:thechancethatsomethingwillhappen.\\nIfyounoticeclosely,“likelihood”istheonlysynonymof“probability”.\\nAnyway.\\nInmyopinion,itiscrucialtounderstandthatprobabilityandlikelihoodconvey\\nverydiﬀerentmeaningsindatascienceandstatistics.\\nLet’sunderstand!\\n178'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 179, 'page_label': '180', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nProbabilityisusedincontextswhereyouwishtoknowthepossibility/oddsofan\\nevent.\\nForinstance,whatisthe:\\n● Probabilityofobtaininganevennumberinadieroll?\\n● Probabilityofdrawinganaceofdiamondsfromadeck?\\n● andsoon…\\nWhentranslatedtoML,probabilitycanbethoughtofas:\\n● Whatistheprobabilitythatatransactionisfraud?\\n● Whatistheprobabilitythatanimagedepictsacat?\\n● andsoon…\\nEssentially,manyclassiﬁcationmodels,likelogisticregressionoraclassiﬁcation\\nneuralnetwork,etc.,assigntheprobabilityofaspeciﬁclabeltoaninput.\\nWhencalculatingprobability,themodel’sparametersareknown.Also,we\\nassumethattheyaretrustworthy.\\nForinstance,todeterminetheprobabilityofaheadinacointoss,wemostly\\nassumeandtrustthatitisafaircoin.\\nLikelihood,ontheotherhand,isaboutexplainingeventsthathavealready\\noccurred.\\nUnlikeprobability(whereparametersareknownandassumedtobe\\ntrustworthy)...\\n179'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 180, 'page_label': '181', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n…likelihoodhelpsusdetermineifwecantrusttheparametersinamodelbased\\nontheobserveddata.\\nLetmeelaboratemoreonthat.\\nAssumeyouhavecollectedsome2Ddataandwishtoﬁtastraightlinewithtwo\\nparameters—slope(m)andintercept(c).\\nHere,likelihoodisdeﬁnedasthesupportprovidedbyadatapointforsome\\nparticularparametervaluesinyourmodel.\\nHere,youwillaskquestionslike:\\n● IfImodelthisdatawiththeparameters:\\n○ \\x00=\\x00and\\x00=\\x00,whatisthelikelihoodofobservingthedata?\\n○ \\x00=\\x00and\\x00=\\x00,whatisthelikelihoodofobservingthedata?\\n○ andsoon…\\nTheaboveformulationpopularlytranslatesintothemaximumlikelihood\\nestimation(MLE),whichwediscussedhere:(\\x00L\\x00\\x00s\\x00\\x00M—\\x00h\\x00\\x00’\\x00\\x00h\\x00\\x00iﬀ\\x00r\\x00\\x00c\\x00\\x00)\\nInmaximumlikelihoodestimation,youhavesomeobserveddataandyouare\\ntryingtodeterminethespeciﬁcsetofparameters(θ)thatmaximizethelikelihood\\nofobservingthedata.\\n180'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 181, 'page_label': '182', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nUsingtheterm“likelihood”islike:\\n● Ihaveapossibleexplanationformydata.(Intheaboveillustration,\\n“explanation”canbethoughtofastheparametersyouaretryingto\\ndetermine)\\n● HowwelldoesmyexplanationexplainwhatI’vealreadyobserved?Thisis\\npreciselyquantiﬁedwithlikelihood.\\nForinstance:\\n● Observation:Theoutcomesof\\x000cointossesare“\\x00H\\x00\\x00\\x00H\\x00\\x00H\\x00”.\\n● Explanation:Ithinkitisafaircoin(\\x00=\\x00\\x005).\\n● Whatisthelikelihoodthatmyaboveexplanationistruebasedonthe\\nobserveddata?\\nTosummarize…\\nItisimmenselyimportanttounderstandthatindatascienceandstatistics,\\nlikelihoodandprobabilityDONOTconveythesamemeaning.\\nAsexplainedabove,theyareprettydiﬀerent.\\n181'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 182, 'page_label': '183', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInprobability:\\n● Wedeterminethepossibilityofanevent.\\n● Weknowtheparametersassociatedwiththeeventandassumethemtobe\\ntrustworthy.\\nInlikelihood:\\n● Wehavesomeobservations.\\n● Wehaveanexplanation(orparameters).\\n● Likelihoodhelpsusquantifywhethertheexplanationistrustworthy.\\nHopethathelped!\\n182'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 183, 'page_label': '184', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00e\\x00\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\x00\\x00n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\nStatisticalmodelsassumeanunderlyingdatagenerationprocess.\\nThisisexactlywhatletsusformulatethegenerationprocess,usingwhichwe\\ndeﬁnethemaximumlikelihoodestimation(MLE)step.\\nThus,whendealingwithstatisticalmodels,themodelperformancebecomes\\nentirelydependenton:\\n● Yourunderstandingofthedatagenerationprocess.\\n● Thedistributionyouchosetomodeldatawith,which,inturn,dependson\\nhowwellyouunderstandvariousdistributions.\\n\\x00e\\x00h\\x00\\x00l\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00t\\x00h\\x00\\x00\\x00n\\x00h\\x00\\x00h\\x00\\x00t\\x00\\x00\\x00n\\x00L\\x00\\x00\\x00\\x00h\\x00\\x00\\x00h\\x00\\x00t\\x00\\x00\\x00.\\nThus,itiscrucialtobeawareofsomeofthemostimportantdistributionsand\\nthetypeofdatatheycanmodel.\\nThevisualbelowdepictsthe11mostimportantdistributionsindatascience:\\n183'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 184, 'page_label': '185', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’sunderstandthembrieﬂyandhowtheyareused.\\n184'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 185, 'page_label': '186', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00a\\x00\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Themostwidelyuseddistributionindatascience.\\n● Characterizedbyasymmetricbell-shapedcurve\\n● Itisparameterizedbytwoparameters—meanandstandarddeviation.\\n● Example:Heightofindividuals.\\n\\x00e\\x00\\x00o\\x00\\x00l\\x00\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Adiscreteprobabilitydistributionthatmodelstheoutcomeofabinary\\nevent.\\n● Itisparameterizedbyoneparameter—theprobabilityofsuccess.\\n● Example:Modelingtheoutcomeofasinglecoinﬂip.\\n\\x00i\\x00\\x00m\\x00\\x00l\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● ItisBernoullidistributionrepeatedmultipletimes.\\n● Adiscreteprobabilitydistributionthatrepresentsthenumberofsuccesses\\ninaﬁxednumberofindependentBernoullitrials.\\n● Itisparameterizedbytwoparameters—thenumberoftrialsandthe\\nprobabilityofsuccess.\\n185'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 186, 'page_label': '187', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00s\\x00\\x00\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Adiscreteprobabilitydistributionthatmodelsthenumberofevents\\noccurringinaﬁxedintervaloftimeorspace.\\n● Itisparameterizedbyoneparameter—lambda,therateofoccurrence.\\n● Example:Analyzingthenumberofgoalsateamwillscoreduringaspeciﬁc\\ntimeperiod.\\n\\x00x\\x00\\x00n\\x00\\x00t\\x00\\x00l\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Acontinuousprobabilitydistributionthatmodelsthetimebetweenevents\\noccurringinaPoissonprocess.\\n● Itisparameterizedbyoneparameter—lambda,theaveragerateofevents.\\n● Example:Analyzingthetimebetweengoalsscoredbyateam.\\n\\x00a\\x00\\x00a\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Itisavariationoftheexponentialdistribution.\\n● Acontinuousprobabilitydistributionthatmodelsthewaitingtimefora\\nspeciﬁednumberofeventsinaPoissonprocess.\\n● Itisparameterizedbytwoparameters—alpha(shape)andbeta(rate).\\n● Example:Analysingthetimeitwouldtakeforateamtoscorethreegoals.\\n186'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 187, 'page_label': '188', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Itisusedtomodelprobabilities,thus,itisboundedbetween[0,1].\\n● DiﬀersfromBinomialinthisrespectthatinBinomial,probabilityisa\\nparameter.\\n● ButinBeta,theprobabilityisarandomvariable.\\n\\x00n\\x00\\x00o\\x00\\x00\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Alloutcomeswithinagivenrangeareequallylikely.\\n● Itcanbecontinuousordiscrete.\\n● Itisparameterizedbytwoparameters:a(minvalue)andb(maxvalue).\\n● Example:Simulatingtherollofafairsix-sideddie,whereeachoutcome(1,\\n2,3,4,5,6)hasanequalprobability.\\n187'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 188, 'page_label': '189', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00N\\x00\\x00m\\x00\\x00\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\n● Acontinuousprobabilitydistributionwherethelogarithmofthevariable\\nfollowsanormaldistribution.\\n● Itisparameterizedbytwoparameters—meanandstandarddeviation.\\n● Example:Typically,instockreturns,thenaturallogarithmfollowsa\\nnormaldistribution.\\n\\x00t\\x00\\x00e\\x00\\x00\\x00-\\x00\\x00s\\x00\\x00i\\x00\\x00t\\x00\\x00n\\n● Itissimilartonormaldistributionbutwithlongertails(shownabove).\\n● Itisusedint-SNEtomodellow-dimensionalpairwisesimilarities.\\n\\x00e\\x00\\x00u\\x00\\x00\\n● Modelsthewaitingtimeforanevent.\\n● O\\x00enemployedtoanalyzetime-to-failuredata.\\n188'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 189, 'page_label': '190', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00 \\x00o\\x00\\x00o\\x00 \\x00i\\x00\\x00n\\x00\\x00r\\x00\\x00e\\x00\\x00t\\x00\\x00n \\x00f \\x00o\\x00\\x00i\\x00\\x00o\\x00\\x00\\n\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00i\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\x00\\nConsiderthe\\nfollowing\\nprobabilitydensity\\nfunctionofa\\ncontinuous\\nprobability\\ndistribution.Sayit\\nrepresentsthetime\\nonemaytaketo\\ntravelfrompointA\\ntoB.\\n● Forsimplicity,weareassumingauniformdistributionintheinterval[1,5].\\n● Essentially,itsaysthatitwilltakesomewherebetween1and5minutesto\\ngofromAtoB.Nevermore,neverless.\\nThus,theprobabilitydensityfunction(PDF)canbewrittenasfollows:\\n189'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 190, 'page_label': '191', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAnswerthefollowingquestionforme:\\n\\x00)\\x00h\\x00\\x00\\x00s\\x00h\\x00\\x00r\\x00\\x00a\\x00\\x00l\\x00\\x00y\\x00h\\x00\\x00\\x00n\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00\\x00r\\x00\\x00i\\x00\\x00l\\x00\\x00h\\x00\\x00e\\x00i\\x00\\x00t\\x00\\x00\\n\\x00(\\x00\\x003\\x00\\x00o\\x00e\\x00\\x00h\\x00o\\x00\\x00t\\x00?\\n● \\x00)\\x00/\\x00\\x00o\\x00\\x00.\\x00\\x00)\\n● \\x00)\\x00r\\x00\\x00\\x00n\\x00\\x00r\\x00h\\x00\\x00u\\x00\\x00e\\x00r\\x00\\x00\\x00=\\x00\\x00,\\x00\\x00.\\n● \\x00)\\x00r\\x00\\x00\\x00n\\x00\\x00r\\x00h\\x00\\x00u\\x00\\x00e\\x00r\\x00\\x00\\x00=\\x00\\x00,\\x00\\x00.\\nDecideonananswerbeforeyoureadfurther.\\nWell,alloftheaboveanswersarewrong.\\nThecorrectanswerisZERO.\\nAndIintentionallykeptonlywronganswersheresothatyouneverforget\\nsomethingfundamentallyimportantaboutcontinuousprobabilitydistributions.\\nLet’sdivein!\\nTheprobabilitydensityfunctionofacontinuousprobabilitydistributionmay\\nlookasfollows:\\nSomeconditionsforthisprobabilitydensityfunctionare:\\n190'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 191, 'page_label': '192', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Itshouldbedeﬁnedforallrealnumbers(canbezeroforsomevalues).\\nThisisincontrasttoadiscreteprobabilitydistributionwhichisonlydeﬁnedfor\\nalistofvalues.\\n● Theareashouldbe1.\\n● Thefunctionshouldbenon-negativeforallrealvalues.\\nHere,manyfolkso\\x00enmisinterpretthattheprobabilitydensityfunction\\nrepresentstheprobabilityofobtainingaspeciﬁcvalue.\\n191'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 192, 'page_label': '193', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,bylookingattheaboveprobabilitydensityfunction,many\\nincorrectlyconcludethattheprobabilityoftherandomvariableXbeing\\x00is\\ncloseto\\x00.\\x00\\x00.\\nButcontrarytothiscommonbelief,aprobabilitydensityfunction:\\n● DOESNOTdepicttheprobabilitiesofaspeciﬁcvalue.\\n● isnotmeanttodepictadiscreterandomvariable.\\nInstead,aprobabilitydensityfunction:\\n● depictstherateatwhichprobabilitiesaccumulatearoundeachpoint.\\n● isonlymeanttodepictacontinuousrandomvariable.\\nNow,thereareinﬁnitelypossiblevaluesthatacontinuousrandomvariablemay\\ntake.\\nSotheprobabilityofobtainingaspeciﬁcvalueisalwayszero(orinﬁnitesimally\\nsmall).\\nThus,answeringouroriginalquestion,theprobabilitythatonewilltakethree\\nminutestoreachpointBisZERO.\\nSowhatisthepurposeofusingaprobabilitydensityfunction?\\nInstatistics,aPDFisusedtocalculatetheprobabilityoveranintervalofvalues.\\n192'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 193, 'page_label': '194', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,wecanuseittoanswerquestionssuchas…\\n● Whatistheprobabilitythatitwilltakebetween:\\n○ 3to4minutestoreachpointBfrompointA,or,\\n○ 2to4minutestoreachpointBfrompointA,andsoon…\\nAndwedothisusingintegrals.\\nMoreformally,theprobabilitythatarandomvariable\\x00willtakevaluesinthe\\ninterval\\x00a\\x00\\x00]is:\\nSimplyput,it’stheareaunderthecurvefrom\\x00a\\x00\\x00].\\nFromtheaboveprobabilityestimationoveraninterval,wecanalsoverifythatthe\\nprobabilityofobtainingaspeciﬁcvalueisindeedzero.\\n193'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 194, 'page_label': '195', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nBysubstituting\\x00=\\x00,weget:\\nTosummarize,alwaysrememberthatinacontinuousprobabilitydistribution:\\n● Theprobabilitydensityfunctiondoesnotdepicttheexactprobabilityof\\nobtainingaspeciﬁcvalue.\\n● Estimatingtheprobabilityforaprecisevalueoftherandomvaluemakes\\nnosensebecauseitisinﬁnitesimallysmall.\\nInstead,weusetheprobabilitydensityfunctiontocalculatetheprobabilityover\\nanintervalofvalues.\\n194'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 195, 'page_label': '196', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00u\\x00\\x00\\x00eﬁ\\x00i\\x00\\x00o\\x00\\x00\\x00n\\x00\\x00n\\x00\\x00r\\x00\\x00g\\n\\x00n\\x00\\x00e\\x00\\x00c\\x00\\x00o\\x00\\n\\x001\\x00y\\x00\\x00s\\x00f\\x00a\\x00\\x00a\\x00\\x00e\\x00\\x00n\\x00\\x00a\\x00\\x00s\\x00\\x00\\nInanytabulardataset,wetypicallycategorizethecolumnsaseitherafeatureora\\ntarget.\\nHowever,therearesomanyvariablesthatonemayﬁnd/deﬁneintheirdataset,\\nwhichIwanttodiscussinthischapter.Thesearedepictedintheimagebelow:\\n195'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 196, 'page_label': '197', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00)\\x00n\\x00\\x00p\\x00\\x00d\\x00\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00n\\x00\\x00\\x00t\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nThesearethemostcommonandfundamentaltoML.\\nIndependentvariablesarethefeaturesthatareusedasinputtopredictthe\\noutcome.Theyarealsoreferredtoaspredictors/features/explanatoryvariables.\\nThedependentvariableistheoutcomethatisbeingpredicted.Itisalsocalled\\nthetarget,response,oroutputvariable.\\n\\x003\\x00\\x00)\\x00o\\x00\\x00o\\x00\\x00d\\x00\\x00\\x00\\x00n\\x00\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nConfoundingvariablesaretypicallyfoundinacause-and-eﬀectstudy(causal\\ninference).\\nThesevariablesarenotofprimaryinterestinthecause-and-eﬀectequationbut\\ncanpotentiallyleadtospuriousassociations.\\n196'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 197, 'page_label': '198', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nToexemplify,saywewanttomeasuretheeﬀectoficecreamsalesonthesalesof\\nairconditioners.\\nAsyoumayhaveguessed,thesetwomeasurementsarehighlycorrelated.\\nHowever,there’saconfoundingvariable—temperature,whichinﬂuencesboth\\nicecreamsalesandthesalesofairconditioners.\\nTostudythetruecasualimpact,itisessentialtoconsidertheconfounder\\n(temperature).Otherwise,thestudywillproducemisleadingresults.\\nInfact,itisduetotheconfoundingvariablesthatwehearthestatement:\\n“Correlationdoesnotimplycausation.”\\nIntheaboveexample:\\n● Thereisahighcorrelationbetweenicecreamsalesandsalesofair\\nconditioners.\\n● Butthesalesofairconditioners(eﬀect)areNOTcausedbyicecreamsales.\\n197'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 198, 'page_label': '199', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAlso,inthiscase,theairconditionerandicecreamsalesarecorrelatedvariables.\\nMoreformally,achangeinonevariableisassociatedwithachangeinanother.\\n\\x005\\x00\\x00o\\x00\\x00r\\x00\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nIntheaboveexample,tomeasurethetrueeﬀectoficecreamsalesonair\\nconditionersales,wemustensurethatthetemperatureremainsunchanged\\nthroughoutthestudy.\\nOncecontrolled,temperaturebecomesacontrolvariable.\\nMoreformally,thesearevariablesthatarenottheprimaryfocusofthestudybut\\narecrucialtoaccountfortoensurethattheeﬀectweintendtomeasureisnot\\nbiasedorconfoundedbyotherfactors.\\n\\x006\\x00\\x00a\\x00\\x00n\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nAvariablethatisnotdirectlyobservedbutisinferredfromotherobserved\\nvariables.\\nForinstance,weuseclusteringalgorithmsbecausethetruelabelsdonotexist,\\nandwewanttoinferthemsomehow.\\n198'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 199, 'page_label': '200', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThetruelabelisalatentvariableinthiscase.\\nAnothercommonexampleofalatentvariableis“intelligence.”\\nIntelligenceitselfcannotbedirectlymeasured;itisalatentvariable.\\nHowever,wecaninferintelligencethroughvariousobservableindicatorssuchas\\ntestscores,problem-solvingabilities,andmemoryretention.\\n\\x007\\x00\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00n\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nAsthenamesuggests,thesevariablesrepresenttheinteractioneﬀectbetween\\ntwoormorevariables,andareo\\x00enusedinregressionanalysis.\\nHere’saninstanceIrememberusingthemin.\\nInaproject,Istudiedtheimpactofpopulationdensityandincomelevelson\\nspendingbehavior.\\n● Icreatedthreegroupsforpopulationdensity—HIGH,MEDIUM,and\\nLOW(one-hotencoded).\\n● Likewise,Icreatedthreegroupsforincomelevels—HIGH,MEDIUM,\\nandLOW(one-hotencoded).\\nTodoregressionanalysis,Icreatedinteractionvariablesbycross-multiplying\\nbothone-hotcolumns.\\n199'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 200, 'page_label': '201', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisproduced9interactionvariables:\\n● Population-HighandIncome-High\\n● Population-HighandIncome-Med\\n● Population-HighandIncome-Low\\n● Population-MedandIncome-High\\n● andsoon…\\nConductingtheregressionanalysisoninteractionvariablesrevealedmoreuseful\\ninsightsthanwhatIobservedwithoutthem.\\nTosummarize,thecoreideaistostudytwoormorevariablestogetherrather\\nthanindependently.\\n\\x008\\x00\\x00)\\x00t\\x00\\x00i\\x00\\x00a\\x00\\x00\\x00n\\x00\\x00o\\x00\\x00S\\x00\\x00t\\x00\\x00n\\x00\\x00y\\x00a\\x00\\x00a\\x00\\x00e\\x00\\x00\\nTheconceptofstationarityo\\x00enappearsintime-seriesanalysis.\\nStationaryvariablesarethosewhosestatisticalproperties(mean,variance)DO\\nNOTchangeovertime.\\nOntheﬂipside,ifavariable’sstatisticalpropertieschangeovertime,theyare\\ncallednon-stationaryvariables.\\nPreservingstationarityinstatisticallearningiscriticalbecausethesemodelsare\\nfundamentallyreliantontheassumptionthatsamplesareidenticallydistributed.\\nButiftheprobabilitydistributionofvariablesisevolvingovertime,\\n(non-stationary),theaboveassumptiongetsviolated.\\n200'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 201, 'page_label': '202', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThatiswhy,typically,usingdirectvaluesofthenon-stationaryfeature(likethe\\nabsolutevalueofthestockprice)isnotrecommended.\\nInstead,Ihavealwaysfounditbettertodeﬁnefeaturesintermsofrelative\\nchanges:\\n\\x001\\x00\\x00\\x00a\\x00\\x00e\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nTalkingoftimeseries,laggedvariablesareprettycommonlyusedinfeature\\nengineeringanddataanalytics.\\nAsthenamesuggests,alaggedvariablerepresentsprevioustimepoints’valuesof\\nagivenvariable,essentiallyshi\\x00ingthedataseriesbyaspeciﬁednumberof\\nperiods/rows.\\nForinstance,whenpredictingnextmonth’ssalesﬁgures,wemightincludethe\\nsalesﬁguresfromthepreviousmonthasalaggedvariable.\\nLaggedfeaturesmayinclude:\\n● 7-daylagonwebsitetraﬃctopredictcurrentwebsitetraﬃc.\\n● 30-daylagonstockpricestopredictthenextmonth’sclosingprices.\\n● Andsoon…\\n201'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 202, 'page_label': '203', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00\\x00e\\x00\\x00y\\x00a\\x00\\x00a\\x00\\x00e\\x00\\nYetagain,asthenamesuggests,thesevariables(unintentionally)provide\\ninformationaboutthetargetvariablethatwouldnotbeavailableatthetimeof\\nprediction.\\nThisleadstooverlyoptimisticmodelperformanceduringtrainingbutfailsto\\ngeneralizetonewdata.\\nConsideradatasetcontainingmedicalimagingdata.\\nEachsampleconsistsofmultipleimages(e.g.,diﬀerentviewsofthesame\\npatient’sbodypart),andthemodelisintendedtodetecttheseverityofadisease.\\nInthiscase,randomlysplittingtheimagesintotrainandtestsetswillresultin\\ndataleakage.\\nThisisbecauseimagesofthesamepatientwillendupinboththetrainingand\\ntestsets,allowingthemodelto“see”informationfromthesamepatientduring\\ntrainingandtesting.\\n202'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 203, 'page_label': '204', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHere’sapaperthatcommittedthismistake(andlatercorrectedit):\\nToavoidthis,apatientmustonlybelongtothetestortrain/valset,notboth.\\nThisiscalledgroupsplitting:\\nCreatingforward-lagfeaturesisanotherwayleakyvariablesgetcreated\\nunintentionallyattimes:\\nLet’sgetintomoredetailabouttheissuewithrandomsplittingbelow.\\n203'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 204, 'page_label': '205', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00y\\x00\\x00i\\x00\\x00l\\x00e\\x00\\x00u\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\nIntypicalmachinelearningdatasets,wemostlyﬁndfeaturesthatprogressfrom\\nonevaluetoanother:Forinstance:\\n● Numericalfeatureslikeage,income,transactionamount,etc.\\n● Categoricalfeaturesliket-shirtsize,incomegroups,agegroups,etc.\\nHowever,thereisonemoretypeoffeature,which,inmostcases,deservesspecial\\nfeatureengineeringeﬀortbutiso\\x00enoverlooked.Thesearecyclicalfeatures,i.e.,\\nfeatureswitharecurringpattern(orcycle).\\nUnlikeotherfeaturesthatprogresscontinuously(orhavenoinherentorder),\\ncyclicalfeaturesexhibitperiodicbehaviorandrepeata\\x00eraspeciﬁcinterval.For\\ninstance,thehour-of-the-day,theday-of-the-week,andthemonth-of-an-yearare\\nallcommonexamplesofcyclicalfeatures.Talkingspeciﬁcallyabout,say,the\\nhour-of-the-day,itsvaluecanrangebetween0to23:\\nIfweDON’Tconsiderthisasacyclicalfeatureanddon’tutilizeappropriate\\nfeatureengineeringtechniques,wewilllosesomereallycriticalinformation.\\nTounderstandbetter,considerthis:\\n204'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 205, 'page_label': '206', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nRealisticallyspeaking,thevalues“23”\\nand“0”mustbeclosetoeachotherin\\nour“ideal”featurerepresentationof\\nthehour-of-the-day.\\nMoreover,thedistancebetween“0”and“1”mustbethesameasthedistance\\nbetween“23”and“0”.\\nHowever,standardrepresentationdoesnotfulﬁlltheseproperties.Thus,the\\nvalue“23”isfarfrom“0”.Infact,thedistancepropertyisn’tsatisﬁedeither.\\nNow,thinkaboutitforasecond.Intuitivelyspeaking,don’tyouthinkthisfeature\\ndeservesspecialfeatureengineering,i.e.,onethatpreservestheinherentnatural\\nproperty?\\nIamsureyoudo!\\nLet’sunderstandhowwetypicallydoit.\\n\\x00y\\x00\\x00i\\x00\\x00l\\x00e\\x00\\x00u\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\nOneofthemostcommontechniquestoencodesuchafeatureisusing\\ntrigonometricfunctions,speciﬁcally,sineandcosine.Thesearehelpfulbecause\\nsineandcosineareperiodic,bounded,anddeﬁnedforallrealvalues.\\n205'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 206, 'page_label': '207', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00f\\x00o\\x00\\x00s\\x00\\x00\\x00v\\x00\\x00\\x00t\\x00\\x00r\\x00r\\x00\\x00o\\x00\\x00m\\x00\\x00r\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00\\x00r\\x00\\x00l\\x00\\x00\\x00e\\x00\\x00o\\x00\\x00\\x00,\\x00u\\x00\\x00h\\x00\\x00\\n\\x00r\\x00\\x00l\\x00\\x00\\x00n\\x00\\x00ﬁ\\x00e\\x00\\x00o\\x00\\x00o\\x00\\x00\\x00a\\x00\\x00e\\x00\\x00\\x00i\\x00\\x00,\\x00a\\x00\\x00p\\x00\\x002\\x00\\x00\\nForinstance,consider\\nrepresentingthelinear\\nhour-of-the-dayfeatureasa\\ncyclicalfeature:\\nThecentralangle(2π)represents24hours.Thus,thelinearfeaturevaluescanbe\\neasilyconvertedintocyclicalfeaturesasfollows:\\nThebeneﬁtofdoingthisishowneatlytheengineeredfeaturesatisﬁesthe\\npropertieswediscussedearlier:\\nAsdepictedabove,thedistancebetweenthecyclicalfeaturerepresentationof\\n“23”and“0”isthesameasthedistancebetween“0”and“1”.Thestandardlinear\\nrepresentationofthehour-of-the-dayfeature,however,violatesthisproperty,\\nwhichresultsinlossofinformation…\\n206'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 207, 'page_label': '208', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n…orrather,Ishouldsaythatthestandardlinearrepresentationofthe\\nhour-of-the-dayfeatureresultsinanunderutilizationofinformation,whichthe\\nmodelcanbeneﬁtfrom.Haditbeentheday-of-the-weekinstead,thecentral\\nangle(2π)musthaverepresented7days.\\nThesameideacanbeextendedtoallsortsofcyclicalfeaturesyoumayﬁndin\\nyourdataset:\\n● Winddirection,ifrepresentedcategorically,willgointhisorder:N,NE,E,\\nSE,S,SW,W,NW,andthenbacktoN.\\n● Phasesofthemoon,likenewmoon,ﬁrstquarter,fullmoon,andlast\\nquarter,canberepresentedascategorieswithacyclicalorder.\\n● Seasons,suchasspring,summer,fall,andwinter,arecategoricalfeatures\\nwithacyclicalpatternastheyrepeatannually.\\nThepointisthatasyouwillinspectthedatasetfeatures,youwillintuitively\\nknowwhichfeaturesarecyclicalandwhicharenot.\\nTypically,themodelwillﬁnditeasiertointerprettheengineeredfeaturesand\\nutilizetheminmodelingthedatasetaccurately.\\n207'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 208, 'page_label': '209', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00u\\x00\\x00\\x00i\\x00\\x00r\\x00\\x00i\\x00\\x00t\\x00\\x00n\\nDuringmodeldevelopment,oneofthetechniquesthatmanydon’texperiment\\nwithisfeaturediscretization.Asthenamesuggests,theideabehind\\ndiscretizationistotransformacontinuousfeatureintodiscretefeatures.\\nWhy,when,andhowwouldyoudothat?Let’sunderstandinthischapter.\\n\\x00o\\x00\\x00v\\x00\\x00i\\x00\\x00\\nMyrationaleforusingfeaturediscretizationhasalmostalwaysbeensimple:“It\\njustmakessensetodiscretizeafeature.”\\nForinstance,consideryourdatasethasanagefeature:\\nInmanyusecases,likeunderstandingspendingbehaviorbasedontransaction\\nhistory,suchcontinuousvariablesarebetterunderstoodwhentheyare\\ndiscretizedintomeaningfulgroups→youngsters,adults,andseniors.\\nForinstance,saywemodelthistransactiondatasetwithoutdiscretization.This\\nwouldresultinsomecoeﬃcientsforeachfeature,whichwouldtellusthe\\ninﬂuenceofeachfeatureontheﬁnalprediction.\\n208'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 209, 'page_label': '210', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nButifyouthinkagain,inourgoalofunderstandingspendingbehavior,arewe\\nreallyinterestedinlearningthecorrelationbetweenexactageandspending\\nbehavior?\\nItmakesverylittlesensetodothat.Instead,itmakesmoresensetolearnthe\\ncorrelationbetweendiﬀerentagegroupsandspendingbehavior.\\nAsaresult,discretizingtheagefeaturecanpotentiallyunveilmuchmore\\nvaluableinsightsthanusingitasarawfeature.\\n\\x00\\x00o\\x00\\x00o\\x00\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\nNowthatweunderstandtherationale,thereare2techniquesthatarewidely\\npreferred.\\nOnewayofdiscretizingfeaturesinvolvesdecomposingafeatureintoequally\\nsizedbins.\\nAnothertechniqueinvolvesdecomposingafeatureintoequalfrequencybins:\\n209'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 210, 'page_label': '211', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nA\\x00erthat,thediscretevaluesareone-hotencoded.\\nOneadvantageoffeaturediscretizationisthatitenablesnon-linearbehavior\\neventhoughthemodelislinear.Thiscanpotentiallyleadtobetteraccuracy,\\nwhichisalsoevidentfromtheimagebelow:\\nAlinearmodelwithfeaturediscretizationresultsina:\\n● non-lineardecisionboundary.\\n● bettertestaccuracy.\\nSo,inaway,wegettouseasimplelinearmodelbutstillgettolearnnon-linear\\npatterns.\\nAnotheradvantageofdiscretizingcontinuousfeaturesisthatithelpsusimprove\\nthesignal-to-noiseratio.\\n210'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 211, 'page_label': '212', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSimplyput,“signal”referstothemeaningfulorvaluableinformationinthedata.\\nBinnngafeaturehelpsusmitigatetheinﬂuenceofminorﬂuctuations,whichare\\no\\x00enmerenoise.\\nEachbinactsasameansof“smoothing”outthenoisewithinspeciﬁcdata\\nsegments.\\nBeforeIconclude,dorememberthatfeaturediscretizationwithone-hot\\nencodingincreasesthenumberoffeatures→therebyincreasingthedata\\ndimensionality.\\nAndtypically,asweprogresstowardshigherdimensions,databecomemore\\neasilylinearlyseparable.Thus,featurediscretizationcanleadtooverﬁtting.\\nToavoidthis,don’toverlydiscretizeallfeatures.Instead,useitwhenitmakes\\nintuitivesense,aswesawearlier.\\nOfcourse,itsutilitycanvastlyvaryfromoneapplicationtoanother,butattimes,\\nIhavefoundthat:\\n● Discretizinggeospatialdatalikelatitudeandlongitudecanbeuseful.\\n● Discretizingage/weight-relateddatacanbeuseful.\\n● Featuresthataretypicallyconstrainedbetweenarangemakessense,like\\nsavings/income(practicallyspeaking),etc.\\n211'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 212, 'page_label': '213', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00a\\x00\\x00g\\x00\\x00i\\x00\\x00l\\x00a\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00\\nHereare7waystoencodecategoricalfeatures:\\nWecoveredthemindetailhere:https://bit.ly/3LkfVq5.\\n212'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 213, 'page_label': '214', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Eachcategoryisrepresentedbyabinaryvectorof0sand1s.\\n● Eachcategorygetsitsownbinaryfeature,andonlyoneofthemis\"hot\"\\n(setto1)atatime,indicatingthepresenceofthatcategory.\\n● Numberoffeatures=Numberofuniquecategoricallabels\\n\\x00u\\x00\\x00y\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Sameasone-hotencodingbutwithoneadditionalstep.\\n● A\\x00erone-hotencoding,wedropafeaturerandomly.\\n● Wedothistoavoidthedummyvariabletrap(discussedinthischapter).\\n● Numberoffeatures=Numberofuniquecategoricallabels-1\\n\\x00ﬀ\\x00c\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Similartodummyencodingbutwithoneadditionalstep.\\n● Altertherowwithallzerosto-1.\\n● Thisensuresthattheresultingbinaryfeaturesrepresentnotonlythe\\npresenceorabsenceofspeciﬁccategoriesbutalsothecontrastbetween\\nthereferencecategoryandtheabsenceofanycategory.\\n● Numberoffeatures=Numberofuniquecategoricallabels-1.\\n\\x00a\\x00\\x00l\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Assigneachcategoryauniquelabel.\\n● Labelencodingintroducesaninherentorderingbetweencategories,which\\nmaynotbethecase.\\n● Numberoffeatures=1.\\n\\x00r\\x00\\x00n\\x00\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Similartolabelencoding—assignauniqueintegervaluetoeachcategory.\\n● Theassignedvalueshaveaninherentorder,meaningthatonecategoryis\\nconsideredgreaterorsmallerthananother.\\n● Numberoffeatures=1.\\n213'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 214, 'page_label': '215', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00t\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Alsoknownasfrequencyencoding.\\n● Encodescategoricalfeaturesbasedonthefrequencyofeachcategory.\\n● Thus,insteadofreplacingthecategorieswithnumericalvaluesorbinary\\nrepresentations,countencodingdirectlyassignseachcategorywithits\\ncorrespondingcount.\\n● Numberoffeatures=1.\\n\\x00i\\x00\\x00r\\x00\\x00n\\x00\\x00d\\x00\\x00g\\x00\\n● Combinationofone-hotencodingandordinalencoding.\\n● Itrepresentscategoriesasbinarycode.\\n● Eachcategoryisﬁrstassignedanordinalvalue,andthenthatvalueis\\nconvertedtobinarycode.\\n● Thebinarycodeisthensplitintoseparatebinaryfeatures.\\n● Usefulwhendealingwithhigh-cardinalitycategoricalfeatures(orahigh\\nnumberoffeatures)asitreducesthedimensionalitycomparedtoone-hot\\nencoding.\\n● Numberoffeatures=log(n)(inbase2).\\nWhilethesearesomeofthemostpopulartechniques,donotethatthesearenot\\ntheonlytechniquesforencodingcategoricaldata.\\nYoucantryplentyoftechniqueswiththecategory-encoderslibrary:\\nhttps://pypi.org/project/category-encoders.\\n214'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 215, 'page_label': '216', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00ﬄ\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00\\nIo\\x00enﬁnd“ShuﬄeFeatureImportance”tobeahandyandintuitivetechnique\\ntomeasurefeatureimportance.\\nAsthenamesuggests,itobserveshowshuﬄingafeatureinﬂuencesthemodel\\nperformance.Thevisualbelowillustratesthistechniqueinfoursimplesteps:\\n215'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 216, 'page_label': '217', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHere’showitworks:\\n● Trainthemodelandmeasureitsperformance→\\x001.\\n● Shuﬄeonefeaturerandomlyandmeasureperformanceagain→\\x002(model\\nisNOTtrainedagain).\\n● Measurefeatureimportanceusingperformancedrop=(\\x001-\\x002).\\n● Repeatforallfeatures.\\nThismakesintuitivesenseaswell,doesn’tit?\\nSimplyput,ifwerandomlyshuﬄejustonefeatureandeverythingelsestaysthe\\nsame,thentheperformancedropwillindicatehowimportantthatfeatureis.\\n● Iftheperformancedropislow→Thismeansthefeaturehasaverylow\\ninﬂuenceonthemodel’spredictions.\\n● Iftheperformancedropishigh→Thismeansthatthefeaturehasavery\\nhighinﬂuenceonthemodel’spredictions.\\nDonotethattoeliminateanypotentialeﬀectsofrandomnessduringfeature\\nshuﬄing,itisrecommendedto:\\n● Shuﬄethesamefeaturemultipletimes\\n● Measureaverageperformancedrop.\\nAfewthingsthatIloveaboutthistechniqueare:\\n216'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 217, 'page_label': '218', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Itrequiresnorepetitivemodeltraining.Justtrainthemodelonceand\\nmeasurethefeatureimportance.\\n● Itisprettysimpletouseandquiteintuitivetointerpret.\\n● ThistechniquecanbeusedforallMLmodelsthatcanbeevaluated.\\nOfcourse,thereisonecaveataswell.\\nSaytwofeaturesarehighlycorrelated,andoneofthemispermuted/shuﬄed.\\nInthiscase,themodelwillstillhaveaccesstothefeaturethroughitscorrelated\\nfeature.\\nThiswillresultinalowerimportancevalueforbothfeatures.\\nOnewaytohandlethisistoclusterhighlycorrelatedfeaturesandonlykeepone\\nfeaturefromeachcluster.\\n217'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 218, 'page_label': '219', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00r\\x00\\x00e\\x00e\\x00\\x00o\\x00\\x00o\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00e\\x00\\x00c\\x00\\x00o\\x00\\nReal-worldMLdevelopmentisallaboutachievingasweetbalancebetween\\nspeed,modelsize,andperformance.\\nOnecommonwayto:\\n● Improvespeed,\\n● Reducesize,and\\n● Maintain(orminimallydegrade)performance…\\n…isbyusingfeaturingselection.Theideaistoselectthemostusefulsubsetof\\nfeaturesfromthedataset.\\nWhiletherearemanymanymethodsforfeatureselection,Ihaveo\\x00enfoundthe\\n“ProbeMethod”tobeprettyreliable,practicalandintuitivetouse.\\n218'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 219, 'page_label': '220', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheimagebelowdepictshowitworks:\\n● Step1)Addarandomfeature(noise).\\n● Step2)Trainamodelonthenewdataset.\\n● Step3)Measurefeatureimportance(canuseshuﬄefeatureimportance)\\n● Step4)Discardoriginalfeaturesthatrankbelowtherandomfeature.\\n● Step5)Repeatuntilconvergence.\\nThiswholeideamakesintuitivesenseaswell.\\n219'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 220, 'page_label': '221', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nMorespeciﬁcally,ifafeature’simportanceisrankedbelowarandomfeature,itis\\nprobablyauselessfeatureforthemodel.\\nThiscanbeespeciallyusefulincaseswherewehaveplentyoffeatures,andwe\\nwishtodiscardthosethatdon’tcontributetothemodel.\\nOfcourse,oneshortcomingisthatwhenusingtheProbeMethod,wemusttrain\\nmultiplemodels:\\n1. Traintheﬁrstmodelwiththerandomfeatureanddiscarduselessfeatures.\\n2. Keeptrainingnewmodelsuntiltherandomfeatureisrankedastheleast\\nimportantfeature(althoughtypically,convergencedoesnotresultinplenty\\nofmodels).\\n3. Traintheﬁnalmodelwithouttherandomfeature.\\nNonetheless,theapproachcanbequiteusefultoreducemodelcomplexity.\\n220'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 221, 'page_label': '222', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\n\\x00h\\x00\\x00e\\x00\\x00\\x00q\\x00\\x00r\\x00\\x00\\x00r\\x00\\x00r\\x00M\\x00\\x00)\\x00\\nSayyouwishtotrainalinearregression\\nmodel.Weknowthatwetrainitby\\nminimizingthesquarederror:\\nButhaveyoueverwonderedwhywespeciﬁcallyusethesquarederror?\\nSee,manyfunctionscanpotentiallyminimizethediﬀerencebetweenobserved\\nandpredictedvalues.Butofallthepossiblechoices,whatissospecialaboutthe\\nsquarederror?\\nInmyexperience,peopleo\\x00ensay:\\n● Squarederrorisdiﬀerentiable.Thatiswhyweuseitasalossfunction.\\nWRONG.\\n● Itisbetterthanusingabsoluteerrorassquarederrorpenalizeslargeerrors\\nmore.WRONG.\\nSadly,eachoftheseexplanationsareincorrect.\\nButapproachingitfromaprobabilisticperspectivehelpsusunderstandwhythe\\nsquarederroristheidealchoice.\\nLet’sunderstandinthischapter.\\n221'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 222, 'page_label': '223', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInlinearregression,wepredictourtargetvariableyusingtheinputsXas\\nfollows:\\nHere,epsilonisanerrortermthatcapturestherandomnoiseforaspeciﬁcdata\\npoint(i).\\nWeassumethenoiseisdrawnfromaGaussiandistributionwithzeromean\\nbasedonthecentrallimittheorem:\\nThus,theprobabilityofobservingtheerrortermcanbewrittenas:\\nSubstitutingtheerrortermfromthelinearregressionequation,weget:\\nForaspeciﬁcsetofparametersθ,theabovetellsustheprobabilityofobservinga\\ndatapoint(i).\\nNext,wecandeﬁnethelikelihoodfunctionasfollows:\\n222'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 223, 'page_label': '224', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItmeansthatbyvaryingθ,wecanﬁtadistributiontotheobserveddataand\\nquantifythelikelihoodofobservingit.\\nWefurtherwriteitasaproductforindividualdatapointsbecauseweassumeall\\nobservationsareindependent.\\nThus,weget:\\nSincethelogfunctionismonotonic,weusethelog-likelihoodandmaximizeit.\\nThisiscalledmaximumlikelihoodestimation(MLE).\\n223'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 224, 'page_label': '225', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSimplifying,weget:\\nToreiterate,theobjectiveistoﬁndtheθthatmaximizestheaboveexpression.\\nButtheﬁrsttermisindependentofθ.Thus,maximizingtheaboveexpressionis\\nequivalenttominimizingthesecondterm.Andifyounoticeclosely,it’sprecisely\\nthesquarederror.\\nThus,youcanmaximizethelog-likelihoodbyminimizingthesquarederror.And\\nthisistheoriginofleast-squaresinlinearregression.See,there’sclearproofand\\nreasoningbehindusingsquarederrorasalossfunctioninlinearregression.\\nNothingcomesfromthinairinmachinelearning.\\n224'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 225, 'page_label': '226', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00k\\x00\\x00a\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00o\\x00y\\x00\\x00r\\x00\\x00r\\x00\\x00e\\x00\\x00r\\x00\\nAlmostallMLmodelsweworkwithhavesomehyperparameters,suchas:\\n● Learningrate\\n● Regularization\\n● Layersize(forneuralnetwork),etc.\\nButasshownintheimagebelow,whydon’tweseeanyhyperparameterin\\nSklearn’sLinearRegressionimplementation?\\nItmusthavelearningrateasahyperparameter,right?\\nTounderstandthereasonwhyithasnohyperparameters,weﬁrstneedtolearn\\nthattheLinearRegressioncanmodeldataintwodiﬀerentways:\\n225'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 226, 'page_label': '227', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n1. GradientDescent(whichmanyotherMLalgorithmsuseforoptimization):\\n○ Itisastochasticalgorithm,i.e.,involvessomerandomness.\\n○ Itﬁndsanapproximatesolutionusingoptimization.\\n○ Ithashyperparameters.\\n2. OrdinaryLeastSquare(OLS):\\n○ Itisadeterministicalgorithm.Thus,ifrunmultipletimes,itwill\\nalwaysconvergetothesameweights.\\n○ Italwaysﬁndstheoptimalsolution.\\n○ Ithasnohyperparameters.\\nNow,insteadofthetypicalgradientdescentapproach,Sklearn’sLinear\\nRegressionclassimplementstheOLSmethod.\\nThatiswhyithasnohyperparameters.\\n\\x00o\\x00\\x00o\\x00\\x00\\x00L\\x00\\x00o\\x00\\x00?\\nWithOLS,theideaistoﬁndthesetof\\nparameters(Θ)suchthat:\\n● X:inputdatawithdimensions(n,m).\\n● Θ:parameterswithdimensions(m,1).\\n● y:outputdatawithdimensions(n,1).\\n● n:numberofsamples.\\n● m:numberoffeatures.\\n226'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 227, 'page_label': '228', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nOnewaytodeterminetheparameter\\nmatrixΘisbymultiplyingbothsidesof\\ntheequationwiththeinverseofX,as\\nshownbelow:\\nButbecauseXmightbeanon-squarematrix,itsinversemaynotbedeﬁned.\\nToresolvethis,ﬁrst,wemultiplywiththetransposeofXonbothsides,asshown\\nbelow:\\nThismakestheproductofXwithitstransposeasquarematrix.\\nTheobtainedmatrix,beingsquare,canbeinverted(provideditisnon-singular).\\nNext,wetakethecollectiveinverseoftheproducttogetthefollowing:\\n227'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 228, 'page_label': '229', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIt’sclearthattheabovedeﬁnitionhas:\\n● Nohyperparameters.\\n● Norandomness.Thus,itwillalwaysreturnthesamesolution,whichisalso\\noptimal.\\nThisispreciselywhattheLinearRegressionclassofSklearnimplements.\\nTosummarize,itusestheOLSmethodinsteadofgradientdescent.\\nThatiswhyithasnohyperparameters.\\nOfcourse,donotethatthereisasigniﬁcanttradeoﬀbetweenruntimeand\\nconveniencewhenusingOLSvs.gradientdescent.\\nThisisalsoclearfromthetime-complexitytablewediscussedinanearlier\\nchapter.\\nAsdepictedabove,therun-timeofOLSiscubicallyrelatedtothenumberof\\nfeatures(m).\\nThus,whenwehavemanyfeatures,itmaynotbeagoodideatousethe\\n\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00(\\x00class.Instead,usethe\\x00G\\x00\\x00e\\x00\\x00e\\x00\\x00o\\x00\\x00)classfromSklearn.\\nOfcourse,thegoodthingaboutLinearRegression()classisthatitinvolvesno\\nhyperparametertuning.\\nThus,whenweuseOLS,wetraderun-timeforﬁndinganoptimalsolution\\nwithouthyperparametertuning.\\n228'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 229, 'page_label': '230', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00s\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00s\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\nLinearregressioncomeswithitsownsetofchallenges/assumptions.For\\ninstance,a\\x00ermodeling,theoutputcanbenegativeforsomeinputs.\\nButthismaynotmakesenseattimes—predictingthenumberofgoalsscored,\\nnumberofcallsreceived,etc.Thus,itisclearthatitcannotmodelcount(or\\ndiscrete)data.\\nFurthermore,inlinearregression,residualsareexpectedtobenormally\\ndistributedaroundthemean.Sotheoutcomesoneithersideofthemean(\\x00-\\x00\\x00\\n\\x00+\\x00)areequallylikely.\\nForinstance:\\n● iftheexpectednumber(mean)ofcallsreceivedis1...\\n● ...then,accordingtolinearregression,receiving3calls(1+2)isjustaslikely\\nasreceiving-1(1-2)calls.(Thisrelatestotheconceptofprediction\\nintervals,whichwecoveredintheanearlierchapter.)\\n● Butinthiscase,anegativepredictiondoesnotmakeanysense.\\nThus,iftheaboveassumptionsdonothold,linearregressionwon’thelp.\\nInstead,inthisspeciﬁccase,whatyoumayneedisPoissonregression.\\n229'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 230, 'page_label': '231', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nPoissonregressionismoresuitableifyourresponse(oroutcome)iscount-based.\\nItassumesthattheresponsecomesfromaPoissondistribution.\\nItisatypeofgeneralizedlinearmodel(GLM)thatisusedtomodelcountdata.It\\nworksbyestimatingaPoissondistributionparameter(λ),whichisdirectlylinked\\ntotheexpectednumberofeventsinagiveninterval.\\nContrarytolinearregression,inPoissonregression,residualsmayfollowan\\nasymmetricdistributionaroundthemean(λ).Hence,outcomesoneithersideof\\nthemean(λ-x,λ+x)areNOTequallylikely.\\nForinstance:\\n● iftheexpectednumber(mean)ofcallsreceivedis1...\\n● ...then,accordingtoPoissonregression,itispossibletoreceive3(1+2)\\ncalls,butitisimpossibletoreceive-1(1-2)calls.\\n● Thisisbecauseitsoutcomeisalsonon-negative.\\nTheregressionﬁtismathematicallydeﬁnedasfollows:\\n230'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 231, 'page_label': '232', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThefollowingvisualneatlysummarizesthispost:\\nWeshallcontinuethisdiscussioninthenextchapter.\\n231'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 232, 'page_label': '233', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00o\\x00u\\x00\\x00d\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00l\\x00\\x00\\nInthischapter,IwillhelpyoucultivatewhatIthinkisoneoftheMOST\\noverlookedandunderappreciatedskillsindevelopinglinearmodels.\\nIcanguaranteethatharnessingthisskillwillgiveyoualotofclarityand\\nintuitioninthemodelingstages.\\nTobegin,understandthatthePoissonregressionwedisussedaboveisnomagic.\\nIt’sjustthat,inourspeciﬁcusecase,thedatagenerationprocessdidn’tperfectly\\nalignwithwhatlinearregressionisdesignedtohandle.Inotherwords,earlier\\nwhenwetrainedalinearregressionmodel,weinherentlyassumedthatthedata\\nwassampledfromanormaldistribution.ButthatwasnottrueinthisPoisson\\nregressioncase.\\nInstead,itcamefromaPoissondistribution,whichiswhyPoissonregression\\nworkedbetter.Thus,thetakeawayisthatwheneveryoutrainlinearmodels,\\nalwaysalwaysandalwaysthinkaboutthedatagenerationprocess.Itgoeslike\\nthis:\\n● Okay,Ihavethisdata.\\n● Iwanttoﬁtalinearmodelthroughit.\\n● WhatinformationdoIgetaboutthedatagenerationprocessthatcanhelp\\nmeselectanappropriatelinearmodel?\\n232'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 233, 'page_label': '234', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nYou’dstartappreciatingtheimportanceofdatagenerationwhenyourealizethat\\nliterallyeverymemberofthegeneralizedlinearmodelfamilystemsfromaltering\\nthedatagenerationprocess.\\nForinstance:\\n● IfthedatagenerationprocessinvolvesaNormaldistribution→youget\\nlinearregression.\\n● Ifthedatahasonlypositiveintegersintheresponsevariable,maybeit\\ncamefromaPoissondistribution→andthisgivesusPoissonregression.\\nThisispreciselywhatwediscussedyesterday.\\n● Ifthedatahasonlytwotargets—0and1,maybeitwasgeneratedusing\\nBernoullidistribution→andthisgivesrisetologisticregression.\\n● Ifthedatahasﬁniteandﬁxedcategories(0,1,2,…n),thenthishints\\ntowardsBinomialdistribution→andwegetBinomialregression.\\n233'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 234, 'page_label': '235', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSee…\\nEverylinearmodelmakesanassumptionandisthenderivedfromanunderlying\\ndatagenerationprocess.\\nThus,developingahabitofholdingforasecondandthinkingaboutthedata\\ngenerationprocesswillgiveyousomuchclarityinthemodelingstages.\\nIamconﬁdentthiswillhelpyougetridofthatannoyingandhelplesshabitof\\nrelentlesslyusingaspeciﬁcsklearnalgorithmwithouttrulyknowingwhyyouare\\nusingit.\\nConsequently,you’dknowwhichalgorithmtouseand,mostimportantly,why.\\nThisimprovesyourcredibilityasadatascientistandallowsyoutoapproachdata\\nscienceproblemswithintuitionandclarityratherthanhit-and-trial.\\nInfact,onceyouunderstandthedatagenerationprocess,youwillautomatically\\ngettoknowaboutmostoftheassumptionsofthatspeciﬁclinearmodel.\\n234'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 235, 'page_label': '236', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00y\\x00a\\x00\\x00a\\x00\\x00e\\x00r\\x00\\x00\\nWithone-hotencoding,we\\nintroduceabigprobleminthedata.\\nWhenweone-hotencode\\ncategoricaldata,weunknowingly\\nintroduceperfectmulticollinearity.\\nMulticollinearityariseswhentwoor\\nmorefeaturescanpredictanother\\nfeature.Inthiscase,asthesumof\\none-hotencodedfeaturesisalways\\n1,itleadstoperfect\\nmulticollinearity.\\nThisiso\\x00encalledtheDummyVariableTrap.Itisbadbecausethemodelhas\\nredundantfeatures.Morevero,theregressionscoeﬃcientsaren’treliableinthe\\npresenceofmulticollinearity.\\nSohowtoresolvethis?\\nThesolutionissimple.Dropanyarbitrary\\nfeaturefromtheone-hotencoded\\nfeatures.\\nThisinstantlymitigatesmulticollinearity\\nandbreaksthelinearrelationshipwhich\\nexistedbefore.\\n235'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 236, 'page_label': '237', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00a\\x00\\x00y\\x00s\\x00\\x00s\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00o\\x00\\x00a\\x00\\x00e\\nLinearregressionassumesthatthemodelresiduals(\\x00a\\x00\\x00u\\x00\\x00-\\x00\\x00e\\x00\\x00c\\x00\\x00d)are\\nnormallydistributed.Ifthemodelisunderperforming,itmaybeduetoa\\nviolationofthisassumption.\\nHere,Io\\x00enusearesidualdistributionplottoverifythisanddeterminethe\\nmodel’sperformance.Asthenamesuggests,thisplotdepictsthedistributionof\\nresiduals(\\x00a\\x00\\x00u\\x00\\x00-\\x00\\x00e\\x00\\x00c\\x00\\x00d),asshownbelow:\\nAgoodresidualplotwill:\\n● Followanormaldistribution\\n● NOTrevealtrendsinresiduals\\nAbadresidualplotwill:\\n● Showskewness\\n● Revealpatternsinresiduals\\n236'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 237, 'page_label': '238', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,themorenormallydistributedtheresidualplotlooks,themoreconﬁdent\\nwecanbeaboutourmodel.Thisisespeciallyusefulwhentheregressionlineis\\ndiﬃculttovisualize,i.e.,inahigh-dimensionaldataset.\\nWhy?\\nBecausearesidualdistributionplotdepictsthedistributionofresiduals,whichis\\nalwaysone-dimensional.\\nThus,itcanbeplottedandvisualizedeasily.\\nOfcourse,thiswasjustaboutvalidatingoneassumption—thenormalityof\\nresiduals.\\nHowever,linearregressionreliesonmanyotherassumptions,whichmustbe\\ntestedaswell.\\nStatsmodelprovidesaprettycomprehensivereportforthis,whichweshall\\ndiscussinthenextchapter.\\n237'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 238, 'page_label': '239', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00t\\x00\\x00s\\x00\\x00d\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00u\\x00\\x00a\\x00\\x00\\nStatsmodelprovidesoneofthemostcomprehensivesummariesforregression\\nanalysis.\\nYet,Ihaveseensomanypeoplestrugglingtointerpretthecriticalmodeldetails\\nmentionedinthisreport.Inthischapter,let’sunderstandtheentiresummary\\nsupportprovidedbystatsmodelandwhyitissoimportant.\\n\\x00e\\x00\\x00i\\x00\\x00\\x001\\nTheﬁrstcolumnoftheﬁrstsectionliststhemodel’ssettings(orconﬁg).This\\nparthasnothingtodowiththemodel’sperformance.\\n238'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 239, 'page_label': '240', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Dependentvariable:Thevariablewearepredicting.\\n● ModelandMethod:WeareusingOLStoﬁtalinearmodel.\\n● Dateandtime:Youknowit.\\n● No.ofobservations:Thedataset’ssize.\\n● Dfresiduals:Thedegreesoffreedomassociatedwiththeresiduals.Itis\\nessentiallythenumberofdatapointsminusthenumberofparameters\\nestimatedinthemodel(includinginterceptterm).\\n● DfModel:Thisrepresentsthedegreesoffreedomassociatedwiththe\\nmodel.Itisthenumberofpredictors,2inourcase—\\x00and\\x00i\\x00\\x00X.\\nIfyourdatahascategoricalfeatures,statsmodelwillone-hotencodethem.Butin\\nthatprocess,itwilldroponeoftheone-hotencodedfeatures.\\nThisisdonetoavoidthedummyvariabletrap,whichwediscussedinanearlier\\nchapter(thischapter).\\n● Covariancetype:Thisisrelatedtotheassumptionsaboutthedistribution\\noftheresidual.\\n○ Inlinearregression,weassumethattheresidualshaveaconstant\\nvariance(homoscedasticity).\\n○ Weuse“nonrobust”totrainamodelunderthatassumption.\\nInthesecondcolumn,statsmodelprovidestheoverallperformance-related\\ndetails:\\n239'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 240, 'page_label': '241', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● R-squared:Thefractionoforiginaldatavariabilitycapturedbythemodel.\\n○ Forinstance,inthiscase,0.927meansthatthecurrentmodel\\ncaptures92.7%oftheoriginalvariabilityinthetrainingdata.\\n○ StatsmodelreportsR2ontheinputdata,soyoumustnotoverly\\noptimizeforit.Ifyoudo,itwillleadtooverﬁtting.\\n● Adj.R-squared:\\n○ ItissomewhatsimilartoR-squared,butitalsoaccountsforthe\\nnumberofpredictors(features)inthemodel.\\n○ TheproblemisthatR-squaredalwaysincreasesasweaddmore\\nfeatures.\\n○ Soevenifweaddtotallyirrelevantfeatures,R-squaredwillnever\\ndecrease.Adj.R-squaredpenalizesthisbehaviorofR-squared.\\n● F-statisticandProb(F-statistic):\\n○ Theseassesstheoverallsigniﬁcanceofaregressionmodel.\\n○ TheycomparetheestimatedcoeﬃcientsbyOLSwithamodelwhose\\nallcoeﬃcients(exceptfortheintercept)arezero.\\n○ F-statistictestswhethertheindependentvariablescollectivelyhave\\nanyeﬀectonthedependentvariableornot.\\n○ Prob(F-statistic)istheassociatedp-valuewiththeF-statistic.\\n○ Asmallp-value(typicallylessthan0.05)indicatesthatthemodelasa\\nwholeisstatisticallysigniﬁcant.\\n○ Thismeansthatatleastoneindependentvariablehasasigniﬁcant\\neﬀectonthedependentvariable.\\n● Log-Likelihood:\\n240'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 241, 'page_label': '242', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n○ Thistellsusthelog-likelihoodthatthegivendatawasgeneratedby\\ntheestimatedmodel.\\n○ Thehigherthevalue,themorelikelythedatawasgeneratedbythis\\nmodel.\\n● AICandBIC:\\n○ LikeadjustedR-squared,theseareperformancemetricsto\\ndeterminegoodnessofﬁtwhilepenalizingcomplexity.\\n○ LowerAICandBICvaluesindicateabetterﬁt.\\n\\x00e\\x00\\x00i\\x00\\x00\\x002\\nThesecondsectionprovidesdetailsrelatedtothefeatures:\\n● coef:Theestimatedcoeﬃcientforafeature.\\n● tandP>|t|:\\n○ Earlier,weusedF-statistictodeterminethestatisticalsigniﬁcance\\nofthemodelasawhole.\\n○ t-statisticismoregranularonthatfrontasitdeterminesthe\\nsigniﬁcanceofeveryindividualfeature.\\n○ P>|t|istheassociatedp-valuewiththet-statistic.\\n○ Asmallp-value(typicallylessthan0.05)indicatesthatthefeatureis\\nstatisticallysigniﬁcant.\\n○ Forinstance,thefeature“X”hasap-valueof~0.6.Thissuggeststhat\\nthereisa60%chancethatthefeature“X”hasnoeﬀecton“Y”.\\n● [0.025,0.975]andstderr:\\n241'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 242, 'page_label': '243', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n○ See,thecoeﬃcientswehaveobtainedfromthemodelarejust\\nestimates.Theymaynotbeabsolutetruecoeﬃcientsoftheprocess\\nthatgeneratedthedata.\\n○ Thus,theestimatedparametersaresubjecttouncertainty,aren’t\\nthey?\\n○ Note:Thewidthoftheinterval[0.025,0.975]is0.95→or95%.This\\nconstitutestheareabetween2standarddeviationsfromthemeanin\\nanormaldistribution.\\n○ A95%conﬁdenceintervalprovidesarangeofvalueswithinwhich\\nyoucanbe95%conﬁdentthatthetruevalueoftheparameterlies.\\n○ Forinstance,theintervalforsin_Xis(0.092,6.104).Soalthoughthe\\nestimatedcoeﬃcientis3.09,wecanbe95%conﬁdentthatthetrue\\ncoeﬃcientliesintherange(0.092,6.104).\\n242'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 243, 'page_label': '244', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00i\\x00\\x00\\x003\\nDetailsinthelastsectionofthereporttesttheassumptionsoflinearregression.\\n● OmnibusandProb(Omnibus):\\n○ Theytestthenormalityoftheresiduals.\\n○ Omnibusvalueofzeromeansresidualsareperfectlynormal.\\n○ Prob(Omnibus)isthecorrespondingp-value.\\n■ Inthiscase,Prob(Omnibus)is0.001.Thismeansthereisa\\n0.1%chancethattheresidualsarenormallydistributed.\\n● SkewandKurtosis:\\n○ Theyalsoprovideinformationaboutthedistributionofthe\\nresiduals.\\n○ Skewnessmeasurestheasymmetryofthedistributionofresiduals.\\n■ Zeroskewnessmeansperfectsymmetry.\\n■ Positiveskewnessindicatesadistributionwithalongright\\ntail.Thisindicatesaconcentrationofresidualsonlower\\nvalues.Goodtocheckforoutliersinthiscase.\\nNegativeskewnessindicatesa\\ndistributionwithalongle\\x00tail.Thisis\\nmostlyindicativeofpoorfeatures.For\\ninstance,considerﬁttingasincurvewith\\nalinearfeature(X).Mostresidualswillbe\\nhigh,resultinginnegativeskewness.\\n243'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 244, 'page_label': '245', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Durbin-Watson:\\n○ Thismeasuresautocorrelationbetweenresiduals.\\n○ Autocorrelationoccurswhentheresidualsarecorrelated,indicating\\nthattheerrortermsarenotindependent.\\n○ Butlinearregressionassumesthatresidualsarenotcorrelated.\\n○ TheDurbin-Watsonstatisticrangesbetween0and4.\\n■ Avaluecloseto2indicatesnoautocorrelation.\\n■ Valuescloserto0indicatepositiveautocorrelation.\\n■ Valuescloserto4indicatenegativeautocorrelation.\\n● Jarque-Bera(JB)andProb(JB):\\n○ TheysolvethesamepurposeasOmnibusandProb(Omnibus)—\\nmeasuringthenormalityofresiduals.\\n● ConditionNumber:\\n○ Thistestsmulticollinearity.\\n○ Multicollinearityoccurswhentwofeaturesarecorrelated,ortwoor\\nmorefeaturesdeterminethevalueofanotherfeature.\\n○ AstandalonevalueforConditionNumbercanbediﬃcultto\\ninterpretsohere’showIuseit:\\n■ Addfeaturesonebyonetotheregressionmodelandnotice\\nanyspikesintheConditionNumber.\\nAsdiscussedabove,everysectionofthisreporthasitsimportance:\\n● Theﬁrstsectiontellsusaboutthemodel’sconﬁg,theoverallperformance\\nofthemodel,anditsstatisticalsigniﬁcance.\\n● Thesecondsectiontellsusaboutthestatisticalsigniﬁcanceofindividual\\nfeatures,themodel’sconﬁdenceinﬁndingthetruecoeﬃcient,etc.\\n● Thelastsectionletsusvalidatethemodel’sassumptions,whichare\\nimmenselycriticaltolinearregression’sperformance.\\nNowyouknowhowtointerprettheentireregressionsummaryfromstatsmodel.\\n244'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 245, 'page_label': '246', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n  \\x00e\\x00\\x00r\\x00\\x00i\\x00\\x00d\\x00i\\x00\\x00a\\x00\\x00o\\x00\\x00l\\x00\\x00G\\x00\\x00s\\x00\\nAlinearregressionmodelisundeniablyanextremelypowerfulmodel,inmy\\nopinion.However,itmakessomestrictassumptionsaboutthetypeofdataitcan\\nmodel,asdepictedbelow.\\nTheseconditionso\\x00enrestrictitsapplicabilitytodatasituationsthatdonot\\nobeytheaboveassumptions.Thatiswhybeingawareofitsextensionsis\\nimmenselyimportant.\\nGeneralizedlinearmodels(GLMs)preciselydothat.Theyrelaxtheassumptions\\noflinearregressiontomakelinearmodelsmoreadaptabletoreal-worlddatasets.\\n\\x00h\\x00\\x00L\\x00\\x00?\\nLinearregressionisprettyrestrictedintermsofthekindofdataitcanmodel.\\nForinstance,itsassumeddatagenerationprocesslookslikethis:\\n245'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 246, 'page_label': '247', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheassumeddatagenerationprocessoflinearregression\\n● Firstly,itassumesthattheconditionaldistributionofYgivenXisa\\nGaussian.\\n● Next,itassumesaveryspeciﬁcformforthemeanoftheaboveGaussian.It\\nsaysthatthemeanshouldalwaysbealinearcombinationofthefeatures\\n(orpredictors).\\n● Lastly,itassumesaconstantvariancefortheconditionaldistribution\\nP(Y|X)acrossalllevelsofX.Agraphicalwayofillustratingthisisas\\nfollows:\\nTheseconditionso\\x00enrestrictitsapplicabilitytodatasituationsthatdonot\\nobeytheaboveassumptions.\\nInotherwords,nothingstopsreal-worlddatasetsfromviolatingthese\\nassumptions.\\nInfact,inmanyscenarios,thedatamightexhibitcomplexrelationships,\\nheteroscedasticity(varyingvariance),orevenfollowentirelydiﬀerent\\ndistributionsaltogether.\\n246'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 247, 'page_label': '248', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nYet,ifweintendtobuildlinearmodels,weshouldformulatebetteralgorithms\\nthatcanhandlethesepeculiarities.\\nGeneralizedlinearmodels(GLMs)preciselydothat.\\nTheyrelaxtheassumptionsoflinearregressiontomakelinearmodelsmore\\nadaptabletoreal-worlddatasets.\\nMorespeciﬁcally,theyconsiderthefollowing:\\n● Whatifthedistributionisn’tnormalbutsomeotherdistribution?\\n● WhatifXhasamoresophisticatedrelationshipwiththemean?\\n● WhatifthevariancevarieswithX?\\n247'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 248, 'page_label': '249', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheeﬀectivenessofaspeciﬁcGLM—Poissonregression(whichwediscussedin\\nanearlierchapter)overlinearregressionisevidentfromtheimagebelow:\\n● LinearregressionassumesthedataisdrawnfromaGaussian,whenin\\nreality,itisn’t.Hence,itunderperforms.\\n● Poissonregressionadaptsitsregressionﬁttoanon-Gaussiandistribution.\\nHence,itperformssigniﬁcantlybetter.\\n248'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 249, 'page_label': '250', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00-\\x00\\x00ﬂ\\x00t\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\nThetargetvariableoftypicalregressiondatasetsissomewhatevenlydistributed.\\nBut,attimes,thetargetvariablemayhaveplentyofzeros.Suchdatasetsare\\ncalledzero-inﬂateddatasets.\\nTheymayraisemanyproblemsduringregressionmodeling.Thisisbecausea\\nregressionmodelcannotalwayspredictexact“zero”valueswhen,ideally,it\\nshould.Forinstance,considersimplelinearregression.Theregressionlinewill\\noutputexactly“zero”onlyonce(ifithasanon-zeroslope).\\nThisissuepersistsnotonlyinhigherdimensionsbutalsoincomplexmodelslike\\nneuralnetsforregression.\\nOnegreatwaytosolvethisisbytrainingacombinationofaclassiﬁcationanda\\nregressionmodel.\\n249'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 250, 'page_label': '251', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisgoesasfollows:\\n● Markallnon-zerotargetsas“1”andtherestas“0”.\\n● Trainabinaryclassiﬁeronthisdataset.\\n● Next,trainaregressionmodelonlyonthosedatapointswithanon-zero\\ntruetarget.\\nDuringprediction:\\n250'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 251, 'page_label': '252', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\n● Iftheclassiﬁer'soutputis“0”,theﬁnaloutputisalsozero.\\n● Iftheclassiﬁer'soutputis“1”,usetheregressionmodeltopredicttheﬁnal\\noutput.\\nItseﬀectivenessovertheregularregressionmodelisevidentfromtheimage\\nbelow:\\nRegressionvs.Regression+Classiﬁcationresults\\n● Linearregressionaloneunderﬁtsthedata.\\n● Linearregressionwithaclassiﬁerperformsasexpected.\\n251\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 252, 'page_label': '253', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00r\\x00e\\x00\\x00e\\x00\\x00i\\x00\\x00\\nOnebigproblemwithregressionmodelsisthattheyaresensitivetooutliers.\\nConsiderlinearregression.EvenafewoutlierscansigniﬁcantlyimpactLinear\\nRegressionperformance,asshownbelow:\\nAnditisn’thardtoidentifythecauseofthisproblem.Essentially,theloss\\nfunction(MSE)scalesquicklywiththeresidualterm(true-predicted). \\nThus,evenafewdatapointswithalargeresidualcanimpactparameter\\nestimation.\\n252'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 253, 'page_label': '254', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHuberloss(orHuberRegression)preciselyaddressesthisproblem.Inagist,it\\nattemptstoreducetheerrorcontributionofdatapointswithlargeresiduals.\\nOnesimple,intuitive,andobviouswaytodothisisbyapplyingathreshold(δ)on\\ntheresidualterm:\\n● Iftheresidualissmallerthanthethreshold,useMSE(nochangehere).\\n● Otherwise,usealossfunctionwhichhasasmalleroutputthanMSE—\\nlinear,forinstance.\\nThisisdepictedbelow:\\n● Forresiduals\\nsmallerthanthe\\nthreshold(δ)→we\\nuseMSE.\\n● Otherwise,weuse\\nalinearloss\\nfunctionwhichhas\\nasmalleroutput\\nthanMSE.\\nMathematically,Huberlossisdeﬁnedasfollows:\\n253'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 254, 'page_label': '255', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItseﬀectivenessis\\nevidentfromthe\\nfollowingimage:\\n● LinearRegression\\nisaﬀectedby\\noutliers\\n● HuberRegression\\nismorerobust.\\n\\x00o\\x00\\x00o\\x00e\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00h\\x00\\x00h\\x00\\x00s\\x00\\x00l\\x00\\x00δ\\x00?\\nWhiletrialanderrorisoneway,Io\\x00enliketocreatearesidualplot.Thisis\\ndepictedbelow:Thebelowplotisgenerallycalledalollipopplotbecauseofits\\nappearance.\\n● Trainalinearregressionmodelasyouusuallywould.\\n● Computetheresiduals(=true-predicted)onthetrainingdata.\\n● Plottheabsoluteresidualsforeverydatapoint.\\nOnegoodthingisthatwecancreatethisplotforanydimensionaldataset.The\\nobjectiveisjusttoplot(true-predicted)values,whichwillalwaysbe1D.\\n254'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 255, 'page_label': '256', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNext,youcansubjectivelydecideareasonablethresholdvalueδ.\\n\\x00e\\x00\\x00’\\x00\\x00n\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00g\\x00d\\x00\\x00.\\nByusingalinearlossfunctioninHuberregressor,weintendedtoreducethe\\nlargeerrorcontributionsthatwouldhavehappenedotherwisebyusingMSE.\\nThus,wecanfurtherreducethaterrorcontributionbyusing,say,asquareroot\\nlossfunction,asshownbelow:\\nIamunsureifthishasbeenproposedbefore,soIdecidedtocallitthe\\n\\x00a\\x00\\x00y\\x00\\x00s\\x00\\x00\\x00D\\x00\\x00a\\x00\\x00i\\x00\\x00c\\x00\\x00e\\x00\\x00e\\x00\\x00o\\x00\\n .\\nItisclearthattheerrorcontributionofthesquarerootlossfunctionisthelowest\\nforallresidualsabovethethresholdδ.\\n255'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 256, 'page_label': '257', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\x00n\\x00\\x00n\\x00\\x00m\\x00\\x00e\\n\\x00e\\x00\\x00o\\x00\\x00\\n\\x00o\\x00\\x00e\\x00\\x00e\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00s\\x00\\x00n\\x00\\x00\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00\\nThere’saninterestingtechnique,usingwhich,wecancondenseanentirerandom\\nforestmodelintoasingledecisiontree.\\nThebeneﬁts?\\nThistechniquecan:\\n● Decreasethepredictionrun-time.\\n● Improveinterpretability.\\n● Reducethememoryfootprint.\\n● Simplifythemodel.\\n● Preservethegeneralizationpoweroftherandomforestmodel.\\nLet’sunderstandinthischapter.\\n256'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 257, 'page_label': '258', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00a\\x00\\x00t\\x00\\x00o\\x00\\x00h\\nLet’sﬁtadecisiontreemodelonthefollowingdummydataset.Itproducesa\\ndecisionregionplotshownontheright.\\nIt’sclearthatthereishighoverﬁtting.\\nInfact,wemustnotethat,bydefault,adecisiontreecanalways100%overﬁtany\\ndataset(wewillusethisinformationshortly).Thisisbecauseitisalwaysallowed\\ntogrowuntilallsampleshavebeenclassiﬁedcorrectly.\\nThisoverﬁttingproblemisresolvedbyarandomforestmodel,asdepictedbelow:\\n257'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 258, 'page_label': '259', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\nThistime,thedecisionregionplotsuggeststhatwedon’thaveacomplex\\ndecisionboundary.Thetestaccuracyhasalsoimproved(69.5%to74%).\\nNow,here’saninterestingthingwecando.\\nWeknowthattherandomforestmodelhaslearnedsomerulesthatgeneralizeon\\nunseendata.\\nSo,howaboutwetrainadecisiontreeonthepredictionsgeneratedbythe\\nrandomforestmodelonthetrainingset?\\nMorespeciﬁcally,givenadataset(X,y):\\n● Trainarandomforestmodel.Thiswilllearnsomerulesfromthetraining\\nsetwhichareexpectedtogeneralizeonunseendata(duetoBagging).\\n● GeneratepredictionsonX,whichproducestheoutputy'.These\\npredictionswillcapturetheessenceoftheruleslearnedbytherandom\\nforestmodel.\\n● Finally,trainadecisiontreemodelon(X,y').Here,wewantto\\nintentionallyoverﬁtthismappingasthismappingfrom(X)to(y')isaproxy\\nfortheruleslearnedbytherandomforestmodel.\\n258\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 259, 'page_label': '260', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisideaisimplementedbelow:\\nThedecisionregionplotwegetwiththenewdecisiontreeisprettysimilarto\\nwhatwesawwiththerandomforestearlier:\\nMeasuringthetestaccuracyofthedecisiontreeandrandomforestmodel,we\\nnoticethemtobesimilartoo:\\n259'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 260, 'page_label': '261', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInfact,thisapproachalsosigniﬁcantlyreducestherun-time,asdepictedbelow:\\nIsn’tthatcool?\\nAnotherrationaleforconsideringdoingthisisthatitaddsinterpretability.\\nThisisbecauseifwehave100treesinarandomforest,there’snowaywecan\\ninterpretthem.\\nHowever,ifwehavecondensedittoadecisiontree,nowwecaninspectit.\\n260'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 261, 'page_label': '262', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00o\\x00\\x00\\nIdevisedthisveryrecently.Ialsotestedthisapproachonacoupleofdatasets,\\nandtheyproducedpromisingresults.\\nButitwon’tbefairtomakeanyconclusionsbasedonjusttwoinstances.\\nWhiletheideamakesintuitivesense,Iunderstandtherecouldbesomepotential\\nﬂawsthatarenotevidentrightnow.\\nSo,Inotsayingthatyoushouldadoptthistechniquerightaway.\\nInstead,itisadvisedtotestthisapproachonyourrandomforestusecases.\\nConsideringrevertingbacktomewithwhatyoudiscovered.\\nThecodeforthischapterisavailablehere:https://bit.ly/3XSPejD.\\nInthisnextchapter,let’sunderstandatechniquetotransformadecisiontreeinto\\nmatrixoperationswhichcanrunonGPUs.\\n261'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 262, 'page_label': '263', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00s\\x00\\x00r\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00\\x00n\\x00\\x00\\x00a\\x00\\x00i\\x00\\x00p\\x00\\x00a\\x00\\x00o\\x00\\x00\\nInferenceusingadecisiontreeisaniterativeprocess.Wetraverseadecisiontree\\nbyevaluatingtheconditionataspeciﬁcnodeinalayeruntilwereachaleaf\\nnode.\\nInthischapter,let’slearnasuperbtechniquethattorepresentinferencesfroma\\ndecisiontreeintheformofmatrixoperations.\\nAsaresult:\\n1. Itmakesinferencemuchfasterasmatrixoperationscanberadically\\nparallelized.\\n2. TheseoperationscanbeloadedonaGPUforevenfasterinference,making\\nthemmoredeployment-friendly.\\n\\x00e\\x00\\x00p\\nConsiderabinaryclassiﬁcationdatasetwith5features.\\n262'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 263, 'page_label': '264', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’ssaywegetthefollowingtreestructurea\\x00erﬁttingadecisiontreeonthe\\nabovedataset:\\n\\x00o\\x00\\x00t\\x00\\x00n\\x00\\nBeforeproceedingahead,let’sassumethat:\\n● \\x00→Totalfeaturesinthedataset(5inthedatasetabove).\\n● \\x00→Totalevaluationnodesinthetree(4bluenodesinthetreeabove).\\n● \\x00→Totalleafnodesinthetree(5greennodesinthetree).\\n● \\x00→Totalclassesinthedataset(2inthedatasetabove).\\n\\x00r\\x00\\x00\\x00o\\x00a\\x00\\x00i\\x00\\x00s\\nThecoreideain\\nthisconversionis\\ntoderiveﬁve\\nmatrices(\\x00,\\x00,\\x00,\\n\\x00,\\x00)thatcapture\\nthestructureofthe\\ndecisiontree.\\n263'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 264, 'page_label': '265', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’sunderstandthemonebyone!\\n\\x00o\\x00\\x00:\\x00h\\x00\\x00t\\x00\\x00s\\x00e\\x00h\\x00\\x00l\\x00e\\x00e\\x00\\x00o\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00w\\x00i\\x00\\x00t\\x00o\\x00\\x00a\\x00\\x00\\x00m\\x00\\x00d\\x00\\x00t\\x00\\n\\x00e\\x00\\x00\\x00\\x00o\\x00l\\x00\\x00s\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00i\\x00\\x00.\\x00o\\x00\\x00l\\x00\\x00n\\x00\\x00r\\x00\\x00a\\x00\\x00\\x00v\\x00\\x00y\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00\\x00e\\x00a\\x00\\x00\\n\\x00e\\x00\\x00v\\x00\\x00\\x00l\\x00\\x00\\x00a\\x00\\x00i\\x00\\x00s\\x00\\n\\x001\\x00\\x00a\\x00\\x00i\\x00\\x00\\nThismatrixcaptures\\ntherelationship\\nbetweeninputfeatures\\nandevaluationnodes\\n(bluenodesabove).\\nSoit’san(\\x00×\\x00)shaped\\nmatrix.\\nAspeciﬁcentryissetto“1”ifthecorrespondingnodeinthecolumnevaluates\\nthecorrespondingfeatureintherow.Forinstance,inourdecisiontree,“Node0”\\nevaluates“Feature2”.\\nThus,thecorrespondingentrywillbe“1”andallotherentrieswillbe“0.”\\n264'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 265, 'page_label': '266', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFillingouttheentirematrixthisway,weget:\\n\\x002\\x00\\x00a\\x00\\x00i\\x00\\x00\\nTheentriesofmatrixBarethethresholdvalueateachnode.Thus,itsshapeis\\n1×e.\\nThisisvectorthough,buttheterminologyisnotimportanthere.\\n\\x003\\x00\\x00a\\x00\\x00i\\x00\\x00\\nThisisamatrixbetweeneverypairofleafnodesandevaluationnodes.Thus,its\\ndimensionsare\\x00×\\x00.\\n265'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 266, 'page_label': '267', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAspeciﬁcentryissetto:\\n● “1”ifthecorrespondingleafnodeinthecolumnliesinthele\\x00sub-treeof\\nthecorrespondingevaluationnodeintherow.\\n● “-1”ifthecorrespondingleafnodeinthecolumnliesintherightsub-tree\\nofthecorrespondingevaluationnodeintherow.\\n● “0”ifthecorrespondingleafnodeandevaluationnodehavenolink.\\nForinstance,inourdecisiontree,the“leafnode4”liesonthele\\x00sub-treeof\\nboth“evaluationnode0”and“evaluationnode1”.Thus,thecorrespondingvalues\\nwillbe1.\\n266'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 267, 'page_label': '268', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFillingouttheentirematrixthisway,weget:\\n\\x004\\x00\\x00a\\x00\\x00i\\x00\\x00o\\x00\\x00e\\x00\\x00o\\x00\\x00\\x00\\nTheentriesofvectorDarethesumofnon-negativeentriesineverycolumnof\\nMatrixC:\\n\\x005\\x00\\x00a\\x00\\x00i\\x00\\x00\\nFinally,thismatrix\\nholdsthemapping\\nbetweenleafnodesand\\ntheircorresponding\\noutputlabels.Thus,its\\ndimensionsare\\x00×\\x00.\\n267'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 268, 'page_label': '269', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfaleafnodeclassiﬁesasampleto“Class1”,thecorrespondingentrywillbe1,\\nandtheothercellentrywillbe0.\\nForinstance,“leadnode4”outputs“Class1”,thusthecorrespondingentriesfor\\ntheﬁrstrowwillbe(1,0):\\nWerepeatthisforallotherleafnodestogetthefollowingmatrixasMatrixE:\\nWiththis,wehavecompiledourdecisiontreeintomatrices.Torecall,theseare\\ntheﬁvematriceswehavecreatedsofar:\\n268'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 269, 'page_label': '270', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● MatrixAcaptureswhichinputfeaturewasusedateachevaluationnode.\\n● MatrixBstoresthethresholdofeachevaluationnode.\\n● MatrixCcaptureswhetheraleafnodeliesonthele\\x00orrightsub-treeofa\\nspeciﬁcevaluationnodeorhasnorelationtoit.\\n● MatrixDstoresthesumofnon-negativeentriesineverycolumnofMatrix\\nC.\\n● Finally,MatrixEmapsfromleafnodestotheirclasslabels.\\n\\x00n\\x00\\x00r\\x00\\x00c\\x00\\x00s\\x00\\x00g\\x00a\\x00\\x00i\\x00\\x00s\\nSaythisisourinputfeaturevectorX(5dimensions):\\nThewholeinferencecannowbedoneusingjustthesematrixoperations:\\n● XA<Bgives:\\n269'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 270, 'page_label': '271', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● TheaboveresultmultipliedbyCgives:\\n● Theaboveresult,whenmatchedtoD,gives:\\n● Finally,multiplyingwithE,weget:\\nTheﬁnalpredictioncomesouttobe“Class1,”whichisindeedcorrect!Notice\\nthatwecarriedouttheentireinferenceprocessusingonlymatrixoperations:\\n270'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 271, 'page_label': '272', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsaresult,theinferenceoperationcanlargelybeneﬁtfromparallelizationand\\nGPUcapabilities.\\nTherun-timeeﬃcacyofthistechniqueisevidentfromtheimagebelow:\\n● Here,wehavetrainedarandomforestmodel.\\n● Thecompiledmodelruns:\\n○ OvertwiceasfastonaCPU(TensorCPUModel).\\n○ ~40timesfasteronaGPU,whichishuge(TensorGPUModel).\\n● Allmodelshavethesameaccuracy—indicatingnolossofinformation\\nduringcompilation.\\n271'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 272, 'page_label': '273', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00e\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00\\nOnethingIalwaysappreciate\\naboutdecisiontreesistheirease\\nofvisualinterpretability.No\\nmatterhowmanyfeaturesour\\ndatasethas,wecanALWAYS\\nvisualizeandinterpretadecision\\ntree.\\nThisisnotalwayspossiblewithotherintuitiveandsimplemodelslikelinear\\nregression.Butdecisiontreesstandoutinthisrespect.Nonetheless,onethingI\\no\\x00enﬁndabittime-consumingandsomewhathit-and-trial-drivenispruninga\\ndecisiontree.\\n\\x00h\\x00\\x00r\\x00\\x00e\\x00\\nTheproblemis\\nthatunderdefault\\nconditions,\\ndecisiontrees\\nALWAYS100%\\noverﬁtthedataset,\\nasdepictedinthis\\nimage:\\n272'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 273, 'page_label': '274', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,pruningisALWAYSnecessarytoreducemodelvariance.Scikit-learn\\nalreadyprovidesamethodtovisualizethemasshownbelow:\\nButtheabovevisualisationisprettynon-elegant,tedious,messy,andstatic(or\\nnon-interactive).IrecommendusinganinteractiveSankeydiagramtoprune\\ndecisiontrees.Thisisdepictedbelow:\\n273'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 274, 'page_label': '275', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsshownabove,theSankeydiagramallowsustointeractivelyvisualizeand\\npruneadecisiontreebycollapsingitsnodes.\\nAlso,thenumberofdatapoints\\nfromeachclassissizeand\\ncolor-encodedineachnode,as\\nshownbelow.\\nThisinstantlygivesanestimateofthenode’simpurity,basedonwhich,wecan\\nvisuallyandinteractivelyprunethetreeinseconds.Forinstance,inthefull\\ndecisiontreeshownbelow,pruningthetreeatadepthoftwoappearsreasonable:\\nNext,wecantrainanewdecisiontreea\\x00erobtaininganestimatefor\\nhyperparametervalues.Thiswillhelpusreducethevarianceofthedecisiontree.\\nYoucandownloadthecodenotebookfortheinteractivedecisiontreehere:\\nhttps://bit.ly/4bBwY1p.Instructionsareavailableinthenotebook.\\nNext,let’sunderstandapointofcautionwhenusingdecisiontrees.\\n274'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 275, 'page_label': '276', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00 \\x00e\\x00\\x00s\\x00\\x00n \\x00r\\x00\\x00s \\x00u\\x00\\x00 \\x00e\\x00h\\x00\\x00o\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00e\\x00\\n\\x00f\\x00\\x00r\\x00r\\x00\\x00n\\x00\\x00g\\nIfweweretovisualizethe\\ndecisionrules(theconditions\\nevaluatedateverynode)ofANY\\ndecisiontree,wewould\\nALWAYSﬁndthemtobe\\nperpendiculartothefeature\\naxes,asdepictedintheimage.\\nInotherwords,everydecisiontreeprogressivelysegregatesfeaturespacebased\\nonsuchperpendicularboundariestosplitthedata.\\nOfcourse,thisisnota“problem”perse.\\nInfact,thisperpendicularsplittingiswhatmakesitsopowerfultoperfectly\\noverﬁtanydataset.However,thisalsobringsupaprettyinterestingpointthatis\\no\\x00enoverlookedwhenﬁttingdecisiontrees.Morespeciﬁcally,whatwould\\nhappenifourdatasethadadiagonaldecisionboundary,asdepictedbelow:\\n275'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 276, 'page_label': '277', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItiseasytoguessthatinsuchacase,the\\ndecisionboundarylearnedbyadecisiontreeis\\nexpectedtoappearasfollows:\\nInfact,ifweplotthisdecisiontree,wenoticethatitcreatessomanysplitsjustto\\nﬁtthiseasilyseparabledataset,whichamodellikelogisticregression,support\\nvectormachine(SVM),orevenasmallneuralnetworkcaneasilyhandle:\\nItbecomesmoreevidentifwezoomintothisdecisiontreeandnoticehowclose\\nthethresholdsofitssplitconditionsare:\\n276'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 277, 'page_label': '278', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisabitconcerningbecauseitclearlyshowsthatthedecisiontreeis\\nmeticulouslytryingtomimicadiagonaldecisionboundary,whichhintsthatit\\nmightnotbethebestmodeltoproceedwith.Todouble-checkthis,Io\\x00endothe\\nfollowing:\\n● Takethetrainingdata\\x00X\\x00\\x00);\\n○ Shapeof\\x00:\\x00n\\x00\\x00).\\n○ Shapeof\\x00:\\x00n\\x00\\x00).\\n● RunPCAon\\x00toprojectdataintoanorthogonalspaceofmdimensions.\\nThiswillgive\\x00_\\x00\\x00a,whoseshapewillalsobe\\x00n\\x00\\x00).\\n● Fitadecisiontreeon\\x00_\\x00\\x00aandvisualizeit(thankfully,decisiontreesare\\nalwaysvisualizable).\\n● Ifthedecisiontreedepthissigniﬁcantlysmallerinthiscase,itvalidates\\nthatthereisadiagonalseparation.\\nForinstance,thePCAprojectionsontheabovedatasetareshownbelow:\\nItisclearthatthedecisionboundaryonPCAprojectionsisalmostperpendicular\\ntothe\\x002\\x00feature(the2ndprincipalcomponent).\\n277'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 278, 'page_label': '279', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFittingadecisiontreeonthis\\x00_\\x00\\x00adrasticallyreducesitsdepth,asdepicted\\nbelow:\\nThisletsusdeterminethatwemightbebetteroﬀusingsomeotheralgorithm\\ninstead.\\nOr,wecanspendsometimeengineeringbetterfeaturesthatthedecisiontree\\nmodelcaneasilyworkwithusingitsperpendiculardatasplits.\\nAtthispoint,ifyouarethinking,whycan’tweusethedecisiontreetrainedon\\n\\x00_\\x00\\x00a?\\nWhilenothingstopsusfromdoingthat,donotethatPCAcomponentsarenot\\ninterpretable,andmaintainingfeatureinterpretabilitycanbeimportantattimes.\\nThus,wheneveryoutrainyournextdecisiontreemodel,considerspendingsome\\ntimeinspectingwhatit’sdoing.\\n\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00n\\x00\\x00h\\x00\\x00\\x00h\\x00\\x00t\\x00\\x00\\n\\x00\\x00o\\x00\\x00t\\x00n\\x00\\x00n\\x00\\x00o\\x00i\\x00\\x00o\\x00\\x00a\\x00\\x00\\x00h\\x00\\x00s\\x00\\x00f\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\x00\\x00h\\x00\\x00\\x00r\\x00\\x00h\\x00\\n\\x00u\\x00\\x00d\\x00\\x00g\\x00l\\x00\\x00k\\x00\\x00f\\x00o\\x00\\x00\\x00f\\x00h\\x00\\x00o\\x00\\x00\\x00o\\x00\\x00r\\x00\\x00l\\x00n\\x00\\x00\\x00b\\x00\\x00\\x00o\\x00\\x00l\\x00\\x00e\\x00s\\x00\\n\\x00o\\x00\\x00y\\x00\\n\\x00y\\x00o\\x00\\x00t\\x00s\\x00o\\x00r\\x00\\x00g\\x00o\\x00\\x00a\\x00\\x00\\x00h\\x00\\x00t\\x00\\x00c\\x00\\x00\\x00a\\x00\\x00o\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00f\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\n\\x00n\\x00\\x00h\\x00\\x00w\\x00\\x00n\\x00h\\x00\\x00\\x00i\\x00\\x00t\\x00o\\x00\\x00e\\x00n\\x00d\\x00\\x00l\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00o\\x00o\\x00\\x00\\x00i\\x00\\x00.\\n278'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 279, 'page_label': '280', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00s\\x00\\x00n\\x00r\\x00\\x00s\\x00L\\x00\\x00\\x00S\\x00v\\x00\\x00ﬁ\\x00!\\nInadditiontotheaboveinspection,there’sonemorethingyouneedtobecareful\\nofwhenusingdecisiontrees.Thisisaboutoverﬁtting.\\nThethingisthat,bydefault,adecisiontree(insklearn’simplementation,for\\ninstance),isallowedtogrowuntilallleavesarepure.Thishappensbecausea\\nstandarddecisiontreealgorithmgreedilyselectsthebestsplitateachnode.\\nThismakesitsnodesmoreandmorepureaswetraversedownthetree.Asthe\\nmodelcorrectlyclassiﬁesALLtraininginstances,itleadsto100%overﬁtting,and\\npoorgeneralization.\\nForinstance,considerthisdummydataset:\\n279'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 280, 'page_label': '281', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFittingadecisiontreeonthisdatasetgivesusthefollowingdecisionregionplot:\\nItisprettyevidentfromthedecisionregionplot,thetrainingandtestaccuracy\\nthatthemodelhasentirelyoverﬁttedourdataset.\\n\\x00o\\x00\\x00-\\x00\\x00m\\x00\\x00e\\x00\\x00t\\x00\\x00p\\x00\\x00n\\x00\\x00g\\x00C\\x00\\x00)isaneﬀectivetechniquetopreventthis.\\nCCPconsidersacombinationoftwofactorsforpruningadecisiontree:\\n● Cost(C):Numberofmisclassiﬁcations\\n● Complexity(C):Numberofnodes\\nThecoreideaistoiterativelydropsub-trees,which,a\\x00erremoval,leadtoa\\nminimalincreaseinclassiﬁcationcostANDamaximumreductionofcomplexity\\n(ornodes).Inotherwords,iftwosub-treesleadtoasimilarincreasein\\nclassiﬁcationcost,thenitiswisetoremovethesub-treewithmorenodes.\\n280'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 281, 'page_label': '282', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInsklearn,youcancontrolcost-complexity-pruningusingtheccp_alpha\\nparameter:\\n● largevalueof\\x00c\\x00\\x00a\\x00\\x00h\\x00→resultsinunderﬁtting\\n● smallvalueof\\x00c\\x00\\x00a\\x00\\x00h\\x00→resultsinoverﬁtting\\nTheobjectiveistodeterminetheoptimalvalueof\\x00c\\x00\\x00a\\x00\\x00h\\x00,whichgivesa\\nbettermodel.Theeﬀectivenessofcost-complexity-pruningisevidentfromthe\\nimagebelow:\\nAsdepictedabove,CCPresultsinamuchsimplerandacceptabledecisionregion\\nplot.\\n281'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 282, 'page_label': '283', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00O\\x00\\x00a\\x00\\x00d\\x00\\x00i\\x00\\x00\\x00n\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00s\\x00\\nA\\x00ertraininganMLmodelonatrainingset,wealwayskeepaheld-out\\nvalidation/testsetforevaluation.Iamsureyoualreadyknowthepurpose,sowe\\nwon’tdiscussthat.\\nButdoyouknowthatrandomforestsareanexceptiontothat?Inotherwords,\\nonecansomewhat“evaluate”arandomforestusingthetrainingsetitself.\\nLet’sunderstandhow.\\nTorecap,arandomforestistrainedasfollows:\\n● First,wecreatediﬀerentsubsetsofdatawithreplacement(thisprocessis\\ncalledbootstrapping).\\n● Next,wetrainonedecisiontreepersubset.\\n● Finally,weaggregateallpredictionstogettheﬁnalprediction.\\nThisprocessisdepictedbelow:\\n282'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 283, 'page_label': '284', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfwelookcloselyabove,everysubsethassomemissingdatapointsfromthe\\noriginaltrainingset.\\nWecanusetheseobservationstovalidatethemodel.Thisisalsocalled\\nout-of-bagvalidation.Calculatingtheout-of-bagscoreforthewholerandom\\nforestissimpletoo.\\nButonethingtorememberisthatweCANNOTevaluateindividualdecision\\ntreesontheirspeciﬁcout-of-bagsampleandgeneratesomesortof“aggregated\\nscore”fortheentirerandomforestmodel.\\n283'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 284, 'page_label': '285', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisbecausearandomforestisnotaboutwhatadecisiontreesays\\nindividually.Instead,it’saboutwhatalldecisiontreessaycollectively.\\nSohere’showwecangeneratetheout-of-bagscorefortherandomforestmodel.\\nForeverydatapointinthetrainingset:\\n● Gatherpredictionsfromalldecisiontreesthatdidnotuseitasatraining\\ndatapoint.\\n● Aggregatepredictionstogettheﬁnalprediction.\\nForinstance,consideraRFmodelwith5decisiontrees→\\x00P\\x00\\x00,\\x00,\\x00,\\x00).Saya\\nspeciﬁcdatapoint\\x00wasusedasatrainingdataindecisiontrees\\x00and\\x00.\\nSoweshallgathertheout-of-bagpredictionfordatapointXfromdecisiontrees\\nQ,SandT.\\nA\\x00erobtainingout-of-bagpredictionsforallsamples,wescorethemtogetthe\\nout-of-bagscore.\\nSee…thistechniqueallowedustoevaluatearandomforestmodelonthetraining\\nset.Ofcourse,Idon’twantyoutoblindlyadoptout-of-bagvalidationwithout\\nunderstandingsomeofitsadvantagesandconsiderations.\\nIhavefoundout-of-bagvalidationtobeparticularlyusefulinthefollowing\\nsituations:\\n284'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 285, 'page_label': '286', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Inlow-datasituations,out-of-bagvalidationpreventsdatasplittingwhilst\\nobtainingagoodproxyformodelvalidation.\\n● Inlarge-datasituations,traditionalcross-validationtechniquesare\\ncomputationallyexpensive.Here,out-of-bagvalidationprovidesan\\neﬃcientalternative.Thisisbecause,byitsverynature,even\\ncross-validationprovidesanout-of-foldmetric.Out-of-bagvalidationis\\nalsobasedonasimilarprinciple.\\nAnd,ofcourse,aninherentadvantageofout-of-bagvalidationisthatit\\nguaranteesnodataleakage.Luckily,out-of-bagvalidationisalsoneatlytiedin\\nsklearn’srandomforestimplementation.\\nThemostsigniﬁcantconsiderationaboutout-of-bagscoreistouseitwith\\ncautionformodelselection,modelimprovement,etc.\\nThisisbecauseifwedo,wetypicallytendtooverﬁttheout-of-bagscoreasthe\\nmodelisessentiallybeingtunedtoperformwellonthedatapointsthatwerele\\x00\\noutduringitstraining.Andifweconsistentlyimprovethemodelbasedonthe\\nout-of-bagscore,weobtainanoverlyoptimisticevaluationofitsgeneralization\\nperformance.\\nIfIweretosharejustonelessonherebasedonmyexperience,itwouldbethatif\\nwedon’thaveatrue(andentirelydiﬀerent)held-outsetforvalidation,wewill\\noverﬁttosomeextent.\\nThedecisionsmademaybetoospeciﬁctotheout-of-bagsampleandmaynot\\ngeneralizewelltonewdata.\\n285'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 286, 'page_label': '287', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00n\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00s\\x00\\x00n\\x00a\\x00\\x00e\\x00a\\x00\\x00s\\x00\\x00s\\nMostclassicalMLalgorithmscannotbetrainedwithabatchimplementation.\\nThislimitstheirusagetoonlysmall/intermediatedatasets.Forinstance,thisis\\nthelistofsklearnimplementationsthatsupportabatchAPI:\\nIt’sprettysmall,isn’tit?\\nThisisconcerningbecause,intheenterprisespace,thedataisprimarilytabular.\\nClassicalMLtechniques,suchastree-basedensemblemethods,arefrequently\\nusedformodeling.\\nHowever,typicalimplementationsofthesemodelsarenot“big-data-friendly”\\nbecausetheyrequiretheentiredatasettobepresentinmemory.\\n286'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 287, 'page_label': '288', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTherearetwowaystoapproachthis:\\n1. Usebig-dataframeworkslikeSparkMLlibtotrainthem.\\n2. There’sonemoreway,whichDr.GillesLouppediscussedinhisPhDthesis\\n—UnderstandingRandomForests.\\nHere’swhatheproposed.\\n\\x00a\\x00\\x00o\\x00\\x00a\\x00\\x00h\\x00\\x00\\nBeforeexplaining,notethatthisapproachwillonlyworkinanensemblesetting.\\nSo,youwouldhavetotrainmultiplemodels.Theideaistosamplerandomdata\\npatches(rowsandcolumns)andtrainatreemodelonthem.\\nRepeatthisstepmultipletimesbygeneratingdiﬀerentpatchesofdatarandomly\\ntoobtaintheentirerandomforestmodel.\\nTheeﬃcacy?\\nThethesispresentsmanybenchmarks(checkpages174and178ifyouneedmore\\ndetails)on13datasets,andtheresultsareshownbelow:\\n287'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 288, 'page_label': '289', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFromle\\x00toright→\\x00i\\x00\\x00r\\x00\\x00,\\x00n\\x00\\x00t\\x00\\x008\\x00\\x00n\\x00\\x00t\\x00\\x009\\x00\\x00n\\x00\\x00t\\x00\\x00s\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00n\\x00\\x00\\n\\x00r\\x00\\x00s\\x00\\x00,\\x00a\\x00\\x00l\\x00\\x00,\\x00a\\x00\\x00i\\x00\\x00e\\x00\\x00d\\x00\\x00e\\x00\\x00n\\x00\\x00\\x00h\\x00\\x00,and\\x00i\\x00\\x00.\\nFromtheaboveimage,itisclearthatinmostcases,therandompatches\\napproachperformsbetterthanthetraditionalrandomforest.\\nInothercases,therewasn’tasigniﬁcantdiﬀerenceinperformance.\\nAndthisishowwecantrainarandomforestmodelonlargedatasetsthatdonot\\nﬁtintomemory.\\n\\x00h\\x00\\x00o\\x00\\x00\\x00t\\x00o\\x00\\x00?\\nWhilethethesisdidnotprovideaclearintuitionbehindthis,Icanunderstand\\nwhysuchanapproachwouldstillbeaseﬀectiveasrandomforest.\\nInagist,buildingtreesthatareasdiﬀerentaspossibleguaranteesagreater\\nreductioninvariance.\\nInthiscase,thedatasetoverlapbetweenanytwotreesisNOTexpectedtobe\\nhugecomparedtothetypicalrandomforest.ThisaidsintheBaggingobjective.\\n288'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 289, 'page_label': '290', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00i\\x00\\x00a\\x00\\x00u\\x00\\x00e\\x00o\\x00d\\x00\\x00o\\x00\\x00t\\nThefollowingvisualsummarizeshowBoostingmodelswork:\\n● Boostingisaniterativetrainingprocess.\\n● Thesubsequentmodelputsmorefocusonmisclassiﬁedsamplesfromthe\\npreviousmodel.\\n● Theﬁnalpredictionisaweightedcombinationofallpredictions\\nHowever,manyﬁnditdiﬃculttounderstandhowthismodelispreciselytrained\\nandhowinstancesarereweighedforsubsequentmodels.AdaBoostisacommon\\nBoostingmodel,sointhischapter,let’sunderstandhowitworks.\\n\\x00d\\x00\\x00o\\x00\\x00t\\x00n\\x00\\x00r\\x00\\x00l\\x00o\\x00\\x00i\\x00\\x00\\nThecoreideabehindAdaboostistotrainmanyweaklearnerstobuildamore\\npowerfulmodel.Thistechniqueisalsocalledensembling.\\nSpeciﬁcallytalkingaboutAdaboost,theweakclassiﬁersprogressivelylearnfrom\\nthepreviousmodel’smistakes,creatingapowerfulmodelwhenconsideredasa\\nwhole.\\n289'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 290, 'page_label': '291', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheseweaklearnersareusuallydecisiontrees.\\nLetmemakeitmoreclearbyimplementingAdaBoostusingthe\\n\\x00e\\x00\\x00s\\x00\\x00n\\x00\\x00e\\x00\\x00l\\x00\\x00s\\x00ﬁ\\x00rclassfromsklearn.\\nConsiderwehavethefollowingclassiﬁcationdataset:\\nTobegin,everyrowhasanequalweight,anditisequalto(1/n),wherenisthe\\nnumberoftraininginstances.\\n\\x00t\\x00\\x00\\x00:\\x00r\\x00\\x00n\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00\\nInAdaboost,everydecisiontree\\nhasaunitdepth,andtheyarealso\\ncalledstumps.\\n290'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 291, 'page_label': '292', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,wedeﬁneDecisionTreeClassiﬁerwithamax_depthof1,andtrainitonthe\\nabovedataset.\\n\\x00t\\x00\\x00\\x00:\\x00a\\x00\\x00u\\x00\\x00t\\x00\\x00h\\x00\\x00e\\x00\\x00n\\x00\\x00\\x00s\\x00o\\x00\\x00\\nOfcourse,therewillbesomecorrectandincorrectpredictions.\\nThetotalcost(orerror/loss)ofthisspeciﬁcweaklearneristhesumofthe\\nweightsoftheincorrectpredictions.Inourcase,wehavetwoerrors,sothetotal\\nerroris:\\n291'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 292, 'page_label': '293', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNow,asdiscussedabove,theideaistolettheweaklearnprogressivelylearnfrom\\npreviouslearner’smistakes.So,goingahead,wewantthesubsequentmodelto\\nfocusmoreontheincorrectpredictionsproducedearlier.\\nHere’showwedothis:\\n\\x00t\\x00\\x00\\x00:\\x00a\\x00\\x00u\\x00\\x00t\\x00\\x00h\\x00\\x00e\\x00\\x00n\\x00\\x00\\x00s\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00\\nFirst,wedeterminetheimportanceoftheweaklearner.Quiteintuitively,we\\nwanttheimportancetobeinverselyrelatedtotheaboveerror.\\n● Iftheweaklearnerhasahigherror,itmusthavealowerimportance.\\n● Iftheweaklearnerhasalowerror,itmusthaveahigherimportance.\\nOnechoiceisthefollowingfunction:\\n● Thisfunctionisonlydeﬁnedbetween[0,1].\\n● Whentheerrorishigh(~1),thismeanstherewerenocorrectpredictions→\\nThisgivesanegativeimportancetotheweaklearner.\\n● Whentheerrorislow(~0),thismeanstherewerenoincorrectpredictions\\n→Thisgivesapositiveimportancetotheweaklearner.\\n\\x00f\\x00o\\x00\\x00e\\x00\\x00\\x00h\\x00\\x00e\\x00o\\x00\\x00d\\x00e\\x00\\x00e\\x00\\x00e\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00o\\x00s\\x00\\x00e\\x00\\x00,\\x00o\\x00\\x00r\\x00\\x00r\\x00\\x00\\x00o\\n\\x00s\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00a\\x00\\x00\\x00t\\x00o\\x00\\x00\\x00w\\x00\\x00o\\x00\\x00t\\x00\\x00g\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00\\n292'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 293, 'page_label': '294', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNow,wehavetheimportanceofthelearner.\\nTheimportancevalueisusedduringmodelinferencetoweighthepredictions\\nfromtheweaklearners.Sothenextstepisto…\\n\\x00t\\x00\\x00\\x00:\\x00e\\x00\\x00i\\x00\\x00\\x00h\\x00\\x00r\\x00\\x00n\\x00\\x00g\\x00n\\x00\\x00a\\x00\\x00e\\x00\\nAllthecorrectpredictionsareweigheddownasfollows:\\nAndalltheincorrectpredictionsareweighedupasfollows:\\nOncedone,wenormalizethenewweightstoadduptoone.\\nThat’sit!\\n\\x00t\\x00\\x00\\x00:\\x00a\\x00\\x00l\\x00\\x00r\\x00\\x00\\x00e\\x00\\x00i\\x00\\x00e\\x00\\x00a\\x00\\x00s\\x00\\x00\\nFromstep4,wehavethereweigheddataset.\\n293'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 294, 'page_label': '295', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWesampleinstances\\nfromthisdatasetin\\nproportiontothenew\\nweightstocreateanew\\ndataset.\\nNext,gobacktostep1—Trainthenextweaklearner.\\nAndrepeattheaboveprocessoverandoverforsomepre-deﬁnedmaxiterations.\\nThat’showwebuildtheAdaBoostmodel.\\nAllwehavetodoisconsidertheerrorsfromthepreviousmodel,reweighand\\nsamplethetraininginstancesforthenextmodel,andrepeat.\\n294'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 295, 'page_label': '296', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00n\\x00\\x00o\\x00\\x00l\\x00\\x00y\\x00e\\x00\\x00c\\x00\\x00o\\x00\\n\\x00h\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00f\\x00V\\x00\\x00i\\x00\\x00c\\x00\\x00\\x00n\\x00C\\x00\\nThecoreobjectiveofPCAistoretainthemaximumvarianceoftheoriginaldata\\nwhilereducingthedimensionality.Therationaleisthatifweretainvariance,we\\nwillretainmaximuminformation.\\nButwhy?\\nManypeoplestruggletointuitivelyunderstandthemotivationforusing\\n“variance”here.Inotherwords:\\n\\x00h\\x00\\x00e\\x00\\x00i\\x00\\x00n\\x00\\x00a\\x00\\x00m\\x00\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00s\\x00n\\x00n\\x00\\x00\\x00a\\x00\\x00r\\x00o\\x00\\x00e\\x00\\x00i\\x00\\x00n\\x00\\x00a\\x00\\x00m\\x00\\x00\\n\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00n\\x00\\nThischapterprovidesanintuitiveexplanationofthis.\\nImaginesomeonegaveusthe\\nfollowingweightandheight\\ninformationaboutthree\\nindividuals:\\nIt’sclearthattheheightcolumnhasmorevariationthanweight.\\n295'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 296, 'page_label': '297', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,evenifwediscardtheweightcolumn,wecanstillidentifythesepeople\\nsolelybasedontheirheights.\\nDroppingtheweightcolumn\\n● TheoneinthemiddleisNick.\\n● Thele\\x00mostpersonisJonas.\\n● TherightmostoneisAndrew.\\nThatwassupersimple.Butwhatifwediscardedtheheightcolumninstead?\\nCanyouidentifythemnow?\\nNo,right?\\nAndwhy?\\nThisisbecausetheirheightshavemorevariationthantheirweights.\\n296'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 297, 'page_label': '298', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAndit’sclearfromtheaboveexamplethat,typically,ifacolumnhasmore\\nvariation,itholdsmoreinformation.\\nThatisthecoreideaPCAisbuiltaround,andthatiswhyittriestoretain\\nmaximumdatavariance.Simplyput,PCAisdevisedonthepremisethatmore\\nvariancemeansmoreinformation.\\nMorevariancemeansmoreinformationandlessvariancemeanslessinformation\\nThus,duringdimensionalityreduction,wecan(somewhat)saythatweare\\nretainingmaximuminformationifweretainmaximumvariance.\\nOfcourse,asweareusingvariance,thisalsomeansthatitcanbeeasily\\ninﬂuencedbyoutliers.ThatiswhywesaythatPCAisinﬂuencedbyoutliers.\\nAsaconcludingnote,alwaysrememberthatwhenusingPCA,wedon’tjust\\nmeasurecolumn-wisevarianceanddropthecolumnswiththeleastvariance.\\nInstead,wemustﬁrsttransformthedatatocreateuncorrelatedfeatures.A\\x00er\\nthat,wedropthenewfeaturesbasedontheirvariance.\\n297'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 298, 'page_label': '299', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00e\\x00\\x00C\\x00\\x00s\\x00\\x00C\\x00\\nDuringdimensionalityreduction,principalcomponentanalysis(PCA)triesto\\nﬁndalow-dimensionallinearsubspacethatthegivendataconformsto.\\nForinstance,considerthefollowingdummydataset:\\nIt’sprettyclearfromtheabovevisualthatthereisalinearsubspacealongwhich\\nthedatacouldberepresentedwhileretainingmaximumdatavariance.Thisis\\nshownbelow:\\nButwhatifourdataconformstoalow-dimensionalyetnon-linearsubspace.\\n298'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 299, 'page_label': '300', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,considerthefollowingdataset:\\nDoyouseealow-dimensionalnon-linearsubspacealongwhichourdatacouldbe\\nrepresented?\\nNo?\\nDon’tworry.Letmeshowyou!\\nTheabovecurveisacontinuousnon-linearandlow-dimensionalsubspacethat\\nwecouldrepresentourdatagivenalong.\\nOkay…sowhydon’twedoitthen?\\nTheproblemisthatPCAcannotdeterminethissubspacebecausethedatapoints\\narenon-alignedalongastraightline.\\n299'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 300, 'page_label': '301', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInotherwords,PCAisalineardimensionalityreductiontechnique.\\nThus,itfallsshortinsuchsituations.\\nNonetheless,ifweconsidertheabovenon-lineardata,don’tyouthinkthere’sstill\\nsomeintuitiontellingusthatthisdatasetcanbereducedtoonedimensionifwe\\ncancapturethisnon-linearcurve.\\nKernelPCA(whichusesthekerneltrick)preciselyaddressesthislimitationof\\nPCA.\\nTheideaisprettysimple:\\n● Projectthedatatoanotherhigh-dimensionalspaceusingakernel\\nfunction,wherethedatabecomeslinearlyrepresentable.Sklearnprovides\\naKernelPCAwrapper,supportingmanypopularlyusedkernelfunctions.\\n● ApplythestandardPCAalgorithmtothetransformeddata.\\nTheeﬃcacyofKernelPCAoverPCAisevidentfromthedemobelow.\\nAsshownbelow,eventhoughthedataisnon-linear,PCAstillproducesalinear\\nsubspaceforprojection:\\n300'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 301, 'page_label': '302', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHowever,KernelPCAproducesanon-linearsubspace:\\nWhat’sthecatch,youmightbewondering?\\nThecatchistheruntime.\\nPleasenotethattheruntimeofPCAisalreadycubicallyrelatedtothenumberof\\ndimensions.\\nKernelPCAinvolvesthekerneltrick,whichisquadraticallyrelatedtothenumber\\nofdatapoints(n).\\nThus,itincreasestheoverallruntime.\\nThisissomethingtobeawareofwhenusingKernelPCA.\\n301'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 302, 'page_label': '303', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00C\\x00\\x00s\\x00o\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00z\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00u\\x00\\nPCA,byitsverynature,isadimensionalityreductiontechnique.Yet,attimes,\\nmanyusePCAforvisualizinghigh-dimensionaldatasets.Thisisdoneby\\nprojectingthegivendataintotwodimensionsandvisualizingit.\\nWhilethismayappearlikeafairthingtodo,there’sabigproblemherethato\\x00en\\ngetsoverlooked.\\nTounderstandthisproblem,weﬁrstneedtounderstandabitabouthowPCA\\nworks.\\n\\x00o\\x00\\x00C\\x00\\x00o\\x00\\x00s\\x00\\nThecoreideainPCAistolinearlyprojectthedatatoanotherspaceusingthe\\neigenvectorsofthecovariancematrix.\\nWhyeigenvectors?\\n1. Itcreatesuncorrelatedfeatures,whichisusefulbecauseiffeaturesare\\nindependent,thefeatureswiththeleastvariancecanbedroppedfor\\ndimensionalityreduction.\\n2. Itensuresthatnewfeaturescollectivelypreservetheoriginaldatavariance.\\n302'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 303, 'page_label': '304', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nComingbacktothevisualizationtopic…\\nAsdiscussedabove,a\\x00erapplyingPCA,eachnewfeaturecapturesafractionof\\ntheoriginaldatavariance.Thus,ifweintendtousePCAforvisualizationby\\nprojectingthedatato2-dimensions…\\n…thenthisvisualizationwillonlybeusefuliftheﬁrsttwoprincipalcomponents\\ncollectivelycapturemostoftheoriginaldatavariance.\\nIftheydon’t,thenthetwo-dimensionalvisualizationwillbehighlymisleading\\nandincorrect.\\n\\x00o\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00n\\x00\\x00h\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00o\\x00\\x00r\\x00\\x00u\\x00\\x00o\\x00\\x00f\\x00h\\x00ﬁ\\x00s\\x00\\n\\x00w\\x00\\x00r\\x00\\x00c\\x00\\x00a\\x00\\x00o\\x00\\x00o\\x00\\x00n\\x00\\x00?\\nWecanavoidthismistakebyplottingacumulativeexplainedvariance(CEV)\\nplot.Asthenamesuggests,itplotsthecumulativevarianceexplainedby\\nprincipalcomponents.\\nInsklearn,forinstance,theexplainedvariancefractionisavailableinthe\\n\\x00x\\x00\\x00a\\x00\\x00e\\x00\\x00v\\x00\\x00i\\x00\\x00c\\x00\\x00r\\x00\\x00i\\x00\\x00attribute:\\n303'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 304, 'page_label': '305', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWecancreateacumulativeplotofexplainedvarianceandcheckwhethertheﬁrst\\ntwocomponentsexplainthemajorityofvariance.\\nForinstance,intheplotbelow,theﬁrsttwocomponentsonlyexplain55%ofthe\\noriginaldatavariance.\\nThus,visualizingthisdatasetin2DusingPCAmaynotbeagoodchoicebecause\\nplentyofdatavarianceismissing.\\nHowever,inthebelowplot,theﬁrsttwocomponentsexplain90%oftheoriginal\\ndatavariance.\\n304'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 305, 'page_label': '306', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThus,usingPCAforvisualizationlookslikeafairthingtodo.Asatakeaway,use\\nPCAfor2Dvisualizationonlywhentheaboveplotsuggestsso.\\nIfitdoesnot,thenrefrainfromusingPCAfor2Dvisualizationanduseother\\ntechniquesspeciﬁcallymeantforvisualization,liket-SNE,UMAP,etc.The\\neﬃcacyoft-SNEoverPCAisdepictedbelow:\\n2Dprojectionscreatedby\\nPCAdonotconsiderlocal\\nstructure.Instead,its\\nprincipalcomponents\\nprimarilyfocusonpreserving\\nthemaximumvariance.But\\nt-SNEprojectionsprovide\\nmuchmoreclarityintodata\\nclusters.Theclusters\\nproducedbyt-SNEarewell\\nseparated.ButthoseinPCA\\nhaveasigniﬁcantoverlap,and\\ntheyhardlyconveyanything.\\n305'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 306, 'page_label': '307', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\n\\x00-\\x00\\x00E\\x00s\\x00\\x00N\\x00—\\x00h\\x00\\x00'\\x00\\x00h\\x00\\x00iﬀ\\x00r\\x00\\x00c\\x00\\x00\\nInthischapter,let’scontinuediscussingthet-SNEalgorithm.\\nThet-SNEalgorithmisanimprovedversionoftheSNEalgorithm,bothusedfor\\ndimensionalityreduction.\\nThecoreideaofSNE(nott-SNE)isthefollowing:\\n● Step1)Foreverypoint(\\x00)inthegivenhigh-dimensionaldata,convertthe\\nhigh-dimensionalEuclideandistancestoallotherpoints(\\x00)into\\nconditionalGaussianprobabilities.\\n○ Forinstance,considerthemarkedredpointinthedatasetonthele\\x00\\nbelow.\\n○ ConvertingEuclideandistancetoallotherpointsintoGaussian\\nprobabilities(thedistributionontherightabove)showsthatother\\nredpointshaveahigherprobabilityofbeingitsneighborthanother\\npoints.\\n306\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 307, 'page_label': '308', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Step2)Foreverydatapointxᵢ,randomlyinitializeitscounterpartyᵢin\\n2-dimensionalspace.Thesewillbeourprojections.\\n● Step3)Justlikewedeﬁnedconditionalprobabilitiesinthe\\nhigh-dimensionalspaceinStep1,wedeﬁnetheconditionalprobabilities\\ninthelow-dimensionalspace,usingGaussiandistributionagain.\\n● Step4)Now,everydatapoint(i)hasahigh-dimensionalprobability\\ndistributionandacorrespondinglow-dimensionaldistribution:\\n○ Theobjectiveistomatchthesetwoprobabilitydistributions.Thus,\\nwecanmakethepositionsofcounterpartyᵢ’slearnablesuchthatthis\\ndiﬀerenceisminimized.\\n○ UsingKLdivergenceasalossfunctionhelpsusachievethis.It\\nmeasureshowmuchinformationislostwhenweusedistributionQ\\ntoapproximatedistributionP.\\n307'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 308, 'page_label': '309', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n○ Ideally,wewanttohavetheminimumlossvalue(whichiszero),and\\nthiswillbeachievedwhenP=Q.\\nThemodelcanbetrainedusinggradientdescent,anditworksprettywell.\\nForinstance,thefollowingimagedepictsa2-dimensionalvisualizationproduced\\nbytheSNEalgorithmon256-dimensionalhandwrittendigits:\\nSNEproducesgoodclusters.\\nWhat’sevenmoreastonishingisthatpropertieslikeorientation,skew,and\\nstrokethicknessvarysmoothlyacrossthespacewithineachcluster.Thisis\\ndepictedbelow:\\n308'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 309, 'page_label': '310', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNonetheless,ithassomelimitations,whichthet-SNEalgorithmaddresses.\\nNoticethattheclustersproducedbySNEarenotwellseparated.\\nHere,itcouldbefairtoassumethattheoriginaldataclusters,theoneinthe\\n256-dimensionalspace,mostlikelywouldhavebeenwellseparated.Thus:\\n● Allzerosmusthavebeentogetherbutwellseparatedfromotherdigits.\\n● Allonesmusthavebeentogetherbutwellseparatedfromotherdigits.\\n● Andsoon.\\n309'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 310, 'page_label': '311', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nYet,SNEstillproducestightlypackedclusters.Thisisalsocalledthe“crowding\\nproblem.”\\nToeliminatethisproblem,t-SNEwasproposed,standingfort-distributed\\nStochasticNeighborEmbedding(t-SNE).\\nHere’sthediﬀerence.\\nRecallthatinSNE,weusedaGaussiandistributiontodeﬁnethe\\nlow-dimensionalconditionalprobabilities.Butitisnotproducingwell-separated\\nclusters.\\nOnesolutionistousesomeotherprobabilitydistribution,suchthatfordistant\\npoints,wegetthesamevalueoftheconditionalprobabilityaswewouldhave\\nobtainedfromaGaussiandistributionbutatalargerEuclideandistance.\\nLetmesimplifythatabit.Comparethefollowingtwodistributions:\\nNoticethatGaussianachievesaspeciﬁcvalueoflowprobabilitydensityata\\nsmallerdistance.Butt-distributionachievesitatalargerdistance.\\nThisispreciselywhatweintendtoachieve.\\nWeneedaheaviertaildistributionsothatwecanstillminimizethediﬀerence\\nbetweenthetwoprobabilitydistributionsbutatalargerdistanceinthe\\nlow-dimensionalspace.\\n310'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 311, 'page_label': '312', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheStudentt-distributionisaperfectﬁtforit.Thefollowingimagedepictsthe\\ndiﬀerencethischangebrings:\\nAsshownabove:\\n● SNEproducescloselypackedclusters.\\n● t-SNEproduceswell-separatedclusters.\\nAndthat’swhyt-distributionisusedint-SNE.\\nThatsaid,besidesproducingwell-separatedclusters,usingtheStudent\\nt-distributionhasmanymoreadvantages.\\nForinstance,itiscomputationallymuchfastertoevaluatethedensityofapoint\\nunderaStudentt-distributionthanunderaGaussian.\\n311'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 312, 'page_label': '313', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00o\\x00v\\x00\\x00d\\x00e\\x00\\x00i\\x00\\x00\\x00i\\x00\\x00e\\x00\\x00y\\x00-\\x00\\x00E\\x00r\\x00\\x00e\\x00\\x00i\\x00\\x00s\\x00\\nFromtheabovediscussion,t-SNEsoundspromising,doesn’tit?Whilethe\\nalgorithmisquitepowerful,manyconsistentlymakemisleadingconclusions\\nfromthet-SNEprojectionsoftheirhigh-dimensionaldata.\\nInthischapter,Iwanttopointoutafewofthesemistakessothatyoudon’tmake\\nthosemistakesever.\\nTobegin,theperformanceofthet-SNEalgorithmisprimarilyrelianton\\n\\x00e\\x00\\x00\\x00e\\x00\\x00t\\x00—ahyperparameteroft-SNE.Thatiswhyitisconsideredthemost\\nimportanthyperparameterinthet-SNEalgorithm.\\nSimplyput,theperplexityvalueprovidesaroughestimateforthenumberof\\nneighborsapointmayhaveinacluster.Anddiﬀerentvaluesofperplexitycreate\\nverydiﬀerentlow-dimensionalclusterspaces,asdepictedbelow:\\n312'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 313, 'page_label': '314', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsshownabove,mostprojectionsdodepicttheoriginalclusters.However,they\\nvarysigniﬁcantlyinshape.\\nThereareﬁvetakeawaysfromtheaboveimage:\\n● NEVERmakeanyconclusionsabouttheoriginalclustershapebylooking\\nattheseprojections.\\n○ Diﬀerentprojectionshavediﬀerentlow-dimensionalclustershapes,\\nandtheydonotresembletheoriginalclustershape.\\n○ Forlowperplexityvalues(5and10),clustershapessigniﬁcantly\\ndiﬀerfromtheoriginalones.\\n○ Although,inthiscase,theclusterswerecolor-coded,whichprovided\\nmoreclarity.Butitmaynotalwaysbethecase,astSNEisan\\nunsupervisedalgorithm.\\n● Clustersizesinat-SNEplotdonotconveyanythingeither.\\n● Thedimensions(orcoordinatesofdatapoints)createdbyt-SNEinlow\\ndimensionshavenoinherentmeaning.\\n○ Theaxesticklabelsofthelow-dimensionalplotsarediﬀerentand\\nsomewhatrandom.\\n○ SimilartoPCA’sprincipalcomponents,theyoﬀerno\\ninterpretability.\\n● Thedistancesbetweenclustersinaprojectiondonotmeananything.\\n○ Intheoriginaldataset,theblueandredclustersareclose.\\n○ Yet,mostprojectionsdonotpreservetheglobalstructureofthe\\noriginaldataset.\\n● Strangethingshappenat\\x00e\\x00\\x00\\x00e\\x00\\x00t\\x00\\x002and\\x00e\\x00\\x00\\x00e\\x00\\x00t\\x00\\x001\\x00\\x00.\\n○ At\\x00e\\x00\\x00\\x00e\\x00\\x00t\\x00\\x002,thelow-dimensionalmappingconveysnothing.\\n■ Asdiscussedearlier,theperplexityvalueprovidesarough\\nestimateofthenumberofneighborsapointmayhaveina\\ncluster.\\n■ t-SNEtriestomaintainapprox.2pointspercluster.Thatis\\nwhythedistortion.\\n313'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 314, 'page_label': '315', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n○ At\\x00e\\x00\\x00\\x00e\\x00\\x00t\\x00\\x001\\x00\\x00,theglobalstructureispreserved,butthelocal\\nstructuregetsdistorted.\\n○ Thus,tweakingtheperplexityhyperparameterisextremelycritical\\nhere.\\n○ ThatiswhyImentionedabovethatitisthemostimportant\\nhyperparameterofthisalgorithm.\\nAsaconcludingnote,itisfoundthattheidealperplexityvaluestypicallyliein\\ntherange\\x005\\x00\\x000\\x00.Sotryexperimentinginthatrangeandseewhatlooks\\npromising.\\nNexttimeyouuset-SNE,considertheabovepoints,astheseplotscangettricky\\ntointerpret.Thisisespeciallytrueifyoudon’tunderstandtheinternalworkings\\nofthisalgorithm.\\nNonetheless,understandingthealgorithmwillmassivelyhelpyoudevelopan\\nintuitiononitsinterpretability.\\n314'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 315, 'page_label': '316', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00c\\x00\\x00l\\x00\\x00a\\x00\\x00\\x00S\\x00\\x00\\x00i\\x00\\x00\\x00P\\x00\\nSklearnimplementationsare\\ndrivenbyNumPy,whichtypically\\nrunonasinglecoreofaCPU.\\nThus,theabilitytorun\\nparallelizedoperationsisquite\\nlimitedattimes.\\nAnothermajorlimitationisthatscikit-learnmodelscannotrunonGPUs.\\nThisbottleneckprovidesmassiveroomforrun-timeimprovement.Thesame\\nappliestothetSNEalgorithm,whichisamongthemostpowerfuldimensionality\\nreductiontechniquestovisualizehigh-dimensionaldatasets.\\n315'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 316, 'page_label': '317', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nBecausethebiggestissuewithtSNEis\\nthatitsrun-timeisquadratically\\nrelatedtothenumberofdatapoints.\\nThus,beyond,say,20k-25kdatapoints,itbecomesprettydiﬃculttousetSNE\\nfromSklearnimplementations.Therearetwowaystohandlethis:\\n● Eitherkeepwaiting.\\n● Oruseoptimizedimplementationsthatcouldbepossiblyacceleratedwith\\nGPU.\\nIwasstuckduetothesameissueinoneofmyprojects,andIfoundapretty\\nhandysolution.\\ntSNE-CUDAisanoptimizedCUDAversionofthetSNEalgorithm,which,asthe\\nnamesuggests,canleveragehardwareaccelerators.Asaresult,itprovides\\nimmensespeedupsoverthestandardSklearnimplementation:\\n316'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 317, 'page_label': '318', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove,theGPU-acceleratedimplementation:\\n● Is33timesfasterthantheSklearnimplementation.\\n● ProducessimilarqualityclusteringastheSklearnimplementation.\\nDonotethatthisimplementationonlysupports\\x00_\\x00\\x00m\\x00\\x00\\x00e\\x00\\x00s\\x00\\x00,i.e.,youcan\\nonlyprojecttotwodimensions.Asperthedocs,theauthorshavenoplansto\\nsupportmoredimensions,asthiswillrequiresigniﬁcantchangestothecode.\\nButinmyopinion,thatdoesn’tmatterbecause,formorethan99%ofcases,tSNE\\nisusedtoobtain2Dprojections.Sowearegoodhere.\\nBeforeIconcludethischapter,Ialsofoundthefollowingbenchmarkingresults\\nbytheauthors:\\nItdepictsthatontheCIFAR-10trainingset(50kimages),tSNE-CUDAis700x\\nFasterthanSklearn,whichisaninsanespeedup.\\nHere’smyColabnotebookforyoutogetstarted:https://bit.ly/3WdEone.\\n317'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 318, 'page_label': '319', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00c\\x00\\x00e\\x00S\\x00\\x00\\x00o\\x00i\\x00\\x00i\\x00\\x00s\\x00f\\x00a\\x00\\x00\\x00o\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00p\\x00\\x00T\\x00\\x00E\\ntSNE-CUDAdiscussedaboveprovidesimmensespeedupsoverthestandard\\nSklearnimplementationusingaGPU.Butwhatifyoudon’thaveaccesstoa\\nGPU?\\nopenTSNEisanotheroptimizedPythonimplementationoft-SNE,which\\nprovidesmassivespeedimprovementsandenablesustoscalet-SNEtomillions\\nofdatapoints—aplacewhereSklearnimplementationmayneverreach.\\nAsdepictedabove,theopenTSNEimplementation:\\n● is20timesfasterthantheSklearnimplementation.\\n● producessimilarqualityclusteringastheSklearnimplementation.\\n318'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 319, 'page_label': '320', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheauthorshavealsoprovidedthefollowingbenchmarkingresults:\\nAsdepictedabove,openTSNEcanproducelowdimensionalvisualizationofa\\nmilliondatapointsinjust~15minutes.\\nHowever,itisclearfromtheirbenchmarksthattherun-timeoftheSklearn\\nimplementationhasalreadyreachedacoupleofhourswithjust~250kdata\\npoints.\\nDownloadthenotebookheretotryoutopenTSNE:https://bit.ly/3zAHCZ8.\\n319'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 320, 'page_label': '321', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00C\\x00\\x00s\\x00\\x00-\\x00\\x00E\\nFinally,itwouldbegoodtolookatthecorediﬀerencesbetweenPCAandt-SNE\\nalgorithms.Thefollowingtableforyouwhichneatlysummarizesthemajor\\ndiﬀerencesbetweenthetwoalgorithms:\\n\\x001\\x00\\x00u\\x00\\x00o\\x00\\x00\\n● WhilemanyinterpretPCA\\nasadatavisualization\\nalgorithm,itisprimarilya\\ndimensionalityreduction\\nalgorithm.\\n● t-SNE,however,isadata\\nvisualizationalgorithm.We\\nuseittoproject\\nhigh-dimensionaldatato\\nlowdimensions(primarily\\n2D).\\n320'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 321, 'page_label': '322', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x002\\x00\\x00y\\x00\\x00\\x00f\\x00l\\x00\\x00r\\x00\\x00h\\x00\\n● PCAisadeterministicalgorithm.Thus,ifwerunthealgorithmtwiceon\\nthesamedataset,wewillALWAYSgetthesameresult.\\n● t-SNE,however,isastochasticalgorithm.Thus,rerunningthealgorithm\\ncanleadtoentirelydiﬀerentresults.\\n\\x003\\x00\\x00n\\x00\\x00u\\x00\\x00e\\x00\\x00\\x00f\\x00o\\x00\\x00t\\x00\\x00n\\nAsfarasuniquenessandinterpretationofresultsisconcerned…\\n● PCAalwayshasauniquesolutionfortheprojectionofdatapoints.Simply\\nput,PCAisjustarotationofaxessuchthatthenewfeatureswegetare\\nuncorrelated.\\n● t-SNE,asdiscussedabove,canprovideentirelydiﬀerentresults,andits\\ninterpretationissubjectiveinnature.\\n\\x004\\x00\\x00r\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00y\\x00\\x00\\n● PCAisalineardimensionalityreductionapproach.Itcanonlyﬁndalinear\\nsubspacetoprojectthegivendataset.KernelPCAaddressesthis,whichwe\\ncovereearlierinthissection.\\n● t-SNEisanon-linearapproach.Itcanhandlenon-lineardatasets.\\n321'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 322, 'page_label': '323', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x005\\x00\\x00r\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00h\\n● PCAonlyaimstoretaintheglobalvarianceofthedata.Thus,local\\nrelationships(suchasclusters)areo\\x00enlosta\\x00erprojection,asshown\\nbelow:\\n● t-SNEpreserveslocalrelationships.Thus,datapointsinaclusterinthe\\nhigh-dimensionalspacearemuchmorelikelytolietogetherinthe\\nlow-dimensionalspace.\\n○ Int-SNE,wedonotexplicitlyspecifyglobalstructurepreservation.\\nButittypicallydoescreatewell-separatedclusters.\\n○ Nonetheless,asdiscussedinthepreviouschapter,thedistance\\nbetweentwoclustersinlow-dimensionalspaceisNEVERan\\nindicatorofclusterseparationinhigh-dimensionalspace.\\n322'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 323, 'page_label': '324', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00l\\x00\\x00t\\x00\\x00i\\x00\\x00\\n\\x00e\\x00\\x00n\\x00 \\x00M\\x00\\x00n\\x00\\x00 \\x00\\x00u\\x00\\x00-\\x00\\x00o\\x00\\x00y\\x00\\x00s \\x00f \\x00l\\x00\\x00t\\x00\\x00i\\x00\\x00\\n\\x00l\\x00\\x00r\\x00\\x00h\\x00\\x00\\nClusteringisoneofthecorebranchesofunsupervisedlearninginML.Theﬁrst\\n(andsometimestheonly)clusteringalgorithmfolkslearnisKMeans.Yet,itis\\nimportanttonotethatKMeansisnotauniversalsolutiontoallclustering\\nproblems.Infact,there’sawholeworldofclusteringalgorithmsbeyondKMeans,\\nwhichwemustbefamiliarwith.Thevisualbelowsummarizes6diﬀerenttypesof\\nclusteringalgorithms:\\nCentroid-based:Clusterdata\\nbasedonproximitytocentroids.\\nConnectivity-based:Cluster\\npointsbasedonproximity\\nbetweenclusters.\\nDensity-based:Clusterpoints\\nbasedontheirdensity.Itismore\\nrobusttoclusterswithvarying\\ndensitiesandshapesthan\\ncentroid-basedclustering.\\nGraph-based:Clusterpoints\\nbasedongraphdistance.\\nDistribution-based:Clusterpointsbasedontheirlikelihoodofbelongingtothe\\nsamedistribution.GaussianMixtureModelinoneexample.Wewilldiscuss\\nGaussianmixturemodels(GMMs)inanupcomingchapter.\\nCompression-based:Transformdatatoalowerdimensionalspaceandthen\\nperformclustering\\n323'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 324, 'page_label': '325', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00i\\x00\\x00i\\x00\\x00e\\x00\\x00u\\x00\\x00s\\x00o\\x00\\x00l\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00v\\x00\\x00u\\x00\\x00i\\x00\\x00\\nWithoutlabeleddata,itisnotpossibletoobjectivelymeasurehowwellthe\\nclusteringalgorithmhasgroupedsimilardatapointstogetherandseparated\\ndissimilarones.\\nAlso,mostclusteringdatasetsaremulti-dimensional,sodirectlyvisualizingthe\\nresultsisnotpossible.\\nThus,wemustrelyonintrinsicmeasurestodetermineclusteringqualityinsuch\\ncases.\\nThewayIliketousethemisasfollows:\\n● SayIamusingKMeans:\\n○ RunKMeanswitharangeofkvalues.\\n○ Evaluatetheperformance.\\n○ Selectthevalueofkbasedonthemostpromisingclusterquality\\nmetric.\\nLet’sunderstandafewofthesemetrics.\\n324'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 325, 'page_label': '326', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00i\\x00\\x00o\\x00\\x00t\\x00\\x00\\x00o\\x00ﬃ\\x00i\\x00\\x00t\\x00\\nTheSilhouetteCoeﬃcientindicateshowwelleachdatapointﬁtsintoits\\nassignedcluster.\\nTheideaisthatiftheaveragedistancetoalldatapointsinthesameclusteris\\nsmallbutthattoanotherclusterislarge,thisintuitivelyindicatesthatthe\\nclustersarewellseparatedandsomewhatreliable.\\n325'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 326, 'page_label': '327', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItismeasuredasfollows:\\n● Foreverydatapoint:\\n○ ﬁndtheaveragedistancetoallotherpointswithinitscluster→A\\n○ ﬁndtheaveragedistancetoallpointsinthenearestcluster→B\\n○ point’sscore=(B-A)/max(B,A)\\n● Computetheaverageofallindividualscorestogettheoverallclustering\\nscore.\\nNowconsiderthis:\\n● If,foralldatapoints,BismuchgreaterthanA,thiswouldmeanthat:\\n○ Theaveragedistancetopointsintheclosestclusterislarge.\\n○ Theaveragedistancetopointsinthesameclusterissmall.\\n○ Thus,theoverallscorewillbecloseto1,indicatingthattheclusters\\narewellseparated.\\nSo,ahigherSilhouetteCoeﬃcientwouldmeanthat,generallyspeaking,alldata\\npointsﬁtintotheiridealclusters.\\nMeasuringitacrossarangeofcentroids(k)canrevealwhichclusteringresults\\naremostpromising:\\n326'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 327, 'page_label': '328', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x002\\x00\\x00a\\x00\\x00n\\x00\\x00i\\x00\\x00a\\x00\\x00b\\x00\\x00z\\x00n\\x00\\x00x\\nTheissuewiththeSilhouettescoreisthatitsrun-timegrowsquadraticallywith\\nthenumberofdatapoints.\\nCalinski-HarabaszIndexisanothermetricthatismeasuredquitesimilarlytothe\\nSilhouetteCoeﬃcient.\\nHere’swhatitcomputes:\\n● A→sumofsquareddistancebetweenallcentroidsandoveralldataset\\ncenter.\\n● B→sumofsquareddistancebetweenallpointsandtheirspeciﬁccentroid.\\n● metriciscomputedasA/B(withanadditionalscalingfactor).\\nYetagain,isAismuchgreaterthanB:\\n● Thedistanceofcentroidstothedatasetcenterislarge.\\n● Thedistanceofdatapointstotheirspeciﬁccentroidissmall\\n● Thus,itwillresultinahigherscore,indicatingthattheclustersarewell\\nseparated.\\n327'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 328, 'page_label': '329', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThereasonIprefertheCalinski-HarabaszIndexovertheSilhouetteCoeﬃcient\\nisthat:\\n● ItisrelativelymuchfastertocomputethantheSilhouetteCoeﬃcient.\\n● ItmakesthesameintuitivesenseforinterpretationastheSilhouette\\nCoeﬃcient.\\n\\x003\\x00\\x00B\\x00\\x00\\nTheissuewiththeSilhouettescoreandCalinski-Harabaszindexisthattheyare\\ntypicallyhigherforconvex(orsomewhatspherical)clusters.\\nHowever,usingthemtoevaluatearbitrary-shapedclustering,likethoseobtained\\nwithdensity-basedclustering,canproducemisleadingresults.Thisisevident\\nfromtheimagebelow:\\n328'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 329, 'page_label': '330', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove,whiletheclusteringoutputofKMeansisworse,the\\nSilhouettescoreisstillhigherthanDensity-basedclustering.\\nDBCV—density-basedclusteringvalidationisabettermetricinsuchcases.\\nAsthenamesuggests,itisspeciﬁcallymeanttoevaluatedensity-based\\nclustering.Simplyput,DBCVcomputestwovalues:\\n● Thedensitywithinacluster\\n● Thedensityoverlapbetweenclusters\\nAhighdensitywithinaclusterandalowdensityoverlapbetweenclusters\\nindicategoodclusteringresults.TheeﬀectivenessofDBCVisalsoevidentfrom\\ntheimagebelow:\\nWithDBCV,thescorefortheclusteringoutputofKMeansisworse,andthatof\\ndensity-basedclusteringishigher.\\n329'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 330, 'page_label': '331', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00t\\x00\\x00n\\x00\\x00M\\x00\\x00n\\x00\\x00s\\x00M\\x00\\x00n\\x00\\nKMeansiswidelyusedforitssimplicityandeﬀectivenessasaclustering\\nalgorithm.Yet,weallknowthatitsperformanceisentirelydependentonthe\\ncentroidinitializationstep.Thus,itislikelythatwemayobtaininaccurate\\nclusters,asdepictedbelow:\\nOfcourse,rerunningwithdiﬀerentinitializationdoeshelpattimes.ButIhave\\nneverlikedtheunnecessaryrun-timeoverheaditintroduces.\\nInthischapter,let’slearnasuperchargedupgradetoKMeans,whichaddresses\\nthisissuewhilealsoproducingbetterclusteringresults.It’scalledtheBreathing\\nKMeansalgorithm.\\n\\x00t\\x00\\x00\\x00:\\x00u\\x00\\x00m\\x00\\x00n\\x00\\nFirst,weruntheusualKMeansclusteringonlyonce,i.e.,withoutrerunningthe\\nalgorithmwithadiﬀerentinitialization.\\n330'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 331, 'page_label': '332', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nClusteringresultsofKMeansa\\x00errunningthealgorithmwithoutrepetitionand\\nuntilconvergence\\nThisgivesusthelocationof“k”centroids,whichmayormaynotbeaccurate.\\n\\x00t\\x00\\x00\\x00:\\x00r\\x00\\x00t\\x00\\x00\\x00n\\x00t\\x00\\x00\\nTothe“k”centroidsobtainedfromStep1,weadd“m”newcentroids.\\nAspertheresearchpaperofBreathingKmeans,m=5wasfoundtobegoodvalue.\\nNow,youmightbethinking,wheredoweaddthese“m”centroids?\\nTheadditionofnewcentroidsisdecidedbasedontheerrorassociatedwitha\\ncentroid.Simplyput,acentroid’serroristhesumofthesquareddistancetothe\\npointsassociatedwiththatcentroid.\\nThus,weadd“m”centroidsinthevicinityofcentroidswithhigherror.Let’s\\nunderstandmoreintuitivelywhythismakessense.\\n331'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 332, 'page_label': '333', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIntheaboveKMeansclusteringresults:\\n● Thecentroidatthetophasahigherror.\\n● Allothercentroidshaverelativelylowerror.\\nIntuitivelyspeaking,ifacentroidhasaveryhigherror,itispossiblethatmultiple\\nclustersareassociatedwithit.Thus,wewouldwanttosplitthiscluster.Adding\\nnewcentroidsnearclusterswithhigherrorwillpreciselyfulﬁllthisobjective.\\nA\\x00eradding“m”newcentroids,wegetatotalof“k+m”centroids.\\nFinally,werunKMeansagainwith“k+m”centroidsonlyonce.Thisgivesusthe\\nlocationof“k+m”centroids.\\n\\x00t\\x00\\x00\\x00:\\x00r\\x00\\x00t\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00\\nNext,wewanttoremove“m”centroidsfromthe“k+m”centroidsobtainedabove.\\nHere,youmightbethinking,which“m”centroidsshouldweremove?The\\nremovalofcentroidsisdecidedbasedonthe“utility”ofacentroid.\\nSimplyput,acentroid’sutilityisproportionaltoitsdistancefromallother\\ncentroids.Thegreaterthedistance,themoreisolateditwillbe.Hence,themore\\ntheutility.\\nThismakesintuitivesenseaswell.Iftwocentroidsareprettyclose,theywill\\nhavelowutility.\\nThus,theyarelikelyinthesamecluster,andwemustremoveoneofthem.\\n332'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 333, 'page_label': '334', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisdemonstratedbelow:\\nA\\x00erremovingoneofthelow-utilitycentroids,theothercentroidbecomesvery\\nuseful.So,inpractice,a\\x00erremovingonecentroid,weupdatetheutilityvaluesof\\nallothercentroids.\\nWerepeattheprocessuntilall“m”low-utilitycentroidshavebeenremoved.This\\ngivesback“k”centroids.\\nFinally,werunKMeanswiththese“k”centroidsonlyonce.\\n\\x00t\\x00\\x00\\x00:\\x00e\\x00\\x00e\\x00\\x00e\\x00\\x00y\\x00.\\n\\x00t\\x00\\x00\\x00:\\x00e\\x00\\x00a\\x00\\x00t\\x00\\x00s\\x00\\x00o\\x00\\x00n\\x00\\x00l\\x00=\\x00\\x00\\nDone!\\nTheserepeatedbreathingcycles(breathe-inandbreathe-outsteps)almostalways\\nprovideafasterandbettersolutionthanstandardKMeanswithrepetitions.\\nIneachcycle:\\n● Newcentroidsareaddedat“good”locations.Thishelpsinsplitting\\nclustersoccupiedbyasinglecentroid.\\n● Low-utilitycentroidsareremoved.Thishelpsineliminatingcentroidsthat\\narelikelyinthesamecluster.\\n333'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 334, 'page_label': '335', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='Python\\nPython\\nDailyDoseofDS.com\\nAsaresult,itisexpectedtoconvergetotheoptimalsolutionfaster.The\\neﬀectivenessofBreathingKMeansoverKMeansisevidentfromtheimage\\nbelow:\\n● KMeansproducedtwomisplacedcentroids\\n● BreathingKMeansaccuratelyclusteredthedatawitha50%run-time\\nimprovement.\\nThereisalsoanopen-sourceimplementationofBreathingKMeans,witha\\nsklearn-likeAPI.Togetstarted,installthebkmeanslibrary:\\npipinstallbkmeans\\nNext,runthealgorithmasfollows:\\nfrombkmeansimportBKMeans\\nbkm=BKMeans(n_clusters=100)\\nbkm.fit(X)\\n334'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 335, 'page_label': '336', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00o\\x00\\x00\\x00i\\x00\\x00B\\x00\\x00c\\x00\\x00M\\x00\\x00n\\x00\\x00o\\x00\\x00s\\x00\\nManyMLalgorithmsimplementedbySklearnsupportincrementallearning,i.e.,\\nmini-batch-basedtraining.Thismakesthemquiteusefulwhentheentiredata\\ndoesnotﬁtintomemory.\\nThesearethesupportedalgorithmsforyourreference:\\nIfyoulookclosely,there’sKMeansinthatlisttoo.Beingapopularclustering,it’s\\ngoodthatwecanrunitonlargedatasetsifneeded.\\nButwhenIﬁrstlearnedthatKMeanscanbeimplementedinamini-batch\\nfashion,IwonderedhowexactlyabatchimplementationofKMeansmightwork?\\nGivingitathoughtforaminuteorsogavemeanidea,whichIwishtosharewith\\nyouinthischapter.\\n335'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 336, 'page_label': '337', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00o\\x00\\x00c\\x00\\x00f\\x00v\\x00\\x00h\\x00\\x00d\\x00n\\x00M\\x00\\x00n\\x00\\nBeforeunderstandingthemini-batchimplementation,wemustknowtheissue\\nwiththeusualKMeansimplementation.\\nTorecall,KMeansworksasfollows:\\n● Step1)Initializecentroids.\\n● Step2)Findthenearestcentroidforeachpoint.\\n● Step3)Reassigncentroidsastheaverageofpointsassignedtothem.\\n● Step4)Repeatuntilconvergence.\\nInthisimplementation,theprimarymemorybottleneckoriginatesfromStep3.\\nThisisbecause,inthisstep,wecomputetheaverageofallpointsassignedtoa\\ncentroidtoshi\\x00thecentroid.\\nThus,traditionally,alldatapointsassignedtoacentroidmustbeavailablein\\nmemorytotakeanaverageover.\\n336'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 337, 'page_label': '338', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nButinourcase,alldatapointsDONOTﬁtintomemoryatonce.Thus,direct\\naveragingisn’tfeasible.Nonetheless,here’showwecansmartlyaddressthis\\nissue.\\n\\x00e\\x00\\x00r\\x00\\x00\\x00r\\x00\\x00e\\x00\\x00\\x00u\\x00\\x00h\\x00\\x00:\\n\\x00o\\x00\\x00s\\x00\\x00y\\x00p\\x00\\x00k\\x00\\x00g\\x00\\x00\\x00a\\x00\\x00n\\x00\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00h\\x00\\x00r\\x00\\x00i\\x00\\x00\\x00m\\x00\\x00e\\x00\\x00n\\x00\\x00t\\x00\\x00n\\x00f\\n\\x00i\\x00\\x00B\\x00\\x00c\\x00\\x00M\\x00\\x00n\\x00\\x00n\\x00k\\x00\\x00a\\x00\\x00.\\x00h\\x00\\x00o\\x00\\x00\\x00o\\x00\\x00t\\x00e\\x00\\x00\\x00u\\x00\\x00\\x00i\\x00\\x00e\\x00\\x00\\n\\x00h\\x00\\x00o\\x00\\x00t\\x00\\x00n\\x00\\x00m\\x00b\\x00\\x00t\\x00o\\x00h\\x00\\x00e\\x00s\\x00h\\x00\\x00n\\x00\\x00\\x00a\\x00\\x00\\x00p\\x00i\\x00\\x00\\x00h\\x00\\x00\\x00\\x00a\\x00\\n\\x00r\\x00\\x00n\\x00\\x00oﬁ\\x00u\\x00\\x00\\x00u\\x00\\x00o\\x00\\x00\\x00i\\x00\\x00-\\x00\\x00t\\x00\\x00\\x00M\\x00\\x00n\\x00\\x00a\\x00\\x00e\\x00o\\x00\\x00i\\x00\\x00y\\n\\x00m\\x00\\x00e\\x00\\x00n\\x00\\x00\\x00.\\n\\x00o\\x00f\\x00h\\x00\\x00e\\x00\\x00\\x00\\x00e\\x00\\x00e\\x00\\x00\\x00v\\x00\\x00l\\x00\\x00\\x00i\\x00\\x00\\x00k\\x00\\x00a\\x00\\x00,\\x00r\\x00\\x00t\\x00\\n\\x00f\\x00o\\x00\\x00\\x00t\\x00\\x00ﬁ\\x00e\\x00e\\x00\\x00u\\x00\\x00\\x00h\\x00\\x00o\\x00\\x00t\\x00\\x00n\\x00\\x00m\\x00b\\x00\\x00t\\x00o\\x00h\\x00\\x00e\\x00s\\x00e\\x00\\x00e\\x00\\x00l\\x00\\n\\x00i\\x00\\x00l\\x00\\x00o\\x00\\x00\\n\\x00i\\x00\\x00B\\x00\\x00c\\x00\\x00M\\x00\\x00n\\x00\\nWebeginthealgorithmbyselectingavalueofk—thenumberofcentroids.\\nNext,weneedaninitiallocationforthecentroidssowecansamplem(m<<n)\\nrandomdatapointstoinitializethecentroids.\\nNow,recallthebottleneckwediscussedabove—Alldatapointsmustbe\\navailableinmemorytoaverageover.\\nThus,toaddressthis,weneedawaytokeeptrackofthedatapointsassignedto\\neachcentroidasweloadandprocesseachmini-batch.\\n337'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 338, 'page_label': '339', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWecannotmaintainagrowinglistofdatapointsassignedtoeachcentroid\\nbecausethatwouldeventuallytakeupthememoryoftheentiredataset.\\nSohere’showwecansmartlyfulﬁllthisobjective.\\nWecanutilizethefollowingmathematicalpropertythatrelatessumandaverage:\\nHow?\\nSee,inatypicalKMeanssetting,weaverageoveralldatapointsassignedtoa\\ncentroidtodetermineitsnewlocation.Butasdiscussedabove,wecan’tkeepall\\ndatapointsinmemoryatonce.\\nSohowaboutwedothisinthemini-batchsettingofKMeans:\\nForeverycentroid:\\n● Wemaintaina“sum-vector”,whichstoresthesumofthevectorsofalldata\\npointsassignedtothatcentroid.Tobegin,thiswillbeazerovector.\\n● A“count”variable,whichtracksthenumberofdatapointsassignedtoa\\ncentroid.Tobegin,thecountwillbezero.\\n338'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 339, 'page_label': '340', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWhileiteratingthroughmini-batches:\\n● Every“sum-vector”ofacentroidwillcontinuetoaccumulatethesumofall\\ndatapointsthathavebeenassignedtoitsofar.\\n● Every“count”variableofacentroidwillcontinuetotrackthenumberof\\ndatapointsthathavebeenassignedtoitsofar.\\nA\\x00eriteratingthroughallmini-batches,wecanperformascalardivision\\nbetweensum-vectorandcounttogettheaveragevector:\\nThisaveragewillpreciselytellusthenewlocationofthecentroidasifalldata\\npointswerepresentinmemory.Wecanrepeatthisuntilconvergence.\\nAlso,beforestartingthenextiterationofMiniBatchKMeans,wemustresetthe\\nsum-vectortoazerovectorandcounttozero.Andthat’showamini-batch\\nversionofKMeanscanbeimplemented.\\n339'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 340, 'page_label': '341', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00N\\x00\\x00d\\x00\\x00v\\x00\\x00\\x00M\\x00\\x00n\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00s\\nAsdiscussedinthepreviouschapter,KMeansistrainedasfollows:\\n● Step1)Initializecentroids\\n● Step2)Findthenearestcentroidforeachpoint\\n● Step3)Reassigncentroids\\n● Step4)Repeatuntilconvergence\\nButinthisimplementation,“Step2”hasarun-timebottleneck,asthisstep\\ninvolvesabrute-forceandexhaustivesearch.Inotherwords,thisﬁndsthe\\ndistanceofeverydatapointfromeverycentroid.\\nAsaresult,thisstepisn’toptimized,andittakesplentyoftimetotrainand\\npredict.Thisisespeciallychallengingwithlargedatasets.\\nTospeedupKMeans,oneoftheimplementationsIusuallyprefer,especiallyon\\nlargedatasets,isFaissbyFacebookAIResearch.\\nToelaboratefurther,Faissprovidesamuchfasternearest-neighborsearchusing\\napproximatenearest-neighborsearchalgorithms.\\nItusesan“InvertedIndex,”whichisanoptimizeddatastructuretostoreand\\nindexthedatapoint.\\n340'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 341, 'page_label': '342', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThismakesperformingclusteringextremelyeﬃcient,especiallyonlarge\\ndatasets,whichisalsoevidentfromtheimagebelow:\\nAsshownabove,onadatasetof500kdatapoints(1024dimensions),Faissis\\nroughly20xfasterthanKMeansfromSklearn,whichisaninsanespeedup.\\nWhat’smore,FaisscanalsorunonaGPU,whichcanfurtherspeedupyour\\nclusteringrun-timeperformance.\\nGetstartedwithFaisshere:https://github.com/facebookresearch/faiss.\\n341'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 342, 'page_label': '343', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00M\\x00\\x00n\\x00\\x00s\\x00\\x00a\\x00\\x00s\\x00\\x00n\\x00i\\x00\\x00u\\x00\\x00\\x00o\\x00\\x00l\\x00\\nIliketothinkofGaussianMixtureModelsasamoregeneralizedversionof\\nKMeans,whichaddressessomeofthewidelyknownlimitationsofKMeans.\\n\\x00i\\x00\\x00t\\x00\\x00i\\x00\\x00s\\x00f\\x00M\\x00\\x00n\\x00\\nTobegin,KMeans\\ncanonlyproduce\\nglobularclusters.\\nForinstance,as\\nshownbelow,even\\nifthedatahas\\nnon-circular\\nclusters,itstill\\nproducesround\\nclusters.\\nMoreover,itperformsahardassignment.Therearenoprobabilisticestimatesof\\neachdatapointbelongingtoeachcluster.\\nLastly,itonlyreliesondistance-basedmeasurestoassigndatapointstoclusters.\\n342'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 343, 'page_label': '344', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTounderstandthispointbetter,considertwoclustersin2D—AandB.Cluster\\nAhasahigherspreadthanB.\\nNowconsideralinethatismid-waybetweencentroidsofAandB.\\nAlthoughAhasahigherspread,evenifapointisslightlyrighttothemidline,it\\nwillgetassignedtoclusterB.\\nIdeally,however,clusterAshouldhavehadalargerareaofinﬂuence.\\n343'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 344, 'page_label': '345', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00s\\x00\\x00n\\x00i\\x00\\x00u\\x00\\x00\\x00o\\x00\\x00l\\x00\\nTheselimitationso\\x00enmakeKMeansanon-idealchoiceforclustering.Gaussian\\nMixtureModelsareo\\x00enasuperioralgorithminthisrespect.Asthename\\nsuggests,theycanclusteradatasetthathasamixtureofmanyGaussian\\ndistributions.TheycanbethoughtofasamoreﬂexibletwinofKMeans.\\nTheprimarydiﬀerenceisthat,\\nKMeanslearnscentroidsbut\\nGaussianmixturemodelslearna\\ndistribution.Forinstance,in2\\ndimensions,KMeanscanonly\\ncreatecircularclustersbutGMM\\ncancreateoval-shapedclusters.\\nTheeﬀectivenessofGMMsoverKMeansisevidentfromtheimagebelow.\\n● KMeansjustreliesondistanceandignoresthedistributionofeachcluster.\\n● GMMlearnsthedistributionandproducesbetterclustering.\\n344'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 345, 'page_label': '346', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00B\\x00\\x00A\\x00\\x00+\\x00 \\x00 \\x00a\\x00\\x00e\\x00 \\x00n\\x00 \\x00c\\x00\\x00a\\x00\\x00\\x00 \\x00l\\x00\\x00r\\x00\\x00t\\x00\\x00e \\x00o\\n\\x00B\\x00\\x00A\\x00\\nNon-parametricunsupervisedalgorithmsarewidelyusedinindustrytoanalyze\\nlargedatasets.Thisisbecauseinmanypracticalapplications,gatheringtrue\\nlabelsisprettydiﬃcultorinfeasible.\\nInsuchcases:\\n● Eitherdatateamsannotatethedata,whichcanbepracticallyimpossibleat\\ntimes,\\n● Oruseunsupervisedmethodstoidentifypatterns.\\nWhileKMeansiswidelyusedhereduetoitssimplicityandeﬀectivenessasa\\nclusteringalgorithm,ithasmanylimitationsaswealsodiscussedintheprevious\\nchapter:\\n● Itdoesnotaccountforclustercovariance.\\n● Itcanonlyproducesphericalclusters.Asshownbelow,evenifthedatahas\\nnon-circularclusters,itstillproducesroundclusters.\\nDensity-basedalgorithms,likeDBSCAN,quiteeﬀectivelyaddressthese\\nlimitations.\\nThecoreideabehindDBSCANistogrouptogetherdatapointsbasedon\\n“density”,i.e.,pointsthatareclosetoeachotherinahigh-densityregionandare\\nseparatedbylower-densityregions.\\n345'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 346, 'page_label': '347', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nButunfortunately,withtheuseofDBSCAN,weagainrunintoanoverlooked\\nproblem.OneofthethingsthatmakesDBSCANinfeasibletouseattimesisits\\nrun-time.\\nUntilrecently,itwasbelievedthatDBSCANhadarun-timeofO(nlogn),butit\\nwasproventobeO(n²)intheworstcase.\\nInfact,thiscanalsobeveriﬁedfromtheﬁgurebelow.Itdepictstherun-timeof\\nDBSCANwiththenumberofsamples:\\n346'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 347, 'page_label': '348', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItisclearthatDBSCANhasaquadraticrelationwiththedataset’ssize.\\nThus,thereisanincreasingneedtoestablishmoreeﬃcientversionsof\\nDBSCAN.\\nDBSCAN++isamajorsteptowardsafastandscalableDBSCAN.\\nSimplyput,\\nDBSCAN++is\\nbasedonthe\\nobservationthat\\nweonlyneedto\\ncomputethe\\ndensityestimates\\nforasubset“m”of\\nthe“n”datapoints\\ninthegiven\\ndataset,where“m”\\ncanbemuch\\nsmallerthan“n”to\\nclusterproperly.\\nTheeﬀectiveness\\nofDBSCAN++is\\nevidentfromthis\\nimage.\\nAsdepictedabove,onadatasetof60kdatapoints:\\n● DBSCAN++is20xfasterthanDBSCAN.\\n● DBSCAN++producesbetterclusteringscoresthanDBSCAN.\\n347'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 348, 'page_label': '349', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00D\\x00\\x00C\\x00\\x00\\x00s\\x00\\x00B\\x00\\x00A\\x00\\nInthischapter,let’sdiveintoHDBSCANandunderstandhowitdiﬀersfrom\\nDBSCAN.\\nLikeanyotheralgorithm,DBSCANalsohassomelimitations.Tobegin,\\nDBSCANassumesthatthelocaldensityofdatapointsis(somewhat)globally\\nuniform.Thisisgovernedbyitsepsparameter.\\nThus,itmaystruggletoidentifyclusterswithvaryingdensities.Thismayneed\\nseveralhyperparametertuningattemptstogetpromisingresults.\\nHDBSCANcanbeabetterchoicefordensity-basedclustering.Itrelaxesthe\\nassumptionoflocaluniformdensity,whichmakesitmorerobusttoclustersof\\nvaryingdensitiesbyexploringmanydiﬀerentdensityscales.\\nForinstance,considertheclusteringresultsobtainedwithDBSCANonthe\\ndummydatasetbelow,whereeachclusterhasdiﬀerentdensities:\\nItisclearthat\\nDBSCAN\\nproducesbad\\nclustering\\nresults.\\n348'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 349, 'page_label': '350', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNowcompareitwithHDBSCANresultsdepictedbelow:\\nOnadatasetwith\\nthreeclusters,each\\nwithvarying\\ndensities,\\nHDBSCANisfound\\ntobemorerobust.\\nThere’sonemorethingIloveaboutHDBSCAN:\\n● DBSCANisascalevariantalgorithm.Thus,clusteringresultsfordataX,\\n2X,3X,etc.,canbeentirelydiﬀerent.\\n● Ontheotherhand,HDBSCANisscale-invariant.So,clusteringresults\\nremainthesameacrossdiﬀerentscalesofdata.\\nWehaveDBSCAN\\nonthele\\x00,andwe\\ncanseethatthe\\nresultsvarywith\\nthescaleofthe\\ndata.\\nHowever,\\nclusteringfrom\\nHDBSCAN(onthe\\nright)remains\\nunalteredwiththe\\nscalefor\\nHDBSCAN.\\n349'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 350, 'page_label': '351', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00n\\x00\\x00y\\x00\\x00s\\n\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00=\\x00r\\x00\\x00i\\x00\\x00i\\x00\\x00n\\x00\\x00s\\nCorrelationmeasureshowtwofeaturesvarywithoneanotherlinearly(or\\nmonotonically).\\nThismakescorrelationsymmetric:corr(A,B)=corr(B,A).\\nYet,associationsareo\\x00enasymmetric.Forinstance,givenadate,itiseasytotell\\nthemonth.Butgivenamonth,youcannevertellthedate.\\nCorrelation,beingsymmetric,entirelyignoresthisnotion.What’smore,itisnot\\nmeanttoquantifyhowwellafeaturecanpredicttheoutcome,asdemonstrated\\nbelow:\\n350'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 351, 'page_label': '352', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nYet,attimes,itismisinterpretedasameasureof“predictiveness”.Lastly,\\ncorrelationismostlylimitedtonumericaldata.Butcategoricaldataisequally\\nimportantforpredictivemodels.\\nThePredictivePowerScore(PPS)addresseseachoftheselimitations.Asthe\\nnamesuggests,itmeasuresthepredictivepowerofafeature.\\n\\x00P\\x00\\x00a→\\x00)iscalculatedasfollows:\\n● Ifthetarget(b)isnumeric:\\n○ TrainaDecisionTreeRegressorthatpredicts\\x00using\\x00.\\n○ FindPPSbycomparingitsMAEtotheMAEofabaselinemodel\\n(medianprediction).\\n● Ifthetarget(b)iscategorical:\\n○ TrainaDecisionTreeClassiﬁerthatpredictsbusinga.\\n○ FindPPSbycomparingitsF1totheF1ofabaselinemodel(random\\normostfrequentprediction).\\nThus,PPS:\\n● isasymmetric,meaningPPS(a,b)!=PPS(b,a).\\n● canbeusedoncategoricaltargets(b).\\n351'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 352, 'page_label': '353', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● canbeusedtomeasurethepredictivepowerofcategoricalfeatures(a).\\n● workswellforlinearandnon-linearrelationships.\\n● workswellformonotonicandnon-monotonicrelationships.\\nItseﬀectivenessisevidentfromtheimagebelow.Forallthreedatasets:\\n● Correlationislow.\\n● PPS(x→y)ishigh.\\n● PPS(y→x)iszero.\\nThatbeingsaid,itisimportanttonotethatcorrelationhasitsplace.\\nWhenselectingbetweenPPSandcorrelation,ﬁrstsetaclearobjectiveabout\\nwhatyouwishtolearnaboutthedata:\\n● Doyouwanttoknowthegeneralmonotonictrendbetweentwovariables?\\nCorrelationwillhelp.\\n● Doyouwanttoknowthepredictivenessofafeature?PPSwillhelp.\\nGetstartedwithPPS:https://github.com/8080labs/ppscore.\\n352'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 353, 'page_label': '354', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00r\\x00\\x00f\\x00u\\x00\\x00a\\x00\\x00\\x00t\\x00\\x00i\\x00\\x00\\x00c\\x00\\nManydatascientistssolelyrelyonthecorrelationmatrixtostudytheassociation\\nbetweenvariables.Butunknowntothem,theobtainedstatisticcanbeheavily\\ndrivenbyoutliers.Thisisevidentfromtheimagebelow.\\nAddingjusttwooutliersdrasticallychangedthecorrelationcoeﬃcientandthe\\nregressionﬁt.Thus,plottingthedataishighlyimportant.\\nThiscansaveyoufromdrawingwrongconclusions,whichyoumayhavedrawn\\notherwisebysolelylookingatthesummarystatistics.\\nOnethingthatIo\\x00endowhenusingacorrelationmatrixiscreatingaPairPlot\\naswell(shownbelow).\\n353'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 354, 'page_label': '355', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisletsmeinferifthescatterplotoftwovariablesandtheircorresponding\\ncorrelationmeasureresonatewitheachotherornot.Weshallcontinuethis\\ndiscussioninthenextchapter.\\n354'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 355, 'page_label': '356', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00s\\x00\\x00\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00a\\x00\\x00n\\x00\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00i\\x00\\x00a\\x00\\n\\x00s\\x00\\x00c\\x00\\x00t\\x00\\x00n\\nPearsoncorrelationiscommonlyusedtodeterminetheassociationbetweentwo\\ncontinuousvariables.Manyframeworks(likePandas,forinstance)haveitastheir\\ndefaultcorrelationmetric.\\nYet,unknowntomany,Pearsoncorrelation:\\n● Onlymeasuresthelinearrelationship.\\n● Penalizesanon-linearyetmonotonicassociation.\\nInstead,Spearmancorrelationisabetteralternative.Itassessesmonotonicity,\\nwhichcanbelinearaswellasnon-linear.\\n355'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 356, 'page_label': '357', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisevidentfromtheillustrationbelow:\\nPearsonvs.Spearmanonlinearandnon-lineardata\\n● PearsonandSpearmancorrelationisthesameonlineardata.\\n● ButPearsoncorrelationunderestimatesanon-linearassociation.\\nSpearmancorrelationisalsousefulwhendataisrankedorordinal,whichwill\\nshalldiscussinthenextchapter.\\nAlso,beforeweendthischapter,remembertoalwaysbecautiousbeforedrawing\\nanyconclusionsusingsummarystatistics.Wealsosawthisinthelastchapter.\\nWhileanalyzingdata,somanypeoplegettemptedtodrawconclusionssolely\\nbasedonitsstatistics.Yet,theactualdatamightbeconveyingatotallydiﬀerent\\nstory.\\n356'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 357, 'page_label': '358', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisalsoevidentfromtheimagebelow:\\nAllninedatasetshaveapprox.zerocorrelationbetweenthetwovariables.\\nHowever,thesummarystatistic,Pearsoncorrelationinthiscase,givesnoclue\\naboutwhat’sinsidethedatabecauseitisalwayszero.\\nInfact,thisisnotjustaboutPearsoncorrelationbutappliestoallsummary\\nstatistics.Theideaisthatwheneveryougenerateanysummarystatistic,youlose\\nessentialinformation.\\nThus,theimportanceoflookingatthedatacannotbestressedenough.Itsaves\\nusfromdrawingwrongconclusions,whichwecouldhavemadeotherwiseby\\nlookingatthestatisticsalone.\\n357'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 358, 'page_label': '359', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00e\\x00\\x00t\\x00\\x00n\\x00i\\x00\\x00\\x00r\\x00\\x00n\\x00\\x00\\x00a\\x00\\x00g\\x00\\x00i\\x00\\x00l\\x00a\\x00\\x00\\nImagineyouhaveanordinalcategoricalfeature.Youwanttomeasureits\\ncorrelationwithothercontinuousfeatures.\\n\\x00r\\x00\\x00n\\x00\\x00\\x00e\\x00\\x00u\\x00\\x00\\x00s\\x00a\\x00\\x00g\\x00\\x00i\\x00\\x00l\\x00a\\x00\\x00\\x00i\\x00\\x00\\x00\\x00a\\x00\\x00r\\x00\\x00\\x00r\\x00\\x00r\\x00\\x00g\\x00n\\x00a\\x00\\x00g\\x00\\x00i\\x00\\x00.\\nBeforeproceeding\\nwiththecorrelation\\nanalysis,youwill\\nencodethefeature,\\nwhichisafairthing\\ntodo.\\nYet,unknowntomany,thechoiceofencodingcanlargelyaﬀectthecorrelation\\nresults.Forinstance,considerthedatasetbelow:\\n358'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 359, 'page_label': '360', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHere,wehave:\\n● Anordinalcategoricalfeature:t-shirtsize(S,M,L,XL).\\n● Acontinuousfeature:weight.\\nIfwelookatitgraphically,theredoesexistamonotonicrelationshipbetweenthe\\ntwofeatures.However,asdepictedbelow,alteringthecategoricalencoding\\naﬀectsthePearsoncorrelation.\\n● Inthele\\x00plot,wehavethefollowingencoding→S(1),M(2),L(3)andXL(4).\\n● Intherightplot,wehavethefollowingencoding→S(1),M(2),L(4)and\\nXL(8).\\nItisclearthattheSpearmancorrelationisabetteralternativetoassessthe\\nmonotonicitybetweenordinalandcontinuousfeatures.\\n359'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 360, 'page_label': '361', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItalwaysremainsthesame,irrespectiveofthechoiceofcategoricalencoding.\\nThisisbecausetheSpearmancorrelationisrank-based.\\nItoperatesontheranksofthedata,whichmakesitmoresuitableforsuchcases\\nofcorrelationanalysis.\\nUsingSpearmancorrelationisprettysimpleaswell.Forinstance,ifyouare\\nusingPandas,justspecifythedesiredcorrelationmeasureasfollows:\\n360'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 361, 'page_label': '362', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00t\\n\\x00u\\x00\\x00i\\x00\\x00r\\x00\\x00\\x00e\\x00o\\x00\\x00r\\x00\\x00\\x00e\\x00h\\x00\\x00t\\nAlmostallreal-world\\nMLmodelsgradually\\ndegradein\\nperformancedueto\\ncovariateshi\\x00.\\nForstarters,covariateshi\\x00happenswhenthedistributionoffeatureschanges\\novertime,butthetrue(natural)relationshipbetweeninputandoutputremains\\nthesame.\\nItisaseriousproblembecausewetrainedthemodelononedistribution,butitis\\nbeingusedtopredictonanotherdistributioninproduction.Thus,itiscriticalto\\ndetectcovariateshi\\x00earlysothatmodelscontinuetoworkwell.\\n\\x00s\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00c\\x00\\x00o\\x00\\x00e\\x00\\x00o\\x00\\x00\\nOneofthemostcommonandintuitive\\nwaystodetectcovariateshi\\x00isby\\nsimplycomparingthefeature\\ndistributionintrainingdatatothatin\\nproduction.\\n361'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 362, 'page_label': '363', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThiscouldbedoneinmanyways,suchas:\\n● Comparetheirsummarystatistics—mean,median,etc.\\n● Inspectvisuallyusingdistributionplots.\\n● Performhypothesistesting.\\n● Measuredistancesbetweentraining/productiondistributionsusing\\nBhattacharyyadistance,KStest,etc.\\nWhiletheseapproachesareo\\x00eneﬀective,thebiggestproblemisthattheywork\\nonasinglefeatureatatime.But,inreallife,wemayobservemultivariate\\ncovariateshi\\x00(MCS)aswell.Ithappenswhen:\\n● Thedistributionofindividualdistributionsremainsthesame:\\n○ \\x00(\\x00\\x00)and\\x00(\\x00\\x00)individuallyremainthesame.\\n● Buttheirjointdistributionchanges:\\n○ \\x00(\\x00\\x00,\\x002\\x00changes.\\n● FromtheKDEplotsonthetopandtheright,itisclearthatthe\\ndistributionofbothfeatures(covariates)isalmostthesame.\\n362'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 363, 'page_label': '364', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● But,thescatterplotrevealsthattheirjointdistributionintraining(Blue)\\ndiﬀersfromthatinproduction(Red).\\nAnditiseasytoguessthattheunivariatecovariateshi\\x00detectionmethods\\ndiscussedabovewillproducemisleadingresults.Forinstance,asdemonstrated\\nbelow,measuringtheBhattacharyyadistancebetweenatrainingandproduction\\nfeaturegivesaverylowdistancevalue,indicatinghighsimilarity:\\nInfact,eventhoughtheindividualfeaturedistributionisthesame,wecan\\nconﬁrmexperimentallythatthiswillresultinadropinmodelperformance:\\nLet’ssaythetrueoutput\\x00\\x00\\x00*\\x00\\x00a\\x00\\x00r\\x00\\x00A\\x00\\x00*\\x00\\x00a\\x00\\x00r\\x00\\x00B\\x00\\x00o\\x00\\x00e.\\n363'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 364, 'page_label': '365', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInmostcases,thetrueoutputpredictionsonproductiondataarenot\\nimmediatelyavailable.Intheabovedemonstration,justassumethatwehave\\nalreadygatheredthetruepredictionssomehow.\\nAsdepictedabove,themodelperformancedropsby20%inproduction,whichis\\nhuge.Here,wemaycompletelyruleoutthepossibilityofcovariateshi\\x00ifwe\\nthinkthatcovariateshi\\x00canneverbemultivariateinnature.\\nOfcourse,intheﬁgurebelow,itwaseasytoidentifymultivariatecovariateshi\\x00\\nbecauseweareonlylookingattwofeatures.\\nButmultivariatecovariateshi\\x00canhappenwithmorethantwofeaturesaswell.\\nUnlikethebivariatecaseabove,visualinspectionwillnotbepossibleforhigher\\ndimensions.\\n\\x00e\\x00\\x00c\\x00\\x00n\\x00\\x00u\\x00\\x00i\\x00\\x00r\\x00\\x00\\x00e\\x00o\\x00\\x00r\\x00\\x00\\x00e\\x00h\\x00\\x00t\\nTobegin,itisimportanttounderstandthatmultivariatecovariateshi\\x00isabig\\nproblem,andthereisnodirect(orsingle)approachtohandlethis.Below,Iwill\\nshareacoupleofideasthatIo\\x00enusemyselfandhaveseenothersuseaswellto\\nhandlemultivariatecovariateshi\\x00.\\n364'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 365, 'page_label': '366', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00d\\x00\\x00\\x001\\x00\\x00C\\x00\\x00s\\x00a\\x00\\x00\\nAslongaswearecheckingtwo(oreventhree)featuresatatime,visual\\ninspectioncanbeusedtodetectcovariateshi\\x00.Sohere,attimes,manysimply\\nignorethepossibilityofanycovariateshi\\x00beyondthreefeatures.\\nTherationaleisthatbeyondthreefeatures,itisprettyunlikelythat:\\n● \\x00(\\x00\\x00)\\x00\\x00(\\x00\\x00)\\x00\\x00(\\x00\\x00)\\x00\\x00(\\x00\\x00)\\x00…→allofthemindividuallywillalmostremain\\nthesame.\\n● Buttheirjointdistribution\\x00(\\x00\\x00,\\x002\\x00\\x003\\x00\\x004\\x00…\\x00willchange.\\nThus,itmightbefairtolimitmultivariatecovariateshi\\x00analysistojustone,\\ntwo,andthreefeaturesatatime.Butofcourse,thismaynotbealwaystrue,\\nwhichbringsustoanotheridea:\\n\\x00d\\x00\\x00\\x002\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00n\\nThisisanothercoolandpracticalideathatIusedinoneofmyprojects.Data\\nreconstruction,asthenamesuggests,revolvesaroundlearningamappingthat\\nprojectsthedatatolowdimensionsandthenreconstructstheoriginaldatafrom\\nthelowdimensions.\\n365'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 366, 'page_label': '367', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisispreciselywhatAutoencodersdo.Theyareaclassofneuralnetworksthat\\nlearntoencodedatainalower-dimensionalspace,andthendecodeitbacktothe\\noriginaldataspace.\\nTheobjectiveistominimizethedatareconstructionerror.It’slikeaskingthe\\nmodeltolearnamappingthat:\\n● Takessomedata.\\n● Projectsthedatatolowdimensions.\\n● Andthengivesustheexactinputdataback.\\nSohere’swhatwecando:\\n● TrainanAutoencoderontheoriginaltrainingdataset.Thiswillgiveusthe\\nweightsfortheneuralnetworkmodelthatreconstructsthedataset.\\n● Usethismodelonnewdatatocheckmultivariatecovariateshi\\x00:\\n○ Ifthereconstructionlossishigh,thedistributionhaschanged.\\n○ Ifthereconstructionlossislow,thedistributionisalmostthesame.\\n366'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 367, 'page_label': '368', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nOneofthebestthingsaboutthisapproachisthatitdoesnotneedalabeled\\ndataset.Essentially,Autoencodersaimtoreconstructthedatasetwithoutlabels.\\nThisisquiteusefulinreal-worldmodelsbecause,aswediscussedyesterday,in\\nmostcases,thetrueoutputpredictionsonproductiondataareneverimmediately\\navailable.Instead,theyalwaystakesometime.\\nForinstance,IrememberwhenIwasworkingonatransactionalfrauddetection\\nmodelatMastercard,acardholder’sissuerbankmaytakeupto45-50daysto\\nsendthefraudlabelforthetransactionsthatwentthroughMastercard’snetwork.\\nThisisalotoftime,isn’tit?\\nButusingAutoencoders,wecanstillcheckdatareconstructionerrorsonthe\\nunlabeleddata.Infact,wedon’tnecessarilyhavetouseAutoencoders.Itwasjust\\nanexamplehere.\\nOtherdatareconstructiontechniquescanalsobeused,suchasPCAisoneof\\nthem.Nonetheless,whenusinganydatareconstructionapproach,itisimportant\\ntotakecareofonething.Earlier,wediscussedthat:\\n● Ifthereconstructionlossishigh,thedistributionhaschanged.\\n● Ifthereconstructionlossislow,thedistributionisalmostthesame.\\nButinterpretingreconstructionlosscanbeprettysubjective,anditalsoneeds\\nsomecontext.\\n367'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 368, 'page_label': '369', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,ifthereconstructionlossis0.4(say),howdowedeterminewhether\\nthisissigniﬁcant?Contrarytowhatmanythink,covariateshi\\x00happens\\ngraduallyinMLmodels.Inotherwords,itisunlikelythatourmodelis\\nperformingprettywelloneday,andthenextday,itstartsunderperformingDUE\\nTOCOVARIATESHIFT.\\nOfcourse,abruptdegradationispossiblebutitisrarelyduetocovariateshi\\x00.\\nMostofthetime,ithappensduetootherissueslike-Maybeduetosome\\ncomplianceissue,theteamhasstoppedcollectingafeature.Asaresult,your\\nmodelnolongerhasaccesstoafeatureyoutraineditwith.Ormaybetherewere\\nsomeupdatestotheinferencelogicwhichwasnothoroughlytested.andmore.\\nButperformancedegradationhappensgraduallyduetocovariateshi\\x00.Andin\\nthatphaseofperformancedegradation,wemustdecidetoupdatethemodelat\\nsomepointtoadjustfordistributionalchanges.\\n368'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 369, 'page_label': '370', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWhiledatareconstructionlossishelpfulindecidingmodelupdates,itconveys\\nverylittlemeaningalone.Simplyput,weneedabaselinereconstructionloss\\nvaluetocompareourfuturereconstructionlosses.\\nSohere’swhatwecando.\\nLet’ssaywedecidedtoreviewourmodeleveryweek.A\\x00ertrainingthemodelon\\nthegathereddata:\\n● Wedetermineourbaselinereconstructionscoreusingthenewdata\\ngatheredovertheﬁrstweekinthepost-trainingphase.\\n● Weusethisbaselinescoretocomparefuturelossesofeverysubsequent\\nweek.\\n369'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 370, 'page_label': '371', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAhighdiﬀerencebetweenreconstructionlossandthebaselinescorewill\\nindicateacovariateshi\\x00.\\nSimpleandintuitive,right?Theabovereconstructionmetric,coupledwiththe\\nmodelperformance,becomesareliablewaytodetermineifcovariateshi\\x00has\\nsteppedin.Ofcourse,asdiscussedabove,truelabelsmightnotbeimmediately\\navailableattimes.\\nThus,determiningperformanceinproductioncanbediﬃcult.Tocounterthis,\\nallMLteamsconsistentlytrytogatherfeedbackfromtheendusertolearnifthe\\nmodeldidwellornot.\\nThisisespeciallyseeninrecommendationsystems:\\n● Wasthisadvertisementrelevanttoyou?\\n● Wasthisnewvideorelevanttoyou?\\n● Andmore.\\nThesethingsareapartofthemodelloggingphase,wherewetrackmodel\\nperformanceinproduction.Thishelpsusdecidewhetherweshouldupdateour\\nmodelornot.\\n370'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 371, 'page_label': '372', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00s\\x00\\x00g\\x00r\\x00\\x00y\\x00\\x00a\\x00\\x00l\\x00\\x00n\\x00\\x00o\\x00d\\x00\\x00t\\x00\\x00y\\x00r\\x00\\x00t\\nAlmostallreal-worldMLmodelsgraduallydegradeinperformanceduetoadri\\x00\\ninfeaturedistribution:\\nItisaserious\\nproblembecausewe\\ntrainedthemodel\\nononedistribution,\\nbutitisbeingused\\ntogenerate\\npredictionson\\nanotherdistribution\\ninproduction.\\nThus,itiscriticalto\\ndetectdri\\x00earlyso\\nthatmodels\\ncontinuetowork\\nwell.Theimage\\ndepictsanintuitive\\ntechnique Io\\x00en\\nusetodetermine\\nwhichfeaturesare\\ndri\\x00inginmy\\ndataset.\\n371'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 372, 'page_label': '373', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nConsiderwehavetwoversionsofthedataset—theoldversion(oneonwhichthe\\nmodelwastrained)andthecurrentversion(oneonwhichthemodelisgenerating\\npredictions):\\nThecoreideaistoassessifthereareanydistributionaldissimilaritiesbetween\\nthesetwoversions.\\nSohere’swhatwedo:\\n● Appenda\\x00a\\x00\\x00l\\x00\\x00columntotheolddataset.\\n● Appenda\\x00a\\x00\\x00l\\x00\\x00columntothecurrentdataset.\\nNow,mergethesetwodatasetsandtrainasupervisedlearningclassiﬁcation\\nmodelonthecombineddataset:\\n372'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 373, 'page_label': '374', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThechoiceoftheclassiﬁcationmodelcouldbeabitarbitrary,butitshouldbe\\nensuredthatitispossibletoreliablydeterminefeatureimportance.\\nThus,Ipersonallypreferarandomforestclassiﬁerbecauseithasaninherent\\nmechanismtodeterminefeatureimportance:\\nThatsaid,itisnotnecessarytousearandomforest.\\nTechniqueslikeshuﬄefeatureimportance(whichwediscussedabove)ontopof\\naclassiﬁcationmodelcanbeusedaswell.\\nIfthefeatureimportancevaluessuggestthattherearefeatureswithhighfeature\\nimportance,thismeansthatthosefeaturesaredri\\x00ing.\\nWhy?\\n373'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 374, 'page_label': '375', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisbecauseifsomefeaturescanreliablydistinguishbetweenthetwo\\nversionsofthedataset,thenitisprettylikelythattheirdistribution\\ncorrespondingto\\x00a\\x00\\x00l\\x00\\x00and\\x00a\\x00\\x00l\\x00\\x00(conditionaldistribution)arevarying.\\n● Iftherearedistributionaldiﬀerences,themodelwillcapturethem.\\n● Iftherearenodistributionaldiﬀerences,themodelwillstruggleto\\ndistinguishbetweentheclasses.\\nThisideamakesintuitivesenseaswell.\\nAtthispoint,onequestionthatmanyhaveisthat…\\nWhycan’twejustmonitorthemodelaccuracytodeterminedri\\x00?\\nOfcourse,wecandothataslongaswehavethetruelabelsforthecurrent\\nversionorhaveawaytocomparethemodelperformanceontheoldversionand\\nthenewversion.\\nButinmanycases,thetrueoutputpredictionsonproductiondataarenever\\nimmediatelyavailable.\\nInstead,theyalwaystakesometime.\\nForinstance,whenIwasworkingonatransactionalfrauddetectionmodelat\\nMastercard,acardholder’sissuerbankmaytakeupto45-50daystosendthe\\nfraudlabelforthetransactionsthatwentthroughMastercard’snetwork.\\nThisisalotoftime,isn’tit?\\nThus,onemustrelyonsomerelevantfeedbackfromthesystemtodetermine\\nwhetherthemodel’sperformanceisdroppingornot.\\nUsingthis“proxy-labelingtechnique”discussedaboveissomethingIhavefound\\ntobeimmenselyuseful.\\n374'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 375, 'page_label': '376', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00N\\x00\\n\\x00s\\x00\\x00g\\x00N\\x00\\x00\\x00n\\x00m\\x00\\x00l\\x00\\x00c\\x00\\x00\\x00a\\x00\\x00s\\x00\\x00s\\nOneofthethingsthatalwaysmakesmeabitcautiousandskepticalwhenusing\\nkNNisitsHIGHsensitivitytotheparameter\\x00.Tounderstandbetter,consider\\nthisdummy2Ddatasetbelow.Thereddatapointisatestinstanceweintendto\\ngenerateapredictionforusingkNN.\\nSaywesetthevalueof\\x00=\\x00.Thepredictionfortheredinstanceisgeneratedin\\ntwosteps:\\n● First,wecountthe7nearestneighborsofthereddatapoint.\\n● Next,weassignittotheclasswiththehighestcountamongthose7\\nnearestneighbors.\\nThisisdepictedbelow:\\n375'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 376, 'page_label': '377', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheproblemisthatstep2isentirelybasedonthenotionofclasscontribution—\\ntheclassthatmaximallycontributestotheknearestneighborsisassignedtothe\\ndatapoint.Butthiscanmiserablyfailattimes,especiallywhenwehaveaclass\\nwithfewsamples.\\nForinstance,asshownin\\nthisimage,withk=7,thered\\ndatapointcanNEVERbe\\nassignedtotheyellowclass,\\nnomatterhowcloseitisto\\nthatcluster.\\nWhileitiseasytotweakthehyperparameterkvisuallyintheabovedemo,this\\napproachisinfeasibleinhigh-dimensionaldatasets.Therearetwowaysto\\naddressthis.\\n376'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 377, 'page_label': '378', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00t\\x00\\x00n\\x001\\x00\\x00s\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00e\\x00\\x00h\\x00\\x00\\x00N\\x00\\nDistance-weighted\\nkNNsareamuchmore\\nrobustalternativeto\\ntraditionalkNNs.As\\nthenamesuggests,in\\nstep2,theyconsiderthe\\ndistancetothenearest\\nneighbor.\\nAsaresult,theclosera\\nspeciﬁcneighboris,the\\nmoreimpactitwillhave\\nontheﬁnalprediction.\\nItseﬀectivenessis\\nevidentfromthisimage.\\n● Intheﬁrstplot,traditionalkNN(withk=7)canneverpredicttheblueclass.\\n● Inthesecondplot,distance-weightedkNNisfoundtobemorerobustin\\nitsprediction.\\nAspermyobservation,adistance-weightedkNNtypicallyworksbetterthana\\ntraditionalkNN.Yet,thismaygounnoticedbecause,bydefault,thekNN\\nimplementationofsklearnconsiders“uniform”weighting.\\n377'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 378, 'page_label': '379', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00t\\x00\\x00n\\x002\\x00\\x00y\\x00\\x00m\\x00\\x00a\\x00\\x00y\\x00p\\x00\\x00t\\x00\\x00h\\x00\\x00y\\x00\\x00r\\x00\\x00r\\x00\\x00e\\x00\\x00r\\x00\\nRecalltheabovedemoagain:\\nHere,onemayarguethatwemustrefrainfromsettingthehyperparameterkto\\nanyvaluegreaterthantheminimumnumberofsamplesthatbelongtoaclassin\\nthedataset.\\nOfcourse,Iagreewiththistoanextent.\\nButletmetellyouthedownsideofdoingthat.Settingaverylowvalueofkcan\\nbehighlyproblematicinthecaseofextremelyimbalanceddatasets.\\nTogiveyoumoreperspective,IhavepersonallyusedkNNondatasetsthathad\\nmerelyoneortwoinstancesforaparticularclassinthetrainingset.AndI\\n378'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 379, 'page_label': '380', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\ndiscoveredthatsettingalowofk(say,1or2)ledtosuboptimalperformance\\nbecausethemodelwasnotasholisticallyevaluatingthenearestneighbor\\npatternsasitwaswhenalargevalueofkwasused.\\nInotherwords,settingarelativelylargervalueofktypicallygivesmoreinformed\\npredictionsthanusinglowervalues.Butwejustdiscussedabovethatifweseta\\nlargevalueofk,themajorityclasscandominatetheclassiﬁcationresult:\\nToaddressthis,Ifounddynamicallyupdatingthehyperparameterktobemuch\\nmoreeﬀective.Morespeciﬁcally,therearethreestepsinthisapproach.Forevery\\ntestinstance:\\n1. Beginwithastandardvalueofkasweusuallywouldandﬁndtheknearest\\nneighbors.\\n2. Next,updatethevalueofthekasfollows:\\na. Foralluniqueclassesthatappearintheknearestneighbor,ﬁndthe\\ntotalnumberoftraininginstancestheyhave.\\n379'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 380, 'page_label': '381', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\nb. Updatethevalueofkto:\\n3. Nowperformmajorityvotingonlyontheﬁrstk'neighborsonly.\\nThismakesanintuitivesenseaswell:\\n● Ifaminorityclassappearsinthetopknearestneighbor,wemustreduce\\nthevalueofksothatthemajorityclassdoesnotdominate.\\n● IfaminorityclassDOESNOTappearsinthetopknearestneighbor,we\\nwilllikelynotupdatethevalueofkandproceedwithaholistic\\nclassiﬁcation.\\nTheonlyshortcomingisthatyouwouldn’tﬁndthisapproachinanyopen-source\\nimplementations.Infact,inmyprojectsaswell,Ihadtowriteacustom\\nimplementation,sotakethatintoaccount.\\n380\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 381, 'page_label': '382', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00p\\x00\\x00o\\x00\\x00m\\x00\\x00e \\x00e\\x00\\x00e\\x00\\x00 \\x00e\\x00\\x00h\\x00\\x00r \\x00e\\x00\\x00c\\x00 \\x00s\\x00\\x00g\\n\\x00n\\x00\\x00r\\x00\\x00\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00x\\nOneofthebiggestissueswithnearestneighborsearchusingkNNisthatit\\nperformsanexhaustivesearch.\\nInotherwords,thequerydatapointmustbematchedacrossalldatapointsto\\nﬁndthenearestneighbor(s).Thisishighlyineﬃcient,especiallywhenwehave\\nmanydatapointsandanear-real-timeresponseisnecessary.\\nThatiswhyapproximatenearestneighborsearchalgorithmsarebecoming\\nincreasinglypopular.Thecoreideaistonarrowdownthesearchspaceusing\\nindexingtechniques,therebyimprovingtheoverallrun-timeperformance.\\n381'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 382, 'page_label': '383', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInvertedFileIndex(IVF)ispossiblyoneofthesimplestandmostintuitive\\ntechniqueshere,whichyoucanimmediatelystartusing.\\n\\x00e\\x00\\x00’\\x00\\x00o\\x00\\x00t\\x00o\\x00\\x00s\\nGivenasetofdatapointsinahigh-dimensionalspace,theideaistoorganize\\nthemintodiﬀerentpartitions,typicallyusingclusteringalgorithmslikek-means.\\nAsaresult,eachpartitionhasacorrespondingcentroid,andeverydatapoint\\ngetsassociatedwithonlyonepartitioncorrespondingtoitsnearestcentroid.\\nAlso,everycentroidmaintainsinformationaboutallthedatapointsthatbelong\\ntoitspartition.\\n382'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 383, 'page_label': '384', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIndexingdone!\\n\\x00e\\x00\\x00’\\x00\\x00o\\x00\\x00e\\x00e\\x00\\x00c\\x00\\x00\\nWhensearchingforthenearestneighbor(s)tothequerydatapoint,insteadof\\nsearchingacrosstheentiredataset,weﬁrstﬁndtheclosestcentroidtothequery:\\nOnceweﬁndthenearestcentroid,thenearestneighborissearchedinonlythose\\ndatapointsthatbelongtotheclosestpartitionfound:\\nLet’sseehowtherun-timecomplexitystandsincomparisontotraditionalkNN.\\nConsiderthefollowing:\\n● ThereareNdatapoints\\n● EachdatapointisDdimensional\\n● WecreateKpartitions.\\n● Lastly,forsimplicity,let’sassumethateachpartitiongetsequaldata\\npoints.\\n383'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 384, 'page_label': '385', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInkNN,thequerydatapointismatchedtoallNdatapoints,whichmakesthe\\ntimecomplexity→\\x00(\\x00\\x00).\\nInIVF,however,therearetwosteps:\\n1. Matchtoallcentroids→\\x00(\\x00\\x00).\\n2. Findthenearestneighborinthenearestpartition→\\x00(\\x00\\x00/\\x00\\x00.\\nTheﬁnaltimecomplexitycomesouttobethefollowing:\\n…whichissigniﬁcantlylowerthanthatofkNN.Togetsomeperspective,assume\\nwehave10Mdatapoints.ThesearchcomplexityofkNNwillbeproportionalto\\n10M.ButwithIVF,saywedividethedatainto100centroids,andeachpartition\\ngetsroughly100kdatapoints.\\nThus,thetimecomplexitycomesouttobeproportionalto100+100k=100100,\\nwhichisnearly100timesfaster.\\nOfcourse,itisessentialtonotethatif\\nsomedatapointsareactuallycloseto\\ntheinputdatapointbutstillhappento\\nbeintheneighboringpartition,wewill\\nmissthemduringthenearestneighbor\\nsearch,asshowninthisimage.\\nButthisaccuracytradeoﬀissomethingwewillinglyacceptforbetterrun-time\\nperformance,whichispreciselywhythesetechniquesarecalled“approximate\\nnearestneighborssearch.”\\n384'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 385, 'page_label': '386', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00e\\x00\\x00\\n\\x00h\\x00\\x00s\\x00e\\x00\\x00e\\x00\\x00r\\x00\\x00k\\x00a\\x00\\x00e\\x00\\x00\\x00T\\x00\\x00c\\x00\\x00?\\nSomanyMLalgorithmsusekernelsforrobustmodeling,likeSVM,KernelPCA,\\netc.\\nInagist,akernelfunctionletsuscomputedotproductsinsomeotherfeature\\nspace(mostlyhigh-dimensional)withoutevenknowingthemappingfromthe\\ncurrentspacetotheotherspace.\\nButhowdoesthatevenhappen?\\nLet’sunderstand!\\n\\x00h\\x00\\x00b\\x00\\x00c\\x00\\x00v\\x00\\nFirstly,itisimportanttonotethatthekernelprovidesawaytocomputethedot\\nproductbetweentwovectors,\\x00and\\x00,insomehigh-dimensionalspacewithout\\nprojectingthevectorstothatspace.\\nThisisdepictedbelow,wheretheoutputofthekernelfunctionisexpectedtobe\\nthesameasthedotproductbetweenprojectedvectors:\\n385'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 386, 'page_label': '387', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThekeyadvantageisthatthekernelfunctionisappliedtothevectorsinthe\\noriginalfeaturespace.\\nHowever,thatequalsthedotproductbetweenthetwovectorswhenprojected\\nintoahigher-dimensional(yetunknown)space.\\nIfthatisabitconfusing,letmegiveanexample.\\n\\x00\\x00o\\x00\\x00v\\x00\\x00i\\x00\\x00\\x00x\\x00\\x00p\\x00\\x00\\nLet’sassumethefollowingpolynomialkernelfunction:\\nForsimplicity,let’ssaybothXandYaretwo-dimensionalvectors:\\nSimplifyingthekernelexpressionabove,wegetthefollowing:\\nExpandingthesquareterm,weget:\\n386'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 387, 'page_label': '388', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNownoticetheﬁnalexpression:\\nTheaboveexpressionisthedotproductbetweenthefollowing6-dimensional\\nvectors:\\nThus,ourprojectionfunctioncomesouttobe:\\nThisshowsthatthekernelfunctionwechoseearliercomputesthedotproductin\\na6-dimensionalspacewithoutexplicitlyvisitingthatspace.\\nAndthatistheprimaryreasonwhywealsocallitthe“kerneltrick.”\\nMorespeciﬁcally,it’sframedasa“trick”sinceitallowsustooperatein\\nhigh-dimensionalspaceswithoutexplicitlycomputingthecoordinatesofthe\\ndatainthatspace.\\n387'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 388, 'page_label': '389', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00a\\x00\\x00e\\x00\\x00t\\x00\\x00s\\x00e\\x00\\x00n\\x00\\x00B\\x00\\x00e\\x00\\x00e\\x00\\nAswesawabove,akernelprovidesawaytocomputethedotproductbetween\\ntwovectors,XandY,insomehigh-dimensionalspacewithoutprojectingthe\\nvectorstothatspace.\\nInthatpost,welookedatthepolynomialkernelandsawthatitcomputesthedot\\nproductofa2-dimensionalvectorina6-dimensionalspacewithoutexplicitly\\nvisitingthatspace.\\nNext,let’stalkabouttheRBFkernel,anotherinsanelypowerfulkernel,whichis\\nalsothedefaultkernelinasupportvectorclassiﬁerclassimplementedby\\nsklearn:\\n388'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 389, 'page_label': '390', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTobegin,themathematicalexpressionoftheRBFkernelisdepictedbelow(and\\nconsiderthatwehavejusta1-dimensionalfeaturevector):\\nYoumayrememberfromhighschoolmathematicsthattheexponentialfunction\\nisdeﬁnedasfollows:\\nExpandingthesquaretermintheRBFkernelexpression,weget:\\nDistributingthegammatermandexpandingtheexponentialtermusingthe\\nexponentrule,weget:\\nNext,weapplytheexponentialexpansiontothelasttermandgetthefollowing:\\n389'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 390, 'page_label': '391', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNoticecloselythattheexponentialexpansionabovecanberewrittenasthedot\\nproductbetweenthefollowingtwovectors:\\nAndtherewegetourprojectionfunction:\\nItisevidentthatthisfunctionmapsthe1-dimensionalinputtoan\\ninﬁnite-dimensionalfeaturespace.\\nThisshowsthattheRBFkernel\\nfunctionwechoseearlier\\ncomputesthedotproductinan\\ninﬁnite-dimensionalspace\\nwithoutexplicitlyvisitingthat\\nspace.\\nThisiswhytheRBFkernelisconsideredsopowerful,allowingittoeasilymodel\\nhighlycomplexdecisionboundaries.\\nHere,Iwanttoremindyouthateventhoughthekernelisequivalenttothedot\\nproductbetweentwoinﬁnite-dimensionalvectors,weNEVERcomputethatdot\\nproduct,sothecomputationcomplexityisnevercompromised.\\nThatiswhythekerneltrickiscalleda“trick.”Inotherwords,itallowsusto\\noperateinhigh-dimensionalspaceswithoutexplicitlycomputingthecoordinates\\nofthedatainthatspace.\\n390'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 391, 'page_label': '392', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00\\n\\x00\\x00y\\x00\\x00s\\x00f\\x00i\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00e\\x00\\nA\\x00erseeingmissingvaluesinadataset,mostpeoplejumpdirectlyintoimputing\\nthem.Butascounterintuitiveasitmaysound,theﬁrststeptowardsimputing\\nmissingdatashouldNEVERbeimputation.\\nInstead,theﬁrststepmustbetounderstandthereasonbehinddatamissingness.\\nThisisbecausethedataimputationstrategylargelydependsonWHYdatais\\nmissing.\\nMorespeciﬁcally,missingnesscouldbeofthreetypes:\\n● Missingcompletelyatrandom(MCAR).\\n● Missingatrandom(MAR).\\n● Missingnotatrandom(MNAR).\\nOnlywhenwedeterminethereasoncanweproceedwiththeappropriate\\nimputationtechniques.\\nWhatareMCAR,MAR,andMNAR?\\nLet’sunderstandtheminthischapterindetail.\\n391'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 392, 'page_label': '393', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00)\\x00i\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00l\\x00\\x00e\\x00\\x00\\x00t\\x00a\\x00\\x00o\\x00\\x00M\\x00\\x00R\\x00\\nMCARisasituationinwhichthedataisgenuinelymissingatrandomandhasno\\nrelationtoanyobservedorunobservedvariables.Inotherwords,themissing\\ndatapointsfollownodiscernablepattern.\\nForinstance,insurveyresponseswithmissingvalues,assumingMCARwould\\nmeanthatsomeparticipantsmayhaveunintentionallyskippedtoanswerany\\nrandomquestion.\\nHowever,inmyexperience,MCARhasmostlybeenanunrealisticassumptionfor\\nmissingness,i.e.,dataisusuallynotMCARformostreal-worlddatasetswith\\nmissingvalues.Thisisbecause,inreal-worldscenarios,theoccurrenceof\\nmissingdatao\\x00entendstobeinﬂuencedbysomefactor,eitherobservedor\\nunobserved.\\nHumanbehavior,surveyadministration,oranyexternaleventscanbemotivating\\nfactorsformissingvaluestoappearinadataset.Forinstance,participantsmay\\nselectivelyomitsensitiveinformation,orcertaingroupsmaybemoreproneto\\nnon-response,causingmissingness.\\nThatiswhyassumingMCARformissingdataisnotasensibleassumptionunless\\nyouknowtheend-to-enddatacollectionprocessand/orhavedomainexpertisein.\\nHere,itbecomesessentialfordatascientiststotalktodataengineersand\\nunderstandthedatacollectionprocess.Infact,thisisnotjustaboutMCARbut\\napplicabletotheothertwosituationsaswell,whichweshalldiscussshortly.\\n392'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 393, 'page_label': '394', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nUnderstandingthedatacollectionprocesswillNEVERhurt.\\nThatsaid,nothingstopsusfromassumingthatmissingvaluesareMCARifthat\\nappearstobeafairthingtodobasedontheanalysisand/orinputfromthe\\ndomainexpertsand/ora\\x00erunderstandingthedatacollectionmechanism.You\\ncanproceedwiththesimplestunivariateimputationtechniques.\\n\\x00)\\x00i\\x00\\x00i\\x00\\x00\\x00t\\x00a\\x00\\x00o\\x00\\x00M\\x00\\x00)\\nMARisasituationinwhichthemissingnessofonefeaturecanbeexplainedby\\notherobservedfeaturesinthedataset.\\nIncontrasttoMCAR,MARismorepracticallyobserved.\\nInthiscase,themissingnesscanbeaccountedforthroughappropriatestatistical\\nmethodswithreasonableaccuracy.\\nThus,eventhoughthedataismissing,itsoccurrencecanstillbe(somewhat)\\nestimatedbasedontheinformationavailableinthedataset.\\nAcommonwaytodetermineMARisbyconditioningonanotherobserved\\nfeaturesandnoticinganyincreaseintheprobabilityofmissingness.\\n393'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 394, 'page_label': '395', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,inanacademicperformancesurvey,studentswithhighergrades\\nmightbelesslikelytodiscloseinformationaboutthenumberofhourstheystudy\\n(duetocompetition,maybe).\\nThisiswheretechniqueslikekNNimputation,MissForest,etc.,arequite\\neﬀective.Theyuseotherobservedfeaturestoimputethemissingfeature.\\nIfyouwanttolearnaboutthesetechniques,thechapterispreciselyaboutthis\\ntopic.\\n\\x00)\\x00i\\x00\\x00i\\x00\\x00\\x00o\\x00\\x00t\\x00a\\x00\\x00o\\x00\\x00M\\x00\\x00R\\x00\\nMNARisthemostcomplicatedsituationofallthree.InMNAR,missingnessis\\neitherattributedtothemissingvalueitselforthefeature(s)thatwedidn’tcollect\\ndatafor.\\nInotherwords,withinMNAR,thereisadeﬁnitepatterninmissingvariables.\\nHowever,thepatternisunrelatedtoanyobservedfeature(s).Thisisdiﬀerent\\nfromMCARwherethereisnodeﬁnitepattern.\\n394'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 395, 'page_label': '396', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,inahealthsurvey,participantswithveryhighstresslevelsmight\\nconsciouslychoosenottodisclosetheirstresslevelduetostigma,orfearof\\njudgment.Asaresult,themissingdataonstresslevelisnotrandom;itis\\ninﬂuencedbythestresslevelitself.\\nSo,inaway,thehigherthestresslevel,thelesslikelyonewilldiscloseit,andthe\\nmorelikelythevaluewillbemissingfromthecollecteddataset.Thus,the\\nmissingnessisdirectlydependentontheveryvariablethatismissingintheﬁrst\\nplace.That’stricky,isn’tit?\\nThere’snotmuchwecandotoaddressthis,exceptforcollectingmore\\ndata/features.What’smore,domainexpertisebecomeextremelyimportantto\\nsmartlytackleMNARandimprovethedatacollectionprocess.\\nAttimes,Ihavealsopreferredproceedingwithtypicalimputationtechniques\\n(usedinMCARandMAR)becausefurtherdatacollectioncanbeinfeasiblein\\nmostcases.Butasdiscussedabove,thereisadeﬁnitemissingnesspatternin\\nMNAR,whichcanbeimportant.Butdirectimputationwilldiscardthat\\ninformation.\\nOnewaytoretainthatmissingnesspatternisbyaddingabinaryfeature,\\nindicatingwhetherthefeaturewasimputed.\\nThisway,theMLalgorithmcanstillaccessandlearnthemissingdatapatterns.\\n395'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 396, 'page_label': '397', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00F\\x00\\x00e\\x00\\x00\\x00n\\x00\\x00N\\x00\\x00m\\x00\\x00t\\x00\\x00i\\x00\\x00\\nTheimputationstrategyformissingdatalargelydependsonthetypeof\\nmissingness,whichcanbeofthreetypes:\\n● Missingcompletelyatrandom(MCAR):Dataisgenuinelymissingat\\nrandomandhasnorelationtoanyobservedorunobservedvariables.\\n● Missingatrandom(MAR):Themissingnessofonefeaturecanbe\\nexplainedbyotherobservedfeaturesinthedataset.\\n● Missingnotatrandom(MNAR):Missingnessiseitherattributedtothe\\nmissingvalueitselforthefeature(s)thatwedidn’tcollectdatafor.\\nIhaveobservedmissingatrandom(MAR)appearingrelativelymuchmorein\\npracticalsituationsthantheothertwo,sointhischapter,Iwanttosharetwo\\nimputationstrategiesItypicallyprefertouseinsuchacase.\\n396'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 397, 'page_label': '398', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00N\\x00\\x00m\\x00\\x00t\\x00\\x00i\\x00\\x00\\nAsthenamesuggests,itimputesmissingvaluesusingthek-nearestneighbors\\nalgorithm.\\nMorespeciﬁcally,missingfeaturesareimputedbyrunningakNNon\\nnon-missingfeaturevalues.\\nThefollowingimagedepictshowitworks:\\n● Step1:Selectarow(r)withamissingvalue.\\n● Step2:Finditsknearestneighborsusingthenon-missingfeaturevalues.\\n● Step3:Imputethemissingfeatureoftherow(r)usingthecorresponding\\nnon-missingvaluesofknearestneighborrows.\\n● Step4:Repeatforallrowswithmissingvalues.\\nItseﬀectivenessovermean/zeroimputationisevidentfromthedemobelow.\\nOnthele\\x00,wehaveafeaturedistributionwithmissingvalues.Assumingwe\\nhavealreadyvalidatedthatthedatawasmissingatrandom(MAR),using\\nmean/zeroaltersthesummarystatisticsanddistribution,asdepictedbelow.\\n397'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 398, 'page_label': '399', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHowever,asdepictedbelow,kNNimputerappearstobemorereliable,andit\\npreservesthesummarystatistics:\\n\\x002\\x00\\x00i\\x00\\x00F\\x00\\x00e\\x00\\x00\\nSomemajorissueswithkNNimputationare:\\n1. Ithasahighrun-timeforimputation—especiallyforhigh-dimensional\\ndatasets.\\n2. Itraisesissueswithdistancecalculationinthecaseofcategorical\\nnon-missingfeatures.\\n3. Itrequiresfeaturescaling,etc.\\n398'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 399, 'page_label': '400', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nMissForestimputerisanotherreliablechoiceformissingvalueimputationwhen\\ndataismissingatrandom(MAR).Asthenamesuggests,itimputesmissing\\nvaluesusingtheRandomForestalgorithm.\\n1. Tobegin,imputethemissingfeaturewitharandomguess—Mean,\\nMedian,etc.\\n2. ModelthemissingfeatureusingRandomForest.\\n3. ImputeONLYoriginallymissingvaluesusingRandomForest’sprediction.\\n4. BacktoStep2.UsetheimputeddatasetfromStep3totrainthenext\\nRandomForestmodel.\\n5. Repeatuntilconvergence(ormaxiterations).\\nIncaseofmultiplemissingfeatures,theidea(somewhat)staysthesame:\\n399'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 400, 'page_label': '401', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nImputefeaturessequentiallyinincreasingordermissingness—featureswith\\nfewermissingvaluesareimputedﬁrst.\\nLikebefore,onthele\\x00,wehaveafeaturedistributionwithmissingvalues.\\nAgain,assumingwehavealreadyvalidatedthatthedatawasmissingatrandom\\n(MAR),usingmean/zeroaltersthesummarystatisticsanddistribution:\\nHowever,asdepictedbelow,MissForestappearsmorereliableandpreservesthe\\nsummarystatistics.\\nkNNimputationandMissForestJupyternotebook:https://bit.ly/3RW3i8k.\\n400'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 401, 'page_label': '402', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00a\\x00\\x00s\\x00n\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00\\n\\x00h\\x00\\x00\\x00s\\x00a\\x00\\x00o\\x00\\x00p\\x00\\x00t\\x00\\x00\\x00g\\x00a\\x00\\x00l\\x00o\\x00\\x00L\\x00o\\x00\\x00l\\x00\\x00\\nOnethingwehearinalmostallintroductoryMLlessonsistosplitthegivendata\\nRANDOMLYintotrainandvalidationsets.\\n\\x00o\\x00\\x00:\\x00g\\x00\\x00r\\x00\\x00h\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00o\\x00\\x00o\\x00\\x00\\nRandomsplittingmakessensebecauseitensuresthatthedataisdividedwithout\\nanybias.\\nHowever,Ihavecomeacrossmanysituationswhererandomsplittingisfatalfor\\nmodelbuilding.Yet,manypeopledon’trealizeit.\\nAndIamnottalkingabouttemporaldatasetshere.\\n\\x00c\\x00\\x00a\\x00\\x00o\\nConsideryouarebuildingamodelthatgeneratescaptionsforimages.\\n401'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 402, 'page_label': '403', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nDuetotheinherentnatureoflanguage,everyimagecanhavemanydiﬀerent\\ncaptions:\\nNow,youmightrealizewhatwouldhappenifwerandomlysplitthisdatasetinto\\ntrainandvalidationsets.\\nDuringtherandomsplit,thesamedatapoint(image)willbeavailableinthetrain\\nandvalidationsets.\\nThisisatypicalexampleofdataleakage,whichresultsinhighoverﬁtting!\\nThistypeofleakageisalsoknownasgroupleakage.\\n\\x00o\\x00\\x00t\\x00\\x00n\\nFromtheabovediscussion,itisclearthatrandomsplittingisthecauseofthe\\nproblem.\\nGroupshuﬄesplithelpsussolvethis.\\n402'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 403, 'page_label': '404', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTherearetwosteps:\\n● Groupalltraininginstancescorrespondingtooneimage(orfeaturesthat\\nmayresultinleakage,anyothergroupingcriteria,etc.).\\n● A\\x00ergrouping,thewholegroupmustberandomlysenttoeitherthe\\ntrainingsetorthevalidationset.\\nThiswillpreventthegroupleakagewewitnessedearlierandpreventoverﬁtting.\\nOnethingtonotehereisthatintheaboveexample,allfeaturesinthedataset,\\ni.e.,theimagepixels,contributedtothegroupingcriteria.\\nButmoregenerallyspeaking,therecouldonlybeasubsetoffeaturesthatmustbe\\ngroupedtogetherfordatasplitting.\\nForinstance,consideradatasetcontainingmedicalimagingdata.Eachsample\\nconsistsofmultipleimages(e.g.,diﬀerentviewsofthesamepatient’sbodypart),\\nandthemodelisintendedtodetecttheseverityofadisease.\\nInthiscase,itiscrucialtogroupallimagescorrespondingtothesamepatient\\ntogetherandthenperformdatasplitting.Otherwise,itwillresultindataleakage\\nandthemodelwillnotgeneralizewelltonewpatients.\\n403'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 404, 'page_label': '405', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00\\nIfyouuseSklearn,the\\x00r\\x00\\x00p\\x00\\x00uﬄ\\x00S\\x00\\x00i\\x00implementsthisidea.\\nConsiderwehavethefollowingdataset:\\n● \\x001and\\x002arethefeatures.\\n● \\x00isthetargetvariable.\\n● groupdenotesthegroupingcriteria.\\nFirst,weimportthe\\x00r\\x00\\x00p\\x00\\x00uﬄ\\x00S\\x00\\x00i\\x00fromsklearnandinstantiatetheobject:\\n404'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 405, 'page_label': '406', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThe\\x00p\\x00\\x00t\\x00\\x00methodofthisobjectletsusperformgroupsplitting:\\nThisreturnsagenerator,andwecanunpackittogetthefollowingoutput:\\nAsdemonstratedabove:\\n● Thedatapointsingroups“\\x00”and“\\x00”aretogetherinthetrainingset.\\nThedatapointsingroup“\\x00”aretogetherinthevalidation/testset.\\n405'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 406, 'page_label': '407', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00u\\x00\\x00\\x00c\\x00\\x00i\\x00\\x00\\x00s\\x00O\\x00\\x00l\\x00\\x00y\\x00\\x00e\\x00\\x00s\\x00\\x00r\\x00\\nFeaturescalingiscommonlyusedto\\nimprovetheperformanceandstabilityof\\nMLmodels.Thisisbecauseitscalesthe\\ndatatoastandardrange.Thispreventsa\\nspeciﬁcfeaturefromhavingastrong\\ninﬂuenceonthemodel’soutput.\\nForinstance,intheimageabove,thescaleofincomecouldmassivelyimpactthe\\noverallprediction.Scalingbothfeaturestothesamerangecanmitigatethisand\\nimprovethemodel’sperformance.Iamsureyoualreadyknowthis,sowewon’t\\ngetintomoredetailhere.However,haveyoueverwonderedthefollowing:\\n\\x00s\\x00e\\x00\\x00u\\x00\\x00\\x00c\\x00\\x00i\\x00\\x00\\x00l\\x00\\x00\\x00s\\x00e\\x00\\x00s\\x00\\x00r\\x00\\x00h\\x00\\x00\\x00u\\x00\\x00a\\x00\\x00s\\x00\\x00’\\x00\\x00e\\x00\\x00u\\x00\\x00s\\x00a\\x00\\x00\\x00\\n\\x00i\\x00\\x00r\\x00\\x00\\x00a\\x00\\x00e\\x00\\nWhilefeaturescalingiso\\x00encrucial,weo\\x00enoverlookwhetheritiseven\\nneededornot.ThisisbecausemanyMLalgorithmsareunaﬀectedbyscale.\\nThisisevidentfromtheimage,\\nwhichdepictsthetestaccuracy\\nofsomeclassiﬁcationalgorithms\\nwithandwithoutfeaturescaling.\\nLogisticregression(trainedusing\\nSGD),SVMClassiﬁer,MLP,and\\nkNNdobetterwithfeature\\nscaling.\\nDecisiontrees,Randomforests,\\nNaivebayes,andGradient\\nboostingareunaﬀectedbyscale.\\n406'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 407, 'page_label': '408', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTounderstandbetter,consideradecisiontree.Itsplitsthedatabasedon\\nthresholdsdeterminedsolelybythefeaturevalues,regardlessoftheirscale.\\nAsaresult,itsperformanceisunaﬀectedbythescale.Thismakesintuitivesense\\naswell.\\nThus,thetakeawayisthatwhenwedofeaturescaling,it’simportantto\\nunderstandnotjustthenatureofourdatabutalsothealgorithmweintendto\\nuse.Wemayneverneedfeaturescalingifthealgorithmisinsensitivetothescale\\nofthedata.\\n407'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 408, 'page_label': '409', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00b\\x00\\x00t\\x00o\\x00\\x00r\\x00\\x00s\\x00\\x00r\\x00\\nLogtransformiscommonlyusedtoeliminateskewnessin\\ndata.Yet,itisnotalwaystheidealsolutionforeliminating\\nskewness.Itisimportanttonotethatlogtransform:\\n● DOESNOTeliminatele\\x00-skewness.\\n● Onlyworksforright-skewness,thattoowhenthe\\nvaluesaresmallandpositive.\\nTounderstandbetter,considerthele\\x00-skeweddistributionanditslogtransform\\nbelow:\\nItisclearthatlogtransformdidnoteliminateskewness.\\nHowever,nowconsideraright-skeweddistribution:\\nThistime,iteliminatestheskewness.\\n408'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 409, 'page_label': '410', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\nThishappensbecausethelogfunctiongrowsfasterforlowervalues.Thus,it\\nstretchesoutthelowervaluesmorethanthehighervalues.\\nMorespeciﬁcally,inthecaseofle\\x00-skewness,thetailexiststothele\\x00,which\\ngetsstretchedmorethanthemajorityoftheprobabilitymassthatexiststothe\\nright.Thus,skewnessisn'taﬀectedmuch.\\nHowever,inthecaseofright-skewness,themajorityofprobabilitymassexiststo\\nthele\\x00,whichgetsstretchedoutmore.\\n409\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 410, 'page_label': '411', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThatsaid,itisimportanttonotethat\\nevenifwehavearight-skewed\\ndistribution,logtransformwillnotbe\\neﬀectiveifthevaluesarelarge.\\nThishappensbecausethestretcheﬀect\\nofthelogfunctiondiminishesatlarge\\nvalues:\\nIhaveo\\x00enfoundthebox-coxtransformtobequiteeﬀectiveateliminatingboth\\nright-skewnessandle\\x00-skewness,asdepictedbelow:\\n410'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 411, 'page_label': '412', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00 \\x00r\\x00\\x00 \\x00u\\x00\\x00o\\x00\\x00 \\x00f \\x00e\\x00\\x00u\\x00\\x00 \\x00c\\x00\\x00i\\x00\\x00 \\x00n\\x00\\n\\x00t\\x00\\x00d\\x00\\x00d\\x00\\x00a\\x00\\x00o\\x00\\nFeaturescalingandstandardizationaretwocommonwaystoalterafeature’s\\nrange.Forinstance:\\n● MinMaxScalerchangestherangeofafeatureto[0,1]:\\n● Standardizationmakesafeature’smeanzeroandstandarddeviationone:\\nAsyoumayalreadyknow,theseoperationsarenecessarybecause:\\n● Theypreventaspeciﬁcfeaturefromstronglyinﬂuencingthemodel’s\\noutput.\\n● Theyensurethatthemodelismorerobusttowidevariationsinthedata.\\nForinstance,intheimagebelow,thescaleof“income”couldmassivelyimpact\\ntheoverallprediction.\\n411'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 412, 'page_label': '413', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nScaling(orstandardizing)thedatacanmitigatethisandimprovethemodel’s\\nperformance.Wealsolearnedthisinthefeaturescalingchapter:\\nAsdepictedabove,featurescalingisnecessaryforthebetterperformanceof\\nmanyMLmodels.Sowhiletheimportanceoffeaturescalingandstandardization\\nisprettyclearandwell-known,Ihaveseenmanypeoplemisinterpretingthemas\\ntechniquestoeliminateskewness.\\nButcontrarytothiscommonbelief,featurescalingandstandardizationNEVER\\nchangetheunderlyingdistribution.Instead,theyjustaltertherangeofvalues.\\nThus,a\\x00erscaling(orstandardization):\\n● Normaldistribution→staysNormal\\n● Uniformdistribution→staysUniform\\n● Skeweddistribution→staysSkewed\\n● andsoon…\\nWecanalsoverifythisfromthetwoillustrationsbelow:\\n412'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 413, 'page_label': '414', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItisclearthatscalingandstandardizationhavenoeﬀectontheunderlying\\ndistribution.Thus,alwaysrememberthatifyouintendtoeliminateskewness,\\nscaling/standardizationwillneverhelp.Tryfeaturetransformationsinstead.\\nTherearemanyofthem,butthemostcommonlyusedtransformationsare:\\n● Logtransform\\n● Sqrttransform\\n● Box-coxtransform\\nTheireﬀectivenessisevidentfromtheimagebelow:\\nAsdepictedinthisimage,\\napplyingtheseoperations\\ntransformstheskeweddata\\nintoa(somewhat)normally\\ndistributedvariable.\\nBeforeIconclude,please\\nnotethatwhilelog\\ntransformiscommonly\\nusedtoeliminatedata\\nskewness,itisnotalways\\ntheidealsolution.\\n413'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 414, 'page_label': '415', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x002\\x00e\\x00\\x00l\\x00\\x00i\\x00\\x00t\\x00\\x00n\\x00s\\x00o\\x00\\x00u\\x00\\x00\\x00s\\x00\\x00\\x00o\\x00\\n\\x00e\\x00\\x00l\\x00\\x00i\\x00\\x00t\\x00\\x00n\\nAlmosteverytutorial/course/blogmentioningL2regularizationIhaveseentalks\\naboutjustonething:\\n\\x002\\x00e\\x00\\x00l\\x00\\x00i\\x00\\x00t\\x00\\x00n\\x00s\\x00\\x00a\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00g\\x00e\\x00\\x00n\\x00\\x00u\\x00\\x00h\\x00\\x00\\x00v\\x00\\x00d\\x00\\x00v\\x00\\x00ﬁ\\x00t\\x00\\x00g\\n\\x00y\\x00n\\x00\\x00o\\x00\\x00c\\x00\\x00g\\x00\\x00e\\x00\\x00l\\x00\\x00\\x00e\\x00\\x00\\x00n\\x00\\x00\\x00h\\x00\\x00o\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00u\\x00\\x00t\\x00\\x00n\\x00a\\x00\\x00d\\x00n\\x00h\\x00\\n\\x00q\\x00\\x00r\\x00\\x00\\x00f\\x00h\\x00\\x00o\\x00\\x00l\\x00\\x00\\x00a\\x00\\x00m\\x00\\x00e\\x00\\x00.\\n\\x00n\\x00l\\x00\\x00s\\x00ﬁ\\x00a\\x00\\x00o\\x00\\x00a\\x00\\x00s\\x00\\x00o\\x00\\x00n\\x00\\x00a\\x00\\x00e\\x00\\x00n\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00h\\x00\\x00ﬀ\\x00c\\x00\\x00f\\x00e\\x00\\x00l\\x00\\x00i\\x00\\x00t\\x00\\x00n\\n\\x00i\\x00\\x00\\x00r\\x00\\x00u\\x00\\x00\\x00i\\x00\\x00l\\x00\\x00\\x00e\\x00\\x00s\\x00\\x00n\\x00o\\x00\\x00d\\x00\\x00i\\x00\\x00.\\nOfcourse,theabovestatementsareindeedcorrect,andIamnotdenyingthat.In\\nfact,wecanalsoverifythisfromthediagrambelow:\\nIntheimageabove,aswemovetotheright,theregularizationparameter\\nincreases,andthemodelcreatesasimplerdecisionboundaryonall5datasets.\\n414'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 415, 'page_label': '416', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00n\\x00\\x00a\\x00\\x00\\x00o\\x00h\\x00\\x00o\\x00\\x00c…\\nHowever,whatdisappointsmethemostisthatmostresourcesdon’tpointout\\nthatL2regularizationisagreatremedyformulticollinearity.\\nMulticollinearityariseswhentwo(ormore)featuresarehighlycorrelatedORtwo\\n(ormore)featurescanpredictanotherfeature:\\nWhenweuseL2regularizationinlinearregression,thealgorithmisalsocalled\\nRidgeregression.\\nButhowdoesL2regularizationeliminatemulticollinearity?\\nInthischapter,let’sbuildademonstrativeintuitionintothistopic,whichwill\\nalsoexplainwhy“ridgeregression”iscalled“ridgeregression.”\\n415'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 416, 'page_label': '417', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00y\\x00a\\x00\\x00s\\x00\\x00\\nFordemonstrationpurposes,considerthisdummydatasetoftwofeatures:\\nAsshownabove,wehaveintentionallymadefeatureBhighlycorrelatedwith\\nfeatureA.Thisgivesusadummydatasettoworkwith.\\n\\x00o\\x00\\x00g\\x00h\\x00\\x00\\x00,\\x00e\\x00h\\x00\\x00l\\x00e\\x00g\\x00\\x00r\\x00\\x00g\\x00n\\x00\\x00n\\x00\\x00r\\x00\\x00\\x00t\\x00e\\x00\\x00\\x00o\\x00\\x00i\\x00\\x00l\\x00\\x00i\\x00\\x00.\\n\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00\\x00s\\x00\\x00o\\x00\\x00i\\x00\\x00o\\x00\\x00\\x002\\x00e\\x00\\x00l\\x00\\x00\\nDuringregressionmodeling,thegoalistodeterminethosespeciﬁcparameters\\n(θ₁,θ₂),whichminimizestheresidualsumofsquares(RSS):\\nSohowaboutwedothefollowing:\\n● WeshallplottheRSSvalueformanydiﬀerentcombinationsof(θ₁,θ₂)\\nparameters.Thiswillcreatea3Dplot:\\n○ x-axis→θ₁\\n○ y-axis→θ₂\\n○ z-axis→RSSvalue\\n● Next,weshallvisuallyassessthisplottolocatethosespeciﬁcparameters\\n(θ₁,θ₂)thatminimizetheRSSvalue.\\n416'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 417, 'page_label': '418', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’sdothis.\\nWithouttheL2penalty,wegetthefollowingplot(it’sthesameplotbutviewed\\nfromdiﬀerentangles):\\nDidyounoticesomething?\\nThe3Dplothasavalley.Therearemultiplecombinationsofparametervalues(θ₁,\\nθ₂)forwhichRSSisminimum.\\nThus,obtainingasinglevaluefortheparameters(θ₁,θ₂)thatminimizetheRSSis\\nimpossible.\\n\\x00i\\x00\\x00a\\x00\\x00e\\x00\\x00\\x00s\\x00\\x00o\\x00\\x00i\\x00\\x00\\x002\\x00e\\x00\\x00l\\x00\\x00\\nWhenusinganL2penalty,thegoalistominimizethefollowing:\\nCreatingthesameplotagainaswedidabove,wegetthefollowing:\\n417'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 418, 'page_label': '419', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nDidyounoticesomethingdiﬀerentthistime?\\nAsdepictedabove,usingL2regularizationremovesthevalleywesawearlierand\\nprovidesaglobalminimatotheRSSerror.\\nNow,obtainingasinglevaluefortheparameters(θ₁,θ₂)thatminimizestheRSSis\\npossible.\\nOutofnowhere,L2regularizationhelpeduseliminatemulticollinearity.\\n\\x00h\\x00\\x00h\\x00\\x00a\\x00\\x00\\x00r\\x00\\x00g\\x00\\x00e\\x00\\x00\\x00s\\x00\\x00o\\x00\\x00?\\nInfact,thisiswhere“ridgeregression”alsogetsitsnamefrom—iteliminates\\ntheridgeinthelikelihoodfunctionwhentheL2penaltyisused.\\n418'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 419, 'page_label': '420', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nOfcourse,inthedemonstrationswediscussedearlier,wenoticedavalley,nota\\nridge.\\nHowever,inthatcase,wewereconsideringtheresidualsumoferror—\\nsomethingwhichisminimizedtoobtaintheoptimalparameters.\\nThus,theerrorfunctionwillobviouslyresultinavalley.\\nIfweweretouselikelihoodinstead—somethingwhichismaximized,itwould\\n(somewhat)invertthegraphupsidedownandresultinaridgeinstead:\\nApparently,whilenamingthealgorithm,thelikelihoodfunctionwasconsidered.\\nAndthatiswhyitwasnamed“ridgeregression.”\\nWhenIﬁrstlearnedaboutthissomeyearsback,Iliterallyhadnoideathatsuch\\ndeepthoughtwentintonamingridgeregression.\\n419'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 420, 'page_label': '421', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00e\\x00\\x00a\\x00\\x00o\\x00\\x00\\n\\x00s\\x00o\\x00\\x00\\x00o\\x00\\x00l\\x00a\\x00\\x00\\x00eﬁ\\x00i\\x00\\x00t\\x00\\nDuringmodeldevelopment,manypeopleﬁndthemselvesinsituationswhere,no\\nmatterhowmuchtheytry,themodelperformancebarelyimproves:\\n● Featureengineeringisgivingmarginalimprovement.\\n● Tryingdiﬀerentmodelsdoesnotproducesatisfactoryresultseither.\\n● andmore…\\nThisisusually(notalways)anindicatorthatthemodelisdatadeﬁcient.\\n420'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 421, 'page_label': '422', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nInotherwords,wedon’thaveenoughdatatoworkwith.\\nHowever,gatheringnewdatacanbeatime-consumingandtediousprocess.So\\nbeforeventuringintothatdirection,itwouldbegoodtogetsomeinsightsabout\\nwhethernewdatawillhelp.Here’satrickIhaveo\\x00enusedtodeterminethis.\\n\\x00a\\x00\\x00\\x00u\\x00\\x00\\x00t\\x00\\x00n\\x00\\x00n\\x00\\x00o\\x00\\x00l\\x00u\\x00\\x00d\\x00\\x00g\\nLet’ssaythisisyourfulltrainingandvalidationset:\\nDividethetrainingdatasetinto“k”equalparts.Thevalidationsetremainsasis.\\n“k”doesnothavetobesuperlarge.Anynumberbetween7to12isﬁne\\ndependingonhowmuchdatayouhave.Ifthere’splentyofdata,settingalow\\nvalueinthisrangeisrecommended(youwillunderstandwhyshortly).\\n421'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 422, 'page_label': '423', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNext,trainmodelscumulativelyontheabovesubsetsandmeasuretheir\\nperformanceonthevalidationset:\\n● Trainamodelontheﬁrstsubsetandevaluatethevalidationset.\\n● Trainamodelontheﬁrsttwosubsetsandevaluatethevalidationset.\\n● Trainamodelontheﬁrstthreesubsetsandevaluatethevalidationset.\\n● Andsoon…\\nPlottingthevalidationperformanceofthesemodels(inorderofincreasing\\ntrainingdata)islikelytoproducetwotypesoflines:\\n422'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 423, 'page_label': '424', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● LineAconveysthataddingmoredataislikelytoincreasemodel\\nperformance.\\n● LineBconveysthatthemodelperformancehasalreadysaturated.Adding\\nmoredatawillmostlikelynotresultinanyconsiderablegains.\\nNow,youmightalsounderstandwhyImentionedthisabove:“Ifthere’splentyof\\ndata,settingalowvalueinthisrangeisrecommended.”\\n● Becausewetrainmultiplemodels,settingahighvalueof“k”meansmore\\nsubsets,whichinturnmeansmoremodels.\\nThisway,youcandeterminewhetherthemodelisdatadeﬁcientandwhether\\ngatheringdatawillbehelpfulornot.\\n423'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 424, 'page_label': '425', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00s\\x00\\x00n\\x00p\\x00\\x00m\\x00\\x00a\\x00\\x00o\\x00\\nHyperparametertuningisatediousandtime-consumingtaskintrainingML\\nmodels.\\nTypically,weusetwocommonapproachesforthis–GridsearchandRandom\\nsearch.\\nButtheyhavemanylimitations.Forinstance:\\n● Gridsearchperformsanexhaustivesearchoverallcombinations.Thisis\\ncomputationallyexpensive.\\n● Gridsearchandrandom\\nsearcharerestrictedtothe\\nspeciﬁedhyperparameter\\nrange.Yet,theideal\\nhyperparametermayexist\\noutsidethatrange.\\n● TheycanONLYperformdiscretesearches,evenifthehyperparameteris\\ncontinuous.\\nTothisend,BayesianOptimizationisahighlyunderappreciatedyetimmensely\\npowerfulapproachfortuninghyperparameters.\\n424'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 425, 'page_label': '426', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nItusesBayesianstatisticstoestimatethedistributionofthebest\\nhyperparameters.Here’showitdiﬀersfromGridsearchandRandomSearch:\\nBothGridsearchandRandomSearchevaluateeveryhyperparameter\\nconﬁgurationindependently.Thus,theyiterativelyexploreallhyperparameter\\nconﬁgurationstoﬁndthemostoptimalone.\\nHowever,Bayesian\\nOptimizationtakesinformed\\nstepsbasedontheresultsofthe\\nprevioushyperparameter\\nconﬁgurations.Thisletsit\\nconﬁdentlydiscardnon-optimal\\nconﬁgurations.Consequently,\\nthemodelconvergestoan\\noptimalsetofhyperparameters\\nmuchfaster.Theeﬃcacyof\\nBayesianOptimizationis\\nevidentfromtheimage.\\nBayesianoptimizationleadsthemodeltothesameF1scorebut:\\n● ittakes7xfeweriterations\\n● itexecutes5xfaster\\n● itreachestheoptimalconﬁgurationearlier\\nPrettycool,isn’tit?\\n425'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 426, 'page_label': '427', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00n\\x00n\\x00\\x00e\\x00\\x00-\\x00\\x00m\\x00\\x00a\\x00\\x00\\x00u\\x00\\x00e\\x00\\x00a\\x00\\x00o\\x00\\nDataaugmentationstrategiesaretypicallyusedduringtrainingtime.\\nTheideaistousesomeclevertechniquestocreatemoredatafromexistingdata,\\nwhichisespeciallyusefulwhenyoudon’thavemuchdatatobeginwith:\\nLetmegiveyouanexample.\\nThesedays,language-relatedMLmodelshavebecomequiteadvancedand\\ngeneral-purpose.Thesamemodelcantranslate,summarize,identifyspeechtags\\n(nouns,adjectives,etc.),andmuchmore.\\nButearlier,modelsusedtobetask-speciﬁc(wehavethemnowaswell,butthey\\narefewerthanweusedtohavebefore).\\n● Adedicatedmodelthatwouldtranslate.\\n● Adedicatedmodelthatwouldsummarize,etc.\\nInoneparticularusecase,Iwasbuildinganamedentityrecognition(NER)\\nmodel,andtheobjectivewastoidentifynamedentities.\\n426'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 427, 'page_label': '428', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAnexampleisshownbelow:\\nIhadminimaldata—around8-10klabeledsentences.Thedatasetwasthe\\nCoNLL2003NERdatasetifyouknowit.\\nHere’showIapproacheddataaugmentationinthiscase.\\nOb\\x00e\\x00v\\x00\\x00\\x00o\\x00:InNE\\x00,t\\x00efa\\x00\\x00\\x00\\x00lco\\x00\\x00\\x00c\\x00\\x00\\x00s\\x00oft\\x00ese\\x00\\x00\\x00n\\x00\\x00\\x00do\\x00\\x00no\\x00ma\\x00\\x00\\x00r.\\nRevisitingtheaboveexample,itwouldnothavematteredifIhadthefollowing\\nsentenceinthetrainingdata:\\nThesentenceisfactuallyincorrect,ofcourse,butthatdoesnotmatter.\\nTheonlythingthatmatterstothemodelisthattheoutputlabels(namedentity\\ntagsinthiscase)mustbecorrect.\\nSousingthisobservation,Icreatedmanymoresentencesbyreplacingthenamed\\nentitiesinanexistingsentencewithothernamedentitiesinthewholedataset:\\n427'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 428, 'page_label': '429', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForsuchsubstitutions,Icouldhaveusednamedentitiesfromoutside.However,\\nitwasimportanttoestablishafaircomparisonwithotherapproaches.\\nThistechnique(alongwithacouplemorearchitecturaltweaks)resultedin\\nstate-of-the-artperformance.\\nMovingon…\\nTheabovediscussionwasabouttrainingdataaugmentation.\\nButthere’salsotest-timeaugmentation.\\nTestTimeAugmentation(TTA)iswhenweapplydataaugmentationduring\\ntesting.\\nMorespeciﬁcally,insteadofshowingjustonetestexampletothemodel,we\\nshowmultipleversionsofthetestexamplebyapplyingdiﬀerentoperations.\\nThemodelmakesprobabilitypredictionsforeveryversionofthetestexample,\\nwhicharethenaveragedtogeneratetheﬁnalprediction:\\n428'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 429, 'page_label': '430', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nEssentially,TTAcreatesanensembleofpredictionsbyconsideringmultiple\\naugmentedversionsofthesameinput,whichleadstoamorerobustﬁnal\\nprediction.\\nInfact,inthispaper(https://arxiv.org/html/2402.06892v1),theauthorsprovedthat\\ntheaveragemodelerrorwithTTAneverexceedstheaverageerroroftheoriginal\\nmodel,whichisgreat.\\nAsyoumayhaveguessed,theonlycatchisthatitincreasestheinferencetime.\\n● Dataaugmentationtakessometime.\\n● Generatingmultiplepredictionsincreasestheoverallpredictionrun-time.\\nSo,whenalowinferencetimeisimportanttoyou,thinktwiceaboutTTA.\\nTosummarize,ifyoucancompromiseabitoninferencetime,TTAcanbea\\npowerfulwaytoimprovepredictionsfromanexistingmodelwithouthavingto\\nengineerabettermodel.\\n429'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 430, 'page_label': '431', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00\\x00n\\x00\\x00y\\x00\\x00s\\n\\x005\\x00a\\x00\\x00a\\x00↔\\x00o\\x00\\x00r\\x00↔\\x00Q\\x00↔\\x00y\\x00\\x00a\\x00\\x00\\x00r\\x00\\x00s\\x00\\x00t\\x00\\x00n\\x00\\nThefollowingvisualdepictsthe15mostcommontabularoperationsinPandas\\nandtheircorrespondingtranslationsinSQL,Polars,andPySpark.\\nWhilethemotivationforPandasandSQLisclearandwell-known,letmetellyou\\nwhyyoushouldcareaboutPolarsandPySpark.\\n430'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 431, 'page_label': '432', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00o\\x00\\x00r\\x00\\x00\\nPandashasmanylimitations,whichPolarsaddresses,suchas:\\n● Pandasalwaysadherestosingle-corecomputation→Polarsismulti-core.\\n● Pandasoﬀersnolazyexecution→Polarsdoes.\\n● PandascreatesbulkyDataFrames→Polars’DFsarelightweight.\\n● Pandasisslowonlargedatasets→Polarsisremarkablyeﬃcient.\\nInfact,ifwe\\nlookatthe\\nrun-time\\ncomparisonon\\nsomecommon\\noperations,it’s\\nclearthatPolars\\nismuchmore\\neﬃcientthan\\nPandas.\\n431'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 432, 'page_label': '433', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00p\\x00\\x00k\\x00\\nWhiletabulardataspaceismainlydominatedbyPandasandSklearn,onecan\\nhardlyexpectanybeneﬁtfromthembeyondsomeGBsofdataduetotheir\\nsingle-nodeprocessing.\\nAmorepracticalsolutionistousedistributedcomputinginstead—aframework\\nthatdispersesthedataacrossmanysmallcomputers.\\nSparkisamongthebesttechnologiesusedtoquicklyandeﬃcientlyanalyze,\\nprocess,andtrainmodelsonbigdatasets.\\nThatiswhymostdatasciencerolesatbigtechdemandproﬁciencyinSpark.It’s\\nthatimportant\\n432'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 433, 'page_label': '434', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00l\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00\\x00o\\x00a\\x00\\x00a\\x00\\x00\\x00e\\x00\\x00r\\x00\\x00e\\nProbablytheﬁrst(orsecond)thingIdowhenIloadanyPandasorPolars\\nDataFrameisdescribeit,usingthedf.describe()method.However,Ialwaysﬁnd\\nitsoutputtobeprettynaiveandalmostofnouse.Inotherwords,ithardly\\nhighlightsanykeyinformationaboutthedata.\\nButsometimeback,IcameacrosstwoprettycoollibrariesthatIMMENSELY\\nsuperchargethisDataFramesummary.Sincethen,Idon’tthinkIhaveeverused\\nthedescribe()method.\\n\\x001\\x00\\x00k\\x00\\x00p\\x00\\nItisaJupyter-based\\ntoolthatprovidesa\\nstandardizedand\\ncomprehensivedata\\nsummary.\\nThisincludesdata\\nshape,columndata\\ntypes,column\\nsummarystatistics,\\ndistributioncharts,\\nmissingstats,etc.:\\n433'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 434, 'page_label': '435', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWhat’smore,thesummaryisgroupedbydatatypesforfasteranalysis.Thisisthe\\ncodetouseSkimpy:\\nOnethingIreallyloveaboutSkimpyisthatitworksseamlesslywithPolars,\\nwhichIhavestartedusingmoreo\\x00enthanIusePandasthesedays.\\n\\x002\\x00\\x00u\\x00\\x00a\\x00\\x00T\\x00\\x00l\\x00\\nThesecondoneis\\nSummaryTools,which\\ndoesalmosttheexact\\nsamethingasSkimpy,\\ni.e.,itgeneratesa\\nstandardizedreport:\\n434'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 435, 'page_label': '436', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisthecodetouseSummaryTools:\\nTwoprettycoolthingsaboutSummaryToolsarethatitcancreate:\\n1. Acollapsiblesummaryofthedataset,asillustratedbelow:\\n2. Atabbedsummaryofthedataset,asshownbelow:\\nTheonlythingIdon’tlikeaboutSummaryToolsisthatitisnotcompatiblewith\\nPolars(yet).Nonetheless,Iﬁndbothofthemextremelypromisingfor\\nunderstandingmydatasetwithmoregranularitythanPandas’\\x00e\\x00\\x00r\\x00\\x00e\\x00\\x00\\nmethod.\\nTrybydownloadingthisJupyternotebook:https://bit.ly/45Rheq1.\\n435'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 436, 'page_label': '437', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00c\\x00\\x00l\\x00\\x00a\\x00\\x00\\x00a\\x00\\x00a\\x00\\x00i\\x00\\x00\\x00P\\x00\\x00s\\x00\\x00g\\x00A\\x00\\x00D\\x00\\x00u\\x00\\x00\\nTwoofthebiggestproblemswithPandasisthat:\\n● Italwaysadherestoasingle-corecomputationonaCPU.\\n● ItcreatesbulkyDataFrames.\\nWhilemanylibraries(Polars,forinstance)doaddresstheselimitations,theyare\\nstilllimitedtoCPU-drivencomputations.\\nNVIDIA’sRAPIDScuDFlibraryallowsPandasuserstosuperchargetheirPandas\\nworkﬂowwithGPUs.\\nHowtouseit?\\nWithinaGPUruntime,dothefollowing:\\n● Loadtheextension:\\x00l\\x00\\x00d\\x00\\x00x\\x00\\x00u\\x00\\x00.\\x00\\x00n\\x00\\x00\\x00\\n● ImportPandas: \\x00m\\x00\\x00r\\x00\\x00a\\x00\\x00a\\x00\\x00s\\x00d\\nDone!UsePandas’methodsasyouusuallywould.\\n436'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 437, 'page_label': '438', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nJustloadingthe\\nextensionprovides\\nimmensespeedups.\\nAsperNVIDIA’soﬃcialrelease,thiscanbeasfastas150x.Inmypersonal\\nexperimentation,however,Imostlyobservedittorangebetween50-70x,whichis\\nstillprettygood.\\nThegoodthingisthattheextensionacceleratesmostPandas’methods.Yet,if\\nneeded,itcanautomaticallyfallbacktotheCPU.\\n\\x00o\\x00\\x00o\\x00\\x00\\x00t\\x00o\\x00\\x00?\\nWhenever\\x00u\\x00\\x00.\\x00\\x00n\\x00\\x00\\x00isenabled,theimportpandasaspdstatementdoesnot\\nimporttheoriginalPandaslibrarywhichweuseallthetime.Instead,itimports\\nanotherlibrarythatcontainsGPU-acceleratedimplementationsofallPandas\\nmethods.\\nThisalternativeimplementationpreservestheentiresyntaxofPandas.Soifyou\\nknowPandas,youalreadyknowhowtousecuDF’sPandas.\\nYoucanﬁndthecodehere:https://bit.ly/4cOAPZW.\\n437'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 438, 'page_label': '439', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00\\x00n\\x00\\x00y\\x00\\x00s\\x00i\\x00\\x00\\x00e\\x00\\x00m\\x00\\x00s\\nReal-worlddatasets\\nalmostalwayshave\\nmissingvalues.Inmost\\ncases,itisunknowntous\\nbeforehandwhyvalues\\naremissing.\\nButit’sgoodtoknowthattherecouldbemultiplereasonsformissingvalues:\\n● MissingCompletelyatRandom(MCAR):Thevalueisgenuinelymissingby\\nitselfandhasnorelationtothatoranyotherobservation.\\n● MissingatRandom(MAR):Dataismissingduetoanotherobserved\\nvariable.Forinstance,wemayobservethatthepercentageofmissing\\nvaluesdiﬀerssigniﬁcantlybasedonothervariables.\\n● MissingNOTatRandom(MNAR):Thisoneistricky.MNARoccurswhen\\nthereisadeﬁnitepatterninthemissingvariable.However,itisunrelated\\ntoanyfeaturewecanobserveinourdata.Infact,thismaydependonan\\nunobservedfeature.\\nAndidentifyingthereasonformissingnesscanbeextremelyusefulforfurther\\nanalysis,imputation,andmodeling.\\nConsiderwehaveadailysalesdatasetofastorethathasthefollowing\\ninformation:\\n438'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 439, 'page_label': '440', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● DayandDate\\n● Storeopeningandclosingtime\\n● Numberofcustomers\\n● Totalsales\\n● Accountbalanceatopenandclosetime\\nThereasonformissingvaluesisunknowntous.Here,whendoingEDA,many\\nfolkscomputethecolumn-wisemissingfrequencyasfollows:\\nTheabovetablejusthighlightsthenumberofmissingvaluesineachcolumn.\\nMorespeciﬁcally,wegettoknowthat:\\n● Missingvaluesarerelativelyhighintwocolumnscomparedtoothers.\\n● Missingvaluesintheopeningandclosingtimecolumnsarethesame(53).\\nThat’stheonlyinfoitprovides.However,theproblemwiththisapproachisthat\\nithidesmanyimportantdetailsaboutmissingvalues,suchas:\\n● Theirspeciﬁclocationinthedataset.\\n● Periodicityofmissingvalues(ifany).\\n● Missingvaluecorrelationacrosscolumns,etc.\\n…whichcanbeextremelyusefultounderstandthereasonformissingness.\\nToputitanotherway,theabovetableismorelikesummarystatistics,which\\nrarelydepictthetruepicture.\\nWhy?\\n439'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 440, 'page_label': '441', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWediscussedthisinthestatisticssectionofthisbook.\\nButhere’showIo\\x00enenrichmymissingvalueanalysiswithheatmaps.Compare\\nthemissingvaluetablewediscussedabovewiththefollowingheatmapof\\nmissingvalues:\\n440'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 441, 'page_label': '442', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThewhiteverticallinesdepictthelocationofmissingvaluesinaspeciﬁc\\ncolumn.\\nNow,itisimmediatelyclearthat:\\n● Valuesareperiodicallymissingintheopeningandclosingtimecolumns.\\n● Missingvaluesarecorrelatedintheopeningandclosingtimecolumns.\\n● Themissingvaluesinothercolumnsappeartobe(notnecessarilythough)\\nmissingcompletelyatrandom.\\nFurtheranalysisoftheopeningtimeletsusdiscoverthatthestorealways\\nremainsclosedonSundays:\\nNow,weknowwhytheopeningandclosingtimesaremissinginourdataset.\\nThisinformationcanbebeneﬁcialduringitsimputation.\\nThisspeciﬁcsituationis“MissingatRandom(MAR).”Essentially,aswesaw\\nabove,themissingnessisdrivenbythevalueofanotherobservedcolumn.\\nAsweknowthereason,wecanuseeitherthekNNimputationortheMissForest\\ntechniquestoimputethesevalues,whichwediscussedintheMLsectionofthis\\nbook(\\x00i\\x00\\x00F\\x00\\x00e\\x00\\x00\\x00n\\x00\\x00N\\x00\\x00m\\x00\\x00t\\x00\\x00i\\x00\\x00).\\n441'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 442, 'page_label': '443', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content=\"DailyDoseofDS.com\\n\\x00a\\x00\\x00F\\x00\\x00m\\x00\\x00t\\x00\\x00i\\x00\\x00\\nInsteadof\\npreviewingraw\\nDataFrames,styling\\ncanmakedata\\nanalysismucheasier\\nandfaster.\\nHere'show.\\nJupyterisa\\nweb-basedIDE.\\nAnythingyouprintis\\nrenderedusing\\nHTMLandCSS.\\nThismeansyoucan\\nstyleyouroutputin\\nmanydiﬀerentways.\\nTostylePandasDataFrames,useitsStylingAPI(𝗱𝗳.𝘀𝘁𝘆𝗹𝗲).\\nAsaresult,theDataFrameisrenderedwiththespeciﬁedstyling.\\nPandasdocumentationpageonStyling:https://bit.ly/3zDmxgE.\\n442\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 443, 'page_label': '444', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00u\\x00\\x00m\\x00\\x00e\\x00\\x00D\\x00\\x00o\\x00\\x00s\\nBeloware8powerfulEDAtoolsthatautomatemanyredundantEDAstepsand\\nhelpyouproﬁleyourdataquickly.\\nPleasenotethatthesetoolsarenottheultimateEDAalternativesthatwill\\nanswerallyourquestionsaboutthedataset.\\nButgiventhatthepreliminaryEDAstepsinalmostallprojectsarethesame—\\nplottingtheresponsevariable,checkingimbalance,runningcorrelationanalysis,\\nmissingvalueanalysis,andmore,thesetoolsprettywellautomatethesestepsin\\nmyopinion.\\nAlso,attimes,manualEDAcanbepronetohumanerrorsandonemaymissout\\noncheckingafewthings.Automatedtoolseliminatetheserisksandprovidea\\nstandardizedreportacrossallprojects.\\nFullsummaryabouteachtoolisavailablehere:https://bit.ly/3xZP4fK.\\n443'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 444, 'page_label': '445', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00\\x00i\\x00\\x00a\\x00\\x00s\\x00\\x00i\\x00\\x00\\n\\x00o\\x00\\x00\\x00m\\x00\\x00r\\x00\\x00n\\x00\\x00l\\x00\\x00s\\x00n\\x00a\\x00\\x00\\x00c\\x00\\x00n\\x00\\x00\\nThevisualbelowdepictsthe11mostimportantandmust-knowplotsindata\\nscience:\\n444'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 445, 'page_label': '446', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’sunderstandthembrieﬂyandhowtheyareused.\\n\\x00S\\x00l\\x00\\x00:\\n● Itisusedtoassessthedistributionaldiﬀerences.\\n● Thecoreideaistomeasurethemaximumdistancebetweenthecumulative\\ndistributionfunctions(CDF)oftwodistributions.\\n● Thelowerthemaximumdistance,themorelikelytheybelongtothesame\\ndistribution.\\n● Thus,insteadofa“plot”,itismainlyinterpretedasa“statisticaltest”to\\ndeterminedistributionaldiﬀerences.\\n\\x00H\\x00\\x00\\x00l\\x00\\x00:\\n● Itsummarizesfeatureimportancetoamodel’spredictionsbyconsidering\\ninteractions/dependenciesbetweenthem.\\n445'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 446, 'page_label': '447', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Itisusefulindetermininghowdiﬀerentvalues(loworhigh)ofafeature\\naﬀecttheoveralloutput.\\n\\x00O\\x00\\x00u\\x00\\x00e\\x00\\n● Itdepictsthetradeoﬀbetweenthetruepositiverate(goodperformance)\\nandthefalsepositiverate(badperformance)acrossdiﬀerentclassiﬁcation\\nthresholds.\\n● TheideaistobalanceTPR(goodperformance)vs.FPR(badperformance).\\n\\x00r\\x00\\x00i\\x00\\x00o\\x00\\x00R\\x00\\x00a\\x00\\x00\\x00u\\x00\\x00e\\x00\\n● ItdepictsthetradeoﬀbetweenPrecisionandRecallacrossdiﬀerent\\nclassiﬁcationthresholds.\\n446'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 447, 'page_label': '448', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00Q\\x00l\\x00\\x00:\\n● Itassessesthedistributionalsimilaritybetweenobserveddataand\\ntheoreticaldistribution.\\n● Itplotsthequantilesofthetwodistributionsagainsteachother.\\n● Deviationsfromthestraightlineindicateadeparturefromtheassumed\\ndistribution.\\n\\x00u\\x00\\x00l\\x00\\x00i\\x00\\x00\\x00x\\x00\\x00a\\x00\\x00e\\x00\\x00a\\x00\\x00a\\x00\\x00e\\x00l\\x00\\x00:\\n● Itisusefulindeterminingthenumberofdimensionswecanreduceour\\ndatatowhilepreservingmaxvarianceduringPCA.\\n447'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 448, 'page_label': '449', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00l\\x00\\x00w\\x00u\\x00\\x00e\\x00\\n● Theplothelpsidentifytheoptimalnumberofclustersforthek-means\\nalgorithm.\\n● Thepointoftheelbowdepictstheidealnumberofclusters.\\n\\x00i\\x00\\x00o\\x00\\x00t\\x00\\x00\\x00u\\x00\\x00e\\x00\\n● TheElbowcurveiso\\x00enineﬀectivewhenyouhaveplentyofclusters.\\n● SilhouetteCurveisabetteralternative,asdepictedabove.\\n448'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 449, 'page_label': '450', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00-\\x00\\x00p\\x00\\x00i\\x00\\x00\\x00n\\x00\\x00n\\x00\\x00o\\x00\\x00:\\n● Theyareusedtomeasuretheimpurityordisorderofanodeorsplitina\\ndecisiontree.\\n● TheplotcomparesGiniimpurityandEntropyacrossdiﬀerentsplits.\\n● Thisprovidesinsightsintothetradeoﬀbetweenthesemeasures.\\n\\x00i\\x00\\x00-\\x00\\x00r\\x00\\x00\\x00c\\x00\\x00r\\x00\\x00e\\x00ﬀ\\x00\\n● It’sprobablythemostpopularplotonthislist.\\n● Itisusedtoﬁndtherightbalancebetweenthebiasandthevarianceofa\\nmodelagainstcomplexity.\\n449'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 450, 'page_label': '451', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00i\\x00\\x00\\x00e\\x00\\x00n\\x00\\x00\\x00c\\x00\\x00l\\x00\\x00s\\x00\\n● Depictsthedependencebetweentargetandfeatures.\\n● Aplotbetweenthetargetandonefeatureforms→1-wayPDP.\\n● Aplotbetweenthetargetandtwofeatureforms→2-wayPDP.\\n● Inthele\\x00mostplot,anincreaseintemperaturegenerallyresultsina\\nhighertargetvalue.\\n450'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 451, 'page_label': '452', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00r\\x00\\x00Q\\x00l\\x00\\x00s\\x00r\\x00\\x00t\\x00\\x00?\\nAQQplotisagreat\\nwaytovisuallyassess\\nthesimilaritybetween\\ntwodistributions.\\nItdoesthisbyplottingthequantilesofthetwodistributionsagainsteachother.\\nThedeviationsfromthestraightlineindicatethediﬀerencesbetweenthetwo\\ndistributions.Thefollowingvisualdepictshowitiscreated:\\n451'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 452, 'page_label': '453', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nLet’sdiscussitinmoredetail.\\nConsiderwehavetwodistributions,D1andD2.\\nStep1)Arrangepointsonaxes:\\nAsshownbelow,wearrangepointsofD1onthey-axisandD2onthex-axis.\\nStep2)Drawpercentilelines\\nNext,forbothdistributions,wecreatesomepercentilelines.\\nForinstance,onbothaxes,wecanmarkthepointsof10thpercentile,20th\\npercentile,30thpercentile,etc.,frombothdistributions.Thisisshownbelow:\\nWemarkthepercentilelocationsforbothdistributionsandintersectthe\\ncorrespondinglines.\\n● 10thpercentileofD1isintersectedwith10thpercentileofD2.\\n● 20thpercentileofD1isintersectedwith20thpercentileofD2.\\n● andsoon.\\n452'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 453, 'page_label': '454', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheintersectionpointsofthesepercentilelinesgivesusthepointswetypically\\nseeinaQQplot:\\nNow,wecangetridofthepercentilemarkerlines.\\nInagist,theaboveplotgivesusthelocationwherethecorrespondingpercentiles\\nofthetwodistributionsmatch.\\nStep3)Addthereferenceline\\nFinally,wemustaddareferencelinetodeterminethedeviationsbetweenthetwo\\ndistributions.\\nTherearemanywaystodothis.\\n453'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 454, 'page_label': '455', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance:\\n● Thelineconnectingthe25thand75thpercentilesofbothdistributionscan\\nbeconsideredasareferenceline.\\n● Theregressionﬁtontheabovescatterplotcanbeconsideredasa\\nreferenceline.\\nTypically,thelineconnectingthe25-75thpercentileispreferredbecausethe\\nregressionﬁtcanbeinﬂuencedbyoutliers.\\nA\\x00eraddingthereferenceline,wegetourQQplot:\\nThedeviationsfromthisreferencelineindicatethatthetwodistributionsdiﬀer\\nfromeachother.Inotherwords,thedeviationsmeanthatthecorresponding\\npercentilesdonotalign.\\nThisbecomesanindicatorofdistributionaldissimilarities.\\n454'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 455, 'page_label': '456', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAnd,ofcourse,themorepercentilesweplot,thebetterandmoreaccuratethe\\nQQplotwillbe.\\nTherearemanyapplicationsoftheQQplot.\\nForinstance,saywehaveanobserveddistributionandwanttodetermineifit\\nresemblesanormaldistribution.\\nWecanuseaQQplotforthis:\\n● D1:Theobserveddistribution\\n● D2:Normaldistribution.\\nIfthepercentilepointslieclosertothereferenceline,thiswouldmeanthatthe\\nobserveddistributionismorelikeanormaldistribution.Thisisdepictedbelow:\\n455'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 456, 'page_label': '457', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00l\\x00\\x00a\\x00\\x00\\x00l\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00\\x00o\\x00r\\x00\\x00i\\x00\\x00o\\x00\\x00l\\x00l\\x00\\x00s\\nScatterplots,barplots,lineplots,boxplots,andheatmapsarethemost\\nfrequentlyusedplotsfordatavisualization.Althoughtheyaresimpleandknown\\ntoalmosteveryone,Ibelievetheyarenottherightchoicetocovereverypossible\\nscenario.\\nInstead,manyotherplotsoriginatefromthesestandardplotsthatcanbemuch\\nmoresuitable,ifusedappropriately.Inthischapter,let’sdiscussafew\\nalternativestothesepopularplots.Wewillalsounderstandspeciﬁcsituations\\nwheretheycanbemoreusefuloverstandardplots.\\nThisc\\x00a\\x00t\\x00\\x00isno\\x00in\\x00\\x00\\x00d\\x00\\x00todi\\x00\\x00\\x00\\x00ra\\x00\\x00t\\x00eus\\x00oft\\x00e\\x00\\x00t\\x00a\\x00\\x00t\\x00o\\x00\\x00\\x00p\\x00o\\x00s.The\\x00wi\\x00\\x00\\nal\\x00\\x00\\x00sha\\x00\\x00t\\x00e\\x00\\x00p\\x00a\\x00\\x00.\\nIn\\x00\\x00e\\x00d,itistohi\\x00\\x00l\\x00\\x00\\x00ts\\x00e\\x00\\x00ﬁcsi\\x00\\x00\\x00\\x00i\\x00n\\x00w\\x00e\\x00\\x00t\\x00e\\x00ca\\x00re\\x00\\x00\\x00c\\x00\\x00wi\\x00\\x00be\\x00\\x00\\x00rp\\x00o\\x00t\\x00\\x00\\x00\\nid\\x00\\x00\\x00.\\n\\x001\\x00\\x00i\\x00\\x00-\\x00\\x00c\\x00\\x00e\\x00\\x00e\\x00\\x00m\\x00\\x00s\\nAtraditionalheatmap\\nrepresentsthevalues\\nusingacolorscale.Yet,\\nmappingthecellcolor\\ntoexactnumbersis\\nstillchallenging.\\nEmbeddingasize\\ncomponentto\\nheatmapscanbe\\nextremelyhelpfulin\\nsuchcases.Inessence,\\nthebiggerthesize,the\\nhighertheabsolute\\nvalue\\n456'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 457, 'page_label': '458', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisisespeciallyusefultomakeheatmapscleaner,asmanyvaluesnearertozero\\nwillimmediatelyshrink.\\n\\x002\\x00\\x00a\\x00\\x00r\\x00\\x00l\\x00\\x00h\\x00\\x00t\\x00\\nTovisualizethechangeinvalueovertime,aline(orbar)plotmaynotalwaysbe\\nanaptchoice.\\nThisisbecausealineplot(orbarplot)depictstheactualvaluesinthechart.\\nThus,itisdiﬃculttovisuallyestimatethescaleanddirectionofincremental\\nchanges.\\nInstead,youcanuse\\nawaterfallchart.It\\nelegantlydepicts\\ntheserolling\\ndiﬀerences,as\\ndepictedinthis\\nimage.\\nHere,thestartand\\nﬁnalvaluesare\\nrepresentedbythe\\nﬁrstandlastbars.\\nAlso,theconsecutivechangesareautomaticallycolor-coded,makingthemeasier\\ntointerpret.\\n457'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 458, 'page_label': '459', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x003\\x00\\x00u\\x00\\x00\\x00h\\x00\\x00t\\x00\\nWhenvisualizingthechangeinrankovertimeofmultiplecategories,usingabar\\nchartmaynotbeappropriate.\\nThisisbecausebarchartsquicklybecomeclutteredwithmanycategories.\\nInstead,tryBumpCharts.Theyarespeciﬁcallyusedtovisualizetherankof\\ndiﬀerentitemsovertime.\\nComparingthebarchartandbumpchartabove,itisfareasiertointerpretthe\\nchangeinrankwithabumpchartratherthanabarchart.\\n458'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 459, 'page_label': '460', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x004\\x00\\x00a\\x00\\x00c\\x00\\x00u\\x00\\x00l\\x00\\x00s\\nVisualizingdatadistributionsusingboxplotsandhistogramscanbemisleading\\nattimes.Thisisbecause:\\n● Itispossibletogetthesameboxplotwithentirelydiﬀerentdata.\\n● Alteringthenumberofbinschangestheshapeofahistogram.\\nThus,toavoidmisleadingconclusions,itisalwaysrecommendedtoplotthedata\\ndistributionaspreciselyaspossible.Raincloudplotsprovideaconcisewayto\\ncombineandvisualizethreediﬀerenttypesofplotstogether.\\nTheseinclude:\\n● Boxplotsfordatastatistics.\\n● Stripplotsfordataoverview.\\n● KDEplotsfortheprobabilitydistributionofdata.\\n459'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 460, 'page_label': '461', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWithRaincloudplots,youcan:\\n● Combinemultipleplotstopreventincorrect/misleadingconclusions\\n● Reduceclutterandenhanceclarity\\n● Improvecomparisonsbetweengroups\\n● Capturediﬀerentaspectsofthedatathroughasingleplot\\n\\x005\\x00\\x00)\\x00e\\x00\\x00i\\x00\\x00n\\x00\\x00e\\x00\\x00i\\x00\\x00\\x00l\\x00\\x00s\\nScatterplotscangettoodensetointerpretwhenyouhavethousandsofdata\\npoints.\\nInstead,youcanreplacethemwithHexbinplots.\\nHexbinplotsbintheareaofachartintohexagonalregions.Eachregionis\\nassignedacolorintensitybasedonthemethodofaggregationused(thenumber\\nofpoints,forinstance).\\n460'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 461, 'page_label': '462', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAnotherchoiceisadensityplot,whichillustratesthedistributionofpointsina\\ntwo-dimensionalspace.\\nAcontouriscreatedbyconnectingpointsofequaldensity.Inotherwords,a\\nsinglecontourlinedepictsanequaldensityofdatapoints.\\n\\x007\\x00\\x00)\\x00u\\x00\\x00l\\x00\\x00h\\x00\\x00t\\x00\\x00n\\x00\\x00o\\x00\\x00l\\x00\\x00s\\nAsdiscussedabove,barplotsquicklygetmessyandclutteredasthenumberof\\ncategoriesincreases.\\nAbubbleplotiso\\x00enabetteralternativeinsuchcases.\\nTheyarelikescatterplotsbut:\\n● withonecategoricalaxis\\n● andonecontinuousaxis\\n461'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 462, 'page_label': '463', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove:\\n● Itisdiﬃculttointerpretthebarplotbecauseithastoomanybarspacked\\nintoasmallspace,\\n● Butsize-encodedbubblesmakeitprettyeasytovisualizethechangeover\\ntime.\\n462'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 463, 'page_label': '464', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAnotheralternativetobarplotsinsuchsituationsisdotplots.\\nBothdotplotsandbubblechartsarebasedontheideathat,attimes,whenwe\\nhaveabarplotwithmanybars,we’reo\\x00ennotpayingattentiontotheindividual\\nbarlengths.\\nInstead,wemostlyconsidertheindividualendpointsthatdenotethetotalvalue.\\nTheseplotspreciselyhelpusdepictthatwhilealsoeliminatingthelongbarsof\\nlittletonouse.\\n463'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 464, 'page_label': '465', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00r\\x00\\x00t\\x00\\x00e\\x00o\\x00\\x00r\\x00\\x00s\\nWhileusingJupyter,oneo\\x00enﬁndsthemselvesinsituationswherethey\\nrepeatedlymodifyacellandre-rerunit.Thismakesdataexploration\\nirreproducibleandtime-consuming.What’smore,thenotebookalsogetsmessy\\nandcluttered.\\nToaddressthis,oneofthethingsI\\nactivelyleverageinmyJupyter\\nnotebooksisinteractivecontrols\\nusing\\x00P\\x00\\x00i\\x00\\x00e\\x00\\x00.\\nAsingledecorator(\\x00i\\x00\\x00e\\x00\\x00c\\x00)\\nallowsustoadd:\\n● sliders\\n● dropdowns\\n● textﬁelds,andmore.\\nAsaresult,onecan:\\n● explorethedatainteractively\\n● speed-updataexploration\\n● avoidrepetitivecell\\nmodiﬁcationsandexecutions\\n● organizethedataanalysis.\\nAJupyterNotebookisavailableheretogetstarted:https://bit.ly/3XZtsea.\\n464'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 465, 'page_label': '466', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00i\\x00\\x00l\\x00\\x00s\\nWemostlyuse\\x00l\\x00\\x00s\\x00\\x00p\\x00\\x00t\\x00\\x00)methodtocreatesubplotsusingMatplotlib.But\\nthis,attimes,getsprettytediousandcumbersome.Forinstance,itoﬀerslimited\\nﬂexibilitytocreateacustomlayout,itispronetoindexingerrors,andmore.\\nInstead,usethe\\x00l\\x00\\x00s\\x00\\x00p\\x00\\x00t\\x00\\x00o\\x00\\x00i\\x00\\x00)\\nmethod.Here,youcancreateaplotof\\nanydesiredlayoutbydeﬁningtheplot\\nstructureasastring.\\nForinstance,thestringlayout:\\n● AB\\n● AC\\n…willcreatethreesubplots,wherein:\\n● subplot\"A\"spans1stcolumn\\n● subplot\"B\"spanstophalfofthe\\n2ndcolumn\\n● subplot\"C\"spansthebottom\\nhalfof2ndcolumn\\nNext,createasubplotinaspeciﬁc\\nregionbyindexingtheaxesdictionary\\nwithitssubplotkey(\"A\",\"B\",or\"C\").\\n465'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 466, 'page_label': '467', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00n\\x00\\x00c\\x00 \\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00 \\x00l\\x00\\x00s \\x00i\\x00\\x00 \\x00n\\x00\\x00t \\x00x\\x00\\x00 \\x00n\\x00\\n\\x00n\\x00\\x00t\\x00\\x00i\\x00\\x00s\\nWhilecreatingdatavisualizations,thereareo\\x00encertainpartsthatare\\nparticularlyimportant.\\nYet,theymaynotbeimmediatelyobvioustotheviewer.\\nAgooddatastorytelleralwaysensuresthattheplotguidestheviewer’sattention\\ntothesekeyareas.\\nOnegreatwayistozoominonspeciﬁcregionsofinterestinaplot,asdepicted\\nbelow.\\n466'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 467, 'page_label': '468', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIncontrasttotheusualplot,theotherplotguidestheviewer’sattentiontoa\\nspeciﬁcareaofinterest.\\nSucheﬀortsalwaysensurethattheplotcommunicateswhatweintenditto\\ndepict—eveniftheplot’screatorisnotpresentatthattime.\\nInmatplotlib,wecandosousingindicate_inset_zoom().Itaddsanindicatorbox,\\nwhichcanbezoomed-inforbetterclarity.\\nTheembeddedplotistreatedlikeanyothermatplotlibplot.Thus,wecanadd\\naxislabelstoit,ifneeded.\\nAnothergreatwaytoprovideextrainfoisbyaddingtextannotationstoaplot,as\\ndepictedbelow:\\n467'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 468, 'page_label': '469', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSucheﬀortsalwaysensurethattheplotindeedcommunicateswhatweintendit\\ntodepict—eveniftheplot’screatorisnotpresentatthattime.\\nInmatplotlib,youcanuse𝐚𝐧𝐧𝐨𝐭𝐚𝐭𝐞(),asdepictedbelow:\\nItaddsexplanatorytextstoyourplot,whichletsyouguideaviewer’sattentionto\\nspeciﬁcareasandaidtheirunderstanding.\\nJupyternotebookforplotannotationsisavailablehere:https://bit.ly/4bzQ5Ja.\\n468'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 469, 'page_label': '470', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00e\\x00\\x00i\\x00\\x00a\\x00\\x00z\\x00\\x00a\\x00\\x00l\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00s\\n\\x00o\\x00\\x00e\\x00\\x00\\nIhavebeenusingmatplotlibformanyyearsnow.Basedonthatexperience,I\\nbelievethatoneofthebestyetunderratedpotentialsofmatplotlibistheamount\\nofcustomizabilityitoﬀers.\\nButbeingunawareofthat,mostmatplotlibusersuseitasanaiveplottingutility\\nwithalmostzerocustomization.Andasthedefaultplotsneverappear\\n“appealing”,theyresorttootherlibraries,Plotly,forinstance,tocreateelegant\\nplots.\\nYet,Ibelieve\\nthatin90-95%of\\ncases,youwould\\nNEVERneedto\\nlookbeyond\\nmatplotlib.It\\ncandomuch\\nmorethanwhat\\nmostusers\\nthink.For\\ninstance,\\nconsiderthetwo\\nplotsinthis\\nimage.\\nYes!Bothplotswerecreatedusingmatplotlib.\\nButsomecustomformattingmakesthesecondplotmuchmoreelegant,\\ninformative,appealing,andeasytofollow.\\n● Thetitleandsubtitlesigniﬁcantlyaidthestory.\\n469'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 470, 'page_label': '471', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● Also,thefootnoteoﬀersextraimportantinformation,whichisnowhereto\\nbeseeninthebasicplot.\\n● Lastly,theboldbarimmediatelydrawstheviewer’sattentionandconveys\\nthepurchasecategory’simportance.\\nThus,inmyopinion,theoverwhelmingpotentialforcustomizationmakes\\nmatplotlibfarmorecapablethanwhatmostusersthink.\\n\\x00o\\x00\\x00l\\x00\\x00i\\x00\\x00\\nOneofthethingsIalwaysensuretowardsbeingagoodstorytellerinmydata\\nscienceprojectsisthatmyplotmustdemandminimaleﬀortfromtheviewer.\\nThus,Inevershyawayfromputtinginthatextraeﬀort.\\nThishasbeenespeciallytrueforprofessionalenvironments.Attimes,itisalso\\ngoodtoensurethatourvisualizationsconveytherightstory,eveniftheyare\\nviewedinourabsence.Thebelowplotisaclassicexampleofthat.\\nInthisentirechapter,Ineverdiscussedwhatthatplotisabout—somewhat\\nindicatingmyabsence.Yet,bystaringatthisplotforafewseconds,youcan\\nquicklyﬁgureoutwhatIintendedtohighlighthere,can’tyou?\\nYoucandownloadthecodenotebookforthisposthere:https://bit.ly/3zv8pWG.\\n470'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 471, 'page_label': '472', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00a\\x00\\x00e\\x00\\x00i\\x00\\x00r\\x00\\x00s\\nManytabulardataanalysistaskscanbeinterpretedasaﬂowbetweenseveral\\nsourceandtargetentities.\\nForinstance,considerwehaveasports\\npopularitydataset,whichliststhe\\npopularityindexofasportinacountry.\\nInthisdataset:\\n● Countriesareentities.\\n● Sportsareentities.\\n● Informationﬂowingbetweenthem\\nisthepopularityvalue.\\nWhiletypicalplots,likegroupedbarplots,couldbeusedheretounderstandthe\\npopularitydistributionofcountry-wisesports,asshownbelow:\\n471'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 472, 'page_label': '473', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n…inmyopinion,however,Sankeydiagramsstandoutasaprettycoolandelegant\\nalternativetorepresentsuchﬂowdatasets:\\nTheirlinksarerepresentedwitharcswhosewidthisproportionaltothevalueof\\ntheﬂow.Thisimmenselysimpliﬁesthedataanalysisprocess.Forinstance,from\\ntheSankeydiagramabove,onecanquicklyinferthat:\\n● ThemostpopularsportinIndiaisCricket.\\n● BasketballandFootballarealmostequallypopularintheUS.\\n● BasketballishardlypopularinIndiaandEngland.\\n● England’smostpopularsportisFootball.\\n● CricketandFootballarealmostequallypopularinAustralia.\\n● Overall,Footballisthemostpopularsportinthisdataset.\\n● andmanymanymore.\\nImaginedoingthatbylookingatthetabulardataoragroupedbarchart.\\n● Thisprocesswillbetime-consuming.\\n● Youmaymissoutonafewinsights.\\n● Thegroupedbarchartcanappearprettyclutteredandmessyattimes.\\n472'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 473, 'page_label': '474', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nOfcourse,Sankeydiagramscanhavemultiplelevelsaswell,asshownbelow:\\nTodeterminewhentouseSankeydiagrams,seeifthedatainvolvesanykindof\\nﬂowofresources,energy,orinformationﬂowbetweenmultiplestagesorentities.\\nIfyes,Sankeydiagramscouldbeprettyvaluable.\\nTherearemultiplewaystocreateSankeydiagrams:\\n● Togeneratethemprogrammatically,youmayusetheipysankeywidget\\nlibrary:https://github.com/ricklupton/ipysankeywidget.\\n● IfyoupreferGUI,SankeyMATICisaprettycool,andeasy-to-usetoolto\\ncreateSankeydiagramswhichIo\\x00enuse:https://bit.ly/3WdMAnx.\\n473'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 474, 'page_label': '475', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00i\\x00\\x00e\\x00\\x00n\\x00\\x00l\\x00\\x00s\\nUnderstandingthedistributionaldiﬀerencesofdistinctgroupsinavariableis\\nquiteusefulinuncoveringinsightsaround:\\n● behavioraldisparities,\\n● featureengineering,\\n● predictivemodeling,andmore.\\nButinsuchsituations,manydatascientiststendtocreategroup-level\\ndistributionplots(histogramsordensityplots)onasingleaxisandcompare\\nthem.\\nWhilethisis(somewhat)okaywhentherearelimitedgroups,inthepresenceof\\nmanygroups,itcancreateclutteredplots,whichmaynotrevealmanyinsights\\naboutdistributionaldiﬀerences:\\nRidgelineplots(shownbelow)areaprettycompactandelegantwaytovisualize\\nthedistributionofdiﬀerentvariables(orcategoriesofavariable).\\n474'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 475, 'page_label': '476', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nMorespeciﬁcally,thevertical\\nstackingonacommonaxis\\nprovidesaneasycomparison\\nbetweengroupsandrevealsmany\\ninsightsintotheshapeand\\nvariationofthedistributions,\\nwhichotherwisewouldbe\\ndiﬃculttounderstand.\\nThisallowsusto\\ncomparethe\\ndistributionsof\\nmultiplegroups\\nsidebysideand\\nunderstandhow\\ntheydiﬀer.The\\nimagebelowis\\nanotherclassic\\nexampleof\\nRidgelineplots.It\\ndepictsthesearch\\ninterestacross\\nvariouseventsthat\\nhappenedin2023,\\nandit’ssoeasyto\\nvisualize.\\nWhileSeabornprovidesawaytocreateRidgelineplots,Ihaveo\\x00enfoundthe\\nJoypylibrarytobeprettyusefulandeasytouse.\\n475'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 476, 'page_label': '477', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00\\x00o\\x00o\\x00\\x00i\\x00\\x00r\\x00i\\x00\\x00e\\x00\\x00n\\x00\\x00l\\x00\\x00?\\nTypically,creatingaRidgelineplotmakessensewhenthevariablehasanything\\nabove3-4groups.Thisistoavoidtheoverlapthatmightappearwhenvisualizing\\ntheminasingleplot:\\nAlso,Ridgelinesplotsarerelativelymoreusefulwhenthereisaclearpattern\\nand/orrankingonthecontinuousvariableplottedbetweengroupslike:\\n● monotonicallyincreasing,\\n● monotonicallydecreasing,\\n● increasingthendecreasing,\\n● decreasingthenincreasing,etc.\\nThatiswhytheorderinwhich\\nyouverticallystackthe\\ndistributionofgroupsbecomes\\nquiteimportant.Forinstance,\\nconsidertheabove“Search\\ntrends”plotagainbutwitha\\nrandomarrangementofgroups:\\nIdon’tthinkIhavetoaskyouwhichoneiseasiertovisualizeandunderstandthe\\nﬂowofeventsin2023.Sotheseweresomepointsthatwillhelpyoudetermine\\nwhetheraRidgelineplotwillbeagoodﬁtforvisualizingyourdata.\\nIcreatedthisnotebookforyoutogetstartedwithRidgelineplotsusingJoyPy:\\nhttps://bit.ly/3xMUxXj.\\n476'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 477, 'page_label': '478', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00p\\x00\\x00k\\x00\\x00n\\x00\\x00l\\x00\\x00s\\nWhendoinganydataanalysistaskinJupyter,wemostlycreatestandalonecharts\\nandvisualizations.Ofcourse,there’snothingwrong,butinthischapter,letme\\nintroduceyoutoanotherprettycoolandelegantwayIo\\x00enusetocreate\\nvisualizationsinJupyterNotebook.\\n\\x00a\\x00\\x00g\\x00\\x00u\\x00\\x00\\nWheneverwedisplaya\\nDataFrameinJupyter,itis\\nrenderedusingHTML\\nandCSS.\\nThismeanswecanformatitsoutputjustlikeanyotherwebpage.Moreover,\\nwhenwecreateanyplotinJupyter,itisrenderedasanimage.\\nHowever,thesameimagecanalsoberenderedwithinanHTMLimagetagin\\nJupyter.\\nAndmagichappenswhenwecombinetheabovetwoideas—renderingplots\\ninsideaDataFrame,asdepictedbelow:\\n477'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 478, 'page_label': '479', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nFormally,thesecompactplotsarecalledSparklines,andtheyprovideapretty\\nelegantwaytovisualizedatawithouttakinguptoomuchspace.\\nUnliketraditionalplots,wetypicallydonotcreateaxisticksandlabelsin\\nSparklines.\\nOneofthecoolestwaysIpreferusingSparklinesisbyaddingthemtoa\\nDataFrame’scell,asdepictedintheimageabove.\\nAJupyterNotebookoncreatingSparklineplotsisavailablehere:\\nhttps://bit.ly/4eVhVmf.\\nInthisnotebook,Ihaveprovidedthestep-by-stepproceduretoembedSparklines\\ninaDataFrame.\\n478'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 479, 'page_label': '480', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00Q\\x00\\n\\x00r\\x00\\x00p\\x00\\x00g\\x00e\\x00\\x00,\\x00o\\x00\\x00u\\x00\\x00n\\x00\\x00u\\x00\\x00\\x00n\\x00Q\\x00\\nAtypicalGroupByqueryaggregatesonjustonesetofcolumns.Forinstance:\\n● Groupingdataoncolumn“A”willrequireonequery.\\n● Groupingdataoncolumns“A”and“B”willrequireaseparatequery.\\nNext,ifthetwooutputsmustbegatheredinasingletable,weuseUNIONor\\nUNIONALL(asneeded).\\n479'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 480, 'page_label': '481', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nButthisisnoteﬃcientasitscansthesametabletwice.Instead,therearethree\\nwaystorunmultipleaggregationsonthesametablebyscanningthetablejust\\nonce.Thismakesourquerymuchmoreeﬃcient.TheseareGroupingSets,\\nRollup,andCube,andtheirusagehasbeendepictedintheimagebelow:\\n480'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 481, 'page_label': '482', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00r\\x00\\x00p\\x00\\x00g\\x00e\\x00\\x00\\nTheGROUPINGSETSclauseallowsustodeﬁnemultiplegroupingsinasingle\\nquery.Eachgroupingsetdeﬁnesacombinationofcolumnsbywhichthedatais\\ngrouped.\\nGiventheabovequery,herearethegroupaggregationsthatwillbecreated:\\n● (A):AggregatedbyA,countingallrowsacrossallothercolumns.\\n● (A,B):Aggregatedby(A,B),countingallrowsacrossallothercolumns.\\n● (C):AggregatedbyC,countingallrowsacrossallothercolumns.\\nAdemonstrationisshownbelow:\\n481'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 482, 'page_label': '483', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheabovequery:\\n● GeneratesanaggregationonCitycolumn.\\n● GeneratesanotheraggregationonFruitcolumn.\\n\\x002\\x00\\x00O\\x00\\x00U\\x00\\nROLLUPcreatesaresultsetthatincludessubtotalsandagrandtotalinaddition\\ntotheregulargroupedresults.\\nItdoesthisbygroupingthedataatmultiplelevelsofaggregation.\\nGiventheabovequery,herearethegroupaggregationsthatwillbecreated:\\n● (A):SubtotalforeachA,aggregatedacrossallBandC.\\n● (A,B):SubtotalforeachAandBcombination,aggregatedacrossallC.\\n● (A,B,C):Regulargroupbyallthreecolumns.\\n● ():Grandtotal,aggregatedacrossallA,B,andC.\\nAdemonstrationisshownbelow:\\n482'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 483, 'page_label': '484', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTheabovequery:\\n● GeneratesanaggregationonFruitcolumn.\\n● Generatesanotheraggregationon(Fruit,City)column.\\n● Generatesagrandtotal.\\nUnlikeGroupingSets,theorderisimportantinROLLUP.\\nMorespeciﬁcally,ROLLUP(A,B)willnotbethesameasROLLUP(B,A).\\n\\x003\\x00\\x00U\\x00\\x00\\nFinally,CUBEcreatesaresultsetthatincludesallpossiblecombinationsof\\naggregationsforthespeciﬁedcolumns.\\nGiventheabovequery,herearethegroupaggregationsthatwillbecreated:\\n483'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 484, 'page_label': '485', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n● (A,B,C):Regulargroupbyallthreecolumns.\\n● (A,B):SubtotalforeachAandBcombination,aggregatedacrossallC.\\n● (A,C):SubtotalforeachAandCcombination,aggregatedacrossallB.\\n● (B,C):SubtotalforeachBandCcombination,aggregatedacrossallA.\\n● (A):SubtotalforeachA,aggregatedacrossallBandC.\\n● (B):SubtotalforeachB,aggregatedacrossallAandC.\\n● (C):SubtotalforeachC,aggregatedacrossallAandB.\\n● ():Grandtotal,aggregatedacrossallA,B,andC.\\nAdemonstrationisshownbelow:\\nTheabovequery:\\n● GeneratesanaggregationonFruitcolumn.\\n● GeneratesanaggregationonCitycolumn.\\n● Generatesanotheraggregationon(Fruit,City)column.\\n● Generatesagrandtotal.\\nTryitoutbydownloadingthisJupyterNotebook:https://bit.ly/4bx4SUX.\\n484'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 485, 'page_label': '486', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00,\\x00n\\x00\\x00,\\x00n\\x00\\x00a\\x00\\x00r\\x00\\x00\\x00o\\x00\\x00s\\nWeallhaveheardofLEFTJOIN,RIGHTJOIN,INNERJOIN,andOUTER\\nJOIN,haven’twe?ThesefourarethemostprevalenttypesofSQLjoins.But\\ntherearemore.\\nAndinthischapter,IwanttointroduceyoutothreeofthemwhichIﬁndpretty\\nhandyattimes.Theseare–SemiJoin,AntiJoinandNaturalJoin.\\n485'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 486, 'page_label': '487', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x001\\x00\\x00e\\x00\\x00-\\x00\\x00i\\x00\\nSemi-joinappearsquitesimilartoale\\x00join,buttherearethreenotable\\ndiﬀerences:\\n1. IfthejoinconditionbetweentworowsisTRUE,columnsfromonlythe\\nle\\x00tablearereturned.Comparethistothele\\x00join,whichreturns\\ncolumnsfrombothtables.\\n2. Ifarowinthele\\x00tablehasnomatch,thenthatrowisnotreturned.Inle\\x00\\njoin,however,allrowsfromthele\\x00tablearereturnedirrespectiveof\\nwhethertheyhaveamatchornot.\\n3. Ifarowinthele\\x00tablehasmultiplematches,onlyoneentryisreturned.\\nInle\\x00join,however,multiplematchesarereturnedanequivalentnumber\\noftimes.\\n486'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 487, 'page_label': '488', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForinstance,considerwehavethefollowingtwotables:\\nExecutingasemi-join,wegetthefollowingresults:\\nAsdepictedabove,unlikele\\x00-join:\\n● Itonlyreturnscolumnsfromthele\\x00table.\\n● Itonlyreturnsthematchedrowsfromthele\\x00table.\\nIfarecordhasmultiplematches,likeinthiscase:\\n487'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 488, 'page_label': '489', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n…thenwenoticethatsemi-joinonlyreturnsonerecordfromthele\\x00table:\\nIﬁndsemi-jointobeparticularlyusefulwhenIonlycareabouttheexistenceof\\nrecordsinanothertable.Le\\x00joinreturnsduplicateswhicharenotofinterestat\\nthatpoint.\\n\\x002\\x00\\x00n\\x00\\x00-\\x00\\x00i\\x00\\nTherowsdiscardedbythesemi-joinfromthele\\x00tablearetheresultsofan\\nanti-join.So,inaway,wecansaythat:\\nConsidertheaboveordersanduserstableagain(oneinwhichtherewereno\\nmultiplematches):\\n488'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 489, 'page_label': '490', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nExecutingananti-join,wegetthefollowingresults:\\nItisclearfromthesemi-joinandanti-joinresultsthat:\\nOfcourse,theordercanbediﬀerent.WhenIsay“[SEMIJOIN]+[ANTIJOIN]=\\n[LEFTTABLE]”,Imeanthecollectionofallrecords.\\nIﬁndanti-jointobeparticularlyusefulwhenIwishtoknowwhichrecordsdo\\nnotexistinanothertable.\\n\\x003\\x00\\x00a\\x00\\x00r\\x00\\x00\\x00o\\x00\\x00\\nThisoneissimilartoINNERJOIN,butthere’snoneedtospecifyajoin\\nconditionexplicitly.\\nInstead,itautomaticallyconsidersajoinconditiononALLthematchingcolumn\\nnames.\\n489'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 490, 'page_label': '491', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nConsidertheusersandorderstableyetagain:\\nHere,theUser_IDcolumnispresentinbothtables.\\nExecutinganaturaljoin,wegetthefollowingresults:\\nAsdepictedabove,theresultsaresimilartowhatwewouldgetwithINNER\\nJOIN.\\nHowever,wedidnothavetoexplicitlyspecifyaJOINcondition,which,\\nadmittedly,couldbegoodorbad.\\n● Itisgoodbecauseithelpsuswriteconcisequeries.\\n● Itisbadbecausewearenotexplicitaboutthecolumnsbeingjoined.\\nThesewerethreemoretypesofSQLJoins,whichIuseattimestowriteconcise\\nandelegantSQLqueries.\\nIfyouwishtoexperimentwithwhatwediscussed,downloadthisJupyter\\nNotebook:https://bit.ly/4cRGMWg.\\n490'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 491, 'page_label': '492', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00s\\x00\\x00Q\\x00\\x00N\\x00\\x00\\x00N\\x00\\x00i\\x00\\x00\\x00a\\x00\\x00i\\x00\\x00\\nInmyexperience,manyunexpectederrorsinlibraries/tools/languagescanbe\\nattributedtothepresenceofmissingvalues.Forinstance,considera1DNumPy\\narraywithNaNvalues.\\nWhenweaggregatethisNumPyarraytocalculate,say,itssum,wegetthe\\nfollowingoutput:\\nStrange,right?Althoughanoutputof6mayhavemademoresensehere,NumPy\\nproducesaNaNvalueinstead.\\nAnyway,thisisnotthetopicofthischapter,butIhopeyougetthepoint\\nAsimilarsilentmistakecanbefoundinSQLaswell,speciﬁcallyintheusageof\\n“NOTIN”clause,whichmanySQLusersarenotawareof.\\nLet’sunderstand!\\n491'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 492, 'page_label': '493', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nConsiderwehavethefollowingtables(studentsandnames),inourdatabase:\\nThetaskistoselectrecordsfromstudentstablewhereﬁ\\x00s\\x00\\x00n\\x00\\x00eisnotinthe\\nnamestable.Onewaytodothisisbyusingthe\\x00O\\x00\\x00Nclause:\\nThisprovidestheexpectedresultsaswell.\\nNow,sayournamestablehadaNULLvalue:\\n492'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 493, 'page_label': '494', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nIfweruntheabovequeryagain,wegetnorecordsthistime:\\nOnasidenote,had\\nweusedtheIN\\nclausetoselectrows\\nfromthe\\x00t\\x00\\x00e\\x00\\x00s\\ntablewhere\\nﬁ\\x00s\\x00\\x00n\\x00\\x00ewasin\\nthenamestable,we\\nnoticethatitworks\\nasexpected:\\nThat’sstrange,isn’tit?\\n\\x00h\\x00\\x00o\\x00e\\x00e\\x00\\x00o\\x00e\\x00\\x00r\\x00\\x00\\x00i\\x00\\x00\\x00O\\x00\\x00N\\x00\\nThereasonwegetnorecordswhenweuse\\x00O\\x00\\x00Nbutthe\\x00Nclauseworksas\\nexpectedhastodowithhowthesetwoclausesoperateinternally.\\nForsimplicity,considerweare\\ncurrentlycheckingtheﬁrst\\nrecord(whereﬁrst_nameis\\n“John”)fromthestudents\\ntable:\\n493'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 494, 'page_label': '495', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThe\\x00H\\x00\\x00Eclauseneedsabooleanvaluetodeterminewhetherarecordmustbe\\nﬁlteredornot.Whenweuse“IN”,thisbooleanvalueisevaluatedusingtheOR\\noperatorasfollows:\\nIfanyconditionisTRUE,therowgetsﬁltered.However,whenweuse“NOTIN”,\\nthebooleanvalueisevaluatedusingtheANDoperatorasfollows:\\nFortheaboveexpressiontobeTRUE,allindividualconditionsMUSTbeTRUE.\\nButthe(JOHN!=None)conditionproducesaconﬂictbecause,typically,this\\nconditionresultsinanUNKNOWNvalue.Thus,theentireexpressionevaluates\\ntoUNKNOWN—producingnorecords.Thishappensforeveryrecordinthe\\nstudentstable.Asaresult,thequeryresultsinnorecords:\\n494'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 495, 'page_label': '496', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00\\x00o\\x00o\\x00n\\x00\\x00e\\x00\\x00?\\nTherearemanywaystoavoidthis.Asourtaskistoselectrecordsfrom\\x00t\\x00\\x00e\\x00\\x00s\\ntablewhereﬁ\\x00s\\x00\\x00n\\x00\\x00eisnotinthenamestable,wecan:\\n● FilterouttheNULLvaluesinthesub-query:\\n● UseAntiJoins:Antijoinreturnsonlythoserowsfromthele\\x00tablewhere\\nnomatchisfoundintherighttable.Thisispreciselywhatweneedinour\\ncaseanditisimplementedbelow:\\nAttimes,suchmistakescantakesomeserioustimetodebugifyouarenotaware\\nofthembeforehand.\\nIfyouwishtoexperimentwithwhatwediscussedabouttheNOTINclause,\\ndownloadthisJupyterNotebook:https://bit.ly/3xVOIXF.\\n495'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 496, 'page_label': '497', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00y\\x00\\x00o\\x00\\x00O\\x00\\n\\x00e\\x00\\x00e\\x00\\x00\\x00n\\x00\\x00e\\x00\\x00e\\x00\\x00\\nDotnotationprovidesaconciseandelegantwaytoaccessandmodifyanobject’s\\nattributes.\\nYet,withdotnotation,wecannotvalidatetheupdatesmadetoanattribute.This\\nmeanswecanassigninvalidvaluestoaninstance’sattributes,asshownbelow:\\n496'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 497, 'page_label': '498', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nOnecommonwaytoavoidthisisbydeﬁningasetter(\\x00e\\x00\\x00s\\x00\\x00e\\x00\\x00),which\\nvalidatestheassignmentstep.\\nButexplicitlyinvokingasettermethodisn’taselegantasdotnotation,isit?\\nIdeally,wewouldwanttousedotnotationandstillapplythosevalidationchecks.\\nThe@𝐩𝐫𝐨𝐩𝐞𝐫𝐭𝐲decoratorinPythoncanhelp.Here’showwecanuseithere.\\nFirst,deﬁneagetterasfollows:\\nDeclareamethod\\nwiththeattribute’s\\nname.\\nThere’snoneedto\\nspecifyany\\nparametersforthis\\nmethod.\\nDecorateitwiththe\\n@𝐩𝐫𝐨𝐩𝐞𝐫𝐭𝐲decorator.\\n497'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 498, 'page_label': '499', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNext,deﬁneasetterasfollows:\\n● Declareamethodwiththeattribute’sname.\\n● Specifytheparameteryouwanttoupdatetheattributewith.\\n● Writetheconditionsasyouusuallywouldinanyothersettermethod.\\n● Decorateitwiththe@𝐚𝐭𝐭𝐫𝐢𝐛𝐮𝐭𝐞-𝐧𝐚𝐦𝐞.𝐬𝐞𝐭𝐭𝐞𝐫decorator.\\nNow,youcanusethedotnotationwhilestillhavingvalidationchecksinplace.\\nThisapproachoﬀers\\nboththevalidationand\\ncontrolofexplicitsetters\\nandgettersandthe\\neleganceofdot\\nnotations.\\n498'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 499, 'page_label': '500', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00r\\x00\\x00t\\x00\\x00s\\x00n\\x00y\\x00\\x00o\\x00\\nInthischapter,let’scontinueourdiscussionontheabovetopicanddiscussa\\nlimitationoftheaboveapproach.Movingon,weshallseehowDescriptorsin\\nPythonprovideamuchmoreelegantwayofsettingandgettingvalues.\\n\\x00i\\x00\\x00t\\x00\\x00i\\x00\\x00s\\x00f\\x00p\\x00\\x00p\\x00\\x00t\\x00\\x00e\\x00\\x00r\\x00\\x00o\\x00\\nConsidertheaboveclassimplementationagain:\\nThebiggestissuehereisthatwemustdeﬁneagetterandsetterforevery\\ninstance-levelattribute.\\nSowhatifourclasshas,say,3suchattributes,andallmustbepositive?\\nOfcourse,wewillhave3gettersand3setters,whichmakestheoverall\\nimplementationlong,messy,andredundant.\\n499'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 500, 'page_label': '501', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThere’sredundancybecauseeverysettermethodwillhavealmostthesamelines\\nofcode(theifstatementsforvalidation).\\nAlso,ifyouthinkaboutit,thegettermethodsaresomewhatredundantand\\nunnecessarytoo,astheyjustreturnanattribute.\\nIfthatisclear,there’sonemoreissuewiththeaboveimplementation.\\nRecallwhatImentionedearlier:“Ourclasswillhave3suchinstance-level\\nattributes,andallmustbepositive?”\\n500'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 501, 'page_label': '502', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nSeewhathappenswhenwecreateanobjectwithaninvalidinput:\\nAsdepictedabove,Pythondoesnotraiseanyerror,whenideally,itshould.\\n\\x00n\\x00m\\x00\\x00r\\x00\\x00\\x00t\\x00o\\x00\\x00t\\x00\\x00n\\nOnecommonwayprogrammerstrytoeliminateredundancyisbydeﬁning\\nexplicitvalidationfunctions.\\nForinstance,wecandeﬁneafunctionthatjustvalidatesthevaluereceived,as\\ndemonstratedbelow:\\n501'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 502, 'page_label': '503', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNext,wecaninvokethismethodwhereverneeded:\\nButthisdoesnotsolvetheproblemeither:\\nWestillhaveexplicitandredundantfunctioncallsineachsettermethod.\\nAllgettermethodsstilldothesamethingandhavehighredundancy.\\nAndmostimportantly,the__init__methodisnowmessedupwithmultiple\\nfunctioncalls.\\n502'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 503, 'page_label': '504', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00r\\x00\\x00t\\x00\\x00s\\nSimplyput,Descriptorsareobjectswithmethods(like__get__,__set__,etc.)that\\nareusedtomanageaccesstotheattributesofanotherclass.So,everydescriptor\\nobjectisassignedtoonlyoneattributeofanotherclass.\\nAndjusttobeclear,this“anotherclass”istheclassweareprimarilyinterestedin\\n—theDummyClasswesawearlier,forinstance.\\n● Theattributenumber1→getsitsowndescriptor.\\n● Theattributenumber2→getsitsowndescriptor.\\n● Theattributenumber3→getsitsowndescriptor.\\nAtypicalDescriptorclassisimplementedwiththreemethods,asshownbelow:\\n503'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 504, 'page_label': '505', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThe__set__methodiscalledwhentheattributeisassignedanewvalue.Wecan\\ndeﬁnethecustomcheckshere.\\nThe__set_name__methodiscalledwhenthedescriptorobjectisassignedtoa\\nclassattribute.Itallowsthedescriptortokeeptrackofthenameoftheattribute\\nit’sassignedtowithintheclass.\\nThe__get__methodiscalledwhentheattributeisaccessed.\\nAlso:\\nTheinstanceparameterreferstotheobjectofthedesiredclass—DummyClass().\\nTheownerparameteristhedesiredclassitself—DummyClass.\\nThevalueparameteristhevaluebeingassignedtoanattributeofthedesired\\nclass.\\nThenameparameteristhenameoftheattribute.\\nIfit’sunclear,letmegiveyouasimpledemonstration.\\n504'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 505, 'page_label': '506', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nConsiderthisDescriptorclass:\\nI’llexplainthisimplementationshortly,butbeforethat,let’sconsideritsusage,\\nwhichisdemonstratedbelow:\\n505'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 506, 'page_label': '507', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNow,let’sgobacktotheDescriptorClassimplementation:\\n● __set_name__(self,owner,name):Thismethodiscalledwhenthe\\ndescriptorisassignedtoaclassattribute(line3).Itsavesthenameofthe\\nattributeinthedescriptorforlateruse.\\n● __set__(self,instance,value):Whenavalueisassignedtotheattribute(line\\n6),thismethodiscalled.Itraisesanerrorifthevalueisnegative.\\nOtherwise,itstoresthevalueintheinstance’sdictionaryunderthe\\nattributenamewedeﬁnedearlier.\\n● __get__(self,instance,owner):Whentheattributeisaccessed,thismethod\\niscalled.Itreturnsthevaluefromtheinstance’sdictionary.\\nNow,seehowthissolutionsmartlysolvesalltheproblemswediscussedearlier.\\nLet’screateanobjectoftheDummyClass:\\nAsdepictedabove,assigninganinvalidvaluetotheattributeraisesanerror.\\nNext,let’sseewhathappenswhentheattributespeciﬁedduringtheinitialization\\nisinvalid:\\n506'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 507, 'page_label': '508', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nGreat!Itvalidatestheinitializationtoo.\\nHere,recallthatweneverdeﬁnedanyexplicitchecksinthe__init__method,\\nwhichissupercool.\\nMovingon,let’sdeﬁnemultipleattributesintheDummyClassnow:\\nCreatinganobjectandsettinganinvalidvalueforanyoftheattributesraisesan\\nerror:\\nWorksseamlessly!\\n507'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 508, 'page_label': '509', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nRecallthatweneverdeﬁnedmultiplegettersandsettersforeachattribute\\nindividually,likewedidwiththe@propertydecoratorearlier.\\nThisisgreat,isn’tit?\\nIﬁnddescriptorstobemassivelyhelpfulinreducingworkandcoderedundancy\\nwhilealsomakingtheentireimplementationmuchmoreelegant.\\nIfyouwanttotrythemout,Ipreparedthisnotebookforyoutogetstarted:\\nhttps://bit.ly/4cqsyM6.\\n508'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 509, 'page_label': '510', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x000\\x00o\\x00\\x00\\x00o\\x00\\x00o\\x00\\x00a\\x00\\x00c\\x00e\\x00\\x00o\\x00\\x00\\nHereare20mostcommonmagicmethodsusedinPythonOOP:\\nInmyexperience,thesearepossiblytheonly20magicmethodsyouwouldever\\nneedinmostPythonprojectsutilizingOOP.\\nSyntactically,theyarepreﬁxedandsuﬃxedwithdoubleunderscores,suchas\\n\\x00_\\x00\\x00n\\x00\\x00,\\x00_\\x00\\x00r\\x00\\x00,andmanymore.Thatiswhytheyarealsocalled“Dunder\\nmethods”—shortforDoubleUNDERscore.\\n509'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 510, 'page_label': '511', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00e\\x00\\x00r\\x00\\x00ﬃ\\x00i\\x00\\x00t\\x00l\\x00\\x00s\\x00b\\x00\\x00c\\x00\\x00\\x00s\\x00\\x00g\\x00l\\x00\\x00s\\nWhenwedeﬁneaclass\\ninPython,itis\\npossibleto\\ndynamicallyaddnew\\nattributestoitsobjects\\nduringrun-time.\\nForinstance,consider\\nthefollowingPython\\nclasshere.\\nHere,itisperfectlylegaltoaddnewattributestoanyobjectatrun-time,as\\nshownbelow:\\nHowever,thisisnotalwaysrecommendedbecause:\\n● Itmayleadtobugsifthecodeassumesallclassinstanceswillalwayshave\\nthesameattributes.\\n● Itmakesitdiﬃculttodebugcodewhenobjectskeeponaccumulatingnew\\nattributes.\\n● Itleadstoaconﬂictingschema,etc.\\n510'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 511, 'page_label': '512', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nCanwerestrictthisdynamicity?\\nOfcoursewecan!\\nDeﬁningaslottedclasshelpsusachievethis.Simplyput,itallowsustoﬁxthe\\ninstance-levelattributesaclassobjectcaneverpossess.\\nAslottedclassisdeclaredasfollows:\\nDeﬁnea\\x00_\\x00\\x00o\\x00\\x00_\\x00attributewithallattributesanobjectmaypossess.\\nDone!\\n511'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 512, 'page_label': '513', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nNow,ifwetrytoaddanewattributeatrun-time,itraisesanerror:\\nDeclaringaslottedclasshasmanyadvantagesaswell.Forinstance,itcanhelpus\\navoidtypicalcodetypos.Saywewanttochangethenameof\\x00t\\x00\\x00e\\x00\\x00A.\\nHere,insteadofreferringtotheattributename(withalowercasen),we\\nmistakenlywroteName(withanuppercasen).Anormalunsllotedclasswillnot\\nraiseanerror,asshownbelow:\\n512'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 513, 'page_label': '514', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nButaslottedclasswillcatchthistypo:\\nDeclaringaslottedclassprovidesmemoryadvantagesaswell.Asshownbelow,\\nanobjectoftheslottedclassconsumes\\x00.\\x00\\x00lessmemorythananobjectofa\\nnormalclass:\\nWhy,youmaywonder?\\n513'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 514, 'page_label': '515', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nTypically,Pythoncreatesadictionary\\nforeveryobjectthatmapsattributes\\ntotheirvalues.Asadictionaryis\\nmutable,thisispreciselywhatallows\\nustoadd/deleteattributes\\ndynamically.Butthisintroduces\\nmemoryoverheadsasPythonalways\\ntriestokeeproomfornewattributes\\nthatmaygetaddedatsomepoint.\\nIncaseofslottedclass,however,Pythoncandoawaywiththisdictionary:\\nAsaresult,theobjectconsumeslessmemory.Asadepartingnote,ifyoudon’t\\nwanttodynamicallyaddnewattributestoanobject,itisbettertocreateaslotted\\nclass.\\nInfact,evenifyouknowtheattributesthataclassobjectwilleverpossess,but\\nsomeoftheseattributesarenotavailableduringtheobject’sinitialization,you\\ncanstilldeclaretheminthe\\x00_\\x00\\x00o\\x00\\x00_\\x00classattributeandassignavaluetothem\\nlaterintheprogramwhenevertheyareavailable.\\n514'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 515, 'page_label': '516', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00h\\x00\\x00o\\x00\\x00t\\x00e\\x00n\\x00\\x00k\\x00\\x00o\\x00\\x00l\\x00\\x00o\\x00\\x00a\\x00\\x00(\\x00\\x00n\\x00y\\x00\\x00r\\x00\\x00?\\nInPyTorch,theforwardpassisimplementedinthe\\x00o\\x00\\x00a\\x00\\x00(\\x00method,as\\ndemonstratedbelow:\\nHere,haveyoueverwonderedthatwhenwewanttoruntheforwardpass,we\\nrarelyinvokethis\\x00o\\x00\\x00a\\x00\\x00(\\x00method:\\nInstead,wealwaysinvokethemodel(aclassobject)asdemonstratedbelow,asit\\nwasafunction:\\n515'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 516, 'page_label': '517', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nWecanalsoverifythatmodelisaclassobject:\\nHowcanaclassobjectbeinvokedlikeafunctionandwhatarewemissinghere?\\nLet’sunderstand!\\n\\x00\\x00i\\x00\\x00l\\x00\\x00x\\x00\\x00p\\x00\\x00\\nConsiderwewanttoevaluatethefollowingquadratic:\\n𝑓(𝑥)=𝑎𝑥2+𝑏𝑥+𝑐\\nOnewayistodeﬁneamethodthatacceptstheinputandreturnsthevalueofthe\\nquadratic,asshownbelow:\\nOfcourse,thereisnothingwrongwiththisapproach.\\n516'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 517, 'page_label': '518', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nButthereisonesmartandelegantwayofdoingthisinPython.Insteadof\\nexplicitlyinvokingamethod,wecandeﬁnethe\\x00_\\x00\\x00l\\x00\\x00_\\x00\\x00magicmethod.\\nThismagicmethodallowsyoutodeﬁnethebehavioroftheclassobjectwhenit\\nisinvokedlikeafunction(likethis:\\x00b\\x00\\x00c\\x00\\x00)).\\nLet’srenamethe\\x00v\\x00\\x00u\\x00\\x00e\\x00\\x00methodto\\x00_\\x00\\x00l\\x00\\x00_\\x00\\x00.\\nAsaresult,wecannowinvoketheclassobjectdirectlyinsteadofexplicitly\\ninvokingamethod.Thiscanhavemanyadvantages.Forinstance:\\n● Itallowsustoimplementobjectsthatcanbeusedinaﬂexibleand\\nintuitiveway.\\n● Itallowsustouseaclassobjectincontextswhereacallableobjectis\\nexpected—usingaclassobjectasadecorator,forinstance.\\n\\x00h\\x00\\x00\\x00s\\x00a\\x00\\x00a\\x00\\x00\\x00?\\nInPython,acallableisanyobject\\nthatcanbecalledusingparentheses\\nandmayreturnavalue.For\\ninstance,afunctionisacallable\\nobject(onethatcanbe\\ncalled/invoked).\\n517'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 518, 'page_label': '519', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00o\\x00\\x00n\\x00\\x00a\\x00\\x00\\x00o\\x00y\\x00\\x00r\\x00\\x00\\nThisiswhathappenswhenwebuilddeeplearningmodelswithPyTorch.For\\ninstance,considerthePyTorchclassagain:\\nAsyoumayhavealreadyguessed,themodelobjectcanbeinvokedbecauseall\\nPyTorchclassesimplicitlydeclarethe\\x00_\\x00\\x00l\\x00\\x00_\\x00\\x00methodthemselves.Within\\nthat\\x00_\\x00\\x00l\\x00\\x00_\\x00\\x00method,theyinvoketheuser-deﬁnedforwardpass.\\nAsimpliﬁedversionofthisisdepictedbelow:\\n● PyTorchitselfaddsthe\\x00_\\x00\\x00l\\x00\\x00_\\x00\\x00method.\\n● The\\x00_\\x00\\x00l\\x00\\x00_\\x00\\x00methodinvokestheuser-deﬁned\\x00o\\x00\\x00a\\x00\\x00(\\x00method.\\n518'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 519, 'page_label': '520', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nThisway,Pythongetstoknowthatthemodelobjectcanbeinvokedlikea\\nfunction—\\x00o\\x00\\x00l\\x00\\x00.\\nInfact,wecanverifythatwewillgetthesameoutputnomatterwhichwaywe\\nruntheforwardpass:\\nAllthreewaysreturnthesameoutput.\\n519'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 520, 'page_label': '521', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00r\\x00\\x00\\x00O\\x00\\x00n\\x00\\x00p\\x00\\x00l\\x00\\x00i\\x00\\x00\\x00s\\x00i\\x00\\x00i\\x00\\x00\\x00r\\x00\\x00\\x00y\\x00\\x00o\\x00\\nUsingaccessmodiﬁers(public,protected,andprivate)isfundamentalto\\nencapsulationinOOP.\\nThisisnotjustaboutPythonbutapplicabletoOOPingeneral.However,when\\nwetalkspeciﬁcallyaboutPython,wenoticethatitfailstodelivertrue\\nencapsulationprocedures,whichonewouldwanttoleverageintheirOOPcode.\\nHow?\\nLet’sunderstandthisinthischapter.\\nAsyoumayalreadyknow,classattributesinOOPcanbeofthreetypes:\\n● Apublicmemberisaccessibleeverywhereinsideandoutsidethebase\\nclass,anditisinheritedbyallchildclasses.\\n● Aprotectedmemberisaccessibleeverywhereinsidethebaseclassand\\nchildclass(es).Itisnotaccessibleoutsidetheclass.\\n● Aprivatememberisaccessibleonlyinsidethebaseclass.\\nBut,withPython,therearenosuchstrictenforcements.Thisisunlikemany\\notherprogramminglanguageslikeC++.Thus,protectedmembersbehaveexactly\\nlikepublicmembers.\\nWhat’smore,privatememberscanbe(somehow)accessedoutsidetheclassas\\nwell.\\n520'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 521, 'page_label': '522', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nForbetterclarity,considerthefollowingclassimplementation:\\nSyntacticallyspeaking,inPython:\\n● Apublicmemberisdeclaredwith0leadingunderscores.\\n● Aprotectedmemberisdeclaredwith1leadingunderscore.\\n● Aprivatememberisdeclaredwith2leadingunderscores.\\nNext,weinstantiateaclassobject:\\nMovingon,asonewouldexpect,thepublicattributeisaccessibleoutsidethe\\nclass:\\n521'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 522, 'page_label': '523', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nHowever,inasimilarway,eventheprotectedattributeisaccessibleoutsidethe\\nclass,which,ideally,shouldnothappen:\\nLastly,whendeclaringprivatemembers,Pythonperformsnamemangling.\\nItisatechniqueusedinprogrammingtoavoidnamingconﬂictsbetween\\ndiﬀerentclasses.\\nPythonperformsnamemanglingbyattachingunderscore-preﬁxedclassname\\n(\\x00M\\x00\\x00l\\x00\\x00s)toallmemberswithtwoleadingunderscores.\\nSo,whiletheprivatememberisnotdirectlyaccessibleusingitsoriginalname,it\\ncanstillbeaccessedwiththemodiﬁednameobtainedfromnamemangling.\\nThisisdemonstratedbelow:\\n522'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 523, 'page_label': '524', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdemonstratedinalltheexamplesabove,protectedmembersandprivate\\nmembers(withnamemangling)canbeaccessedlikepublicmembers.\\nThus,thepointthateveryPythonprogrammerusingOOPmustrememberhere\\nisthatPythonneverenforcesencapsulation.\\nInstead,leveragingencapsulationproceduresinPythonmainlyrelieson\\nconventions.\\nTheyareusedtocommunicatetheaccessibilityprotocolsofclassmembersto\\notherprogrammers.\\nThus,itistheprogrammer’sresponsibilitytoobeytheseconventions.\\n523'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 524, 'page_label': '525', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00\\x00o\\x00\\x00o\\x00\\x00i\\x00\\x00o\\x00\\x00\\x00p\\x00\\x00o\\x00\\x00b\\x00\\x00t\\x00_\\x00\\x00i\\x00\\x00_\\x00\\x00\\nMostPythonprogrammersmisinterpretthe\\x00_\\x00\\x00i\\x00\\x00_\\x00\\x00magicmethodin\\nPythonOOP.Theythinkthatitcreatesanewobject,i.e.,allocatesmemorytoit.\\nForinstance,considerthePoint2Dclassbelow:\\nAssumethatthereisalsoa\\x00_\\x00\\x00p\\x00\\x00_\\x00\\x00method,asitisnotshownhere.When\\nwecreateanobject(shownbelow),programmersbelievethatinthebackground,\\nitisthe\\x00_\\x00\\x00i\\x00\\x00_\\x00\\x00methodthatisallocatingmemorytotheirobject:\\nButthatisnottrue.\\nWhenwecreateANYobjectinPython,the\\x00_\\x00\\x00i\\x00\\x00_\\x00\\x00methodNEVER\\nallocatesmemorytoit.\\n524'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 525, 'page_label': '526', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsthenamesuggests,\\x00_\\x00\\x00i\\x00\\x00_\\x00\\x00onlyassignsvaluetoanobject’sattributes,\\ni.e.,initializetheattributes.\\nInstead,it’sthe\\x00_\\x00\\x00w\\x00\\x00(\\x00magicmethodthatcreatesanewobjectand\\nallocatesmemorytoit.Tounderstandbetter,considertheclassimplementation\\nbelow.\\nHere,wehaveimplementedthe\\x00_\\x00\\x00w\\x00\\x00(\\x00method,whichchecksifthepassed\\nargumentsareofintegertype.Now,ifwetrytocreateanobjectofthisclass,\\nPythonwouldﬁrstvalidatethechecksspeciﬁedinthe\\x00_\\x00\\x00w\\x00\\x00(\\x00methodand\\ncreateanewobjectonlywhenthespeciﬁedconditionsaretrue.Thisisevident\\nfromtheimagebelow:\\n525'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 526, 'page_label': '527', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAnotherpopularusecaseofthe\\x00_\\x00\\x00w\\x00\\x00methodistodeﬁnesingletonclasses\\n—classesthatcanonlyhaveoneobject.Forinstance,considerthefollowing\\nclassimplementation:\\nIntheabovecode,the\\x00_\\x00\\x00w\\x00\\x00methoddeﬁneaclassvariable\\x00c\\x00\\x00s\\x00\\x00c\\x00\\x00n\\x00.\\nWhenanewobjectisinstantiated,ifthevalueof\\x00c\\x00\\x00s\\x00\\x00c\\x00\\x00n\\x00\\x000,thevalueof\\n\\x00c\\x00\\x00s\\x00\\x00c\\x00\\x00n\\x00isupdatedto1andtheobjectisreturned.A\\x00ercreatingtheﬁrst\\nobject,anewobjectcanneverbecreatedbecausethevalueof_class_countwill\\nneverbe0.Thisisevidentfromtheimagebelow:\\n526'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 527, 'page_label': '528', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\n\\x00u\\x00\\x00t\\x00\\x00n\\x00v\\x00\\x00l\\x00\\x00d\\x00\\x00g\\x00n\\x00y\\x00\\x00o\\x00\\nFunctionoverloadingiscriticalto\\npolymorphism,wherein,wemayhave\\nmultiplefunctionswith:\\n● Samename,and\\n● Diﬀerentnumber(ortype)of\\nparameters.\\nHowever,Pythonprovidesnonativesupportforfunctionoverloading.Inother\\nwords,ifwedeﬁnetwo(ormore)functionswiththesamenameanddiﬀerent\\nparameters,Pythonwillonlyconsiderthelatestdeﬁnitioncorrespondingtothat\\nfunctionname.Thisisdepictedbelow.\\n527'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 528, 'page_label': '529', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove,thelatestdeﬁnitionofadd()hadthreeparameters.Thatiswhy\\npassingtwoargumentsraisedanerror.Thisrestrictsusfromwritinggood\\npolymorphiccodeinPython.Ofcourse,therearewaystopreventthiserror,as\\ndepictedbelow:\\nAlso,ifthesameparametercantakemultipledatatypes,wecanwrite\\n(somewhat)polymorphiccodebyaddingifstatementsusing\\x00s\\x00\\x00s\\x00\\x00n\\x00\\x00(\\x00.\\nButthisisn’taselegantasdeﬁningmultiplefunctionswithdiﬀerentdatatypes\\nlikewecandoin,say,C++,isit?\\n528'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 529, 'page_label': '530', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nC++automaticallyinvokesthecorrectmethodcorrespondingtotheargument\\ndatatype.Whileexploringthisforoneofmyprojects,Ifoundaprettyhandyway\\ntoenablefunctionoverloadinginPython\\nThe\\x00d\\x00\\x00p\\x00\\x00c\\x00decoratorfromtheMultidispatchlibraryallowsustoleverage\\nthestandardandelegantfunctionoverloadinginPython,asdemonstratedbelow:\\n529'),\n",
       " Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 530, 'page_label': '531', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf'}, page_content='DailyDoseofDS.com\\nAsdepictedabove,wehavemultiplefunctionswiththesamenameanddiﬀerent\\nparameters.The\\x00d\\x00\\x00p\\x00\\x00c\\x00decoratorallowsustoinvokethecorrectfunction\\ncorrespondingtotheparameterspassedduringthefunctioncall.\\nAnothercoolthingaboutthe\\x00d\\x00\\x00p\\x00\\x00c\\x00decoratoristhatitraisesanerrorwhen\\nthefunctioncalldoesnotmatchanyofthefunctiondeﬁnitions:\\nThisallowsustoidentifyerrorswhichcano\\x00engounnoticed.\\nInadummyexperimentation,Ididnoticeaslightincrementintheruntime.But\\ntheorderofincreasewasinnanosecondsorso,whichcanbesafelyignored.\\n530'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 0, 'page_label': '1', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 1, 'page_label': '2', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Hands-On Machine Learning with\\nScikit-Learn, Keras, \\nand\\nTensorFlow\\nTHIRD EDITION\\nConcepts, Tools, and Techniques to \\nBuild Intelligent\\nSystems\\nAurélien Géron'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 2, 'page_label': '3', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby \\nAurélien \\nGéron\\nCopyright © 2023 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by \\nO’Reilly Media, Inc.,\\n 1005 Gravenstein Highway North,\\nSebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales\\npromotional use. Online editions are also available for most titles\\n(\\nhttps://oreilly.com\\n). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or\\ncorporate@oreilly.com\\n.\\nAcquisitions Editor:\\n Nicole Butterfield\\nDevelopment Editors:\\n Nicole Taché \\nand \\nMichele Cronin\\nProduction Editor:\\n Beth Kelly\\nCopyeditor:\\n Kim Cofer\\nProofreader:\\n Rachel Head\\nIndexer:\\n Potomac Indexing, LLC\\nInterior Designer:\\n David Futato\\nCover Designer:\\n Karen Montgomery\\nIllustrator:\\n Kate Dullea\\nMarch 2017:\\n First Edition\\nSeptember 2019:\\n Second Edition'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 3, 'page_label': '4', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='October 2022:\\n Third Edition'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 4, 'page_label': '5', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Revision History for the Third Edition\\n2022-10-03:\\n First Release\\nSee \\nhttps://oreilly.com/catalog/errata.csp?isbn=9781492032649\\n for release\\ndetails.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. \\nHands-\\nOn Machine Learning with Scikit-Learn, Keras, and TensorFlow\\n, the cover\\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the author, and do not represent\\nthe publisher’s views. While the publisher and the author have used good\\nfaith efforts to ensure that the information and instructions contained in this\\nwork are accurate, the publisher and the author disclaim all responsibility for\\nerrors or omissions, including without limitation responsibility for damages\\nresulting from the use of or reliance on this work. Use of the information and\\ninstructions contained in this work is at your own risk. If any code samples or\\nother technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to\\nensure that your use thereof complies with such licenses and/or rights.\\n978-1-098-12597-4\\n[LSI]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 5, 'page_label': '6', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Preface'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 6, 'page_label': '7', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published \\na paper\\n\\u2060 \\u2060\\n showing how to train a\\ndeep neural network capable of recognizing handwritten digits with state-of-\\nthe-art precision (>98%). They branded this technique “deep learning”. \\nA\\ndeep neural network is a (very) simplified model of our cerebral cortex,\\ncomposed of a stack of layers of artificial neurons. Training a deep neural net\\nwas widely considered impossible at the time,\\n\\u2060 \\u2060\\n and most researchers had\\nabandoned the idea in the late 1990s. This paper revived the interest of the\\nscientific community, and before long many new papers demonstrated that\\ndeep learning was not only possible, but capable of mind-blowing\\nachievements that no other machine learning (ML) technique could hope to\\nmatch (with the help of tremendous computing power and great amounts of\\ndata). This enthusiasm soon extended to many other areas of machine\\nlearning.\\nA decade later, machine learning had conquered the industry, and today it is\\nat the heart of much of the magic in high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition,\\nrecommending videos, and perhaps even driving your car.\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 7, 'page_label': '8', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Machine Learning in Your Projects\\nSo, naturally you are excited about machine learning and would love to join\\nthe party!\\nPerhaps you would like to give your homemade robot a brain of its own?\\nMake it recognize faces? Or learn to walk around?\\nOr maybe your company has tons of data (user logs, financial data,\\nproduction data, machine sensor data, hotline stats, HR reports, etc.), and\\nmore than likely you could unearth some hidden gems if you just knew where\\nto look. With machine learning, you could accomplish the following \\nand\\nmuch more\\n:\\nSegment customers and find the best marketing strategy for each group.\\nRecommend products for each client based on what similar clients\\nbought.\\nDetect which transactions are likely to be fraudulent.\\nForecast next year’s revenue.\\nWhatever the reason, you have decided to learn machine learning and\\nimplement it in your projects. Great idea!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 8, 'page_label': '9', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Objective and Approach\\nThis book assumes that you know close to nothing about machine learning.\\nIts goal is to give you the concepts, tools, and intuition you need to\\nimplement programs capable of \\nlearning from data\\n.\\nWe will cover a large number of techniques, from the simplest and most\\ncommonly used (such as linear regression) to some of the deep learning\\ntechniques that regularly win competitions. For this, we will be using\\nproduction-ready Python frameworks:\\nScikit-Learn\\n is very easy to use, yet it implements many machine\\nlearning algorithms efficiently, so it makes for a great entry point to\\nlearning machine learning. \\nIt was created by David Cournapeau in 2007,\\nand is now led by a team of researchers at the French Institute for\\nResearch in Computer Science and Automation (Inria).\\nTensorFlow\\n is a more complex library for distributed numerical\\ncomputation. \\nIt makes it possible to train and run very large neural\\nnetworks efficiently by distributing the computations across potentially\\nhundreds of multi-GPU (graphics processing unit) servers. TensorFlow\\n(TF) was created at Google and supports many of its large-scale\\nmachine learning applications. It was open sourced in November 2015,\\nand version 2.0 was released in September 2019.\\nKeras\\n is a high-level deep learning API that makes it very simple to train\\nand run neural networks. Keras comes bundled with TensorFlow, and it\\nrelies on TensorFlow for all the intensive computations.\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nmachine learning through concrete working examples and just a little bit of\\ntheory.\\nTIP'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 9, 'page_label': '10', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='While you can read this book without picking up your laptop, I highly recommend you\\nexperiment with the code examples.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 10, 'page_label': '11', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Code Examples\\nAll the code examples in this book are open source and available online at\\nhttps://github.com/ageron/handson-ml3\\n, as Jupyter notebooks. These are\\ninteractive documents containing text, images, and executable code snippets\\n(Python in our case). The easiest and quickest way to get started is to run\\nthese notebooks using Google Colab: this is a free service that allows you to\\nrun any Jupyter notebook directly online, without having to install anything\\non your machine. All you need is a web browser and a Google account.\\nNOTE\\nIn this book, I will assume that you are using Google Colab, but I have also tested the\\nnotebooks on other online platforms such as Kaggle and Binder, so you can use those if\\nyou prefer. Alternatively, you can install the required libraries and tools (or the Docker\\nimage for this book) and run the notebooks directly on your own machine. See the\\ninstructions at \\nhttps://homl.info/install\\n.\\nThis book is here to help you get your job done. If you wish to use additional\\ncontent beyond the code examples, and that use falls outside the scope of fair\\nuse guidelines, (such as selling or distributing content from O’Reilly books,\\nor incorporating a significant amount of material from this book into your\\nproduct’s documentation), please reach out to us for permission, at\\npermissions@oreilly.com\\n.\\nWe appreciate, but do not require, attribution. An attribution usually includes\\nthe title, author, publisher, and ISBN. For example: “\\nHands-On Machine\\nLearning with Scikit-Learn, Keras, and TensorFlow\\n by Aurélien Géron.\\nCopyright 2023 Aurélien Géron, 978-1-098-12597-4.”'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 11, 'page_label': '12', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Prerequisites\\nThis book assumes that you have some Python programming experience. If\\nyou don’t know Python yet, \\nhttps://learnpython.org\\n is a great place to start.\\nThe official tutorial on \\nPython.org\\n is also quite good.\\nThis book also assumes that you are familiar with Python’s main scientific\\nlibraries—in particular, \\nNumPy\\n, \\nPandas\\n, and \\nMatplotlib\\n. If you have never\\nused these libraries, don’t worry; they’re easy to learn, and I’ve created a\\ntutorial for each of them. You can access them online at\\nhttps://homl.info/tutorials\\n.\\nMoreover, if you want to fully understand how the machine learning\\nalgorithms work (not just how to use them), then you should have at least a\\nbasic understanding of a few math concepts, especially linear algebra.\\nSpecifically, you should know what vectors and matrices are, and how to\\nperform some simple operations like adding vectors, or transposing and\\nmultiplying matrices. If you need a quick introduction to linear algebra (it’s\\nreally not rocket science!), I provide a tutorial at \\nhttps://homl.info/tutorials\\n.\\nYou will also find a tutorial on differential calculus, which may be helpful to\\nunderstand how neural networks are trained, but it’s not entirely essential to\\ngrasp the important concepts. This book also uses other mathematical\\nconcepts occasionally, such as exponentials and logarithms, a bit of\\nprobability theory, and some basic statistics concepts, but nothing too\\nadvanced. If you need help on any of these, please check out\\nhttps://khanacademy.org\\n, which offers many excellent and free math courses\\nonline.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 12, 'page_label': '13', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Roadmap\\nThis book is organized in two parts. \\nPart I, “The Fundamentals of \\nMachine\\nLearning\\n”\\n, covers the following topics:\\nWhat machine learning is, what problems it tries to solve, and the main\\ncategories and fundamental concepts of its systems\\nThe steps in a typical machine learning project\\nLearning by fitting a model to data\\nOptimizing a cost function\\nHandling, cleaning, and preparing data\\nSelecting and engineering features\\nSelecting a model and tuning hyperparameters using cross-validation\\nThe challenges of machine learning, in particular underfitting and\\noverfitting (the bias/variance trade-off)\\nThe most common learning algorithms: linear and polynomial\\nregression, logistic regression, \\nk\\n-nearest neighbors, support vector\\nmachines, decision trees, random forests, and ensemble methods\\nReducing the dimensionality of the training data to fight the “curse of\\ndimensionality”\\nOther unsupervised learning techniques, including clustering, density\\nestimation, and anomaly detection\\nPart II, “Neural Networks and Deep Learning”\\n, covers the following topics:\\nWhat neural nets are and what they’re good for\\nBuilding and training neural nets using TensorFlow and Keras\\nThe most important neural net architectures: feedforward neural nets for'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 13, 'page_label': '14', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='tabular data, convolutional nets for computer vision, recurrent nets and\\nlong short-term memory (LSTM) nets for sequence processing,\\nencoder–decoders and transformers for natural language processing (and\\nmore!), autoencoders, generative adversarial networks (GANs), and\\ndiffusion models for generative learning\\nTechniques for training deep neural nets\\nHow to build an agent (e.g., a bot in a game) that can learn good\\nstrategies through trial and error, using reinforcement learning\\nLoading and preprocessing large amounts of data efficiently\\nTraining and deploying TensorFlow models at scale\\nThe first part is based mostly on Scikit-Learn, while the second part uses\\nTensorFlow and Keras.\\nCAUTION\\nDon’t jump into deep waters too hastily: while deep learning is no doubt one of the most\\nexciting areas in machine learning, you should master the fundamentals first. Moreover,\\nmost problems can be solved quite well using simpler techniques such as random forests\\nand ensemble methods (discussed in \\nPart I\\n). deep learning is best suited for complex\\nproblems such as image recognition, speech recognition, or natural language processing,\\nand it requires a lot of data, computing power, and patience (unless you can leverage a\\npretrained neural network, as you will see).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 14, 'page_label': '15', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Changes Between the First and the Second Edition\\nIf you have already read the first edition, here are the main changes between\\nthe first and the second edition:\\nAll the code was migrated from TensorFlow 1.x to TensorFlow 2.x, and\\nI replaced most of the low-level TensorFlow code (graphs, sessions,\\nfeature columns, estimators, and so on) with much simpler Keras code.\\nThe second edition introduced the Data API for loading and\\npreprocessing large datasets, the distribution strategies API to train and\\ndeploy TF models at scale, TF Serving and Google Cloud AI Platform to\\nproductionize models, and (briefly) TF Transform, TFLite, TF\\nAddons/Seq2Seq, TensorFlow.js, and TF Agents.\\nIt also introduced many additional ML topics, including a new chapter\\non unsupervised learning, computer vision techniques for object\\ndetection and semantic segmentation, handling sequences using\\nconvolutional neural networks (CNNs), natural language processing\\n(NLP) using recurrent neural networks (RNNs), CNNs and transformers,\\nGANs, and more.\\nSee \\nhttps://homl.info/changes2\\n for more details.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 15, 'page_label': '16', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Changes Between the Second and the Third Edition\\nIf you read the second edition, here are the main changes between the second\\nand the third edition:\\nAll the code was updated to the latest library versions. In particular, this\\nthird edition introduces many new additions to Scikit-Learn (e.g.,\\nfeature name tracking, histogram-based gradient boosting, label\\npropagation, and more). It also introduces the \\nKeras Tuner\\n library for\\nhyperparameter tuning, Hugging Face’s \\nTransformers\\n library for natural\\nlanguage processing, and Keras’s new preprocessing and data\\naugmentation layers.\\nSeveral vision models were added (ResNeXt, DenseNet, MobileNet,\\nCSPNet, and EfficientNet), as well as guidelines for choosing the right\\none.\\nChapter 15\\n now analyzes the Chicago bus and rail ridership data instead\\nof generated time series, and it introduces the ARMA model and its\\nvariants.\\nChapter 16\\n on natural language processing now builds an English-to-\\nSpanish translation model, first using an encoder–decoder RNN, then\\nusing a transformer model. The chapter also covers language models\\nsuch as Switch Transformers, DistilBERT, T5, and PaLM (with chain-\\nof-thought prompting). In addition, it introduces vision transformers\\n(ViTs) and gives an overview of a few transformer-based visual models,\\nsuch as data-efficient image transformers (DeiTs), Perceiver, and DINO,\\nas well as a brief overview of some large multimodal models, including\\nCLIP, DALL·E, Flamingo, and GATO.\\nChapter 17\\n on generative learning now introduces diffusion models, and\\nshows how to implement a denoising diffusion probabilistic model\\n(DDPM) from scratch.\\nChapter 19\\n migrated from Google Cloud AI Platform to Google Vertex'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 16, 'page_label': '17', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='AI, and uses distributed Keras Tuner for large-scale hyperparameter\\nsearch. The chapter now includes TensorFlow.js code that you can\\nexperiment with online. It also introduces additional distributed training\\ntechniques, including PipeDream and Pathways.\\nIn order to allow for all the new content, some sections were moved\\nonline, including installation instructions, kernel principal component\\nanalysis (PCA), mathematical details of Bayesian Gaussian mixtures, TF\\nAgents, and former appendices A (exercise solutions), C (support vector\\nmachine math), and E (extra neural net architectures).\\nSee \\nhttps://homl.info/changes3\\n for more details.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 17, 'page_label': '18', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Other Resources\\nMany excellent resources are available to learn about machine learning. \\nFor\\nexample, Andrew Ng’s \\nML course on Coursera\\n is amazing, although it\\nrequires a significant time investment.\\nThere are also many interesting websites about machine learning, including\\nScikit-Learn’s exceptional \\nUser Guide\\n. \\nYou may also enjoy \\nDataquest\\n, which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on\\nQuora\\n.\\nThere are many other introductory books about machine learning. In\\nparticular:\\nJoel Grus’s \\nData Science from Scratch\\n, 2nd edition (O’Reilly), presents\\nthe fundamentals of machine learning and implements some of the main\\nalgorithms in pure Python (from scratch, as the name suggests).\\nStephen Marsland’s \\nMachine Learning: An Algorithmic Perspective\\n,\\n2nd edition (Chapman & Hall), is a great introduction to machine\\nlearning, covering a wide range of topics in depth with code examples in\\nPython (also from scratch, but using NumPy).\\nSebastian Raschka’s \\nPython Machine Learning\\n, 3rd edition (Packt\\nPublishing), is also a great introduction to machine learning and\\nleverages Python open source libraries (Pylearn 2 and Theano).\\nFrançois Chollet’s \\nDeep Learning with Python\\n, 2nd edition (Manning),\\nis a very practical book that covers a large range of topics in a clear and\\nconcise way, as you might expect from the author of the excellent Keras\\nlibrary. It favors code examples over mathematical theory.\\nAndriy Burkov’s \\nThe Hundred-Page Machine Learning Book\\n (self-\\npublished) is very short but covers an impressive range of topics,\\nintroducing them in approachable terms without shying away from the\\nmath equations.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 18, 'page_label': '19', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin’s\\nLearning from Data\\n (AMLBook) is a rather theoretical approach to ML\\nthat provides deep insights, in particular on the bias/variance trade-off\\n(see \\nChapter 4\\n).\\nStuart Russell and Peter Norvig’s \\nArtificial Intelligence: A Modern\\nApproach\\n, 4th edition (Pearson), is a great (and huge) book covering an\\nincredible amount of topics, including machine learning. It helps put ML\\ninto perspective.\\nJeremy Howard and Sylvain Gugger’s \\nDeep Learning for Coders with\\nfastai and PyTorch\\n (O’Reilly) provides a wonderfully clear and practical\\nintroduction to deep learning using the fastai and PyTorch libraries.\\nFinally, joining ML competition websites such as \\nKaggle.com\\n will allow you\\nto practice your skills on real-world problems, with help and insights from\\nsome of the best ML professionals out there.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 19, 'page_label': '20', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Conventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file\\nextensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to\\nprogram elements such as variable or function names, databases, data\\ntypes, environment variables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values\\ndetermined by context.\\nPunctuation\\nTo avoid any confusion, punctutation appears outside of quotes\\nthroughout the book. My apologies to the purists.\\nTIP\\nThis element signifies a tip or suggestion.\\nNOTE\\nThis element signifies a general note.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 20, 'page_label': '21', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='WARNING\\nThis element indicates a warning or caution.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 21, 'page_label': '22', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='O’Reilly Online Learning\\nNOTE\\nFor more than 40 years, \\nO’Reilly Media\\n has provided technology and business training,\\nknowledge, and insight to help companies succeed.\\nOur unique network of experts and innovators share their knowledge and\\nexpertise through books, articles, and our online learning platform.\\nO’Reilly’s online learning platform gives you on-demand access to live\\ntraining courses, in-depth learning paths, interactive coding environments,\\nand a vast collection of text and video from O’Reilly and 200+ other\\npublishers. For more information, visit \\nhttps://oreilly.com\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 22, 'page_label': '23', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the\\npublisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any\\nadditional information. You can access this page at \\nhttps://homl.info/oreilly3\\n.\\nEmail \\nbookquestions@oreilly.com\\n to comment or ask technical questions\\nabout this book.\\nFor news and information about our books and courses, visit\\nhttps://oreilly.com\\n.\\nFind us on LinkedIn: \\nhttps://linkedin.com/company/oreilly-media\\nFollow us on Twitter: \\nhttps://twitter.com/oreillymedia\\nWatch us on YouTube: \\nhttps://youtube.com/oreillymedia'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 23, 'page_label': '24', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Acknowledgments\\nNever in my wildest dreams did I imagine that the first and second editions of\\nthis book would get such a large audience. I received so many messages from\\nreaders, many asking questions, some kindly pointing out errata, and most\\nsending me encouraging words. I cannot express how grateful I am to all\\nthese readers for their tremendous support. Thank you all so very much!\\nPlease do not hesitate to \\nfile issues on GitHub\\n if you find errors in the code\\nexamples (or just to ask questions), or to submit \\nerrata\\n if you find errors in\\nthe text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on. I\\nfind such feedback incredibly motivating. If you find this book helpful, I\\nwould love it if you could share your story with me, either privately (e.g., via\\nLinkedIn\\n) or publicly (e.g., tweet me at @aureliengeron or write an \\nAmazon\\nreview\\n).\\nHuge thanks as well to all the wonderful people who offered their time and\\nexpertise to review this third edition, correcting errors and making countless\\nsuggestions. This edition is so much better thanks to them: Olzhas\\nAkpambetov, George Bonner, François Chollet, Siddha Gangju, Sam\\nGoodman, Matt Harrison, Sasha Sobran, Lewis Tunstall, Leandro von Werra,\\nand my dear brother Sylvain. You are all amazing!\\nI am also very grateful to the many people who supported me along the way,\\nby answering my questions, suggesting improvements, and contributing to\\nthe code on GitHub: in particular, Yannick Assogba, Ian Beauregard, Ulf\\nBissbort, Rick Chao, Peretz Cohen, Kyle Gallatin, Hannes Hapke, Victor\\nKhaustov, Soonson Kwon, Eric Lebigot, Jason Mayes, Laurence Moroney,\\nSara Robinson, Joaquín Ruales, and Yuefeng Zhou.\\nThis book wouldn’t exist without O’Reilly’s fantastic staff, in particular\\nNicole Taché, who gave me insightful feedback and was always cheerful,\\nencouraging, and helpful: I could not dream of a better editor. Big thanks to\\nMichele Cronin as well, who cheered me on through the final chapters and\\nmanaged to get me past the finish line. Thanks to the whole production team,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 24, 'page_label': '25', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='in particular Elizabeth Kelly and Kristen Brown. Thanks as well to Kim\\nCofer for the thorough copyediting, and to Johnny O’Toole, who managed\\nthe relationship with Amazon and answered many of my questions. Thanks to\\nKate Dullea for greatly improving my illustrations. Thanks to Marie\\nBeaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in\\nthis project and helping me define its scope. Thanks to Matt Hacker and all of\\nthe Atlas team for answering all my technical questions regarding formatting,\\nAsciiDoc, MathML, and LaTeX, and thanks to Nick Adams, Rebecca\\nDemarest, Rachel Head, Judith McConville, Helen Monroe, Karen\\nMontgomery, Rachel Roumeliotis, and everyone else at O’Reilly who\\ncontributed to this book.\\nI’ll never forget all the wonderful people who helped me with the first and\\nsecond editions of this book: friends, colleagues, experts, including many\\nmembers of the TensorFlow team. The list is long: Olzhas Akpambetov,\\nKarmel Allison, Martin Andrews, David Andrzejewski, Paige Bailey, Lukas\\nBiewald, Eugene Brevdo, William Chargin, François Chollet, Clément\\nCourbet, Robert Crowe, Mark Daoust, Daniel “Wolff” Dobson, Julien\\nDubois, Mathias Kende, Daniel Kitachewsky, Nick Felt, Bruce Fontaine,\\nJustin Francis, Goldie Gadde, Irene Giannoumis, Ingrid von Glehn, Vincent\\nGuilbeau, Sandeep Gupta, Priya Gupta, Kevin Haas, Eddy Hung,\\nKonstantinos Katsiapis, Viacheslav Kovalevskyi, Jon Krohn, Allen Lavoie,\\nKarim Matrah, Grégoire Mesnil, Clemens Mewald, Dan Moldovan, Dominic\\nMonn, Sean Morgan, Tom O’Malley, James Pack, Alexander Pak, Haesun\\nPark, Alexandre Passos, Ankur Patel, Josh Patterson, André Susano Pinto,\\nAnthony Platanios, Anosh Raj, Oscar Ramirez, Anna Revinskaya, Saurabh\\nSaxena, Salim Sémaoune, Ryan Sepassi, Vitor Sessak, Jiri Simsa, Iain\\nSmears, Xiaodan Song, Christina Sorokin, Michel Tessier, Wiktor Tomczak,\\nDustin Tran, Todd Wang, Pete Warden, Rich Washington, Martin Wicke,\\nEdd Wilder-James, Sam Witteveen, Jason Zaman, Yuefeng Zhou, and my\\nbrother Sylvain.\\nLast but not least, I am infinitely grateful to my beloved wife, Emmanuelle,\\nand to our three wonderful children, Alexandre, Rémi, and Gabrielle, for\\nencouraging me to work hard on this book. Their insatiable curiosity was'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 25, 'page_label': '26', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='priceless: explaining some of the most difficult concepts in this book to my\\nwife and children helped me clarify my thoughts and directly improved many\\nparts of it. Plus, they keep bringing me cookies and coffee, who could ask for\\nmore?\\n1\\n Geoffrey E. Hinton et al., “A Fast Learning Algorithm for Deep Belief Nets”, \\nNeural\\nComputation\\n 18 (2006): 1527–1554.\\n2\\n Despite the fact that Yann LeCun’s deep convolutional neural networks had worked well for\\nimage recognition since the 1990s, although they were not as general-purpose.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 26, 'page_label': '27', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Part I. \\nThe Fundamentals of\\nMachine Learning'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 27, 'page_label': '28', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 1. \\nThe Machine Learning\\nLandscape\\nNot so long ago, if you had picked up your phone and asked it the way home,\\nit would have ignored you—and people would have questioned your sanity.\\nBut machine learning is no longer science fiction: billions of people use it\\nevery day. \\nAnd the truth is it has actually been around for decades in some\\nspecialized applications, such as optical character recognition (OCR). \\nThe\\nfirst ML application that really became mainstream, improving the lives of\\nhundreds of millions of people, took over the world back in the 1990s: the\\nspam filter\\n. It’s not exactly a self-aware robot, but it does technically qualify\\nas machine learning: it has actually learned so well that you seldom need to\\nflag an email as spam anymore. It was followed by hundreds of ML\\napplications that now quietly power hundreds of products and features that\\nyou use regularly: voice prompts, automatic translation, image search,\\nproduct recommendations, and many more.\\nWhere does machine learning start and where does it end? What exactly does\\nit mean for a machine to \\nlearn\\n something? If I download a copy of all\\nWikipedia articles, has my computer really learned something? Is it suddenly\\nsmarter? In this chapter I will start by clarifying what machine learning is and\\nwhy you may want to use it.\\nThen, before we set out to explore the machine learning continent, we will\\ntake a look at the map and learn about the main regions and the most notable\\nlandmarks: supervised versus unsupervised learning and their variants, online\\nversus batch learning, instance-based versus model-based learning. Then we\\nwill look at the workflow of a typical ML project, discuss the main\\nchallenges you may face, and cover how to evaluate and fine-tune a machine\\nlearning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every\\ndata scientist should know by heart. It will be a high-level overview (it’s the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 28, 'page_label': '29', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='only chapter without much code), all rather simple, but my goal is to ensure\\neverything is crystal clear to you before we continue on to the rest of the\\nbook. So grab a coffee and let’s get started!\\nTIP\\nIf you are already familiar with machine learning basics, you may want to skip directly to\\nChapter 2\\n. If you are not sure, try to answer all the questions listed at the end of the\\nchapter before moving on.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 29, 'page_label': '30', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='What Is Machine Learning?\\nMachine learning is the science (and art) of programming computers so they\\ncan \\nlearn from data\\n.\\nHere is a slightly more general definition:\\n[Machine learning is the] field of study that gives computers the ability to\\nlearn without being explicitly programmed.\\n—\\nArthur Samuel, \\n1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience \\nE\\n with respect to\\nsome task \\nT\\n and some performance measure \\nP\\n, if its performance on \\nT\\n, as\\nmeasured by \\nP\\n, improves with experience \\nE\\n.\\n—\\nTom Mitchell, \\n1997\\nYour spam filter is a machine learning program that, given examples of spam\\nemails (flagged by users) and examples of regular emails (nonspam, also\\ncalled “ham”), can learn to flag spam. \\nThe examples that the system uses to\\nlearn are called the \\ntraining set\\n. Each training example is called a \\ntraining\\ninstance\\n (or \\nsample\\n). The part of a machine learning system that learns and\\nmakes predictions is called a \\nmodel\\n. Neural networks and random forests are\\nexamples of models.\\nIn this case, the task \\nT\\n is to flag spam for new emails, the experience \\nE\\n is the\\ntraining data\\n, and the performance measure \\nP\\n needs to be defined; for\\nexample, you can use the ratio of correctly classified emails. \\nThis particular\\nperformance measure is called \\naccuracy\\n, and it is often used in classification\\ntasks.\\nIf you just download a copy of all Wikipedia articles, your computer has a lot\\nmore data, but it is not suddenly better at any task. This is not machine\\nlearning.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 30, 'page_label': '31', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Why Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming\\ntechniques (\\nFigure 1-1\\n):\\n1\\n. \\nFirst you would examine what spam typically looks like. You might\\nnotice that some words or phrases (such as “4U”, “credit card”, “free”,\\nand “amazing”) tend to come up a lot in the subject line. Perhaps you\\nwould also notice a few other patterns in the sender’s name, the email’s\\nbody, and other parts of the email.\\n2\\n. \\nYou would write a detection algorithm for each of the patterns that you\\nnoticed, and your program would flag emails as spam if a number of\\nthese patterns were detected.\\n3\\n. \\nYou would test your program and repeat steps 1 and 2 until it was good\\nenough to launch.\\nFigure 1-1. \\nThe traditional approach'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 31, 'page_label': '32', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Since the problem is difficult, your program will likely become a long list of\\ncomplex rules—pretty hard to maintain.\\nIn contrast, a spam filter based on machine learning techniques automatically\\nlearns which words and phrases are good predictors of spam by detecting\\nunusually frequent patterns of words in the spam examples compared to the\\nham examples (\\nFigure 1-2\\n). The program is much shorter, easier to maintain,\\nand most likely more accurate.\\nFigure 1-2. \\nThe machine learning approach\\nWhat if spammers notice that all their emails containing “4U” are blocked?\\nThey might start writing “For U” instead. A spam filter using traditional\\nprogramming techniques would need to be updated to flag “For U” emails. If\\nspammers keep working around your spam filter, you will need to keep\\nwriting new rules forever.\\nIn contrast, a spam filter based on machine learning techniques automatically\\nnotices that “For U” has become unusually frequent in spam flagged by users,\\nand it starts flagging them without your intervention (\\nFigure 1-3\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 32, 'page_label': '33', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-3. \\nAutomatically adapting to change\\nAnother area where machine learning shines is for problems that either are\\ntoo complex for traditional approaches or have no known algorithm. For\\nexample, consider speech recognition. \\nSay you want to start simple and write\\na program capable of distinguishing the words “one” and “two”. You might\\nnotice that the word “two” starts with a high-pitch sound (“T”), so you could\\nhardcode an algorithm that measures high-pitch sound intensity and use that\\nto distinguish ones and twos\\n\\u2060\\n—but obviously this technique will not scale to\\nthousands of words spoken by millions of very different people in noisy\\nenvironments and in dozens of languages. The best solution (at least today) is\\nto write an algorithm that learns by itself, given many example recordings for\\neach word.\\nFinally, machine learning can help humans learn (\\nFigure 1-4\\n). ML models\\ncan be inspected to see what they have learned (although for some models\\nthis can be tricky). For instance, once a spam filter has been trained on\\nenough spam, it can easily be inspected to reveal the list of words and\\ncombinations of words that it believes are the best predictors of spam.\\nSometimes this will reveal unsuspected \\ncorrelations\\n or new trends, and\\nthereby lead to a better understanding of the problem. \\nDigging into large\\namounts of data to discover hidden patterns is called \\ndata mining\\n, and\\nmachine learning excels at it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 33, 'page_label': '34', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-4. \\nMachine learning can help humans learn\\nTo summarize, machine learning is great for:\\nProblems for which existing solutions require a lot of fine-tuning or long\\nlists of rules (a machine learning model can often simplify code and\\nperform better than the traditional approach)\\nComplex problems for which using a traditional approach yields no\\ngood solution (the best machine learning techniques can perhaps find a\\nsolution)\\nFluctuating environments (a machine learning system can easily be\\nretrained on new data, always keeping it up to date)\\nGetting insights about complex problems and large amounts of data'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 34, 'page_label': '35', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Examples of Applications\\nLet’s look at some concrete examples of machine learning tasks, along with\\nthe techniques that can tackle them:\\nAnalyzing images of products on a production line to automatically classify\\nthem\\nThis is image classification, typically performed using convolutional\\nneural networks (CNNs; see \\nChapter 14\\n) or sometimes transformers (see\\nChapter 16\\n).\\nDetecting tumors in brain scans\\nThis is semantic image segmentation, where each pixel in the image is\\nclassified (as we want to determine the exact location and shape of\\ntumors), typically using CNNs or transformers.\\nAutomatically classifying news articles\\nThis is natural language processing (NLP), and more specifically text\\nclassification, which can be tackled using recurrent neural networks\\n(RNNs) and CNNs, but transformers work even better (see \\nChapter 16\\n).\\nAutomatically flagging offensive comments on discussion forums\\nThis is also text classification, using the same NLP tools.\\nSummarizing long documents automatically\\nThis is a branch of NLP called text summarization, again using the same\\ntools.\\nCreating a chatbot or a personal assistant\\nThis involves many NLP components, including natural language\\nunderstanding (NLU) and question-answering modules.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 35, 'page_label': '36', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Forecasting your company’s revenue next year, based on many performance\\nmetrics\\nThis is a regression task (i.e., predicting values) that may be tackled using\\nany regression model, such as a linear regression or polynomial\\nregression model (see \\nChapter 4\\n), a regression support vector machine\\n(see \\nChapter 5\\n), a regression random forest (see \\nChapter 7\\n), or an\\nartificial neural network (see \\nChapter 10\\n). If you want to take into\\naccount sequences of past performance metrics, you may want to use\\nRNNs, CNNs, or transformers (see Chapters \\n15\\n and \\n16\\n).\\nMaking your app react to voice commands\\nThis is speech recognition, which requires processing audio samples:\\nsince they are long and complex sequences, they are typically processed\\nusing RNNs, CNNs, or transformers (see Chapters \\n15\\n and \\n16\\n).\\nDetecting credit card fraud\\nThis is anomaly detection, which can be tackled using isolation forests,\\nGaussian mixture models (see \\nChapter 9\\n), or autoencoders (see\\nChapter 17\\n).\\nSegmenting clients based on their purchases so that you can design a\\ndifferent marketing strategy for each segment\\nThis is clustering, which can be achieved using \\nk\\n-means, DBSCAN, and\\nmore (see \\nChapter 9\\n).\\nRepresenting a complex, high-dimensional dataset in a clear and insightful\\ndiagram\\nThis is data visualization, often involving dimensionality reduction\\ntechniques (see \\nChapter 8\\n).\\nRecommending a product that a client may be interested in, based on past\\npurchases'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 36, 'page_label': '37', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='This is a recommender system. One approach is to feed past purchases\\n(and other information about the client) to an artificial neural network\\n(see \\nChapter 10\\n), and get it to output the most likely next purchase. This\\nneural net would typically be trained on past sequences of purchases\\nacross all clients.\\nBuilding an intelligent bot for a game\\nThis is often tackled using reinforcement learning (RL; see \\nChapter 18\\n),\\nwhich is a branch of machine learning that trains agents (such as bots) to\\npick the actions that will maximize their rewards over time (e.g., a bot\\nmay get a reward every time the player loses some life points), within a\\ngiven environment (such as the game). The famous AlphaGo program\\nthat beat the world champion at the game of Go was built using RL.\\nThis list could go on and on, but hopefully it gives you a sense of the\\nincredible breadth and complexity of the tasks that machine learning can\\ntackle, and the types of techniques that you would use for each task.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 37, 'page_label': '38', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Types of Machine Learning Systems\\nThere are so many different types of machine learning systems that it is\\nuseful to classify them in broad categories, based on the following criteria:\\nHow they are supervised during training (supervised, unsupervised,\\nsemi-supervised, self-supervised, and others)\\nWhether or not they can learn incrementally on the fly (online versus\\nbatch learning)\\nWhether they work by simply comparing new data points to known data\\npoints, or instead by detecting patterns in the training data and building a\\npredictive model, much like scientists do (instance-based versus model-\\nbased learning)\\nThese criteria are not exclusive; you can combine them in any way you like.\\nFor example, a state-of-the-art spam filter may learn on the fly using a deep\\nneural network model trained using human-provided examples of spam and\\nham; this makes it an online, model-based, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 38, 'page_label': '39', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Training Supervision\\nML systems can be classified according to the amount and type of\\nsupervision they get during training. There are many categories, but we’ll\\ndiscuss the main ones: supervised learning, unsupervised learning, self-\\nsupervised learning, semi-supervised learning, and reinforcement learning.\\nSupervised learning\\nIn \\nsupervised learning\\n, the training set you feed to the algorithm includes the\\ndesired solutions, called \\nlabels\\n (\\nFigure 1-5\\n).\\nFigure 1-5. \\nA labeled training set for spam classification (an example of supervised learning)\\nA typical supervised learning task is \\nclassification\\n. The spam filter is a good\\nexample of this: it is trained with many example emails along with their \\nclass\\n(spam or ham), and it must learn how to classify new emails.\\nAnother typical task is to predict a \\ntarget\\n numeric value, such as the price of\\na car, given a set of \\nfeatures\\n (mileage, age, brand, etc.). This sort of task is\\ncalled \\nregression\\n (\\nFigure 1-6\\n).\\n\\u2060\\n To train the system, you need to give it\\nmany examples of cars, including both their features and their targets (i.e.,\\ntheir prices).\\nNote that some regression models can be used for classification as well, and\\nvice versa. \\nFor example, \\nlogistic regression\\n is commonly used for\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 39, 'page_label': '40', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='classification, as it can output a value that corresponds to the probability of\\nbelonging to a given class (e.g., 20% chance of being spam).\\nFigure 1-6. \\nA regression problem: predict a value, given an input feature (there are usually multiple\\ninput features, and sometimes multiple output values)\\nNOTE\\nThe words \\ntarget\\n and \\nlabel\\n are generally treated as synonyms in supervised learning, but\\ntarget\\n is more common in regression tasks and \\nlabel\\n is more common in classification\\ntasks. \\nMoreover, \\nfeatures\\n are sometimes called \\npredictors\\n or \\nattributes\\n. These terms may\\nrefer to individual samples (e.g., “this car’s mileage feature is equal to 15,000”) or to all\\nsamples (e.g., “the mileage feature is strongly correlated with price”).\\nUnsupervised learning\\nIn \\nunsupervised learning\\n, as you might guess, the training data is unlabeled\\n(\\nFigure 1-7\\n). The system tries to learn without a teacher.\\nFor example, say you have a lot of data about your blog’s visitors. \\nYou may\\nwant to run a \\nclustering\\n algorithm to try to detect groups of similar visitors\\n(\\nFigure 1-8\\n). At no point do you tell the algorithm which group a visitor\\nbelongs to: it finds those connections without your help. For example, it\\nmight notice that 40% of your visitors are teenagers who love comic books\\nand generally read your blog after school, while 20% are adults who enjoy'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 40, 'page_label': '41', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='sci-fi and who visit during the weekends. \\nIf you use a \\nhierarchical clustering\\nalgorithm, it may also subdivide each group into smaller groups. This may\\nhelp you target your posts for each group.\\nFigure 1-7. \\nAn unlabeled training set for unsupervised learning\\nFigure 1-8. \\nClustering\\nVisualization\\n algorithms are also good examples of unsupervised learning:\\nyou feed them a lot of complex and unlabeled data, and they output a 2D or\\n3D representation of your data that can easily be plotted (\\nFigure 1-9\\n). \\nThese\\nalgorithms try to preserve as much structure as they can (e.g., trying to keep'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 41, 'page_label': '42', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='separate clusters in the input space from overlapping in the visualization) so\\nthat you can understand how the data is organized and perhaps identify\\nunsuspected patterns.\\nA related task is \\ndimensionality reduction\\n, in which the goal is to simplify the\\ndata without losing too much information. One way to do this is to merge\\nseveral correlated features into one. For example, a car’s mileage may be\\nstrongly correlated with its age, so the dimensionality reduction algorithm\\nwill merge them into one feature that represents the car’s wear and tear. \\nThis\\nis called \\nfeature extraction\\n.\\nFigure 1-9. \\nExample of a t-SNE visualization highlighting semantic clusters\\n\\u2060\\nTIP\\nIt is often a good idea to try to reduce the number of dimensions in your training data\\nusing a dimensionality reduction algorithm before you feed it to another machine learning\\nalgorithm (such as a supervised learning algorithm). It will run much faster, the data will\\ntake up less disk and memory space, and in some cases it may also perform better.\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 42, 'page_label': '43', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Yet another important unsupervised task is \\nanomaly detection\\n—for example,\\ndetecting unusual credit card transactions to prevent fraud, catching\\nmanufacturing defects, or automatically removing outliers from a dataset\\nbefore feeding it to another learning algorithm. The system is shown mostly\\nnormal instances during training, so it learns to recognize them; then, when it\\nsees a new instance, it can tell whether it looks like a normal one or whether\\nit is likely an anomaly (see \\nFigure 1-10\\n). \\nA very similar task is \\nnovelty\\ndetection\\n: it aims to detect new instances that look different from all instances\\nin the training set. This requires having a very “clean” training set, devoid of\\nany instance that you would like the algorithm to detect. For example, if you\\nhave thousands of pictures of dogs, and 1% of these pictures represent\\nChihuahuas, then a novelty detection algorithm should not treat new pictures\\nof Chihuahuas as novelties. On the other hand, anomaly detection algorithms\\nmay consider these dogs as so rare and so different from other dogs that they\\nwould likely classify them as anomalies (no offense to Chihuahuas).\\nFigure 1-10. \\nAnomaly detection\\nFinally, another common unsupervised task is \\nassociation rule learning\\n, in\\nwhich the goal is to dig into large amounts of data and discover interesting\\nrelations between attributes. For example, suppose you own a supermarket.\\nRunning an association rule on your sales logs may reveal that people who\\npurchase barbecue sauce and potato chips also tend to buy steak. Thus, you\\nmay want to place these items close to one another.\\nSemi-supervised learning\\nSince labeling data is usually time-consuming and costly, you will often have'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 43, 'page_label': '44', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='plenty of unlabeled instances, and few labeled instances. Some algorithms\\ncan deal with data that’s partially labeled. This is called \\nsemi-supervised\\nlearning\\n (\\nFigure 1-11\\n).\\nFigure 1-11. \\nSemi-supervised learning with two classes (triangles and squares): the unlabeled\\nexamples (circles) help classify a new instance (the cross) into the triangle class rather than the square\\nclass, even though it is closer to the labeled squares\\nSome photo-hosting services, such as Google Photos, are good examples of\\nthis. Once you upload all your family photos to the service, it automatically\\nrecognizes that the same person A shows up in photos 1, 5, and 11, while\\nanother person B shows up in photos 2, 5, and 7. This is the unsupervised\\npart of the algorithm (clustering). Now all the system needs is for you to tell\\nit who these people are. Just add one label per person\\n\\u2060\\n and it is able to\\nname everyone in every photo, which is useful for searching photos.\\nMost semi-supervised learning algorithms are combinations of unsupervised\\nand supervised algorithms. For example, a clustering algorithm may be used\\nto group similar instances together, and then every unlabeled instance can be\\nlabeled with the most common label in its cluster. Once the whole dataset is\\nlabeled, it is possible to use any supervised learning algorithm.\\nSelf-supervised learning\\nAnother approach to machine learning involves actually generating a fully\\nlabeled dataset from a fully unlabeled one. \\nAgain, once the whole dataset is\\nlabeled, any supervised learning algorithm can be used. This approach is\\ncalled \\nself-supervised learning\\n.\\nFor example, if you have a large dataset of unlabeled images, you can\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 44, 'page_label': '45', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='randomly mask a small part of each image and then train a model to recover\\nthe original image (\\nFigure 1-12\\n). During training, the masked images are\\nused as the inputs to the model, and the original images are used as the labels.\\nFigure 1-12. \\nSelf-supervised learning example: input (left) and target (right)\\nThe resulting model may be quite useful in itself—for example, to repair\\ndamaged images or to erase unwanted objects from pictures. But more often\\nthan not, a model trained using self-supervised learning is not the final goal.\\nYou’ll usually want to tweak and fine-tune the model for a slightly different\\ntask—one that you actually care about.\\nFor example, suppose that what you really want is to have a pet classification\\nmodel: given a picture of any pet, it will tell you what species it belongs to. If\\nyou have a large dataset of unlabeled photos of pets, you can start by training\\nan image-repairing model using self-supervised learning. Once it’s\\nperforming well, it should be able to distinguish different pet species: when it\\nrepairs an image of a cat whose \\nface is\\n masked, it must know not to add a\\ndog’s face. Assuming your model’s architecture allows it (and most neural\\nnetwork architectures do), it is then possible to tweak the model so that it\\npredicts pet species instead of repairing images. The final step consists of\\nfine-tuning the model on a labeled dataset: the model already knows what\\ncats, dogs, and other pet species look like, so this step is only needed so the\\nmodel can learn the mapping between the species it already knows and the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 45, 'page_label': '46', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='labels we expect \\nfrom it.\\nNOTE\\nTransferring knowledge from one task to another is called \\ntransfer learning\\n, and it’s one\\nof the most important techniques in machine learning today, especially when using \\ndeep\\nneural networks\\n (i.e., neural networks composed of many layers of neurons). We will\\ndiscuss this in detail in \\nPart II\\n.\\nSome people consider self-supervised learning to be a part of unsupervised\\nlearning, since it deals with fully unlabeled datasets. But self-supervised\\nlearning uses (generated) labels during training, so in that regard it’s closer to\\nsupervised learning. And the term “unsupervised learning” is generally used\\nwhen dealing with tasks like clustering, dimensionality reduction, or anomaly\\ndetection, whereas self-supervised learning focuses on the same tasks as\\nsupervised learning: mainly classification and regression. In short, it’s best to\\ntreat self-supervised learning as its own category.\\nReinforcement learning\\nReinforcement learning\\n is a very different beast. \\nThe learning system, called\\nan \\nagent\\n in this context, can observe the environment, select and perform\\nactions, and get \\nrewards\\n in return (or \\npenalties\\n in the form of negative\\nrewards, as shown in \\nFigure 1-13\\n). It must then learn by itself what is the best\\nstrategy, called a \\npolicy\\n, to get the most reward over time. A policy defines\\nwhat action the agent should choose when it is in a given situation.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 46, 'page_label': '47', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-13. \\nReinforcement learning\\nFor example, many robots implement reinforcement learning algorithms to\\nlearn how to walk. \\nDeepMind’s AlphaGo program is also a good example of\\nreinforcement learning: it made the headlines in May 2017 when it beat Ke\\nJie, the number one ranked player in the world at the time, at the game of Go.\\nIt learned its winning policy by analyzing millions of games, and then\\nplaying many games against itself. Note that learning was turned off during\\nthe games against the champion; AlphaGo was just applying the policy it had\\nlearned. As you will see in the next section, this is called \\noffline learning\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 47, 'page_label': '48', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Batch Versus Online Learning\\nAnother criterion used to classify machine learning systems is whether or not\\nthe system can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn \\nbatch learning\\n, the system is incapable of learning incrementally: it must\\nbe trained using all the available data. \\nThis will generally take a lot of time\\nand computing resources, so it is typically done offline. First the system is\\ntrained, and then it is launched into production and runs without learning\\nanymore; it just applies what it has learned. \\nThis is called \\noffline learning\\n.\\nUnfortunately, a model’s performance tends to decay slowly over time,\\nsimply because the world continues to evolve while the model remains\\nunchanged. \\nThis phenomenon is often called \\nmodel rot\\n or \\ndata drift\\n. The\\nsolution is to regularly retrain the model on up-to-date data. How often you\\nneed to do that depends on the use case: if the model classifies pictures of\\ncats and dogs, its performance will decay very slowly, but if the model deals\\nwith fast-evolving systems, for example making predictions on the financial\\nmarket, then it is likely to decay quite fast.\\nWARNING\\nEven a model trained to classify pictures of cats and dogs may need to be retrained\\nregularly, not because cats and dogs will mutate overnight, but because cameras keep\\nchanging, along with image formats, sharpness, brightness, and size ratios. Moreover,\\npeople may love different breeds next year, or they may decide to dress their pets with tiny\\nhats—who knows?\\nIf you want a batch learning system to know about new data (such as a new\\ntype of spam), you need to train a new version of the system from scratch on\\nthe full dataset (not just the new data, but also the old data), then replace the\\nold model with the new one. Fortunately, the whole process of training,\\nevaluating, and launching a machine learning system can be automated fairly'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 48, 'page_label': '49', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='easily (as we saw in \\nFigure 1-3\\n), so even a batch learning system can adapt to\\nchange. Simply update the data and train a new version of the system from\\nscratch as often as needed.\\nThis solution is simple and often works fine, but training using the full set of\\ndata can take many hours, so you would typically train a new system only\\nevery 24 hours or even just weekly. If your system needs to adapt to rapidly\\nchanging data (e.g., to predict stock prices), then you need a more reactive\\nsolution.\\nAlso, training on the full set of data requires a lot of computing resources\\n(CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a\\nlot of data and you automate your system to train from scratch every day, it\\nwill end up costing you a lot of money. If the amount of data is huge, it may\\neven be impossible to use a batch learning algorithm.\\nFinally, if your system needs to be able to learn autonomously and it has\\nlimited resources (e.g., a smartphone application or a rover on Mars), then\\ncarrying around large amounts of training data and taking up a lot of\\nresources to train for hours every day is a showstopper.\\nA better option in all these cases is to use algorithms that are capable of\\nlearning incrementally.\\nOnline learning\\nIn \\nonline learning\\n, you train the system incrementally by feeding it data\\ninstances sequentially, either individually or in small groups called \\nmini-\\nbatches\\n. \\nEach learning step is fast and cheap, so the system can learn about\\nnew data on the fly, as it arrives (see \\nFigure 1-14\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 49, 'page_label': '50', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-14. \\nIn online learning, a model is trained and launched into production, and then it keeps\\nlearning as new data comes in\\nOnline learning is useful for systems that need to adapt to change extremely\\nrapidly (e.g., to detect new patterns in the stock market). It is also a good\\noption if you have limited computing resources; for example, if the model is\\ntrained on a mobile device.\\nAdditionally, online learning algorithms can be used to train models on huge\\ndatasets that cannot fit in one machine’s main memory (this is called \\nout-of-\\ncore\\n learning). The algorithm loads part of the data, runs a training step on\\nthat data, and repeats the process until it has run on all of the data (see\\nFigure 1-15\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 50, 'page_label': '51', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-15. \\nUsing online learning to handle huge datasets\\nOne important parameter of online learning systems is how fast they should\\nadapt to changing data: \\nthis is called the \\nlearning rate\\n. If you set a high\\nlearning rate, then your system will rapidly adapt to new data, but it will also\\ntend to quickly forget the old data (and you don’t want a spam filter to flag\\nonly the latest kinds of spam it was shown). Conversely, if you set a low\\nlearning rate, the system will have more inertia; that is, it will learn more\\nslowly, but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points (outliers).\\nWARNING\\nOut-of-core learning is usually done offline (i.e., not on the live system), so \\nonline\\nlearning\\n can be a confusing name. \\nThink of it as \\nincremental learning\\n.\\nA big challenge with online learning is that if bad data is fed to the system,\\nthe system’s performance will decline, possibly quickly (depending on the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 51, 'page_label': '52', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='data quality and learning rate). If it’s a live system, your clients will notice.\\nFor example, bad data could come from a bug (e.g., a malfunctioning sensor\\non a robot), or it could come from someone trying to game the system (e.g.,\\nspamming a search engine to try to rank high in search results). To reduce\\nthis risk, you need to monitor your system closely and promptly switch\\nlearning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. You may also want to monitor the input data and react\\nto abnormal data; for example, using an anomaly detection algorithm (see\\nChapter 9\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 52, 'page_label': '53', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Instance-Based Versus Model-Based Learning\\nOne more way to categorize machine learning systems is by how they\\ngeneralize\\n. Most machine learning tasks are about making predictions. This\\nmeans that given a number of training examples, the system needs to be able\\nto make good predictions for (generalize to) examples it has never seen\\nbefore. Having a good performance measure on the training data is good, but\\ninsufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you\\nwere to create a spam filter this way, it would just flag all emails that are\\nidentical to emails that have already been flagged by users—not the worst\\nsolution, but certainly not the best.\\nInstead of just flagging emails that are identical to known spam emails, your\\nspam filter could be programmed to also flag emails that are very similar to\\nknown spam emails. \\nThis requires a \\nmeasure of similarity\\n between two\\nemails. A (very basic) similarity measure between two emails could be to\\ncount the number of words they have in common. The system would flag an\\nemail as spam if it has many words in common with a known spam email.\\nThis is called \\ninstance-based learning\\n: the system learns the examples by\\nheart, then generalizes to new cases by using a similarity measure to compare\\nthem to the learned examples (or a subset of them). For example, in \\nFigure 1-\\n16\\n the new instance would be classified as a triangle because the majority of\\nthe most similar instances belong to that class.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 53, 'page_label': '54', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-16. \\nInstance-based learning\\nModel-based learning and a typical machine learning workflow\\nAnother way to generalize from a set of examples is to build a model of these\\nexamples and then use that model to make \\npredictions\\n. \\nThis is called \\nmodel-\\nbased learning\\n (\\nFigure 1-17\\n).\\nFigure 1-17. \\nModel-based learning\\nFor example, suppose you want to know if money makes people happy, so\\nyou download the Better Life Index data from the \\nOECD’s website\\n and\\nWorld Bank stats\\n about gross domestic product (GDP) per capita. \\nThen you\\njoin the tables and sort by GDP per capita. \\nTable 1-1\\n shows an excerpt of'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 54, 'page_label': '55', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='what you get.\\nTable 1-1. \\nDoes money make people happier?\\nCountry\\nGDP per capita (USD)\\nLife satisfaction\\nTurkey\\n28,384\\n5.5\\nHungary\\n31,008\\n5.6\\nFrance\\n42,026\\n6.5\\nUnited States\\n60,236\\n6.9\\nNew Zealand\\n42,404\\n7.3\\nAustralia\\n48,698\\n7.3\\nDenmark\\n55,938\\n7.6\\nLet’s plot the data for these countries (\\nFigure 1-18\\n).\\nFigure 1-18. \\nDo you see a trend here?\\nThere does seem to be a trend here! Although the data is \\nnoisy\\n (i.e., partly'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 55, 'page_label': '56', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='random), it looks like life satisfaction goes up more or less linearly as the\\ncountry’s GDP per capita increases. \\nSo you decide to model life satisfaction\\nas a linear function of GDP per capita. \\nThis step is called \\nmodel selection\\n:\\nyou selected a \\nlinear model\\n of life satisfaction with just one attribute, GDP\\nper capita (\\nEquation 1-1\\n).\\nEquation 1-1. \\nA simple linear model\\nlife_satisfaction \\n= \\nθ 0 \\n+ \\nθ 1 \\n× \\nGDP_per_capita\\nThis model has two \\nmodel parameters\\n, \\nθ\\n and \\nθ\\n.\\n\\u2060\\n By tweaking these\\nparameters, you can make your model represent any linear function, as shown\\nin \\nFigure 1-19\\n.\\nFigure 1-19. \\nA few possible linear models\\nBefore you can use your model, you need to define the parameter values \\nθ\\nand \\nθ\\n. How can you know which values will make your model perform best?\\nTo answer this question, you need to specify a performance measure. \\nYou\\ncan either define a \\nutility function\\n (or \\nfitness function\\n) that measures how\\ngood\\n your model is, or you can define a \\ncost function\\n that measures how \\nbad\\nit is. \\nFor linear regression problems, people typically use a cost function that\\nmeasures the distance between the linear model’s predictions and the training\\n0\\n1\\n4\\n0\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 56, 'page_label': '57', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='examples; the objective is to minimize this distance.\\nThis is where the linear regression algorithm comes in: you feed it your\\ntraining examples, and it finds the parameters that make the linear model fit\\nbest to your data. \\nThis is called \\ntraining\\n the model. In our case, the algorithm\\nfinds that the optimal parameter values are \\nθ\\n = 3.75 and \\nθ\\n = 6.78 × 10\\n.\\nWARNING\\nConfusingly, the word “model” can refer to a \\ntype of model\\n (e.g., linear regression), to a\\nfully specified model architecture\\n (e.g., linear regression with one input and one output),\\nor to the \\nfinal trained model\\n ready to be used for predictions (e.g., linear regression with\\none input and one output, using \\nθ\\n = 3.75 and \\nθ\\n = 6.78 × 10\\n). Model selection consists\\nin choosing the type of model and fully specifying its architecture. Training a model\\nmeans running an algorithm to find the model parameters that will make it best fit the\\ntraining data, and hopefully make good predictions on new data.\\nNow the model fits the training data as closely as possible (for a linear\\nmodel), as you can see in \\nFigure 1-20\\n.\\nFigure 1-20. \\nThe linear model that fits the training data best\\nYou are finally ready to run the model to make predictions. For example, say\\n0\\n1\\n–5\\n0\\n1\\n–5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 57, 'page_label': '58', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='you want to know how happy Cypriots are, and the OECD data does not have\\nthe answer. Fortunately, you can use your model to make a good prediction:\\nyou look up Cyprus’s GDP per capita, find $37,655, and then apply your\\nmodel and find that life satisfaction is likely to be somewhere around 3.75 +\\n37,655 × 6.78 × 10\\n = 6.30.\\nTo whet your appetite, \\nExample 1-1\\n shows the Python code that loads the\\ndata, separates the inputs \\nX\\n from the labels \\ny\\n, creates a scatterplot for\\nvisualization, and then trains a linear model and makes a prediction.\\n\\u2060\\nExample 1-1. \\nTraining and running a linear model using Scikit-Learn\\nimport\\n \\nmatplotlib.pyplot\\n \\nas\\n \\nplt\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\nimport\\n \\npandas\\n \\nas\\n \\npd\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLinearRegression\\n# Download and prepare the data\\ndata_root\\n \\n=\\n \\n\"https://github.com/ageron/data/raw/main/\"\\nlifesat\\n \\n=\\n \\npd\\n.\\nread_csv\\n(\\ndata_root\\n \\n+\\n \\n\"lifesat/lifesat.csv\"\\n)\\nX\\n \\n=\\n \\nlifesat\\n[[\\n\"GDP per capita (USD)\"\\n]]\\n.\\nvalues\\ny\\n \\n=\\n \\nlifesat\\n[[\\n\"Life satisfaction\"\\n]]\\n.\\nvalues\\n# Visualize the data\\nlifesat\\n.\\nplot\\n(\\nkind\\n=\\n\\'scatter\\'\\n,\\n \\ngrid\\n=\\nTrue\\n,\\n             \\nx\\n=\\n\"GDP per capita (USD)\"\\n,\\n \\ny\\n=\\n\"Life satisfaction\"\\n)\\nplt\\n.\\naxis\\n([\\n23_500\\n,\\n \\n62_500\\n,\\n \\n4\\n,\\n \\n9\\n])\\nplt\\n.\\nshow\\n()\\n# Select a linear model\\nmodel\\n \\n=\\n \\nLinearRegression\\n()\\n# Train the model\\nmodel\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\n# Make a prediction for Cyprus\\nX_new\\n \\n=\\n \\n[[\\n37_655.2\\n]]\\n  \\n# Cyprus\\' GDP per capita in 2020\\nprint\\n(\\nmodel\\n.\\npredict\\n(\\nX_new\\n))\\n \\n# output: [[6.30165767]]\\nNOTE\\nIf you had used an instance-based learning algorithm instead, you would have found that\\nIsrael has the closest GDP per capita to that of Cyprus ($38,341), and since the OECD\\ndata tells us that Israelis’ life satisfaction is 7.2, you would have predicted a life\\n–5\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 58, 'page_label': '59', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='satisfaction of 7.2 for Cyprus. If you zoom out a bit and look at the two next-closest\\ncountries, you will find Lithuania and Slovenia, both with a life satisfaction of 5.9.\\nAveraging these three values, you get 6.33, which is pretty close to your model-based\\nprediction. \\nThis simple algorithm is called \\nk-nearest neighbors\\n regression (in this\\nexample, \\nk\\n = 3).\\nReplacing the linear regression model with \\nk\\n-nearest neighbors regression in the previous\\ncode is as easy as replacing these lines:\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLinearRegression\\nmodel\\n \\n=\\n \\nLinearRegression\\n()\\nwith these two:\\nfrom\\n \\nsklearn.neighbors\\n \\nimport\\n \\nKNeighborsRegressor\\nmodel\\n \\n=\\n \\nKNeighborsRegressor\\n(\\nn_neighbors\\n=\\n3\\n)\\nIf all went well, your model will make good predictions. If not, you may need\\nto use more attributes (employment rate, health, air pollution, etc.), get more\\nor better-quality training data, or perhaps select a more powerful model (e.g.,\\na polynomial regression model).\\nIn summary:\\nYou studied the data.\\nYou selected a model.\\nYou trained it on the training data (i.e., the learning algorithm searched\\nfor the model parameter values that minimize a cost function).\\nFinally, you applied the model to make predictions on new cases (this is\\ncalled \\ninference\\n), \\nhoping that this model will generalize well.\\nThis is what a typical machine learning project looks like. In \\nChapter 2\\n you\\nwill experience this firsthand by going through a project end to end.\\nWe have covered a lot of ground so far: you now know what machine\\nlearning is really about, why it is useful, what some of the most common'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 59, 'page_label': '60', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='categories of ML systems are, and what a typical project workflow looks like.\\nNow let’s look at what can go wrong in learning and prevent you from\\nmaking accurate predictions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 60, 'page_label': '61', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Main Challenges of Machine Learning\\nIn short, since your main task is to select a model and train it on some data,\\nthe two things that can go wrong are “bad model” and “bad data”. Let’s start\\nwith examples of bad data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 61, 'page_label': '62', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Insufficient Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an\\napple and say “apple” (possibly repeating this procedure a few times). Now\\nthe child is able to recognize apples in all sorts of colors and shapes. Genius.\\nMachine learning is not quite there yet; it takes a lot of data for most machine\\nlearning algorithms to work properly. Even for very simple problems you\\ntypically need thousands of examples, and for complex problems such as\\nimage or speech recognition you may need millions of examples (unless you\\ncan reuse parts of an existing model).\\nTHE UNREASONABLE EFFECTIVENESS OF DATA\\nIn a \\nfamous paper\\n published in 2001, Microsoft researchers Michele\\nBanko and Eric Brill showed that very different machine learning\\nalgorithms, including fairly simple ones, performed almost identically\\nwell on a complex problem of natural language disambiguation\\n\\u2060\\n once\\nthey were given enough data (as you can see in \\nFigure 1-21\\n).\\nAs the authors put it, “these results suggest that we may want to\\nreconsider the trade-off between spending time and money on algorithm\\ndevelopment versus spending it on corpus development”.\\nThe idea that data matters more than algorithms for complex problems\\nwas further popularized by Peter Norvig et al. in a paper titled \\n“The\\nUnreasonable Effectiveness of Data”\\n, published in 2009.\\n\\u2060\\n \\nIt should be\\nnoted, however, that small and medium-sized datasets are still very\\ncommon, and it is not always easy or cheap to get extra training data\\n\\u2060\\n—\\nso don’t abandon algorithms just yet.\\n6\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 62, 'page_label': '63', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-21. \\nThe importance of data versus algorithms\\n\\u2060\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 63, 'page_label': '64', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be\\nrepresentative of the new cases you want to generalize to. \\nThis is true\\nwhether you use instance-based learning or model-based learning.\\nFor example, the set of countries you used earlier for training the linear\\nmodel was not perfectly representative; it did not contain any country with a\\nGDP per capita lower than $23,500 or higher than $62,500. \\nFigure 1-22\\nshows what the data looks like when you add such countries.\\nIf you train a linear model on this data, you get the solid line, while the old\\nmodel is represented by the dotted line. As you can see, not only does adding\\na few missing countries significantly alter the model, but it makes it clear that\\nsuch a simple linear model is probably never going to work well. It seems\\nthat very rich countries are not happier than moderately rich countries (in\\nfact, they seem slightly unhappier!), and conversely some poor countries\\nseem happier than many rich countries.\\nBy using a nonrepresentative training set, you trained a model that is unlikely\\nto make accurate predictions, especially for very poor and very rich countries.\\nFigure 1-22. \\nA more representative training sample\\nIt is crucial to use a training set that is representative of the cases you want to\\ngeneralize to. \\nThis is often harder than it sounds: if the sample is too small,\\nyou will have \\nsampling noise\\n (i.e., nonrepresentative data as a result of\\nchance), but even very large samples can be nonrepresentative if the sampling'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 64, 'page_label': '65', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='method is flawed. This is called \\nsampling bias\\n.\\nEXAMPLES OF SAMPLING BIAS\\nPerhaps the most famous example of sampling bias happened during the\\nUS presidential election in 1936, which pitted Landon against Roosevelt:\\nthe \\nLiterary Digest\\n conducted a very large poll, sending mail to about 10\\nmillion people. It got 2.4 million answers, and predicted with high\\nconfidence that Landon would get 57% of the votes. Instead, Roosevelt\\nwon with 62% of the votes. The flaw was in the \\nLiterary Digest\\n’s\\nsampling method:\\nFirst, to obtain the addresses to send the polls to, the \\nLiterary Digest\\nused telephone directories, lists of magazine subscribers, club\\nmembership lists, and the like. All of these lists tended to favor\\nwealthier people, who were more likely to vote Republican (hence\\nLandon).\\nSecond, less than 25% of the people who were polled answered.\\nAgain this introduced a sampling bias, by potentially ruling out\\npeople who didn’t care much about politics, people who didn’t like\\nthe \\nLiterary Digest\\n, and other key groups. \\nThis is a special type of\\nsampling bias called \\nnonresponse bias\\n.\\nHere is another example: say you want to build a system to recognize\\nfunk music videos. One way to build your training set is to search for\\n“funk music” on YouTube and use the resulting videos. But this assumes\\nthat YouTube’s search engine returns a set of videos that are\\nrepresentative of all the funk music videos on YouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you\\nlive in Brazil you will get a lot of “funk carioca” videos, which sound\\nnothing like James Brown). On the other hand, how else can you get a\\nlarge training set?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 65, 'page_label': '66', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Poor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due\\nto poor-quality measurements), it will make it harder for the system to detect\\nthe underlying patterns, so your system is less likely to perform well. It is\\noften well worth the effort to spend time cleaning up your training data. The\\ntruth is, most data scientists spend a significant part of their time doing just\\nthat. The following are a couple examples of when you’d want to clean up\\ntraining data:\\nIf some instances are clearly outliers, it may help to simply discard them\\nor try to fix the errors manually.\\nIf some instances are missing a few features (e.g., 5% of your customers\\ndid not specify their age), you must decide whether you want to ignore\\nthis attribute altogether, ignore these instances, fill in the missing values\\n(e.g., with the median age), or train one model with the feature and one\\nmodel without it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 66, 'page_label': '67', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Irrelevant Features\\nAs the saying goes: garbage in, garbage out. Your system will only be\\ncapable of learning if the training data contains enough relevant features and\\nnot too many irrelevant ones. A critical part of the success of a machine\\nlearning project is coming up with a good set of features to train on. \\nThis\\nprocess, called \\nfeature engineering\\n, involves the following steps:\\nFeature selection\\n (selecting the most useful features to train on among\\nexisting features)\\nFeature extraction\\n (combining existing features to produce a more\\nuseful one\\n\\u2060\\n—as we saw earlier, dimensionality reduction algorithms\\ncan help)\\nCreating new features by gathering new data\\nNow that we have looked at many examples of bad data, let’s look at a\\ncouple examples of bad algorithms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 67, 'page_label': '68', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Overfitting the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. You\\nmight be tempted to say that \\nall\\n taxi drivers in that country are thieves.\\nOvergeneralizing is something that we humans do all too often, and\\nunfortunately machines can fall into the same trap if we are not careful. In\\nmachine learning this is called \\noverfitting\\n: it means that the model performs\\nwell on the training data, but it does not generalize well.\\nFigure 1-23\\n shows an example of a high-degree polynomial life satisfaction\\nmodel that strongly overfits the training data. Even though it performs much\\nbetter on the training data than the simple linear model, would you really\\ntrust its predictions?\\nFigure 1-23. \\nOverfitting the training data\\nComplex models such as deep neural networks can detect subtle patterns in\\nthe data, but if the training set is noisy, or if it is too small, which introduces\\nsampling noise, then the model is likely to detect patterns in the noise itself\\n(as in the taxi driver example). Obviously these patterns will not generalize to\\nnew instances. For example, say you feed your life satisfaction model many\\nmore attributes, including uninformative ones such as the country’s name. In\\nthat case, a complex model may detect patterns like the fact that all countries\\nin the training data with a \\nw\\n in their name have a life satisfaction greater than\\n7: New Zealand (7.3), Norway (7.6), Sweden (7.3), and Switzerland (7.5).\\nHow confident are you that the \\nw\\n-satisfaction rule generalizes to Rwanda or\\nZimbabwe? Obviously this pattern occurred in the training data by pure'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 68, 'page_label': '69', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='chance, but the model has no way to tell whether a pattern is real or simply\\nthe result of noise in the data.\\nWARNING\\nOverfitting happens when the model is too complex relative to the amount and noisiness\\nof the training data. Here are possible solutions:\\nSimplify the model by selecting one with fewer parameters (e.g., a linear model\\nrather than a high-degree polynomial model), by reducing the number of attributes\\nin the training data, or by constraining the model.\\nGather more training data.\\nReduce the noise in the training data (e.g., fix data errors and remove outliers).\\nConstraining a model to make it simpler and reduce the risk of overfitting is\\ncalled \\nregularization\\n. For example, the linear model we defined earlier has\\ntwo parameters, \\nθ\\n and \\nθ\\n. \\nThis gives the learning algorithm two \\ndegrees of\\nfreedom\\n to adapt the model to the training data: it can tweak both the height\\n(\\nθ\\n) and the slope (\\nθ\\n) of the line. If we forced \\nθ\\n = 0, the algorithm would\\nhave only one degree of freedom and would have a much harder time fitting\\nthe data properly: all it could do is move the line up or down to get as close\\nas possible to the training instances, so it would end up around the mean. A\\nvery simple model indeed! If we allow the algorithm to modify \\nθ\\n but we\\nforce it to keep it small, then the learning algorithm will effectively have\\nsomewhere in between one and two degrees of freedom. It will produce a\\nmodel that’s simpler than one with two degrees of freedom, but more\\ncomplex than one with just one. You want to find the right balance between\\nfitting the training data perfectly and keeping the model simple enough to\\nensure that it will generalize well.\\nFigure 1-24\\n shows three models. The dotted line represents the original\\nmodel that was trained on the countries represented as circles (without the\\ncountries represented as squares), the solid line is our second model trained\\nwith all countries (circles and squares), and the dashed line is a model trained\\n0\\n1\\n0\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 69, 'page_label': '70', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='with the same data as the first model but with a regularization constraint. You\\ncan see that regularization forced the model to have a smaller slope: this\\nmodel does not fit the training data (circles) as well as the first model, but it\\nactually generalizes better to new examples that it did not see during training\\n(squares).\\nFigure 1-24. \\nRegularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a\\nhyperparameter\\n. A hyperparameter is a parameter of a learning algorithm\\n(not of the model). As such, it is not affected by the learning algorithm itself;\\nit must be set prior to training and remains constant during training. If you set\\nthe regularization hyperparameter to a very large value, you will get an\\nalmost flat model (a slope close to zero); the learning algorithm will almost\\ncertainly not overfit the training data, but it will be less likely to find a good\\nsolution. Tuning hyperparameters is an important part of building a machine\\nlearning system (you will see a detailed example in the next chapter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 70, 'page_label': '71', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Underfitting the Training Data\\nAs you might guess, \\nunderfitting\\n is the opposite of overfitting: it occurs when\\nyour model is too simple to learn the underlying structure of the data. For\\nexample, a linear model of life satisfaction is prone to underfit; reality is just\\nmore complex than the model, so its predictions are bound to be inaccurate,\\neven on the training \\nexamples\\n.\\nHere are the main options for fixing this problem:\\nSelect a more powerful model, with more parameters.\\nFeed better features to the learning algorithm (feature engineering).\\nReduce the constraints on the model (for example by reducing the\\nregularization \\nhyperparameter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 71, 'page_label': '72', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Stepping Back\\nBy now you know a lot about machine learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and\\nlook at the big \\npicture\\n:\\nMachine learning is about making machines get better at some task by\\nlearning from data, instead of having to explicitly code rules.\\nThere are many different types of ML systems: supervised or not, batch\\nor online, instance-based or model-based.\\nIn an ML project you gather data in a training set, and you feed the\\ntraining set to a learning algorithm. If the algorithm is model-based, it\\ntunes some parameters to fit the model to the training set (i.e., to make\\ngood predictions on the training set itself), and then hopefully it will be\\nable to make good predictions on new cases as well. If the algorithm is\\ninstance-based, it just learns the examples by heart and generalizes to\\nnew instances by using a similarity measure to compare them to the\\nlearned instances.\\nThe system will not perform well if your training set is too small, or if\\nthe data is not representative, is noisy, or is polluted with irrelevant\\nfeatures (garbage in, garbage out). Lastly, your model needs to be\\nneither too simple (in which case it will underfit) nor too complex (in\\nwhich case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model,\\nyou don’t want to just “hope” it generalizes to new cases. You want to\\nevaluate it and fine-tune it if necessary. Let’s see how to do that.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 72, 'page_label': '73', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Testing and Validating\\nThe only way to know how well a model will generalize to new cases is to\\nactually try it out on new cases. One way to do that is to put your model in\\nproduction and monitor how well it performs. This works well, but if your\\nmodel is horribly bad, your users will complain—not the best idea.\\nA better option is to split your data into two sets: the \\ntraining set\\n and the \\ntest\\nset\\n. \\nAs these names imply, you train your model using the training set, and\\nyou test it using the test set. The error rate on new cases is called the\\ngeneralization error\\n (or \\nout-of-sample error\\n), and by evaluating your model\\non the test set, you get an estimate of this error. This value tells you how well\\nyour model will perform on instances it has never seen before.\\nIf the training error is low (i.e., your model makes few mistakes on the\\ntraining set) but the generalization error is high, it means that your model is\\noverfitting the training data.\\nTIP\\nIt is common to use 80% of the data for training and \\nhold out\\n 20% for testing. However,\\nthis depends on the size of the dataset: if it contains 10 million instances, then holding out\\n1% means your test set will contain 100,000 instances, probably more than enough to get a\\ngood estimate of the generalization error.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 73, 'page_label': '74', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Hyperparameter Tuning and Model Selection\\nEvaluating a model is simple enough: just use a test set. But suppose you are\\nhesitating between two types of models (say, a linear model and a polynomial\\nmodel): how can you decide between them? One option is to train both and\\ncompare how well they generalize using the test set.\\nNow suppose that the linear model generalizes better, but you want to apply\\nsome regularization to avoid overfitting. The question is, how do you choose\\nthe value of the regularization hyperparameter? One option is to train 100\\ndifferent models using 100 different values for this hyperparameter. Suppose\\nyou find the best hyperparameter value that produces a model with the lowest\\ngeneralization error\\n\\u2060\\n—say, just 5% error. You launch this model into\\nproduction, but unfortunately it does not perform as well as expected and\\nproduces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on\\nthe test set, and you adapted the model and hyperparameters to produce the\\nbest model \\nfor that particular set\\n. This means the model is unlikely to\\nperform as well on new data.\\nA common solution to this problem is called \\nholdout validation\\n (\\nFigure 1-\\n25\\n): you simply hold out part of the training set to evaluate several candidate\\nmodels and select the best one. \\nThe new held-out set is called the \\nvalidation\\nset\\n (or the \\ndevelopment set\\n, or \\ndev set\\n). More specifically, you train multiple\\nmodels with various hyperparameters on the reduced training set (i.e., the full\\ntraining set minus the validation set), and you select the model that performs\\nbest on the validation set. After this holdout validation process, you train the\\nbest model on the full training set (including the validation set), and this\\ngives you the final model. Lastly, you evaluate this final model on the test set\\nto get an estimate of the generalization error.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 74, 'page_label': '75', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 1-25. \\nModel selection using holdout validation\\nThis solution usually works quite well. However, if the validation set is too\\nsmall, then the model evaluations will be imprecise: you may end up\\nselecting a suboptimal model by mistake. Conversely, if the validation set is\\ntoo large, then the remaining training set will be much smaller than the full\\ntraining set. Why is this bad? Well, since the final model will be trained on\\nthe full training set, it is not ideal to compare candidate models trained on a\\nmuch smaller training set. It would be like selecting the fastest sprinter to\\nparticipate in a marathon. \\nOne way to solve this problem is to perform\\nrepeated \\ncross-validation\\n, using many small validation sets. Each model is\\nevaluated once per validation set after it is trained on the rest of the data. By\\naveraging out all the evaluations of a model, you get a much more accurate\\nmeasure of its performance. There is a drawback, however: the training time\\nis multiplied by the number of validation sets.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 75, 'page_label': '76', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Data Mismatch\\nIn some cases, it’s easy to get a large amount of data for training, but this\\ndata probably won’t be perfectly representative of the data that will be used\\nin production. For example, suppose you want to create a mobile app to take\\npictures of flowers and automatically determine their species. You can easily\\ndownload millions of pictures of flowers on the web, but they won’t be\\nperfectly representative of the pictures that will actually be taken using the\\napp on a mobile device. Perhaps you only have 1,000 representative pictures\\n(i.e., actually taken with the app).\\nIn this case, the most important rule to remember is that both the validation\\nset and the test set must be as representative as possible of the data you\\nexpect to use in production, so they should be composed exclusively of\\nrepresentative pictures: you can shuffle them and put half in the validation set\\nand half in the test set (making sure that no duplicates or near-duplicates end\\nup in both sets). After training your model on the web pictures, if you\\nobserve that the performance of the model on the validation set is\\ndisappointing, you will not know whether this is because your model has\\noverfit the training set, or whether this is just due to the mismatch between\\nthe web pictures and the mobile app pictures.\\nOne solution is to hold out some of the training pictures (from the web) in yet\\nanother set that Andrew Ng dubbed the \\ntrain-dev set\\n (\\nFigure 1-26\\n). \\nAfter the\\nmodel is trained (on the training set, \\nnot\\n on the train-dev set), you can\\nevaluate it on the train-dev set. If the model performs poorly, then it must\\nhave overfit the training set, so you should try to simplify or regularize the\\nmodel, get more training data, and clean up the training data. But if it\\nperforms well on the train-dev set, then you can evaluate the model on the\\ndev set. If it performs poorly, then the problem must be coming from the data\\nmismatch. You can try to tackle this problem by preprocessing the web\\nimages to make them look more like the pictures that will be taken by the\\nmobile app, and then retraining the model. Once you have a model that\\nperforms well on both the train-dev set and the dev set, you can evaluate it\\none last time on the test set to know how well it is likely to perform in'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 76, 'page_label': '77', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='production.\\nFigure 1-26. \\nWhen real data is scarce (right), you may use similar abundant data (left) for training and\\nhold out some of it in a train-dev set to evaluate overfitting; the real data is then used to evaluate data\\nmismatch (dev set) and to evaluate the final model’s performance (test set)\\nNO FREE LUNCH THEOREM\\nA model is a simplified representation of the data. The simplifications are\\nmeant to discard the superfluous details that are unlikely to generalize to\\nnew instances. \\nWhen you select a particular type of model, you are\\nimplicitly making \\nassumptions\\n about the data. For example, if you choose\\na linear model, you are implicitly assuming that the data is fundamentally\\nlinear and that the distance between the instances and the straight line is\\njust noise, which can safely be ignored.\\nIn a \\nfamous 1996 paper\\n,\\n\\u2060\\n David Wolpert demonstrated that if you make\\nabsolutely no assumption about the data, then there is no reason to prefer\\none model over any other. This is called the \\nNo Free Lunch\\n (NFL)\\ntheorem. For some datasets the best model is a linear model, while for\\nother datasets it is a neural network. There is no model that is \\na priori\\nguaranteed to work better (hence the name of the theorem). The only way\\nto know for sure which model is best is to evaluate them all. Since this is\\nnot possible, in practice you make some reasonable assumptions about\\nthe data and evaluate only a few reasonable models. For example, for\\nsimple tasks you may evaluate linear models with various levels of\\nregularization, and for a complex problem you may evaluate various\\nneural networks.\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 77, 'page_label': '78', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\nIn this chapter we have covered some of the most important concepts in\\nmachine learning. In the next chapters we will dive deeper and write more\\ncode, but before we do, make sure you can answer the following questions:\\n1\\n. \\nHow would you define machine learning?\\n2\\n. \\nCan you name four types of applications where it shines?\\n3\\n. \\nWhat is a labeled training set?\\n4\\n. \\nWhat are the two most common supervised tasks?\\n5\\n. \\nCan you name four common unsupervised tasks?\\n6\\n. \\nWhat type of algorithm would you use to allow a robot to walk in\\nvarious unknown terrains?\\n7\\n. \\nWhat type of algorithm would you use to segment your customers into\\nmultiple groups?\\n8\\n. \\nWould you frame the problem of spam detection as a supervised\\nlearning problem or an unsupervised learning problem?\\n9\\n. \\nWhat is an online learning system?\\n10\\n. \\nWhat is out-of-core learning?\\n11\\n. \\nWhat type of algorithm relies on a similarity measure to make\\npredictions?\\n12\\n. \\nWhat is the difference between a model parameter and a model\\nhyperparameter?\\n13\\n. \\nWhat do model-based algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14\\n. \\nCan you name four of the main challenges in machine learning?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 78, 'page_label': '79', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='15\\n. \\nIf your model performs great on the training data but generalizes poorly\\nto new instances, what is happening? Can you name three possible\\nsolutions?\\n16\\n. \\nWhat is a test set, and why would you want to use it?\\n17\\n. \\nWhat is the purpose of a validation set?\\n18\\n. \\nWhat is the train-dev set, when do you need it, and how do you use it?\\n19\\n. \\nWhat can go wrong if you tune hyperparameters using the test set?\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was\\nstudying the fact that the children of tall people tend to be shorter than their parents. \\nSince the\\nchildren were shorter, he called this \\nregression to the mean\\n. This name was then applied to the\\nmethods he used to analyze correlations between variables.\\n2\\n Notice how animals are rather well separated from vehicles and how horses are close to deer but\\nfar from birds. Figure reproduced with permission from Richard Socher et al., “Zero-Shot\\nLearning Through Cross-Modal Transfer”, \\nProceedings of the 26th International Conference on\\nNeural Information Processing Systems\\n 1 (2013): 935–943.\\n3\\n That’s when the system works perfectly. In practice it often creates a few clusters per person,\\nand sometimes mixes up two people who look alike, so you may need to provide a few labels per\\nperson and manually clean up some clusters.\\n4\\n By convention, the Greek letter \\nθ\\n (theta) is frequently used to represent model parameters.\\n5\\n It’s OK if you don’t understand all the code yet; I will present Scikit-Learn in the following\\nchapters.\\n6\\n For example, knowing whether to write “to”, “two”, or “too”, depending on the context.\\n7\\n Peter Norvig et al., “The Unreasonable Effectiveness of Data”, \\nIEEE Intelligent Systems\\n 24, no.\\n2 (2009): 8–12.\\n8\\n Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very Very\\nLarge Corpora for Natural Language Disambiguation”, \\nProceedings of the 39th Annual Meeting\\nof the Association for Computational Linguistics\\n (2001): 26–33.\\n9\\n David Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms”, \\nNeural\\nComputation\\n 8, no. 7 (1996): 1341–1390.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 79, 'page_label': '80', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 2. \\nEnd-to-End Machine\\nLearning Project\\nIn this chapter you will work through an example project end to end,\\npretending to be a recently hired data scientist at a real estate company. This\\nexample is fictitious; the goal is to illustrate the main steps of a machine\\nlearning project, not to learn anything about the real estate business. Here are\\nthe main steps we will walk through:\\n1\\n. \\nLook at the big picture.\\n2\\n. \\nGet the data.\\n3\\n. \\nExplore and visualize the data to gain insights.\\n4\\n. \\nPrepare the data for machine learning algorithms.\\n5\\n. \\nSelect a model and train it.\\n6\\n. \\nFine-tune your model.\\n7\\n. \\nPresent your solution.\\n8\\n. \\nLaunch, monitor, and maintain your system.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 80, 'page_label': '81', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Working with Real Data\\nWhen you are learning about machine learning, it is best to experiment with\\nreal-world data, not artificial datasets. Fortunately, there are thousands of\\nopen datasets to choose from, ranging across all sorts of domains. Here are a\\nfew places you can look to get data:\\nPopular open data repositories:\\nOpenML.org\\nKaggle.com\\nPapersWithCode.com\\nUC Irvine Machine Learning Repository\\nAmazon’s AWS datasets\\nTensorFlow datasets\\nMeta portals (they list open data repositories):\\nDataPortals.org\\nOpenDataMonitor.eu\\nOther pages listing many popular open data repositories:\\nWikipedia’s list of machine learning datasets\\nQuora.com\\nThe datasets subreddit\\nIn this chapter we’ll use the California Housing Prices dataset from the\\nStatLib repository\\n\\u2060\\n (see \\nFigure 2-1\\n). This dataset is based on data from the\\n1990 California census. It is not exactly recent (a nice house in the Bay Area\\nwas still affordable at the time), but it has many qualities for learning, so we\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 81, 'page_label': '82', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='will pretend it is recent data. For teaching purposes I’ve added a categorical\\nattribute and removed a few features.\\nFigure 2-1. \\nCalifornia housing prices'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 82, 'page_label': '83', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Look at the Big Picture\\nWelcome to the Machine Learning Housing Corporation! Your first task is to\\nuse California census data to build a model of housing prices in the state.\\nThis data includes metrics such as the population, median income, and\\nmedian housing price for each block group in California. Block groups are\\nthe smallest geographical unit for which the US Census Bureau publishes\\nsample data (a block group typically has a population of 600 to 3,000\\npeople). I will call them “districts” for short.\\nYour model should learn from this data and be able to predict the median\\nhousing price in any district, given all the other metrics.\\nTIP\\nSince you are a well-organized data scientist, the first thing you should do is pull out your\\nmachine learning project checklist. You can start with the one in \\nAppendix A\\n; it should\\nwork reasonably well for most machine learning projects, but make sure to adapt it to your\\nneeds. In this chapter we will go through many checklist items, but we will also skip a\\nfew, either because they are self-explanatory or because they will be discussed in later\\nchapters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 83, 'page_label': '84', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Frame the Problem\\nThe first question to ask your boss is what exactly the business objective is.\\nBuilding a model is probably not the end goal. How does the company expect\\nto use and benefit from this model? Knowing the objective is important\\nbecause it will determine how you frame the problem, which algorithms you\\nwill select, which performance measure you will use to evaluate your model,\\nand how much effort you will spend tweaking it.\\nYour boss answers that your model’s output (a prediction of a district’s\\nmedian housing price) will be fed to another machine learning system (see\\nFigure 2-2\\n), along with many other signals.\\n\\u2060\\n This downstream system will\\ndetermine whether it is worth investing in a given area. Getting this right is\\ncritical, as it directly affects \\nrevenue\\n.\\nThe next question to ask your boss is what the current solution looks like (if\\nany). The current situation will often give you a reference for performance, as\\nwell as insights on how to solve the problem. Your boss answers that the\\ndistrict housing prices are currently estimated manually by experts: a team\\ngathers up-to-date \\ninformation\\n about a district, and when they cannot get the\\nmedian housing price, they estimate it using complex rules.\\nFigure 2-2. \\nA machine learning pipeline for real estate investments\\nThis is costly and time-consuming, and their estimates are not great; in cases\\nwhere they manage to find out the actual median housing price, they often\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 84, 'page_label': '85', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='realize that their estimates were off by more than 30%. This is why the\\ncompany thinks that it would be useful to train a model to predict a district’s\\nmedian housing price, given other data about that district. The census data\\nlooks like a great dataset to exploit for this purpose, since it includes the\\nmedian housing prices of thousands of districts, as well as other data.\\nPIPELINES\\nA sequence of data processing components is called a data \\npipeline\\n.\\nPipelines are very common in machine learning systems, since there is a\\nlot of data to manipulate and many data transformations to apply.\\nComponents typically run asynchronously. Each component pulls in a\\nlarge amount of data, processes it, and spits out the result in another data\\nstore. Then, some time later, the next component in the pipeline pulls in\\nthis data and spits out its own output. Each component is fairly self-\\ncontained: the interface between components is simply the data store.\\nThis makes the system simple to grasp (with the help of a data flow\\ngraph), and different teams can focus on different components. Moreover,\\nif a component breaks down, the downstream components can often\\ncontinue to run normally (at least for a while) by just using the last output\\nfrom the broken component. This makes the architecture quite robust.\\nOn the other hand, a broken component can go unnoticed for some time if\\nproper monitoring is not implemented. The data gets stale and the overall\\nsystem’s performance drops.\\nWith all this information, you are now ready to start designing your system.\\nFirst, determine what kind of training supervision the model will need: is it a\\nsupervised, unsupervised, semi-supervised, self-supervised, or reinforcement\\nlearning task? And is it a classification task, a regression task, or something\\nelse? Should you use batch learning or online learning techniques? Before\\nyou read on, pause and try to answer these questions for yourself.\\nHave you found the answers? Let’s see. This is clearly a typical supervised\\nlearning task, since the model can be trained with \\nlabeled\\n examples (each'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 85, 'page_label': '86', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='instance comes with the expected output, i.e., the district’s median housing\\nprice). It is a typical regression task, since the model will be asked to predict\\na value. \\nMore specifically, this is a \\nmultiple regression\\n problem, since the\\nsystem will use multiple features to make a prediction (the district’s\\npopulation, the median income, etc.). \\nIt is also \\na \\nunivariate regression\\nproblem, since we are only trying to predict a single value for each district. \\nIf\\nwe were trying to predict multiple values per district, it would be a\\nmultivariate regression\\n problem. Finally, there is no continuous flow of data\\ncoming into the system, there is no particular need to adjust to changing data\\nrapidly, and the data is small enough to fit in memory, so plain batch learning\\nshould do just fine.\\nTIP\\nIf the data were huge, you could either split your batch learning work across multiple\\nservers (using the MapReduce technique) or use an online learning technique.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 86, 'page_label': '87', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Select a Performance Measure\\nYour next step is to select a performance measure. A typical performance\\nmeasure for regression problems is the \\nroot mean square error\\n (RMSE). It\\ngives an idea of how much error the system typically makes in its predictions,\\nwith a higher weight given to large errors. \\nEquation 2-1\\n shows the\\nmathematical formula to compute the RMSE.\\nEquation 2-1. \\nRoot mean square error (RMSE)\\nRMSE \\n( \\nX \\n, \\nh \\n) \\n= \\n1 m \\n∑ i=1 m \\nh(x (i) )-y (i) \\n2\\nNOTATIONS\\nThis equation introduces several very common machine learning\\nnotations that I will use throughout this book:\\nm\\n is the number of instances in the dataset you are measuring the\\nRMSE on.\\nFor example, if you are evaluating the RMSE on a validation\\nset of 2,000 districts, then \\nm\\n = 2,000.\\nx\\n is a vector of all the feature values (excluding the label) of the \\ni\\ninstance in the dataset, and \\ny\\n is its label (the desired output value\\nfor that instance).\\nFor example, if the first district in the dataset is located at\\nlongitude –118.29°, latitude 33.91°, and it has 1,416 inhabitants\\nwith a median income of $38,372, and the median house value\\nis $156,400 (ignoring other features for now), then:\\nx (1) \\n= \\n- \\n118.29 \\n33.91 \\n1,416 \\n38,372\\nand:\\ny (1) \\n= \\n156,400\\n(\\ni\\n)\\nth\\n(\\ni\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 87, 'page_label': '88', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='X\\n is a matrix containing all the feature values (excluding labels) of\\nall instances in the dataset. There is one row per instance, and the \\ni\\nrow is equal to the transpose of \\nx\\n, noted (\\nx\\n)\\n.\\n\\u2060\\nFor example, if the first district is as just described, then the\\nmatrix \\nX\\n looks like this:\\nX \\n= \\n(x (1) ) \\n⊺\\n \\n(x (2) ) \\n⊺\\n \\n⋮\\n \\n(x (1999) ) \\n⊺\\n \\n(x (2000) ) \\n⊺\\n \\n= \\n- \\n118.29\\n33.91 \\n1,416 \\n38,372 \\n⋮\\n \\n⋮\\n \\n⋮\\n \\n⋮\\nh\\n is your system’s prediction function, also called a \\nhypothesis\\n.\\nWhen your system is given an instance’s feature vector \\nx\\n, it\\noutputs a predicted value \\nŷ\\n = \\nh\\n(\\nx\\n) for that instance (\\nŷ\\n is\\npronounced “y-hat”).\\nFor example, if your system predicts that the median housing\\nprice in the first district is $158,400, then \\nŷ\\n = \\nh\\n(\\nx\\n) =\\n158,400. The prediction error for this district is \\nŷ\\n – \\ny\\n =\\n2,000.\\nRMSE(\\nX\\n,\\nh\\n) is the cost function measured on the set of examples\\nusing your hypothesis \\nh\\n.\\nWe use lowercase italic font for scalar values (such as \\nm\\n or \\ny\\n) and\\nfunction names (such as \\nh\\n), lowercase bold font for vectors (such as \\nx\\n),\\nand uppercase bold font for matrices (such as \\nX\\n).\\nAlthough the RMSE is generally the preferred performance measure for\\nregression tasks, in some contexts you may prefer to use another function.\\nFor example, if there are many outlier districts. In that case, you may\\nconsider using the \\nmean absolute error\\n (MAE, also called the \\naverage\\nabsolute deviation\\n), shown in \\nEquation 2-2\\n:\\nEquation 2-2. \\nMean absolute error (MAE)\\nMAE \\n( \\nX \\n, \\nh \\n) \\n= \\n1 m \\n∑ i=1 m \\nh \\n( \\nx (i) \\n) \\n- \\ny (i)\\nBoth the RMSE and the MAE are ways to measure the distance between two\\nth\\n(\\ni\\n)\\n(\\ni\\n)\\n⊺\\n3\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n(1)\\n(1)\\n(1)\\n(1)\\n(\\ni\\n)\\n(\\ni\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 88, 'page_label': '89', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='vectors: \\nthe vector of predictions and the vector of target values. Various\\ndistance measures, or \\nnorms\\n, are possible:\\nComputing the root of a sum of squares (RMSE) corresponds to the\\nEuclidean norm\\n: \\nthis is the notion of distance we are all familiar with. \\nIt\\nis also called the ℓ\\n \\nnorm\\n, noted \\n∥\\n · \\n∥\\n (or just \\n∥\\n · \\n∥\\n).\\nComputing the sum of absolutes (MAE) corresponds to the ℓ\\n \\nnorm\\n,\\nnoted \\n∥\\n · \\n∥\\n.\\n \\nThis is sometimes called the \\nManhattan norm\\n because it\\nmeasures the distance between two points in a city if you can only travel\\nalong orthogonal city blocks.\\nMore generally, the ℓ\\n \\nnorm\\n of a vector \\nv\\n containing \\nn\\n elements is\\ndefined as \\n∥\\nv\\n∥\\n = (|\\nv\\n|\\n + |\\nv\\n|\\n + ... + |\\nv\\n|\\n)\\n. ℓ\\n gives the number of\\nnonzero elements in the vector, and ℓ\\n gives the maximum absolute\\nvalue in the vector.\\nThe higher the norm index, the more it focuses on large values and neglects\\nsmall ones. This is why the RMSE is more sensitive to outliers than the\\nMAE. But when outliers are exponentially rare (like in a bell-shaped curve),\\nthe RMSE performs very well and is generally preferred.\\n2\\n2\\n1\\n1\\nk\\nk\\n1\\nk\\n2\\nk\\nn\\nk\\n1/\\nk\\n0\\n∞'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 89, 'page_label': '90', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Check the Assumptions\\nLastly, it is good practice to list and verify the assumptions that have been\\nmade so far (by you or others); this can help you catch serious issues early\\non. For example, the district prices that your system outputs are going to be\\nfed into a downstream machine learning system, and you assume that these\\nprices are going to be used as such. But what if the downstream system\\nconverts the prices into categories (e.g., “cheap”, “medium”, or “expensive”)\\nand then uses those categories instead of the prices themselves? In this case,\\ngetting the price perfectly right is not important at all; your system just needs\\nto get the category right. If that’s so, then the problem should have been\\nframed as a classification task, not a regression task. You don’t want to find\\nthis out after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system,\\nyou are confident that they do indeed need the actual prices, not just\\ncategories. Great! You’re all set, the lights are green, and you can start coding\\nnow!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 90, 'page_label': '91', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Get the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and\\nwalk through the code examples. As I mentioned in the preface, all the code\\nexamples in this book are open source and available \\nonline\\n as Jupyter\\nnotebooks, which are interactive documents containing text, images, and\\nexecutable code snippets (Python in our case). \\nIn this book I will assume you\\nare running these notebooks on Google Colab, a free service that lets you run\\nany Jupyter notebook directly online, without having to install anything on\\nyour machine. If you want to use another online platform (e.g., Kaggle) or if\\nyou want to install everything locally on your own machine, please see the\\ninstructions on the book’s GitHub page.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 91, 'page_label': '92', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Running the Code Examples Using Google Colab\\nFirst, open a web browser and visit \\nhttps://homl.info/colab3\\n: this will lead\\nyou to Google Colab, and it will display the list of Jupyter notebooks for this\\nbook (see \\nFigure 2-3\\n). \\nYou will find one notebook per chapter, plus a few\\nextra notebooks and tutorials for NumPy, Matplotlib, Pandas, linear algebra,\\nand differential calculus. For example, if you click\\n02_end_to_end_machine_learning_project.ipynb\\n, the notebook from\\nChapter 2\\n will open up in Google Colab (see \\nFigure 2-4\\n).\\nA Jupyter notebook is composed of a list of cells. Each cell contains either\\nexecutable code or text. Try double-clicking the first text cell (which contains\\nthe sentence “Welcome to Machine Learning Housing Corp.!”). This will\\nopen the cell for editing. Notice that Jupyter notebooks use Markdown syntax\\nfor formatting (e.g., \\n**bold**\\n, \\n*italics*\\n, \\n# Title\\n, \\n[url](link text)\\n, and so on).\\nTry modifying this text, then press Shift-Enter to see the result.\\nFigure 2-3. \\nList of notebooks in Google Colab'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 92, 'page_label': '93', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-4. \\nYour notebook in Google Colab\\nNext, create a new code cell by selecting Insert → “Code cell” from the\\nmenu. Alternatively, you can click the + Code button in the toolbar, or hover\\nyour mouse over the bottom of a cell until you see + Code and + Text appear,\\nthen click + Code. In the new code cell, type some Python code, such as\\nprint(\"Hello World\")\\n, then press Shift-Enter to run this code (or click the \\n▷\\nbutton on the left side of the cell).\\nIf you’re not logged in to your Google account, you’ll be asked to log in now\\n(if you don’t already have a Google account, you’ll need to create one). Once\\nyou are logged in, when you try to run the code you’ll see a security warning\\ntelling you that this notebook was not authored by Google. A malicious\\nperson could create a notebook that tries to trick you into entering your\\nGoogle credentials so they can access your personal data, so before you run a\\nnotebook, always make sure you trust its author (or double-check what each\\ncode cell will do before running it). Assuming you trust me (or you plan to\\ncheck every code cell), you can now click “Run anyway”.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 93, 'page_label': '94', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Colab will then allocate a new \\nruntime\\n for you: this is a free virtual machine\\nlocated on Google’s servers that contains a bunch of tools and Python\\nlibraries, including everything you’ll need for most chapters (in some\\nchapters, you’ll need to run a command to install additional libraries). This\\nwill take a few seconds. Next, Colab will automatically connect to this\\nruntime and use it to execute your new code cell. Importantly, the code runs\\non the runtime, \\nnot\\n on your machine. The code’s output will be displayed\\nunder the cell. Congrats, you’ve run some Python code on Colab!\\nTIP\\nTo insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed by A\\n(to insert above the active cell) or B (to insert below). There are many other keyboard\\nshortcuts available: you can view and edit them by typing Ctrl-M (or Cmd-M) then H. If\\nyou choose to run the notebooks on Kaggle or on your own machine using JupyterLab or\\nan IDE such as Visual Studio Code with the Jupyter extension, you will see some minor\\ndifferences—runtimes are called \\nkernels\\n, the user interface and keyboard shortcuts are\\nslightly different, etc.—but switching from one Jupyter environment to another is not too\\nhard.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 94, 'page_label': '95', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Saving Your Code Changes and Your Data\\nYou can make changes to a Colab notebook, and they will persist for as long\\nas you keep your browser tab open. But once you close it, the changes will be\\nlost. To avoid this, make sure you save a copy of the notebook to your\\nGoogle Drive by selecting File → “Save a copy in Drive”. Alternatively, you\\ncan download the notebook to your computer by selecting File → Download\\n→ “Download .ipynb”. Then you can later visit\\nhttps://colab.research.google.com\\n and open the notebook again (either from\\nGoogle Drive or by uploading it from your computer).\\nWARNING\\nGoogle Colab is meant only for interactive use: you can play around in the notebooks and\\ntweak the code as you like, but you cannot let the notebooks run unattended for a long\\nperiod of time, or else the runtime will be shut down and all of its data will be lost.\\nIf the notebook generates data that you care about, make sure you download\\nthis data before the runtime shuts down. To do this, click the Files icon (see\\nstep 1 in \\nFigure 2-5\\n), find the file you want to download, click the vertical\\ndots next to it (step 2), and click Download (step 3). Alternatively, you can\\nmount your Google Drive on the runtime, allowing the notebook to read and\\nwrite files directly to Google Drive as if it were a local directory. For this,\\nclick the Files icon (step 1), then click the Google Drive icon (circled in\\nFigure 2-5\\n) and follow the on-screen instructions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 95, 'page_label': '96', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-5. \\nDownloading a file from a Google Colab runtime (steps 1 to 3), or mounting your Google\\nDrive (circled icon)\\nBy default, your Google Drive will be mounted at \\n/content/drive/MyDrive\\n. If\\nyou want to back up a data file, simply copy it to this directory by running\\n!cp /content/my_great_model /content/drive/MyDrive\\n.\\n Any command\\nstarting with a bang (\\n!\\n) is treated as a shell command, not as Python code: \\ncp\\nis the Linux shell command to copy a file from one path to another. Note that\\nColab runtimes run on Linux (specifically, Ubuntu).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 96, 'page_label': '97', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The Power and Danger of Interactivity\\nJupyter notebooks are interactive, and that’s a great thing: you can run each\\ncell one by one, stop at any point, insert a cell, play with the code, go back\\nand run the same cell again, etc., and I highly encourage you to do so. If you\\njust run the cells one by one without ever playing around with them, you\\nwon’t learn as fast. However, this flexibility comes at a price: it’s very easy\\nto run cells in the wrong order, or to forget to run a cell. If this happens, the\\nsubsequent code cells are likely to fail. For example, the very first code cell\\nin each notebook contains setup code (such as imports), so make sure you run\\nit first, or else nothing will work.\\nTIP\\nIf you ever run into a weird error, try restarting the runtime (by selecting Runtime →\\n“Restart runtime” from the menu) and then run all the cells again from the beginning of\\nthe notebook. This often solves the problem. If not, it’s likely that one of the changes you\\nmade broke the notebook: just revert to the original notebook and try again. If it still fails,\\nplease file an issue on GitHub.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 97, 'page_label': '98', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Book Code Versus Notebook Code\\nYou may sometimes notice some little differences between the code in this\\nbook and the code in the notebooks. This may happen for several reasons:\\nA library may have changed slightly by the time you read these lines, or\\nperhaps despite my best efforts I made an error in the book. Sadly, I\\ncannot magically fix the code in your copy of this book (unless you are\\nreading an electronic copy and you can download the latest version), but\\nI \\ncan\\n fix the notebooks. So, if you run into an error after copying code\\nfrom this book, please look for the fixed code in the notebooks: I will\\nstrive to keep them error-free and up-to-date with the latest library\\nversions.\\nThe notebooks contain some extra code to beautify the figures (adding\\nlabels, setting font sizes, etc.) and to save them in high resolution for\\nthis book. You can safely ignore this extra code if you want.\\nI optimized the code for readability and simplicity: I made it as linear and flat\\nas possible, defining very few functions or classes. The goal is to ensure that\\nthe code you are running is generally right in front of you, and not nested\\nwithin several layers of abstractions that you have to search through. This\\nalso makes it easier for you to play with the code. For simplicity, there’s\\nlimited error handling, and I placed some of the least common imports right\\nwhere they are needed (instead of placing them at the top of the file, as is\\nrecommended by the PEP 8 Python style guide). That said, your production\\ncode will not be very different: just a bit more modular, and with additional\\ntests and error handling.\\nOK! Once you’re comfortable with Colab, you’re ready to download the data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 98, 'page_label': '99', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Download the Data\\nIn typical environments your data would be available in a relational database\\nor some other common data store, and spread across multiple\\ntables/documents/files. To access it, you would first need to get your\\ncredentials and access authorizations\\n\\u2060\\n and familiarize yourself with the data\\nschema. In this project, however, things are much simpler: you will just\\ndownload a single compressed file, \\nhousing.tgz\\n, which contains a comma-\\nseparated values (CSV) file called \\nhousing.csv\\n with all the data.\\nRather than manually downloading and decompressing the data, it’s usually\\npreferable to write a function that does it for you. This is useful in particular\\nif the data changes regularly: you can write a small script that uses the\\nfunction to fetch the latest data (or you can set up a scheduled job to do that\\nautomatically at regular intervals). Automating the process of fetching the\\ndata is also useful if you need to install the dataset on multiple machines.\\nHere is the function to fetch and load the data:\\nfrom\\n \\npathlib\\n \\nimport\\n \\nPath\\nimport\\n \\npandas\\n \\nas\\n \\npd\\nimport\\n \\ntarfile\\nimport\\n \\nurllib.request\\ndef\\n \\nload_housing_data\\n():\\n    \\ntarball_path\\n \\n=\\n \\nPath\\n(\\n\"datasets/housing.tgz\"\\n)\\n    \\nif\\n \\nnot\\n \\ntarball_path\\n.\\nis_file\\n():\\n        \\nPath\\n(\\n\"datasets\"\\n)\\n.\\nmkdir\\n(\\nparents\\n=\\nTrue\\n,\\n \\nexist_ok\\n=\\nTrue\\n)\\n        \\nurl\\n \\n=\\n \\n\"https://github.com/ageron/data/raw/main/housing.tgz\"\\n        \\nurllib\\n.\\nrequest\\n.\\nurlretrieve\\n(\\nurl\\n,\\n \\ntarball_path\\n)\\n        \\nwith\\n \\ntarfile\\n.\\nopen\\n(\\ntarball_path\\n)\\n \\nas\\n \\nhousing_tarball\\n:\\n            \\nhousing_tarball\\n.\\nextractall\\n(\\npath\\n=\\n\"datasets\"\\n)\\n    \\nreturn\\n \\npd\\n.\\nread_csv\\n(\\nPath\\n(\\n\"datasets/housing/housing.csv\"\\n))\\nhousing\\n \\n=\\n \\nload_housing_data\\n()\\nWhen \\nload_housing_data()\\n is called, it looks for the \\ndatasets/housing.tgz\\n file.\\nIf it does not find it, it creates the \\ndatasets\\n directory inside the current\\ndirectory (which is \\n/content\\n by default, in Colab), downloads the \\nhousing.tgz\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 99, 'page_label': '100', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='file from the \\nageron/data\\n GitHub repository, and extracts its content into the\\ndatasets\\n directory; this creates the \\ndatasets\\n/\\nhousing\\n directory with the\\nhousing.csv\\n file inside it. Lastly, the function loads this CSV file into a\\nPandas DataFrame object containing all the data, and returns it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 100, 'page_label': '101', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"Take a Quick Look at the Data Structure\\nYou start by looking at the top five rows of data using the DataFrame’s\\nhead()\\n method (see \\nFigure 2-6\\n).\\nFigure 2-6. \\nTop five rows in the dataset\\nEach row represents one district. There are 10 attributes (they are not all\\nshown in the screenshot): \\nlongitude\\n, \\nlatitude\\n, \\nhousing_median_age\\n,\\ntotal_rooms\\n, \\ntotal_bedrooms\\n, \\npopulation\\n, \\nhouseholds\\n, \\nmedian_income\\n,\\nmedian_house_value\\n, and \\nocean_proximity\\n.\\nThe \\ninfo()\\n method is useful to get a quick description of the data, in particular\\nthe total number of rows, each attribute’s type, and the number of non-null\\nvalues:\\n>>> \\nhousing\\n.\\ninfo\\n()\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 20640 entries, 0 to 20639\\nData columns (total 10 columns):\\n #   Column              Non-Null Count  Dtype\\n---  ------              --------------  -----\\n 0   longitude           20640 non-null  float64\\n 1   latitude            20640 non-null  float64\\n 2   housing_median_age  20640 non-null  float64\\n 3   total_rooms         20640 non-null  float64\\n 4   total_bedrooms      20433 non-null  float64\\n 5   population          20640 non-null  float64\\n 6   households          20640 non-null  float64\\n 7   median_income       20640 non-null  float64\\n 8   median_house_value  20640 non-null  float64\\n 9   ocean_proximity     20640 non-null  object\\ndtypes: float64(9), object(1)\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 101, 'page_label': '102', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='memory usage: 1.6+ MB\\nNOTE\\nIn this book, when a code example contains a mix of code and outputs, as is the case here,\\nit is formatted like in the Python interpreter, for better readability: the code lines are\\nprefixed with \\n>>>\\n (or \\n...\\n for indented blocks), and the outputs have no prefix.\\nThere are 20,640 instances in the dataset, which means that it is fairly small\\nby machine learning standards, but it’s perfect to get started. You notice that\\nthe \\ntotal_bedrooms\\n attribute has only 20,433 non-null values, meaning that\\n207 districts are missing this feature. You will need to take care of this later.\\nAll attributes are numerical, except for \\nocean_proximity\\n. Its type is \\nobject\\n, so\\nit could hold any kind of Python object. But since you loaded this data from a\\nCSV file, you know that it must be a text attribute. When you looked at the\\ntop five rows, you probably noticed that the values in the \\nocean_proximity\\ncolumn were repetitive, which means that it is probably a categorical\\nattribute. \\nYou can find out what categories exist and how many districts\\nbelong to each category by using the \\nvalue_counts()\\n method:\\n>>> \\nhousing\\n[\\n\"ocean_proximity\"\\n]\\n.\\nvalue_counts\\n()\\n<1H OCEAN     9136\\nINLAND        6551\\nNEAR OCEAN    2658\\nNEAR BAY      2290\\nISLAND           5\\nName: ocean_proximity, dtype: int64\\nLet’s look at the other fields. \\nThe \\ndescribe()\\n method shows a summary of the\\nnumerical attributes (\\nFigure 2-7\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 102, 'page_label': '103', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-7. \\nSummary of each numerical attribute\\nThe \\ncount\\n, \\nmean\\n, \\nmin\\n, and \\nmax\\n rows are self-explanatory. Note that the null\\nvalues are ignored (so, for example, the \\ncount\\n of \\ntotal_bedrooms\\n is 20,433,\\nnot 20,640). \\nThe \\nstd\\n row shows the \\nstandard deviation\\n, which measures how\\ndispersed the values are.\\n\\u2060\\n The \\n25%\\n, \\n50%\\n, and \\n75%\\n rows show the\\ncorresponding \\npercentiles\\n: a percentile indicates the value below which a\\ngiven percentage of observations in a group of observations fall. \\nFor\\nexample, 25% of the districts have a \\nhousing_median_age\\n lower than 18,\\nwhile 50% are lower than 29 and 75% are lower than 37. \\nThese are often\\ncalled the 25th percentile (or first \\nquartile\\n), the median, and the 75th\\npercentile (or third \\nquartile\\n).\\nAnother quick way to get a feel of the type of data you are dealing with is to\\nplot a histogram for each numerical attribute. A histogram shows the number\\nof instances (on the vertical axis) that have a given value range (on the\\nhorizontal axis). You can either plot this one attribute at a time, or you can\\ncall the \\nhist()\\n method on the whole dataset (as shown in the following code\\nexample), and it will plot a histogram for each numerical attribute (see\\nFigure 2-8\\n):\\nimport\\n \\nmatplotlib.pyplot\\n \\nas\\n \\nplt\\nhousing\\n.\\nhist\\n(\\nbins\\n=\\n50\\n,\\n \\nfigsize\\n=\\n(\\n12\\n,\\n \\n8\\n))\\nplt\\n.\\nshow\\n()\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 103, 'page_label': '104', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-8. \\nA histogram for each numerical attribute\\nLooking at these histograms, you notice a few things:\\nFirst, the median income attribute does not look like it is expressed in\\nUS dollars (USD). After checking with the team that collected the data,\\nyou are told that the data has been scaled and capped at 15 (actually,\\n15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for\\nlower median incomes. The numbers represent roughly tens of\\nthousands of dollars (e.g., 3 actually means about $30,000). \\nWorking\\nwith preprocessed attributes is common in machine learning, and it is\\nnot necessarily a problem, but you should try to understand how the data\\nwas computed.\\nThe housing median age and the median house value were also capped.\\nThe latter may be a serious problem since it is your target attribute (your\\nlabels). Your machine learning algorithms may learn that prices never\\ngo beyond that limit. You need to check with your client team (the team\\nthat will use your system’s output) to see if this is a problem or not. If\\nthey tell you that they need precise predictions even beyond $500,000,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 104, 'page_label': '105', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='then you have two options:\\nCollect proper labels for the districts whose labels were capped.\\nRemove those districts from the training set (and also from the test\\nset, since your system should not be evaluated poorly if it predicts\\nvalues beyond $500,000).\\nThese attributes have very different scales. We will discuss this later in\\nthis chapter, when we explore feature scaling.\\nFinally, many histograms are \\nskewed right\\n: \\nthey extend much farther to\\nthe right of the median than to the left. This may make it a bit harder for\\nsome machine learning algorithms to detect patterns. Later, you’ll try\\ntransforming these attributes to have more symmetrical and bell-shaped\\ndistributions.\\nYou should now have a better understanding of the kind of data you’re\\ndealing with.\\nWARNING\\nWait! Before you look at the data any further, you need to create a test set, put it aside, and\\nnever look at it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 105, 'page_label': '106', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Create a Test Set\\nIt may seem strange to voluntarily set aside part of the data at this stage. After\\nall, you have only taken a quick glance at the data, and surely you should\\nlearn a whole lot more about it before you decide what algorithms to use,\\nright? \\nThis is true, but your brain is an amazing pattern detection system,\\nwhich also means that it is highly prone to overfitting: if you look at the test\\nset, you may stumble upon some seemingly interesting pattern in the test data\\nthat leads you to select a particular kind of machine learning model. When\\nyou estimate the generalization error using the test set, your estimate will be\\ntoo optimistic, and you will launch a system that will not perform as well as\\nexpected. \\nThis is called \\ndata snooping\\n bias.\\nCreating a test set is theoretically simple; pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set\\nthem aside:\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\ndef\\n \\nshuffle_and_split_data\\n(\\ndata\\n,\\n \\ntest_ratio\\n):\\n    \\nshuffled_indices\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\npermutation\\n(\\nlen\\n(\\ndata\\n))\\n    \\ntest_set_size\\n \\n=\\n \\nint\\n(\\nlen\\n(\\ndata\\n)\\n \\n*\\n \\ntest_ratio\\n)\\n    \\ntest_indices\\n \\n=\\n \\nshuffled_indices\\n[:\\ntest_set_size\\n]\\n    \\ntrain_indices\\n \\n=\\n \\nshuffled_indices\\n[\\ntest_set_size\\n:]\\n    \\nreturn\\n \\ndata\\n.\\niloc\\n[\\ntrain_indices\\n],\\n \\ndata\\n.\\niloc\\n[\\ntest_indices\\n]\\nYou can then use this function like this:\\n>>> \\ntrain_set\\n,\\n \\ntest_set\\n \\n=\\n \\nshuffle_and_split_data\\n(\\nhousing\\n,\\n \\n0.2\\n)\\n>>> \\nlen\\n(\\ntrain_set\\n)\\n16512\\n>>> \\nlen\\n(\\ntest_set\\n)\\n4128\\nWell, this works, but it is not perfect: if you run the program again, it will\\ngenerate a different test set! Over time, you (or your machine learning\\nalgorithms) will get to see the whole dataset, which is what you want to\\navoid.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 106, 'page_label': '107', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='One solution is to save the test set on the first run and then load it in\\nsubsequent runs. \\nAnother option is to set the random number generator’s seed\\n(e.g., with \\nnp.random.seed(42)\\n)\\n\\u2060\\n before calling \\nnp.random.permutation()\\n so\\nthat it always generates the same shuffled indices.\\nHowever, both these solutions will break the next time you fetch an updated\\ndataset. To have a stable train/test split even after updating the dataset, a\\ncommon solution is to use each instance’s identifier to decide whether or not\\nit should go in the test set (assuming instances have unique and immutable\\nidentifiers). For example, you could compute a hash of each instance’s\\nidentifier and put that instance in the test set if the hash is lower than or equal\\nto 20% of the maximum hash value. This ensures that the test set will remain\\nconsistent across multiple runs, even if you refresh the dataset. The new test\\nset will contain 20% of the new instances, but it will not contain any instance\\nthat was previously in the training set.\\nHere is a possible implementation:\\nfrom\\n \\nzlib\\n \\nimport\\n \\ncrc32\\ndef\\n \\nis_id_in_test_set\\n(\\nidentifier\\n,\\n \\ntest_ratio\\n):\\n    \\nreturn\\n \\ncrc32\\n(\\nnp\\n.\\nint64\\n(\\nidentifier\\n))\\n \\n<\\n \\ntest_ratio\\n \\n*\\n \\n2\\n**\\n32\\ndef\\n \\nsplit_data_with_id_hash\\n(\\ndata\\n,\\n \\ntest_ratio\\n,\\n \\nid_column\\n):\\n    \\nids\\n \\n=\\n \\ndata\\n[\\nid_column\\n]\\n    \\nin_test_set\\n \\n=\\n \\nids\\n.\\napply\\n(\\nlambda\\n \\nid_\\n:\\n \\nis_id_in_test_set\\n(\\nid_\\n,\\n \\ntest_ratio\\n))\\n    \\nreturn\\n \\ndata\\n.\\nloc\\n[\\n~\\nin_test_set\\n],\\n \\ndata\\n.\\nloc\\n[\\nin_test_set\\n]\\nUnfortunately, the housing dataset does not have an identifier column. The\\nsimplest solution is to use the row index as the ID:\\nhousing_with_id\\n \\n=\\n \\nhousing\\n.\\nreset_index\\n()\\n  \\n# adds an `index` column\\ntrain_set\\n,\\n \\ntest_set\\n \\n=\\n \\nsplit_data_with_id_hash\\n(\\nhousing_with_id\\n,\\n \\n0.2\\n,\\n \\n\"index\"\\n)\\nIf you use the row index as a unique identifier, you need to make sure that\\nnew data gets appended to the end of the dataset and that no row ever gets\\ndeleted. If this is not possible, then you can try to use the most stable features\\nto build a unique identifier. For example, a district’s latitude and longitude\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 107, 'page_label': '108', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='are guaranteed to be stable for a few million years, so you could combine\\nthem into an ID like so:\\n\\u2060\\nhousing_with_id\\n[\\n\"id\"\\n]\\n \\n=\\n \\nhousing\\n[\\n\"longitude\"\\n]\\n \\n*\\n \\n1000\\n \\n+\\n \\nhousing\\n[\\n\"latitude\"\\n]\\ntrain_set\\n,\\n \\ntest_set\\n \\n=\\n \\nsplit_data_with_id_hash\\n(\\nhousing_with_id\\n,\\n \\n0.2\\n,\\n \\n\"id\"\\n)\\nScikit-Learn provides a few functions to split datasets into multiple subsets in\\nvarious ways. \\nThe simplest function is \\ntrain_test_split()\\n, which does pretty\\nmuch the same thing as the \\nshuffle_and_split_data()\\n function we defined\\nearlier, with a couple of additional features. First, there is a \\nrandom_state\\nparameter that allows you to set the random generator seed. Second, you can\\npass it multiple datasets with an identical number of rows, and it will split\\nthem on the same indices (this is very useful, for example, if you have a\\nseparate DataFrame for labels):\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\ntrain_test_split\\ntrain_set\\n,\\n \\ntest_set\\n \\n=\\n \\ntrain_test_split\\n(\\nhousing\\n,\\n \\ntest_size\\n=\\n0.2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nSo far we have considered purely random sampling methods. This is\\ngenerally fine if your dataset is large enough (especially relative to the\\nnumber of attributes), but if it is not, you run the risk of introducing a\\nsignificant sampling bias. When employees at a survey company decides to\\ncall 1,000 people to ask them a few questions, they don’t just pick 1,000\\npeople randomly in a phone book. They try to ensure that these 1,000 people\\nare representative of the whole population, with regard to the questions they\\nwant to ask. For example, the US population is 51.1% females and 48.9%\\nmales, so a well-conducted survey in the US would try to maintain this ratio\\nin the sample: 511 females and 489 males (at least if it seems possible that\\nthe answers may vary across genders). \\nThis is called \\nstratified sampling\\n: the\\npopulation is divided into homogeneous subgroups called \\nstrata\\n, and the right\\nnumber of instances are sampled from each stratum to guarantee that the test\\nset is representative of the overall population. If the people running the\\nsurvey used purely random sampling, there would be about a 10.7% chance\\nof sampling a skewed test set with less than 48.5% female or more than\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 108, 'page_label': '109', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='53.5% female participants. Either way, the survey results would likely be\\nquite biased.\\nSuppose you’ve chatted with some experts who told you that the median\\nincome is a very important attribute to predict median housing prices. You\\nmay want to ensure that the test set is representative of the various categories\\nof incomes in the whole dataset. Since the median income is a continuous\\nnumerical attribute, you first need to create an income category attribute.\\nLet’s look at the median income histogram more closely (back in \\nFigure 2-8\\n):\\nmost median income values are clustered around 1.5 to 6 (i.e., $15,000–\\n$60,000), but some median incomes go far beyond 6. It is important to have a\\nsufficient number of instances in your dataset for each stratum, or else the\\nestimate of a stratum’s importance may be biased. This means that you\\nshould not have too many strata, and each stratum should be large enough.\\nThe following code uses the \\npd.cut()\\n function to create an income category\\nattribute with five categories (labeled from 1 to 5); category 1 ranges from 0\\nto 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\\nhousing\\n[\\n\"income_cat\"\\n]\\n \\n=\\n \\npd\\n.\\ncut\\n(\\nhousing\\n[\\n\"median_income\"\\n],\\n                               \\nbins\\n=\\n[\\n0.\\n,\\n \\n1.5\\n,\\n \\n3.0\\n,\\n \\n4.5\\n,\\n \\n6.\\n,\\n \\nnp\\n.\\ninf\\n],\\n                               \\nlabels\\n=\\n[\\n1\\n,\\n \\n2\\n,\\n \\n3\\n,\\n \\n4\\n,\\n \\n5\\n])\\nThese income categories are represented in \\nFigure 2-9\\n:\\nhousing\\n[\\n\"income_cat\"\\n]\\n.\\nvalue_counts\\n()\\n.\\nsort_index\\n()\\n.\\nplot\\n.\\nbar\\n(\\nrot\\n=\\n0\\n,\\n \\ngrid\\n=\\nTrue\\n)\\nplt\\n.\\nxlabel\\n(\\n\"Income category\"\\n)\\nplt\\n.\\nylabel\\n(\\n\"Number of districts\"\\n)\\nplt\\n.\\nshow\\n()\\nNow you are ready to do stratified sampling based on the income category.\\nScikit-Learn provides a number of splitter classes in the\\nsklearn.model_selection\\n package that implement various strategies to split\\nyour dataset into a training set and a test set. Each splitter has a \\nsplit()\\n method\\nthat returns an iterator over different training/test splits of the same data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 109, 'page_label': '110', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-9. \\nHistogram of income categories\\nTo be precise, the \\nsplit()\\n method yields the training and test \\nindices\\n, not the\\ndata itself. Having multiple splits can be useful if you want to better estimate\\nthe performance of your model, as you will see when we discuss cross-\\nvalidation later in this chapter. For example, the following code generates 10\\ndifferent stratified splits of the same dataset:\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\nStratifiedShuffleSplit\\nsplitter\\n \\n=\\n \\nStratifiedShuffleSplit\\n(\\nn_splits\\n=\\n10\\n,\\n \\ntest_size\\n=\\n0.2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nstrat_splits\\n \\n=\\n \\n[]\\nfor\\n \\ntrain_index\\n,\\n \\ntest_index\\n \\nin\\n \\nsplitter\\n.\\nsplit\\n(\\nhousing\\n,\\n \\nhousing\\n[\\n\"income_cat\"\\n]):\\n    \\nstrat_train_set_n\\n \\n=\\n \\nhousing\\n.\\niloc\\n[\\ntrain_index\\n]\\n    \\nstrat_test_set_n\\n \\n=\\n \\nhousing\\n.\\niloc\\n[\\ntest_index\\n]\\n    \\nstrat_splits\\n.\\nappend\\n([\\nstrat_train_set_n\\n,\\n \\nstrat_test_set_n\\n])\\nFor now, you can just use the first split:\\nstrat_train_set\\n,\\n \\nstrat_test_set\\n \\n=\\n \\nstrat_splits\\n[\\n0\\n]\\nOr, since stratified sampling is fairly common, there’s a shorter way to get a\\nsingle split using the \\ntrain_test_split()\\n function with the \\nstratify\\n argument:\\nstrat_train_set\\n,\\n \\nstrat_test_set\\n \\n=\\n \\ntrain_test_split\\n(\\n    \\nhousing\\n,\\n \\ntest_size\\n=\\n0.2\\n,\\n \\nstratify\\n=\\nhousing\\n[\\n\"income_cat\"\\n],\\n \\nrandom_state\\n=\\n42\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 110, 'page_label': '111', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Let’s see if this worked as expected. You can start by looking at the income\\ncategory proportions in the test set:\\n>>> \\nstrat_test_set\\n[\\n\"income_cat\"\\n]\\n.\\nvalue_counts\\n()\\n \\n/\\n \\nlen\\n(\\nstrat_test_set\\n)\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114341\\n1    0.039971\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the\\nfull dataset. \\nFigure 2-10\\n compares the income category proportions in the\\noverall dataset, in the test set generated with stratified sampling, and in a test\\nset generated using purely random sampling. As you can see, the test set\\ngenerated using stratified sampling has income category proportions almost\\nidentical to those in the full dataset, whereas the test set generated using\\npurely random sampling is skewed.\\nFigure 2-10. \\nSampling bias comparison of stratified versus purely random sampling\\nYou won’t use the \\nincome_cat\\n column again, so you might as well drop it,\\nreverting the data back to its original state:\\nfor\\n \\nset_\\n \\nin\\n \\n(\\nstrat_train_set\\n,\\n \\nstrat_test_set\\n):\\n    \\nset_\\n.\\ndrop\\n(\\n\"income_cat\"\\n,\\n \\naxis\\n=\\n1\\n,\\n \\ninplace\\n=\\nTrue\\n)\\nWe spent quite a bit of time on test set generation for a good reason: this is an\\noften neglected but critical part of a machine learning project. Moreover,\\nmany of these ideas will be useful later when we discuss cross-validation.\\nNow it’s time to move on to the next stage: exploring the data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 111, 'page_label': '112', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Explore and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general\\nunderstanding of the kind of data you are manipulating. Now the goal is to go\\ninto a little more depth.\\nFirst, make sure you have put the test set aside and you are only exploring the\\ntraining set. Also, if the training set is very large, you may want to sample an\\nexploration set, to make manipulations easy and fast during the exploration\\nphase. In this case, the training set is quite small, so you can just work\\ndirectly on the full set. Since you’re going to experiment with various\\ntransformations of the full training set, you should make a copy of the\\noriginal so you can revert to it afterwards:\\nhousing\\n \\n=\\n \\nstrat_train_set\\n.\\ncopy\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 112, 'page_label': '113', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Visualizing Geographical Data\\nBecause the dataset includes geographical information (latitude and\\nlongitude), it is a good idea to create a scatterplot of all the districts to\\nvisualize the data (\\nFigure 2-11\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n \\nx\\n=\\n\"longitude\"\\n,\\n \\ny\\n=\\n\"latitude\"\\n,\\n \\ngrid\\n=\\nTrue\\n)\\nplt\\n.\\nshow\\n()\\nFigure 2-11. \\nA geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any\\nparticular pattern. Setting the \\nalpha\\n option to \\n0.2\\n makes it much easier to\\nvisualize the places where there is a high density of data points (\\nFigure 2-12\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n \\nx\\n=\\n\"longitude\"\\n,\\n \\ny\\n=\\n\"latitude\"\\n,\\n \\ngrid\\n=\\nTrue\\n,\\n \\nalpha\\n=\\n0.2\\n)\\nplt\\n.\\nshow\\n()\\nNow that’s much better: you can clearly see the high-density areas, namely'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 113, 'page_label': '114', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='the Bay Area and around Los Angeles and San Diego, plus a long line of\\nfairly high-density areas in the Central Valley (in particular, around\\nSacramento and Fresno).\\nOur brains are very good at spotting patterns in pictures, but you may need to\\nplay around with visualization parameters to make the patterns stand out.\\nFigure 2-12. \\nA better visualization that highlights high-density areas\\nNext, you look at the housing prices (\\nFigure 2-13\\n). The radius of each circle\\nrepresents the district’s population (option \\ns\\n), and the color represents the\\nprice (option \\nc\\n). Here you use a predefined color map (option \\ncmap\\n) called\\njet\\n, which ranges from blue (low values) to red (high prices):\\n\\u2060\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n \\nx\\n=\\n\"longitude\"\\n,\\n \\ny\\n=\\n\"latitude\"\\n,\\n \\ngrid\\n=\\nTrue\\n,\\n             \\ns\\n=\\nhousing\\n[\\n\"population\"\\n]\\n \\n/\\n \\n100\\n,\\n \\nlabel\\n=\\n\"population\"\\n,\\n             \\nc\\n=\\n\"median_house_value\"\\n,\\n \\ncmap\\n=\\n\"jet\"\\n,\\n \\ncolorbar\\n=\\nTrue\\n,\\n             \\nlegend\\n=\\nTrue\\n,\\n \\nsharex\\n=\\nFalse\\n,\\n \\nfigsize\\n=\\n(\\n10\\n,\\n \\n7\\n))\\nplt\\n.\\nshow\\n()\\nThis image tells you that the housing prices are very much related to the\\nlocation (e.g., close to the ocean) and to the population density, as you\\nprobably knew already. A clustering algorithm should be useful for detecting\\nthe main cluster and for adding new features that measure the proximity to\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 114, 'page_label': '115', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='the cluster centers. The ocean proximity attribute may be useful as well,\\nalthough in Northern California the housing prices in coastal districts are not\\ntoo high, so it is not a simple rule.\\nFigure 2-13. \\nCalifornia housing prices: red is expensive, blue is cheap, larger circles indicate areas\\nwith a larger population'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 115, 'page_label': '116', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Look for Correlations\\nSince the dataset is not too large, you can easily compute the \\nstandard\\ncorrelation coefficient\\n (also called \\nPearson’s r\\n) between every pair of\\nattributes using the \\ncorr()\\n method:\\ncorr_matrix\\n \\n=\\n \\nhousing\\n.\\ncorr\\n()\\nNow you can look at how much each attribute correlates with the median\\nhouse value:\\n>>> \\ncorr_matrix\\n[\\n\"median_house_value\"\\n]\\n.\\nsort_values\\n(\\nascending\\n=\\nFalse\\n)\\nmedian_house_value    1.000000\\nmedian_income         0.688380\\ntotal_rooms           0.137455\\nhousing_median_age    0.102175\\nhouseholds            0.071426\\ntotal_bedrooms        0.054635\\npopulation           -0.020153\\nlongitude            -0.050859\\nlatitude             -0.139584\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it\\nmeans that there is a strong positive correlation; for example, the median\\nhouse value tends to go up when the median income goes up. When the\\ncoefficient is close to –1, it means that there is a strong negative correlation;\\nyou can see a small negative correlation between the latitude and the median\\nhouse value (i.e., prices have a slight tendency to go down when you go\\nnorth). Finally, coefficients close to 0 mean that there is no linear correlation.\\nAnother way to check for correlation between attributes is to use the Pandas\\nscatter_matrix()\\n function, which plots every numerical attribute against every\\nother numerical attribute. Since there are now 11 numerical attributes, you\\nwould get 11\\n = 121 plots, which would not fit on a page—so you decide to\\nfocus on a few promising attributes that seem most correlated with the\\nmedian housing value (\\nFigure 2-14\\n):\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 116, 'page_label': '117', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='from\\n \\npandas.plotting\\n \\nimport\\n \\nscatter_matrix\\nattributes\\n \\n=\\n \\n[\\n\"median_house_value\"\\n,\\n \\n\"median_income\"\\n,\\n \\n\"total_rooms\"\\n,\\n              \\n\"housing_median_age\"\\n]\\nscatter_matrix\\n(\\nhousing\\n[\\nattributes\\n],\\n \\nfigsize\\n=\\n(\\n12\\n,\\n \\n8\\n))\\nplt\\n.\\nshow\\n()\\nFigure 2-14. \\nThis scatter matrix plots every numerical attribute against every other numerical\\nattribute, plus a histogram of each numerical attribute’s values on the main diagonal (top left to bottom\\nright)\\nThe main diagonal would be full of straight lines if Pandas plotted each\\nvariable against itself, which would not be very useful. So instead, the Pandas\\ndisplays a histogram of each attribute (other options are available; see the\\nPandas documentation for more details).\\nLooking at the correlation scatterplots, it seems like the most promising\\nattribute to predict the median house value is the median income, so you\\nzoom in on their scatterplot (\\nFigure 2-15\\n):\\nhousing\\n.\\nplot\\n(\\nkind\\n=\\n\"scatter\"\\n,\\n \\nx\\n=\\n\"median_income\"\\n,\\n \\ny\\n=\\n\"median_house_value\"\\n,\\n             \\nalpha\\n=\\n0.1\\n,\\n \\ngrid\\n=\\nTrue\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 117, 'page_label': '118', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='plt\\n.\\nshow\\n()\\nFigure 2-15. \\nMedian income versus median house value\\nThis plot reveals a few things. First, the correlation is indeed quite strong;\\nyou can clearly see the upward trend, and the points are not too dispersed.\\nSecond, the price cap you noticed earlier is clearly visible as a horizontal line\\nat $500,000. But the plot also reveals other less obvious straight lines: a\\nhorizontal line around $450,000, another around $350,000, perhaps one\\naround $280,000, and a few more below that. You may want to try removing\\nthe corresponding districts to prevent your algorithms from learning to\\nreproduce these data quirks.\\nWARNING\\nThe correlation coefficient only measures linear correlations (“as \\nx\\n goes up, \\ny\\n generally\\ngoes up/down”). It may completely miss out on nonlinear relationships (e.g., “as \\nx\\napproaches 0, \\ny\\n generally goes up”). \\nFigure 2-16\\n shows a variety of datasets along with\\ntheir correlation coefficient. Note how all the plots of the bottom row have a correlation\\ncoefficient equal to 0, despite the fact that their axes are clearly \\nnot\\n independent: these are\\nexamples of nonlinear relationships. Also, the second row shows examples where the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 118, 'page_label': '119', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='correlation coefficient is equal to 1 or –1; notice that this has nothing to do with the slope.\\nFor example, your height in inches has a correlation coefficient of 1 with your height in\\nfeet or in nanometers.\\nFigure 2-16. \\nStandard correlation coefficient of various datasets (source: Wikipedia; public domain\\nimage)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 119, 'page_label': '120', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Experiment with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can\\nexplore the data and gain insights. You identified a few data quirks that you\\nmay want to clean up before feeding the data to a machine learning\\nalgorithm, and you found interesting correlations between attributes, in\\nparticular with the target attribute. You also noticed that some attributes have\\na skewed-right distribution, so you may want to transform them (e.g., by\\ncomputing their logarithm or square root). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before preparing the data for machine\\nlearning algorithms is to try out various attribute combinations. For example,\\nthe total number of rooms in a district is not very useful if you don’t know\\nhow many households there are. What you really want is the number of\\nrooms per household. Similarly, the total number of bedrooms by itself is not\\nvery useful: you probably want to compare it to the number of rooms. And\\nthe population per household also seems like an interesting attribute\\ncombination to look at. You create these new attributes as follows:\\nhousing\\n[\\n\"rooms_per_house\"\\n]\\n \\n=\\n \\nhousing\\n[\\n\"total_rooms\"\\n]\\n \\n/\\n \\nhousing\\n[\\n\"households\"\\n]\\nhousing\\n[\\n\"bedrooms_ratio\"\\n]\\n \\n=\\n \\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n \\n/\\n \\nhousing\\n[\\n\"total_rooms\"\\n]\\nhousing\\n[\\n\"people_per_house\"\\n]\\n \\n=\\n \\nhousing\\n[\\n\"population\"\\n]\\n \\n/\\n \\nhousing\\n[\\n\"households\"\\n]\\nAnd then you look at the correlation matrix again:\\n>>> \\ncorr_matrix\\n \\n=\\n \\nhousing\\n.\\ncorr\\n()\\n>>> \\ncorr_matrix\\n[\\n\"median_house_value\"\\n]\\n.\\nsort_values\\n(\\nascending\\n=\\nFalse\\n)\\nmedian_house_value    1.000000\\nmedian_income         0.688380\\nrooms_per_house       0.143663\\ntotal_rooms           0.137455\\nhousing_median_age    0.102175\\nhouseholds            0.071426\\ntotal_bedrooms        0.054635\\npopulation           -0.020153\\npeople_per_house     -0.038224\\nlongitude            -0.050859'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 120, 'page_label': '121', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='latitude             -0.139584\\nbedrooms_ratio       -0.256397\\nName: median_house_value, dtype: float64\\nHey, not bad! The new \\nbedrooms_ratio\\n attribute is much more correlated\\nwith the median house value than the total number of rooms or bedrooms.\\nApparently houses with a lower bedroom/room ratio tend to be more\\nexpensive. The number of rooms per household is also more informative than\\nthe total number of rooms in a district—obviously the larger the houses, the\\nmore expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point\\nis to start off on the right foot and quickly gain insights that will help you get\\na first reasonably good prototype. But this is an iterative process: once you\\nget a prototype up and running, you can analyze its output to gain more\\ninsights and come back to this exploration step.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 121, 'page_label': '122', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Prepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your machine learning algorithms. Instead of\\ndoing this manually, you should write functions for this purpose, for several\\ngood reasons:\\nThis will allow you to reproduce these transformations easily on any\\ndataset (e.g., the next time you get a fresh dataset).\\nYou will gradually build a library of transformation functions that you\\ncan reuse in future projects.\\nYou can use these functions in your live system to transform the new\\ndata before feeding it to your algorithms.\\nThis will make it possible for you to easily try various transformations\\nand see which combination of transformations works best.\\nBut first, revert to a clean training set (by copying \\nstrat_train_set\\n once again).\\nYou should also separate the predictors and the labels, since you don’t\\nnecessarily want to apply the same transformations to the predictors and the\\ntarget values (note that \\ndrop()\\n creates a copy of the data and does not affect\\nstrat_train_set\\n):\\nhousing\\n \\n=\\n \\nstrat_train_set\\n.\\ndrop\\n(\\n\"median_house_value\"\\n,\\n \\naxis\\n=\\n1\\n)\\nhousing_labels\\n \\n=\\n \\nstrat_train_set\\n[\\n\"median_house_value\"\\n]\\n.\\ncopy\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 122, 'page_label': '123', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Clean the Data\\nMost machine learning algorithms cannot work with missing features, so\\nyou’ll need to take care of these. For example, you noticed earlier that the\\ntotal_bedrooms\\n attribute has some missing values. You have three options to\\nfix this:\\n1\\n. \\nGet rid of the corresponding districts.\\n2\\n. \\nGet rid of the whole attribute.\\n3\\n. \\nSet the missing values to some value (zero, the mean, the median, etc.).\\nThis is called \\nimputation\\n.\\nYou can accomplish these easily using the Pandas DataFrame’s \\ndropna()\\n,\\ndrop()\\n, and \\nfillna()\\n methods:\\nhousing\\n.\\ndropna\\n(\\nsubset\\n=\\n[\\n\"total_bedrooms\"\\n],\\n \\ninplace\\n=\\nTrue\\n)\\n  \\n# option 1\\nhousing\\n.\\ndrop\\n(\\n\"total_bedrooms\"\\n,\\n \\naxis\\n=\\n1\\n)\\n  \\n# option 2\\nmedian\\n \\n=\\n \\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n.\\nmedian\\n()\\n  \\n# option 3\\nhousing\\n[\\n\"total_bedrooms\"\\n]\\n.\\nfillna\\n(\\nmedian\\n,\\n \\ninplace\\n=\\nTrue\\n)\\nYou decide to go for option 3 since it is the least destructive, but instead of\\nthe preceding code, you will use a handy Scikit-Learn class: \\nSimpleImputer\\n.\\nThe benefit is that it will store the median value of each feature: this will\\nmake it possible to impute missing values not only on the training set, but\\nalso on the validation set, the test set, and any new data fed to the model. To\\nuse it, first you need to create a \\nSimpleImputer\\n instance, specifying that you\\nwant to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom\\n \\nsklearn.impute\\n \\nimport\\n \\nSimpleImputer\\nimputer\\n \\n=\\n \\nSimpleImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)\\nSince the median can only be computed on numerical attributes, you then'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 123, 'page_label': '124', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='need to create a copy of the data with only the numerical attributes (this will\\nexclude the text attribute \\nocean_proximity\\n):\\nhousing_num\\n \\n=\\n \\nhousing\\n.\\nselect_dtypes\\n(\\ninclude\\n=\\n[\\nnp\\n.\\nnumber\\n])\\nNow you can fit the \\nimputer\\n instance to the training data using the \\nfit()\\nmethod:\\nimputer\\n.\\nfit\\n(\\nhousing_num\\n)\\nThe \\nimputer\\n has simply computed the median of each attribute and stored the\\nresult in its \\nstatistics_\\n instance variable. Only the \\ntotal_bedrooms\\n attribute\\nhad missing values, but you cannot be sure that there won’t be any missing\\nvalues in new data after the system goes live, so it is safer to apply the\\nimputer\\n to all the numerical attributes:\\n>>> \\nimputer\\n.\\nstatistics_\\narray([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])\\n>>> \\nhousing_num\\n.\\nmedian\\n()\\n.\\nvalues\\narray([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])\\nNow you can use this “trained” \\nimputer\\n to transform the training set by\\nreplacing missing values with the learned medians:\\nX\\n \\n=\\n \\nimputer\\n.\\ntransform\\n(\\nhousing_num\\n)\\nMissing values can also be replaced with the mean value (\\nstrategy=\"mean\"\\n),\\nor with the most frequent value (\\nstrategy=\"most_frequent\"\\n), or with a\\nconstant value (\\nstrategy=\"constant\", fill_value=\\n…). The last two strategies\\nsupport non-numerical data.\\nTIP\\nThere are also more powerful imputers available in the \\nsklearn.impute\\n package (both for\\nnumerical features only):\\nKNNImputer\\n replaces each missing value with the mean of the \\nk\\n-nearest neighbors’'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 124, 'page_label': '125', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='values for that feature. \\nThe distance is based on all the available features.\\nIterativeImputer\\n trains a regression model per feature to predict the missing values\\nbased on all the other available features. \\nIt then trains the model again on the\\nupdated data, and repeats the process several times, improving the models and the\\nreplacement values at each iteration.\\nSCIKIT-LEARN DESIGN\\nScikit-Learn’s API is remarkably well designed. These are the \\nmain\\ndesign principles\\n:\\n\\u2060\\nConsistency\\nAll objects share a consistent and simple interface:\\nEstimators\\nAny object that can estimate some parameters based on a dataset is\\ncalled an \\nestimator\\n (e.g., a \\nSimpleImputer\\n is an estimator). The\\nestimation itself is performed by the \\nfit()\\n method, and it takes a\\ndataset as a parameter, or two for supervised learning algorithms—the\\nsecond dataset contains the labels. Any other parameter needed to\\nguide the estimation process is considered a hyperparameter (such as\\na \\nSimpleImputer\\n’s \\nstrategy\\n), and it must be set as an instance variable\\n(generally via a constructor parameter).\\nTransformers\\nSome estimators (such as a \\nSimpleImputer\\n) can also transform a\\ndataset; these are called \\ntransformers\\n. Once again, the API is simple:\\nthe transformation is \\nperformed\\n by the \\ntransform()\\n method with the\\ndataset to transform as a parameter. It returns the transformed dataset.\\nThis transformation generally relies on the learned parameters, as is\\nthe case for a \\nSimpleImputer\\n. All transformers also have a\\nconvenience method called \\nfit_transform()\\n, which is equivalent to\\ncalling \\nfit()\\n and then \\ntransform()\\n (but sometimes \\nfit_transform()\\n is\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 125, 'page_label': '126', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='optimized and runs much faster).\\nPredictors\\nFinally, some estimators, given a dataset, are capable of making\\npredictions; they are called \\npredictors\\n. For example, the\\nLinearRegression\\n model in the previous chapter was a predictor:\\ngiven a country’s GDP per capita, it predicted life satisfaction. A\\npredictor has a \\npredict()\\n method that takes a dataset of new instances\\nand returns a dataset of corresponding predictions. It also has a\\nscore()\\n method that measures the quality of the predictions, given a\\ntest set (and the corresponding labels, in the case of supervised\\nlearning algorithms).\\nInspection\\nAll the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., \\nimputer.strategy\\n), and all the estimator’s learned\\nparameters are accessible via public instance variables with an\\nunderscore suffix (e.g., \\nimputer.statistics_\\n).\\nNonproliferation of classes\\nDatasets are represented as NumPy arrays or SciPy sparse matrices,\\ninstead of homemade classes. Hyperparameters are just regular Python\\nstrings or numbers.\\nComposition\\nExisting building blocks are reused as much as possible. For example, it\\nis easy to create a \\nPipeline\\n estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as you will see.\\nSensible defaults\\nScikit-Learn provides reasonable default values for most parameters,\\nmaking it easy to quickly create a baseline working system.\\nScikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse\\nmatrices) even when they are fed Pandas DataFrames as input.\\n\\u2060\\n So, the\\n10\\n11'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 126, 'page_label': '127', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='output of \\nimputer.transform(housing_num)\\n is a NumPy array: \\nX\\n has neither\\ncolumn names nor index. Luckily, it’s not too hard to wrap \\nX\\n in a DataFrame\\nand recover the column names and index from \\nhousing_num\\n:\\nhousing_tr\\n \\n=\\n \\npd\\n.\\nDataFrame\\n(\\nX\\n,\\n \\ncolumns\\n=\\nhousing_num\\n.\\ncolumns\\n,\\n                          \\nindex\\n=\\nhousing_num\\n.\\nindex\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 127, 'page_label': '128', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Handling Text and Categorical Attributes\\nSo far we have only dealt with numerical attributes, but your data may also\\ncontain text attributes. In this dataset, there is just one: the \\nocean_proximity\\nattribute. Let’s look at its value for the first few instances:\\n>>> \\nhousing_cat\\n \\n=\\n \\nhousing\\n[[\\n\"ocean_proximity\"\\n]]\\n>>> \\nhousing_cat\\n.\\nhead\\n(\\n8\\n)\\n      ocean_proximity\\n13096        NEAR BAY\\n14973       <1H OCEAN\\n3785           INLAND\\n14689          INLAND\\n20507      NEAR OCEAN\\n1286           INLAND\\n18078       <1H OCEAN\\n4396         NEAR BAY\\nIt’s not arbitrary text: there are a limited number of possible values, each of\\nwhich represents a category. So this attribute is a categorical attribute. \\nMost\\nmachine \\nlearning\\n algorithms prefer to work with numbers, so let’s convert\\nthese categories from text to numbers. For this, we can use Scikit-Learn’s\\nOrdinalEncoder\\n class:\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nOrdinalEncoder\\nordinal_encoder\\n \\n=\\n \\nOrdinalEncoder\\n()\\nhousing_cat_encoded\\n \\n=\\n \\nordinal_encoder\\n.\\nfit_transform\\n(\\nhousing_cat\\n)\\nHere’s what the first few encoded values in \\nhousing_cat_encoded\\n look like:\\n>>> \\nhousing_cat_encoded\\n[:\\n8\\n]\\narray([[3.],\\n       [0.],\\n       [1.],\\n       [1.],\\n       [4.],\\n       [1.],\\n       [0.],\\n       [3.]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 128, 'page_label': '129', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='You can get the list of categories using the \\ncategories_\\n instance variable. It is\\na list containing a 1D array of categories for each categorical attribute (in this\\ncase, a list containing a single array since there is just one categorical\\nattribute):\\n>>> \\nordinal_encoder\\n.\\ncategories_\\n[array([\\'<1H OCEAN\\', \\'INLAND\\', \\'ISLAND\\', \\'NEAR BAY\\', \\'NEAR OCEAN\\'],\\n       dtype=object)]\\nOne issue with this representation is that ML algorithms will assume that two\\nnearby values are more similar than two distant values. This may be fine in\\nsome cases (e.g., for ordered categories such as “bad”, “average”, “good”,\\nand “excellent”), but it is obviously not the case for the \\nocean_proximity\\ncolumn (for example, categories 0 and 4 are clearly more similar than\\ncategories 0 and 1). To fix this issue, a common solution is to create one\\nbinary attribute per category: one attribute equal to 1 when the category is \\n\"\\n<1H OCEAN\"\\n (and 0 otherwise), another attribute equal to 1 when the\\ncategory is \\n\"INLAND\"\\n (and 0 otherwise), and so on. \\nThis is called \\none-hot\\nencoding\\n, because only one attribute will be equal to 1 (hot), while the others\\nwill be 0 (cold). \\nThe new attributes are sometimes called \\ndummy\\n attributes.\\nScikit-Learn provides a \\nOneHotEncoder\\n class to convert categorical values\\ninto one-hot vectors:\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nOneHotEncoder\\ncat_encoder\\n \\n=\\n \\nOneHotEncoder\\n()\\nhousing_cat_1hot\\n \\n=\\n \\ncat_encoder\\n.\\nfit_transform\\n(\\nhousing_cat\\n)\\nBy default, the output of a \\nOneHotEncoder\\n is a SciPy \\nsparse matrix\\n, instead\\nof a NumPy array:\\n>>> \\nhousing_cat_1hot\\n<16512x5 sparse matrix of type \\'<class \\'numpy.float64\\'>\\'\\n with 16512 stored elements in Compressed Sparse Row format>\\nA sparse matrix is a very efficient representation for matrices that contain\\nmostly zeros. Indeed, internally it only stores the nonzero values and their'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 129, 'page_label': '130', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='positions. When a categorical attribute has hundreds or thousands of\\ncategories, one-hot encoding it results in a very large matrix full of 0s except\\nfor a single 1 per row. In this case, a sparse matrix is exactly what you need:\\nit will save plenty of memory and speed up computations. You can use a\\nsparse matrix mostly like a normal 2D array,\\n\\u2060\\n but if you want to convert it\\nto a (dense) NumPy array, just call the \\ntoarray()\\n method:\\n>>> \\nhousing_cat_1hot\\n.\\ntoarray\\n()\\narray([[0., 0., 0., 1., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 1., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., 0., 1.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.]])\\nAlternatively, you can set \\nsparse=False\\n when creating the \\nOneHotEncoder\\n, in\\nwhich case the \\ntransform()\\n method will return a regular (dense) NumPy array\\ndirectly.\\nAs with the \\nOrdinalEncoder\\n, you can get the list of categories using the\\nencoder’s \\ncategories_\\n instance variable:\\n>>> \\ncat_encoder\\n.\\ncategories_\\n[array([\\'<1H OCEAN\\', \\'INLAND\\', \\'ISLAND\\', \\'NEAR BAY\\', \\'NEAR OCEAN\\'],\\n       dtype=object)]\\nPandas has a function called \\nget_dummies()\\n, which also converts each\\ncategorical feature into a one-hot representation, with one binary feature per\\ncategory:\\n>>> \\ndf_test\\n \\n=\\n \\npd\\n.\\nDataFrame\\n({\\n\"ocean_proximity\"\\n:\\n \\n[\\n\"INLAND\"\\n,\\n \\n\"NEAR BAY\"\\n]})\\n>>> \\npd\\n.\\nget_dummies\\n(\\ndf_test\\n)\\n   ocean_proximity_INLAND  ocean_proximity_NEAR BAY\\n0                       1                         0\\n1                       0                         1\\nIt looks nice and simple, so why not use it instead of \\nOneHotEncoder\\n? Well,\\nthe advantage of \\nOneHotEncoder\\n is that it remembers which categories it was\\n12'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 130, 'page_label': '131', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='trained on. This is very important because once your model is in production,\\nit should be fed exactly the same features as during training: no more, no less.\\nLook what our trained \\ncat_encoder\\n outputs when we make it transform the\\nsame \\ndf_test\\n (using \\ntransform()\\n, not \\nfit_transform()\\n):\\n>>> \\ncat_encoder\\n.\\ntransform\\n(\\ndf_test\\n)\\narray([[0., 1., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nSee the difference? \\nget_dummies()\\n saw only two categories, so it output two\\ncolumns, whereas \\nOneHotEncoder\\n output one column per learned category,\\nin the right order. \\nMoreover, if you feed \\nget_dummies()\\n a DataFrame\\ncontaining an unknown category (e.g., \\n\"<2H OCEAN\"\\n), it will happily\\ngenerate a column for it:\\n>>> \\ndf_test_unknown\\n \\n=\\n \\npd\\n.\\nDataFrame\\n({\\n\"ocean_proximity\"\\n:\\n \\n[\\n\"<2H OCEAN\"\\n,\\n \\n\"ISLAND\"\\n]})\\n>>> \\npd\\n.\\nget_dummies\\n(\\ndf_test_unknown\\n)\\n   ocean_proximity_<2H OCEAN  ocean_proximity_ISLAND\\n0                          1                       0\\n1                          0                       1\\nBut \\nOneHotEncoder\\n is smarter: it will detect the unknown category and raise\\nan exception. If you prefer, you can set the \\nhandle_unknown\\n hyperparameter\\nto \\n\"ignore\"\\n, in which case it will just represent the unknown category with\\nzeros:\\n>>> \\ncat_encoder\\n.\\nhandle_unknown\\n \\n=\\n \\n\"ignore\"\\n>>> \\ncat_encoder\\n.\\ntransform\\n(\\ndf_test_unknown\\n)\\narray([[0., 0., 0., 0., 0.],\\n       [0., 0., 1., 0., 0.]])\\nTIP\\nIf a categorical attribute has a large number of possible categories (e.g., country code,\\nprofession, species), then one-hot encoding will result in a large number of input features.\\nThis may slow down training and degrade performance. If this happens, you may want to\\nreplace the categorical input with useful numerical features related to the categories: for\\nexample, you could replace the \\nocean_proximity\\n feature with the distance to the ocean\\n(similarly, a country code could be replaced with the country’s population and GDP per'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 131, 'page_label': '132', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"capita). Alternatively, you can use one of the encoders provided by the \\ncategory_encoders\\npackage on \\nGitHub\\n. \\nOr, when dealing with neural networks, you can replace each\\ncategory with a learnable, low-dimensional vector called an \\nembedding\\n. This is an\\nexample of \\nrepresentation learning\\n (see Chapters \\n13\\n and \\n17\\n for more details).\\nWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator\\nstores the column names in the \\nfeature_names_in_\\n attribute. Scikit-Learn\\nthen ensures that any DataFrame fed to this estimator after that (e.g., to\\ntransform()\\n or \\npredict()\\n) has the same column names. Transformers also\\nprovide a \\nget_feature_names_out()\\n method that you can use to build a\\nDataFrame around the transformer’s output:\\n>>> \\ncat_encoder\\n.\\nfeature_names_in_\\narray(['ocean_proximity'], dtype=object)\\n>>> \\ncat_encoder\\n.\\nget_feature_names_out\\n()\\narray(['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\\n       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\\n       'ocean_proximity_NEAR OCEAN'], dtype=object)\\n>>> \\ndf_output\\n \\n=\\n \\npd\\n.\\nDataFrame\\n(\\ncat_encoder\\n.\\ntransform\\n(\\ndf_test_unknown\\n),\\n... \\n                         \\ncolumns\\n=\\ncat_encoder\\n.\\nget_feature_names_out\\n(),\\n... \\n                         \\nindex\\n=\\ndf_test_unknown\\n.\\nindex\\n)\\n...\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 132, 'page_label': '133', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Feature Scaling and Transformation\\nOne of the most important transformations you need to apply to your data is\\nfeature scaling\\n. With few exceptions, machine learning algorithms don’t\\nperform well when the input numerical attributes have very different scales.\\nThis is the case for the housing data: the total number of rooms ranges from\\nabout 6 to 39,320, while the median incomes only range from 0 to 15.\\nWithout any scaling, most models will be biased toward ignoring the median\\nincome and focusing more on the number of rooms.\\nThere are two common ways to get all attributes to have the same scale: \\nmin-\\nmax scaling\\n and \\nstandardization\\n.\\nWARNING\\nAs with all estimators, it is important to fit the scalers to the training data only: never use\\nfit()\\n or \\nfit_transform()\\n for anything else than the training set. Once you have a trained\\nscaler, you can then use it to \\ntransform()\\n any other set, including the validation set, the test\\nset, and new data. Note that while the training set values will always be scaled to the\\nspecified range, if new data contains outliers, these may end up scaled outside the range. If\\nyou want to avoid this, just set the \\nclip\\n hyperparameter to \\nTrue\\n.\\nMin-max scaling (many people call this \\nnormalization\\n) is the simplest: for\\neach attribute, the values are shifted and rescaled so that they end up ranging\\nfrom 0 to 1. This is performed by subtracting the min value and dividing by\\nthe difference between the min and the max. \\nScikit-Learn provides a\\ntransformer called \\nMinMaxScaler\\n for this. It has a \\nfeature_range\\nhyperparameter that lets you change the range if, for some reason, you don’t\\nwant 0–1 (e.g., neural networks work best with zero-mean inputs, so a range\\nof –1 to 1 is preferable). It’s quite easy to use:\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nMinMaxScaler\\nmin_max_scaler\\n \\n=\\n \\nMinMaxScaler\\n(\\nfeature_range\\n=\\n(\\n-\\n1\\n,\\n \\n1\\n))\\nhousing_num_min_max_scaled\\n \\n=\\n \\nmin_max_scaler\\n.\\nfit_transform\\n(\\nhousing_num\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 133, 'page_label': '134', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Standardization is different: first it subtracts the mean value (so standardized\\nvalues have a zero mean), then it divides the result by the standard deviation\\n(so standardized values have a standard deviation equal to 1). Unlike min-\\nmax scaling, standardization does not restrict values to a specific range.\\nHowever, standardization is much less affected by outliers. For example,\\nsuppose a district has a median income equal to 100 (by mistake), instead of\\nthe usual 0–15. Min-max scaling to the 0–1 range would map this outlier\\ndown to 1 and it would crush all the other values down to 0–0.15, whereas\\nstandardization would not be much affected. Scikit-Learn provides a\\ntransformer called \\nStandardScaler\\n for \\nstandardization\\n:\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nStandardScaler\\nstd_scaler\\n \\n=\\n \\nStandardScaler\\n()\\nhousing_num_std_scaled\\n \\n=\\n \\nstd_scaler\\n.\\nfit_transform\\n(\\nhousing_num\\n)\\nTIP\\nIf you want to scale a sparse matrix without converting it to a dense matrix first, you can\\nuse a \\nStandardScaler\\n with its \\nwith_mean\\n hyperparameter set to \\nFalse\\n: it will only divide\\nthe data by the standard deviation, without subtracting the mean (as this would break\\nsparsity).\\nWhen a feature’s distribution has a \\nheavy tail\\n (i.e., when values far from the\\nmean are not exponentially rare), both min-max scaling and standardization\\nwill squash most values into a small range. Machine learning models\\ngenerally don’t like this at all, as you will see in \\nChapter 4\\n. So \\nbefore\\n you\\nscale the feature, you should first transform it to shrink the heavy tail, and if\\npossible to make the distribution roughly symmetrical. For example, a\\ncommon way to do this for positive features with a heavy tail to the right is to\\nreplace the feature with its square root (or raise the feature to a power\\nbetween 0 and 1). \\nIf the feature has a really long and heavy tail, such as a\\npower law distribution\\n, then replacing the feature with its logarithm may\\nhelp. For example, the \\npopulation\\n feature roughly follows a power law:\\ndistricts with 10,000 inhabitants are only 10 times less frequent than districts'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 134, 'page_label': '135', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='with 1,000 inhabitants, not exponentially less frequent. \\nFigure 2-17\\n shows\\nhow much better this feature looks when you compute its log: it’s very close\\nto a Gaussian distribution (i.e., bell-shaped).\\nFigure 2-17. \\nTransforming a feature to make it closer to a Gaussian distribution\\nAnother approach to handle heavy-tailed features consists in \\nbucketizing\\n the\\nfeature. This means chopping its distribution into roughly equal-sized\\nbuckets, and replacing each feature value with the index of the bucket it\\nbelongs to, much like we did to create the \\nincome_cat\\n feature (although we\\nonly used it for stratified sampling). For example, you could replace each\\nvalue with its percentile. Bucketizing with equal-sized buckets results in a\\nfeature with an almost uniform distribution, so there’s no need for further\\nscaling, or you can just divide by the number of buckets to force the values to\\nthe 0–1 range.\\nWhen a feature has a multimodal distribution (i.e., with two or more clear\\npeaks, called \\nmodes\\n), such as the \\nhousing_median_age\\n feature, it can also be\\nhelpful to bucketize it, but this time treating the bucket IDs as categories,\\nrather than as numerical values. This means that the bucket indices must be\\nencoded, \\nfor example using a \\nOneHotEncoder\\n (so you usually don’t want to\\nuse too many buckets). This approach will allow the regression model to\\nmore easily learn different rules for different ranges of this feature value. For\\nexample, perhaps houses built around 35 years ago have a peculiar style that\\nfell out of fashion, and therefore they’re cheaper than their age alone would\\nsuggest.\\nAnother approach to transforming multimodal distributions is to add a feature'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 135, 'page_label': '136', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='for each of the modes (at least the main ones), representing the similarity\\nbetween the housing median age and that particular mode. \\nThe similarity\\nmeasure is typically computed using a \\nradial basis function\\n (RBF)—any\\nfunction that depends only on the distance between the input value and a\\nfixed point. The most commonly used RBF is the Gaussian RBF, whose\\noutput value decays exponentially as the input value moves away from the\\nfixed point. For example, the Gaussian RBF similarity between the housing\\nage \\nx\\n and 35 is given by the equation exp(–\\nγ\\n(\\nx\\n – 35)²). The hyperparameter \\nγ\\n(gamma) determines how quickly the similarity measure decays as \\nx\\n moves\\naway from 35. \\nUsing Scikit-Learn’s \\nrbf_kernel()\\n function, you can create a\\nnew Gaussian RBF feature measuring the similarity between the housing\\nmedian age and 35:\\nfrom\\n \\nsklearn.metrics.pairwise\\n \\nimport\\n \\nrbf_kernel\\nage_simil_35\\n \\n=\\n \\nrbf_kernel\\n(\\nhousing\\n[[\\n\"housing_median_age\"\\n]],\\n \\n[[\\n35\\n]],\\n \\ngamma\\n=\\n0.1\\n)\\nFigure 2-18\\n shows this new feature as a function of the housing median age\\n(solid line). It also shows what the feature would look like if you used a\\nsmaller \\ngamma\\n value. As the chart shows, the new age similarity feature\\npeaks at 35, right around the spike in the housing median age distribution: if\\nthis particular age group is well correlated with lower prices, there’s a good\\nchance that this new feature will help.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 136, 'page_label': '137', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-18. \\nGaussian RBF feature measuring the similarity between the housing median age and 35\\nSo far we’ve only looked at the input features, but the target values may also\\nneed to be transformed. For example, if the target distribution has a heavy\\ntail, you may choose to replace the target with its logarithm. \\nBut if you do,\\nthe regression model will now predict the \\nlog\\n of the median house value, not\\nthe median house value itself. You will need to compute the exponential of\\nthe model’s prediction if you want the predicted median house value.\\nLuckily, most of Scikit-Learn’s transformers have an \\ninverse_transform()\\nmethod, making it easy to compute the inverse of their transformations. \\nFor\\nexample, the following code example shows how to scale the labels using a\\nStandardScaler\\n (just like we did for inputs), then train a simple linear\\nregression model on the resulting scaled labels and use it to make predictions\\non some new data, which we transform back to the original scale using the\\ntrained scaler’s \\ninverse_transform()\\n method. \\nNote that we convert the labels\\nfrom a Pandas Series to a DataFrame, since the \\nStandardScaler\\n expects 2D\\ninputs. Also, in this example we just train the model on a single raw input\\nfeature (median income), for simplicity:\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLinearRegression\\ntarget_scaler\\n \\n=\\n \\nStandardScaler\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 137, 'page_label': '138', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='scaled_labels\\n \\n=\\n \\ntarget_scaler\\n.\\nfit_transform\\n(\\nhousing_labels\\n.\\nto_frame\\n())\\nmodel\\n \\n=\\n \\nLinearRegression\\n()\\nmodel\\n.\\nfit\\n(\\nhousing\\n[[\\n\"median_income\"\\n]],\\n \\nscaled_labels\\n)\\nsome_new_data\\n \\n=\\n \\nhousing\\n[[\\n\"median_income\"\\n]]\\n.\\niloc\\n[:\\n5\\n]\\n  \\n# pretend this is new data\\nscaled_predictions\\n \\n=\\n \\nmodel\\n.\\npredict\\n(\\nsome_new_data\\n)\\npredictions\\n \\n=\\n \\ntarget_scaler\\n.\\ninverse_transform\\n(\\nscaled_predictions\\n)\\nThis works fine, but a simpler option is to use a\\nTransformedTargetRegressor\\n. \\nWe just need to construct it, giving it the\\nregression model and the label transformer, then fit it on the training set,\\nusing the original unscaled labels. It will automatically use the transformer to\\nscale the labels and train the regression model on the resulting scaled labels,\\njust like we did previously. Then, when we want to make a prediction, it will\\ncall the regression model’s \\npredict()\\n method and use the scaler’s\\ninverse_transform()\\n method to produce the prediction:\\nfrom\\n \\nsklearn.compose\\n \\nimport\\n \\nTransformedTargetRegressor\\nmodel\\n \\n=\\n \\nTransformedTargetRegressor\\n(\\nLinearRegression\\n(),\\n                                   \\ntransformer\\n=\\nStandardScaler\\n())\\nmodel\\n.\\nfit\\n(\\nhousing\\n[[\\n\"median_income\"\\n]],\\n \\nhousing_labels\\n)\\npredictions\\n \\n=\\n \\nmodel\\n.\\npredict\\n(\\nsome_new_data\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 138, 'page_label': '139', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Custom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to\\nwrite your own for tasks such as custom transformations, cleanup operations,\\nor combining specific attributes.\\nFor transformations that don’t require any training, you can just write a\\nfunction that takes a NumPy array as input and outputs the transformed array.\\nFor example, as discussed in the previous section, it’s often a good idea to\\ntransform features with heavy-tailed distributions by replacing them with\\ntheir logarithm (assuming the feature is positive and the tail is on the right).\\nLet’s create a log-transformer and apply it to the \\npopulation\\n feature:\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nFunctionTransformer\\nlog_transformer\\n \\n=\\n \\nFunctionTransformer\\n(\\nnp\\n.\\nlog\\n,\\n \\ninverse_func\\n=\\nnp\\n.\\nexp\\n)\\nlog_pop\\n \\n=\\n \\nlog_transformer\\n.\\ntransform\\n(\\nhousing\\n[[\\n\"population\"\\n]])\\nThe \\ninverse_func\\n argument is optional. It lets you specify an inverse\\ntransform function, e.g., if you plan to use your transformer in a\\nTransformedTargetRegressor\\n.\\nYour transformation function can take hyperparameters as additional\\narguments. For example, here’s how to create a transformer that computes the\\nsame Gaussian RBF similarity measure as earlier:\\nrbf_transformer\\n \\n=\\n \\nFunctionTransformer\\n(\\nrbf_kernel\\n,\\n                                      \\nkw_args\\n=\\ndict\\n(\\nY\\n=\\n[[\\n35.\\n]],\\n \\ngamma\\n=\\n0.1\\n))\\nage_simil_35\\n \\n=\\n \\nrbf_transformer\\n.\\ntransform\\n(\\nhousing\\n[[\\n\"housing_median_age\"\\n]])\\nNote that there’s no inverse function for the RBF kernel, since there are\\nalways two values at a given distance from a fixed point (except at distance\\n0). Also note that \\nrbf_kernel()\\n does not treat the features separately. If you\\npass it an array with two features, it will measure the 2D distance (Euclidean)\\nto measure similarity. For example, here’s how to add a feature that will\\nmeasure the geographic similarity between each district and San Francisco:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 139, 'page_label': '140', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='sf_coords\\n \\n=\\n \\n37.7749\\n,\\n \\n-\\n122.41\\nsf_transformer\\n \\n=\\n \\nFunctionTransformer\\n(\\nrbf_kernel\\n,\\n                                     \\nkw_args\\n=\\ndict\\n(\\nY\\n=\\n[\\nsf_coords\\n],\\n \\ngamma\\n=\\n0.1\\n))\\nsf_simil\\n \\n=\\n \\nsf_transformer\\n.\\ntransform\\n(\\nhousing\\n[[\\n\"latitude\"\\n,\\n \\n\"longitude\"\\n]])\\nCustom transformers are also useful to combine features. For example, here’s\\na \\nFunctionTransformer\\n that computes the ratio between the input features 0\\nand 1:\\n>>> \\nratio_transformer\\n \\n=\\n \\nFunctionTransformer\\n(\\nlambda\\n \\nX\\n:\\n \\nX\\n[:,\\n \\n[\\n0\\n]]\\n \\n/\\n \\nX\\n[:,\\n \\n[\\n1\\n]])\\n>>> \\nratio_transformer\\n.\\ntransform\\n(\\nnp\\n.\\narray\\n([[\\n1.\\n,\\n \\n2.\\n],\\n \\n[\\n3.\\n,\\n \\n4.\\n]]))\\narray([[0.5 ],\\n       [0.75]])\\nFunctionTransformer\\n is very handy, but what if you would like your\\ntransformer to be trainable, learning some parameters in the \\nfit()\\n method and\\nusing them later in the \\ntransform()\\n method? For this, you need to write a\\ncustom class. \\nScikit-Learn relies on duck typing, so this class does not have\\nto inherit from any particular base class. All it needs is three methods: \\nfit()\\n(which must return \\nself\\n), \\ntransform()\\n, and \\nfit_transform()\\n.\\nYou can get \\nfit_transform()\\n for free by simply adding \\nTransformerMixin\\n as a\\nbase class: the default implementation will just call \\nfit()\\n and then \\ntransform()\\n.\\nIf you add \\nBaseEstimator\\n as a base class (and avoid using \\n*args\\n and\\n**kwargs\\n in your constructor), you will also get two extra methods:\\nget_params()\\n and \\nset_params()\\n. These will be useful for automatic\\nhyperparameter tuning.\\nFor example, here’s a custom transformer that acts much like the\\nStandardScaler\\n:\\nfrom\\n \\nsklearn.base\\n \\nimport\\n \\nBaseEstimator\\n,\\n \\nTransformerMixin\\nfrom\\n \\nsklearn.utils.validation\\n \\nimport\\n \\ncheck_array\\n,\\n \\ncheck_is_fitted\\nclass\\n \\nStandardScalerClone\\n(\\nBaseEstimator\\n,\\n \\nTransformerMixin\\n):\\n    \\ndef\\n \\n__init__\\n(\\nself\\n,\\n \\nwith_mean\\n=\\nTrue\\n):\\n  \\n# no *args or **kwargs!\\n        \\nself\\n.\\nwith_mean\\n \\n=\\n \\nwith_mean\\n    \\ndef\\n \\nfit\\n(\\nself\\n,\\n \\nX\\n,\\n \\ny\\n=\\nNone\\n):\\n  \\n# y is required even though we don\\'t use it\\n        \\nX\\n \\n=\\n \\ncheck_array\\n(\\nX\\n)\\n  \\n# checks that X is an array with finite float values'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 140, 'page_label': '141', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='self\\n.\\nmean_\\n \\n=\\n \\nX\\n.\\nmean\\n(\\naxis\\n=\\n0\\n)\\n        \\nself\\n.\\nscale_\\n \\n=\\n \\nX\\n.\\nstd\\n(\\naxis\\n=\\n0\\n)\\n        \\nself\\n.\\nn_features_in_\\n \\n=\\n \\nX\\n.\\nshape\\n[\\n1\\n]\\n  \\n# every estimator stores this in fit()\\n        \\nreturn\\n \\nself\\n  \\n# always return self!\\n    \\ndef\\n \\ntransform\\n(\\nself\\n,\\n \\nX\\n):\\n        \\ncheck_is_fitted\\n(\\nself\\n)\\n  \\n# looks for learned attributes (with trailing _)\\n        \\nX\\n \\n=\\n \\ncheck_array\\n(\\nX\\n)\\n        \\nassert\\n \\nself\\n.\\nn_features_in_\\n \\n==\\n \\nX\\n.\\nshape\\n[\\n1\\n]\\n        \\nif\\n \\nself\\n.\\nwith_mean\\n:\\n            \\nX\\n \\n=\\n \\nX\\n \\n-\\n \\nself\\n.\\nmean_\\n        \\nreturn\\n \\nX\\n \\n/\\n \\nself\\n.\\nscale_\\nHere are a few things to note:\\nThe \\nsklearn.utils.validation\\n package contains several functions we can\\nuse to validate the inputs. For simplicity, we will skip such tests in the\\nrest of this book, but production code should have them.\\nScikit-Learn pipelines require the \\nfit()\\n method to have two arguments \\nX\\nand \\ny\\n, which is why we need the \\ny=None\\n argument even though we\\ndon’t use \\ny\\n.\\nAll Scikit-Learn estimators set \\nn_features_in_\\n in the \\nfit()\\n method, and\\nthey ensure that the data passed to \\ntransform()\\n or \\npredict()\\n has this\\nnumber of features.\\nThe \\nfit()\\n method must return \\nself\\n.\\nThis implementation is not 100% complete: all estimators should set\\nfeature_names_in_\\n in the \\nfit()\\n method when they are passed a\\nDataFrame. \\nMoreover, all transformers should provide a\\nget_feature_names_out()\\n method, as well as an \\ninverse_transform()\\nmethod when their transformation can be reversed. See the last exercise\\nat the end of this chapter for more details.\\nA custom transformer can (and often does) use other estimators in its\\nimplementation. For example, the following code demonstrates custom\\ntransformer that uses a \\nKMeans\\n clusterer in the \\nfit()\\n method to identify the\\nmain clusters in the training data, and then uses \\nrbf_kernel()\\n in the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 141, 'page_label': '142', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='transform()\\n method to measure how similar each sample is to each cluster\\ncenter:\\nfrom\\n \\nsklearn.cluster\\n \\nimport\\n \\nKMeans\\nclass\\n \\nClusterSimilarity\\n(\\nBaseEstimator\\n,\\n \\nTransformerMixin\\n):\\n    \\ndef\\n \\n__init__\\n(\\nself\\n,\\n \\nn_clusters\\n=\\n10\\n,\\n \\ngamma\\n=\\n1.0\\n,\\n \\nrandom_state\\n=\\nNone\\n):\\n        \\nself\\n.\\nn_clusters\\n \\n=\\n \\nn_clusters\\n        \\nself\\n.\\ngamma\\n \\n=\\n \\ngamma\\n        \\nself\\n.\\nrandom_state\\n \\n=\\n \\nrandom_state\\n    \\ndef\\n \\nfit\\n(\\nself\\n,\\n \\nX\\n,\\n \\ny\\n=\\nNone\\n,\\n \\nsample_weight\\n=\\nNone\\n):\\n        \\nself\\n.\\nkmeans_\\n \\n=\\n \\nKMeans\\n(\\nself\\n.\\nn_clusters\\n,\\n \\nrandom_state\\n=\\nself\\n.\\nrandom_state\\n)\\n        \\nself\\n.\\nkmeans_\\n.\\nfit\\n(\\nX\\n,\\n \\nsample_weight\\n=\\nsample_weight\\n)\\n        \\nreturn\\n \\nself\\n  \\n# always return self!\\n    \\ndef\\n \\ntransform\\n(\\nself\\n,\\n \\nX\\n):\\n        \\nreturn\\n \\nrbf_kernel\\n(\\nX\\n,\\n \\nself\\n.\\nkmeans_\\n.\\ncluster_centers_\\n,\\n \\ngamma\\n=\\nself\\n.\\ngamma\\n)\\n    \\ndef\\n \\nget_feature_names_out\\n(\\nself\\n,\\n \\nnames\\n=\\nNone\\n):\\n        \\nreturn\\n \\n[\\nf\\n\"Cluster \\n{\\ni\\n}\\n similarity\"\\n \\nfor\\n \\ni\\n \\nin\\n \\nrange\\n(\\nself\\n.\\nn_clusters\\n)]\\nTIP\\nYou can check whether your custom estimator respects Scikit-Learn’s API by passing an\\ninstance to \\ncheck_estimator()\\n from the \\nsklearn.utils.estimator_checks\\n package. For the full\\nAPI, check out \\nhttps://scikit-learn.org/stable/developers\\n.\\nAs you will see in \\nChapter 9\\n, \\nk\\n-means is a clustering algorithm that locates\\nclusters in the data. How many it searches for is controlled by the \\nn_clusters\\nhyperparameter. After training, the cluster centers are available via the\\ncluster_centers_\\n attribute. The \\nfit()\\n method of \\nKMeans\\n supports an optional\\nargument \\nsample_weight\\n, which lets the user specify the relative weights of\\nthe samples. \\nk\\n-means is a stochastic algorithm, meaning that it relies on\\nrandomness to locate the clusters, so if you want reproducible results, you\\nmust set the \\nrandom_state\\n parameter. As you can see, despite the complexity\\nof the task, the code is fairly straightforward. Now let’s use this custom\\ntransformer:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 142, 'page_label': '143', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='cluster_simil\\n \\n=\\n \\nClusterSimilarity\\n(\\nn_clusters\\n=\\n10\\n,\\n \\ngamma\\n=\\n1.\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nsimilarities\\n \\n=\\n \\ncluster_simil\\n.\\nfit_transform\\n(\\nhousing\\n[[\\n\"latitude\"\\n,\\n \\n\"longitude\"\\n]],\\n                                           \\nsample_weight\\n=\\nhousing_labels\\n)\\nThis code creates a \\nClusterSimilarity\\n transformer, setting the number of\\nclusters to 10. Then it calls \\nfit_transform()\\n with the latitude and longitude of\\nevery district in the training set, weighting each district by its median house\\nvalue. The transformer uses \\nk\\n-means to locate the clusters, then measures the\\nGaussian RBF similarity between each district and all 10 cluster centers. The\\nresult is a matrix with one row per district, and one column per cluster. Let’s\\nlook at the first three rows, rounding to two decimal places:\\n>>> \\nsimilarities\\n[:\\n3\\n]\\n.\\nround\\n(\\n2\\n)\\narray([[0.  , 0.14, 0.  , 0.  , 0.  , 0.08, 0.  , 0.99, 0.  , 0.6 ],\\n       [0.63, 0.  , 0.99, 0.  , 0.  , 0.  , 0.04, 0.  , 0.11, 0.  ],\\n       [0.  , 0.29, 0.  , 0.  , 0.01, 0.44, 0.  , 0.7 , 0.  , 0.3 ]])\\nFigure 2-19\\n shows the 10 cluster centers found by \\nk\\n-means. The districts are\\ncolored according to their geographic similarity to their closest cluster center.\\nAs you can see, most clusters are located in highly populated and expensive\\nareas.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 143, 'page_label': '144', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 2-19. \\nGaussian RBF similarity to the nearest cluster center'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 144, 'page_label': '145', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Transformation Pipelines\\nAs you can see, there are many data transformation steps that need to be\\nexecuted in the right order. Fortunately, Scikit-Learn provides the \\nPipeline\\nclass to help with such sequences of transformations. Here is a small pipeline\\nfor numerical attributes, which will first impute then scale the input features:\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nPipeline\\nnum_pipeline\\n \\n=\\n \\nPipeline\\n([\\n    \\n(\\n\"impute\"\\n,\\n \\nSimpleImputer\\n(\\nstrategy\\n=\\n\"median\"\\n)),\\n    \\n(\\n\"standardize\"\\n,\\n \\nStandardScaler\\n()),\\n])\\nThe \\nPipeline\\n constructor takes a list of name/estimator pairs (2-tuples)\\ndefining a sequence of steps. The names can be anything you like, as long as\\nthey are unique and don’t contain double underscores (\\n__\\n). They will be\\nuseful later, when we discuss hyperparameter tuning. \\nThe estimators must all\\nbe transformers (i.e., they must have a \\nfit_transform()\\n method), except for the\\nlast one, which can be anything: a transformer, a predictor, or any other type\\nof estimator.\\nTIP\\nIn a Jupyter notebook, if you \\nimport\\n \\nsklearn\\n and run \\nsklearn.\\nset_config(display=\"diagram\")\\n, all Scikit-Learn estimators will be rendered as interactive\\ndiagrams. This is particularly useful for visualizing pipelines. To visualize \\nnum_pipeline\\n,\\nrun a cell with \\nnum_pipeline\\n as the last line. Clicking an estimator will show more details.\\nIf you don’t want to name the transformers, you can use the \\nmake_pipeline()\\nfunction instead; it takes transformers as positional arguments and creates a\\nPipeline\\n using the names of the transformers’ classes, in lowercase and\\nwithout underscores (e.g., \\n\"simpleimputer\"\\n):\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nmake_pipeline'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 145, 'page_label': '146', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='num_pipeline\\n \\n=\\n \\nmake_pipeline\\n(\\nSimpleImputer\\n(\\nstrategy\\n=\\n\"median\"\\n),\\n \\nStandardScaler\\n())\\nIf multiple transformers have the same name, an index is appended to their\\nnames (e.g., \\n\"foo-1\"\\n, \\n\"foo-2\"\\n, etc.).\\nWhen you call the pipeline’s \\nfit()\\n method, it calls \\nfit_transform()\\n sequentially\\non all the transformers, passing the output of each call as the parameter to the\\nnext call until it reaches the final estimator, for which it just calls the \\nfit()\\nmethod.\\nThe pipeline exposes the same methods as the final estimator. In this example\\nthe last estimator is a \\nStandardScaler\\n, which is a transformer, so the pipeline\\nalso acts like a transformer. If you call the pipeline’s \\ntransform()\\n method, it\\nwill sequentially apply all the transformations to the data. If the last estimator\\nwere a predictor instead of a transformer, then the pipeline would have a\\npredict()\\n method rather than a \\ntransform()\\n method. \\nCalling it would\\nsequentially apply all the transformations to the data and pass the result to the\\npredictor’s \\npredict()\\n method.\\nLet’s call the pipeline’s \\nfit_transform()\\n method and look at the output’s first\\ntwo rows, rounded to two decimal places:\\n>>> \\nhousing_num_prepared\\n \\n=\\n \\nnum_pipeline\\n.\\nfit_transform\\n(\\nhousing_num\\n)\\n>>> \\nhousing_num_prepared\\n[:\\n2\\n]\\n.\\nround\\n(\\n2\\n)\\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\\n       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])\\nAs you saw earlier, if you want to recover a nice DataFrame, you can use the\\npipeline’s \\nget_feature_names_out()\\n method:\\ndf_housing_num_prepared\\n \\n=\\n \\npd\\n.\\nDataFrame\\n(\\n    \\nhousing_num_prepared\\n,\\n \\ncolumns\\n=\\nnum_pipeline\\n.\\nget_feature_names_out\\n(),\\n    \\nindex\\n=\\nhousing_num\\n.\\nindex\\n)\\nPipelines support indexing; for example, \\npipeline[1]\\n returns the second\\nestimator in the pipeline, and \\npipeline[:-1]\\n returns a \\nPipeline\\n object\\ncontaining all but the last estimator. You can also access the estimators via\\nthe \\nsteps\\n attribute, which is a list of name/estimator pairs, or via the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 146, 'page_label': '147', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='named_steps\\n dictionary attribute, which maps the names to the estimators.\\nFor example, \\nnum_pipeline[\"simpleimputer\"]\\n returns the estimator named\\n\"simpleimputer\"\\n.\\nSo far, we have handled the categorical columns and the numerical columns\\nseparately. It would be more convenient to have a single transformer capable\\nof handling all columns, applying the appropriate transformations to each\\ncolumn. For this, you can use a \\nColumnTransformer\\n. For example, the\\nfollowing \\nColumnTransformer\\n will apply \\nnum_pipeline\\n (the one we just\\ndefined) to the numerical attributes and \\ncat_pipeline\\n to the categorical\\nattribute:\\nfrom\\n \\nsklearn.compose\\n \\nimport\\n \\nColumnTransformer\\nnum_attribs\\n \\n=\\n \\n[\\n\"longitude\"\\n,\\n \\n\"latitude\"\\n,\\n \\n\"housing_median_age\"\\n,\\n \\n\"total_rooms\"\\n,\\n               \\n\"total_bedrooms\"\\n,\\n \\n\"population\"\\n,\\n \\n\"households\"\\n,\\n \\n\"median_income\"\\n]\\ncat_attribs\\n \\n=\\n \\n[\\n\"ocean_proximity\"\\n]\\ncat_pipeline\\n \\n=\\n \\nmake_pipeline\\n(\\n    \\nSimpleImputer\\n(\\nstrategy\\n=\\n\"most_frequent\"\\n),\\n    \\nOneHotEncoder\\n(\\nhandle_unknown\\n=\\n\"ignore\"\\n))\\npreprocessing\\n \\n=\\n \\nColumnTransformer\\n([\\n    \\n(\\n\"num\"\\n,\\n \\nnum_pipeline\\n,\\n \\nnum_attribs\\n),\\n    \\n(\\n\"cat\"\\n,\\n \\ncat_pipeline\\n,\\n \\ncat_attribs\\n),\\n])\\nFirst we import the \\nColumnTransformer\\n class, then we define the list of\\nnumerical and categorical column names and construct a simple pipeline for\\ncategorical attributes. Lastly, we construct a \\nColumnTransformer\\n. Its\\nconstructor requires a list of triplets (3-tuples), each containing a name\\n(which must be unique and not contain double underscores), a transformer,\\nand a list of names (or indices) of columns that the transformer should be\\napplied to.\\nTIP\\nInstead of using a transformer, you can specify the string \\n\"drop\"\\n if you want the columns\\nto be dropped, or you can specify \\n\"passthrough\"\\n if you want the columns to be left'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 147, 'page_label': '148', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='untouched. By default, the remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the \\nremainder\\n hyperparameter to any transformer (or to\\n\"passthrough\"\\n) if you want these columns to be handled differently.\\nSince listing all the column names is not very convenient, Scikit-Learn\\nprovides a \\nmake_column_selector()\\n function that returns a selector function\\nyou can use to automatically select all the features of a given type, such as\\nnumerical or categorical. You can pass this selector function to the\\nColumnTransformer\\n instead of column names or indices. \\nMoreover, if you\\ndon’t care about naming the transformers, you can use\\nmake_column_transformer()\\n, which chooses the names for you, just like\\nmake_pipeline()\\n does. For example, the following code creates the same\\nColumnTransformer\\n as earlier, except the transformers are automatically\\nnamed \\n\"pipeline-1\"\\n and \\n\"pipeline-2\"\\n instead of \\n\"num\"\\n and \\n\"cat\"\\n:\\nfrom\\n \\nsklearn.compose\\n \\nimport\\n \\nmake_column_selector\\n,\\n \\nmake_column_transformer\\npreprocessing\\n \\n=\\n \\nmake_column_transformer\\n(\\n    \\n(\\nnum_pipeline\\n,\\n \\nmake_column_selector\\n(\\ndtype_include\\n=\\nnp\\n.\\nnumber\\n)),\\n    \\n(\\ncat_pipeline\\n,\\n \\nmake_column_selector\\n(\\ndtype_include\\n=\\nobject\\n)),\\n)\\nNow we’re ready to apply this \\nColumnTransformer\\n to the housing data:\\nhousing_prepared\\n \\n=\\n \\npreprocessing\\n.\\nfit_transform\\n(\\nhousing\\n)\\nGreat! We have a preprocessing pipeline that takes the entire training dataset\\nand applies each transformer to the appropriate columns, then concatenates\\nthe transformed columns horizontally (transformers must never change the\\nnumber of rows). Once again this returns a NumPy array, but you can get the\\ncolumn names using \\npreprocessing.get_feature_names_out()\\n and wrap the\\ndata in a nice DataFrame as we did before.\\nNOTE\\nThe \\nOneHotEncoder\\n returns a sparse matrix and the \\nnum_pipeline\\n returns a dense matrix.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 148, 'page_label': '149', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='When there is such a mix of sparse and dense matrices, the \\nColumnTransformer\\n estimates\\nthe density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse\\nmatrix if the density is lower than a given threshold (by default, \\nsparse_threshold=0.3\\n). In\\nthis example, it returns a dense matrix.\\nYour project is going really well and you’re almost ready to train some\\nmodels! You now want to create a single pipeline that will perform all the\\ntransformations you’ve experimented with up to now. Let’s recap what the\\npipeline will do and why:\\nMissing values in numerical features will be imputed by replacing them\\nwith the median, as most ML algorithms don’t expect missing values. In\\ncategorical features, missing values will be replaced by the most\\nfrequent category.\\nThe categorical feature will be one-hot encoded, as most ML algorithms\\nonly accept numerical inputs.\\nA few ratio features will be computed and added: \\nbedrooms_ratio\\n,\\nrooms_per_house\\n, and \\npeople_per_house\\n. Hopefully these will better\\ncorrelate with the median house value, and thereby help the ML models.\\nA few cluster similarity features will also be added. These will likely be\\nmore useful to the model than latitude and longitude.\\nFeatures with a long tail will be replaced by their logarithm, as most\\nmodels prefer features with roughly uniform or Gaussian distributions.\\nAll numerical features will be standardized, as most ML algorithms\\nprefer when all features have roughly the same scale.\\nThe code that builds the pipeline to do all of this should look familiar to you\\nby now:\\ndef\\n \\ncolumn_ratio\\n(\\nX\\n):\\n    \\nreturn\\n \\nX\\n[:,\\n \\n[\\n0\\n]]\\n \\n/\\n \\nX\\n[:,\\n \\n[\\n1\\n]]\\ndef\\n \\nratio_name\\n(\\nfunction_transformer\\n,\\n \\nfeature_names_in\\n):\\n    \\nreturn\\n \\n[\\n\"ratio\"\\n]\\n  \\n# feature names out'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 149, 'page_label': '150', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='def\\n \\nratio_pipeline\\n():\\n    \\nreturn\\n \\nmake_pipeline\\n(\\n        \\nSimpleImputer\\n(\\nstrategy\\n=\\n\"median\"\\n),\\n        \\nFunctionTransformer\\n(\\ncolumn_ratio\\n,\\n \\nfeature_names_out\\n=\\nratio_name\\n),\\n        \\nStandardScaler\\n())\\nlog_pipeline\\n \\n=\\n \\nmake_pipeline\\n(\\n    \\nSimpleImputer\\n(\\nstrategy\\n=\\n\"median\"\\n),\\n    \\nFunctionTransformer\\n(\\nnp\\n.\\nlog\\n,\\n \\nfeature_names_out\\n=\\n\"one-to-one\"\\n),\\n    \\nStandardScaler\\n())\\ncluster_simil\\n \\n=\\n \\nClusterSimilarity\\n(\\nn_clusters\\n=\\n10\\n,\\n \\ngamma\\n=\\n1.\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ndefault_num_pipeline\\n \\n=\\n \\nmake_pipeline\\n(\\nSimpleImputer\\n(\\nstrategy\\n=\\n\"median\"\\n),\\n                                     \\nStandardScaler\\n())\\npreprocessing\\n \\n=\\n \\nColumnTransformer\\n([\\n        \\n(\\n\"bedrooms\"\\n,\\n \\nratio_pipeline\\n(),\\n \\n[\\n\"total_bedrooms\"\\n,\\n \\n\"total_rooms\"\\n]),\\n        \\n(\\n\"rooms_per_house\"\\n,\\n \\nratio_pipeline\\n(),\\n \\n[\\n\"total_rooms\"\\n,\\n \\n\"households\"\\n]),\\n        \\n(\\n\"people_per_house\"\\n,\\n \\nratio_pipeline\\n(),\\n \\n[\\n\"population\"\\n,\\n \\n\"households\"\\n]),\\n        \\n(\\n\"log\"\\n,\\n \\nlog_pipeline\\n,\\n \\n[\\n\"total_bedrooms\"\\n,\\n \\n\"total_rooms\"\\n,\\n \\n\"population\"\\n,\\n                               \\n\"households\"\\n,\\n \\n\"median_income\"\\n]),\\n        \\n(\\n\"geo\"\\n,\\n \\ncluster_simil\\n,\\n \\n[\\n\"latitude\"\\n,\\n \\n\"longitude\"\\n]),\\n        \\n(\\n\"cat\"\\n,\\n \\ncat_pipeline\\n,\\n \\nmake_column_selector\\n(\\ndtype_include\\n=\\nobject\\n)),\\n    \\n],\\n    \\nremainder\\n=\\ndefault_num_pipeline\\n)\\n  \\n# one column remaining: housing_median_age\\nIf you run this \\nColumnTransformer\\n, it performs all the transformations and\\noutputs a NumPy array with 24 features:\\n>>> \\nhousing_prepared\\n \\n=\\n \\npreprocessing\\n.\\nfit_transform\\n(\\nhousing\\n)\\n>>> \\nhousing_prepared\\n.\\nshape\\n(16512, 24)\\n>>> \\npreprocessing\\n.\\nget_feature_names_out\\n()\\narray([\\'bedrooms__ratio\\', \\'rooms_per_house__ratio\\',\\n       \\'people_per_house__ratio\\', \\'log__total_bedrooms\\',\\n       \\'log__total_rooms\\', \\'log__population\\', \\'log__households\\',\\n       \\'log__median_income\\', \\'geo__Cluster 0 similarity\\', [...],\\n       \\'geo__Cluster 9 similarity\\', \\'cat__ocean_proximity_<1H OCEAN\\',\\n       \\'cat__ocean_proximity_INLAND\\', \\'cat__ocean_proximity_ISLAND\\',\\n       \\'cat__ocean_proximity_NEAR BAY\\', \\'cat__ocean_proximity_NEAR OCEAN\\',\\n       \\'remainder__housing_median_age\\'], dtype=object)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 150, 'page_label': '151', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Select and Train a Model\\nAt last! You framed the problem, you got the data and explored it, you\\nsampled a training set and a test set, and you wrote a preprocessing pipeline\\nto automatically clean up and prepare your data for machine learning\\nalgorithms. You are now ready to select and train a machine learning model.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 151, 'page_label': '152', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Train and Evaluate on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going\\nto be easy! You decide to train a very basic linear regression model to get\\nstarted:\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLinearRegression\\nlin_reg\\n \\n=\\n \\nmake_pipeline\\n(\\npreprocessing\\n,\\n \\nLinearRegression\\n())\\nlin_reg\\n.\\nfit\\n(\\nhousing\\n,\\n \\nhousing_labels\\n)\\nDone! You now have a working linear regression model. You try it out on the\\ntraining set, looking at the first five predictions and comparing them to the\\nlabels:\\n>>> \\nhousing_predictions\\n \\n=\\n \\nlin_reg\\n.\\npredict\\n(\\nhousing\\n)\\n>>> \\nhousing_predictions\\n[:\\n5\\n]\\n.\\nround\\n(\\n-\\n2\\n)\\n  \\n# -2 = rounded to the nearest hundred\\narray([243700., 372400., 128800.,  94400., 328300.])\\n>>> \\nhousing_labels\\n.\\niloc\\n[:\\n5\\n]\\n.\\nvalues\\narray([458300., 483800., 101700.,  96100., 361800.])\\nWell, it works, but not always: the first prediction is way off (by over\\n$200,000!), while the other predictions are better: two are off by about 25%,\\nand two are off by less than 10%. \\nRemember that you chose to use the RMSE\\nas your performance measure, so you want to measure this regression\\nmodel’s RMSE on the whole training set using Scikit-Learn’s\\nmean_squared_error()\\n function, with the \\nsquared\\n argument set to \\nFalse\\n:\\n>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nmean_squared_error\\n>>> \\nlin_rmse\\n \\n=\\n \\nmean_squared_error\\n(\\nhousing_labels\\n,\\n \\nhousing_predictions\\n,\\n... \\n                              \\nsquared\\n=\\nFalse\\n)\\n...\\n>>> \\nlin_rmse\\n68687.89176589991\\nThis is better than nothing, but clearly not a great score: the\\nmedian_housing_values\\n of most districts range between $120,000 and\\n$265,000, so a typical prediction error of $68,628 is really not very'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 152, 'page_label': '153', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='satisfying. \\nThis is an example of a model underfitting the training data. When\\nthis happens it can mean that the features do not provide enough \\ninformation\\nto make good predictions, or that the model is not powerful enough. As we\\nsaw in the previous chapter, the main ways to fix underfitting are to select a\\nmore powerful model, to feed the training algorithm with better features, or to\\nreduce the constraints on the model. This model is not regularized, which\\nrules out the last option. You could try to add more features, but first you\\nwant to try a more complex model to see how it does.\\nYou decide to try a \\nDecisionTreeRegressor\\n, as this is a fairly powerful model\\ncapable of finding complex nonlinear relationships in the data (decision trees\\nare presented in more detail in \\nChapter 6\\n):\\nfrom\\n \\nsklearn.tree\\n \\nimport\\n \\nDecisionTreeRegressor\\ntree_reg\\n \\n=\\n \\nmake_pipeline\\n(\\npreprocessing\\n,\\n \\nDecisionTreeRegressor\\n(\\nrandom_state\\n=\\n42\\n))\\ntree_reg\\n.\\nfit\\n(\\nhousing\\n,\\n \\nhousing_labels\\n)\\nNow that the model is trained, you evaluate it on the training set:\\n>>> \\nhousing_predictions\\n \\n=\\n \\ntree_reg\\n.\\npredict\\n(\\nhousing\\n)\\n>>> \\ntree_rmse\\n \\n=\\n \\nmean_squared_error\\n(\\nhousing_labels\\n,\\n \\nhousing_predictions\\n,\\n... \\n                               \\nsquared\\n=\\nFalse\\n)\\n...\\n>>> \\ntree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect?\\nOf course, it is much more likely that the model has badly overfit the data.\\nHow can you be sure? As you saw earlier, you don’t want to touch the test set\\nuntil you are ready to launch a model you are confident about, so you need to\\nuse part of the training set for training and part of it for model validation.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 153, 'page_label': '154', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Better Evaluation Using Cross-Validation\\nOne way to evaluate the decision tree model would be to use the \\ntrain_\\ntest_split()\\n function to split the training set into a smaller training set and a\\nvalidation set, then train your models against the smaller training set and\\nevaluate them against the validation set. It’s a bit of effort, but nothing too\\ndifficult, and it would work fairly well.\\nA great alternative is to use Scikit-Learn’s \\nk_-fold cross-validation\\n feature.\\nThe following code randomly splits the training set into 10 nonoverlapping\\nsubsets called \\nfolds\\n, then it trains and evaluates the decision tree model 10\\ntimes, picking a different fold for evaluation every time and using the other 9\\nfolds for training. The result is an array containing the 10 evaluation scores:\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\ncross_val_score\\ntree_rmses\\n \\n=\\n \\n-\\ncross_val_score\\n(\\ntree_reg\\n,\\n \\nhousing\\n,\\n \\nhousing_labels\\n,\\n                              \\nscoring\\n=\\n\"neg_root_mean_squared_error\"\\n,\\n \\ncv\\n=\\n10\\n)\\nWARNING\\nScikit-Learn’s cross-validation features expect a utility function (greater is better) rather\\nthan a cost function (lower is better), so the scoring function is actually the opposite of the\\nRMSE. It’s a negative value, so you need to switch the sign of the output to get the RMSE\\nscores.\\nLet’s look at the results:\\n>>> \\npd\\n.\\nSeries\\n(\\ntree_rmses\\n)\\n.\\ndescribe\\n()\\ncount       10.000000\\nmean     66868.027288\\nstd       2060.966425\\nmin      63649.536493\\n25%      65338.078316\\n50%      66801.953094\\n75%      68229.934454\\nmax      70094.778246\\ndtype: float64'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 154, 'page_label': '155', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Now the decision tree doesn’t look as good as it did earlier. In fact, it seems\\nto perform almost as poorly as the linear regression model! Notice that cross-\\nvalidation allows you to get not only an estimate of the performance of your\\nmodel, but also a measure of how precise this estimate is (i.e., its standard\\ndeviation). The decision tree has an RMSE of about 66,868, with a standard\\ndeviation of about 2,061. You would not have this information if you just\\nused one validation set. But cross-validation comes at the cost of training the\\nmodel several times, so it is not always feasible.\\nIf you compute the same metric for the linear regression model, you will find\\nthat the mean RMSE is 69,858 and the standard deviation is 4,182. So the\\ndecision tree model seems to perform very slightly better than the linear\\nmodel, but the difference is minimal due to severe overfitting. We know\\nthere’s an overfitting problem because the training error is low (actually zero)\\nwhile the validation error is high.\\nLet’s try one last model now: \\nthe \\nRandomForestRegressor\\n. As you will see in\\nChapter 7\\n, random forests work by training many decision trees on random\\nsubsets of the features, then averaging out their predictions. \\nSuch models\\ncomposed of many other models are called \\nensembles\\n: they are capable of\\nboosting the performance of the underlying model (in this case, decision\\ntrees). The code is much the same as earlier:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nRandomForestRegressor\\nforest_reg\\n \\n=\\n \\nmake_pipeline\\n(\\npreprocessing\\n,\\n                           \\nRandomForestRegressor\\n(\\nrandom_state\\n=\\n42\\n))\\nforest_rmses\\n \\n=\\n \\n-\\ncross_val_score\\n(\\nforest_reg\\n,\\n \\nhousing\\n,\\n \\nhousing_labels\\n,\\n                                \\nscoring\\n=\\n\"neg_root_mean_squared_error\"\\n,\\n \\ncv\\n=\\n10\\n)\\nLet’s look at the scores:\\n>>> \\npd\\n.\\nSeries\\n(\\nforest_rmses\\n)\\n.\\ndescribe\\n()\\ncount       10.000000\\nmean     47019.561281\\nstd       1033.957120\\nmin      45458.112527\\n25%      46464.031184\\n50%      46967.596354'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 155, 'page_label': '156', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='75%      47325.694987\\nmax      49243.765795\\ndtype: float64\\nWow, this is much better: random forests really look very promising for this\\ntask! However, if you train a \\nRandomForest\\n and measure the RMSE on the\\ntraining set, you will find roughly 17,474: that’s much lower, meaning that\\nthere’s still quite a lot of overfitting going on. Possible solutions are to\\nsimplify the model, constrain it (i.e., regularize it), or get a lot more training\\ndata. Before you dive much deeper into random forests, however, you should\\ntry out many other models from various categories of machine learning\\nalgorithms (e.g., several support vector machines with different kernels, and\\npossibly a neural network), without spending too much time tweaking the\\nhyperparameters. The goal is to shortlist a few (two to five) promising\\nmodels.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 156, 'page_label': '157', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Fine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. \\nYou now\\nneed to fine-tune them. Let’s look at a few ways you can do that.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 157, 'page_label': '158', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Grid Search\\nOne option would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. \\nThis would be very\\ntedious work, and you may not have time to explore many combinations.\\nInstead, you can use Scikit-Learn’s \\nGridSearchCV\\n class to search for you.\\nAll you need to do is tell it which hyperparameters you want it to experiment\\nwith and what values to try out, and it will use cross-validation to evaluate all\\nthe possible \\ncombinations\\n of hyperparameter values. For example, the\\nfollowing code searches for the best combination of hyperparameter values\\nfor the \\nRandomForestRegressor\\n:\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\nGridSearchCV\\nfull_pipeline\\n \\n=\\n \\nPipeline\\n([\\n    \\n(\\n\"preprocessing\"\\n,\\n \\npreprocessing\\n),\\n    \\n(\\n\"random_forest\"\\n,\\n \\nRandomForestRegressor\\n(\\nrandom_state\\n=\\n42\\n)),\\n])\\nparam_grid\\n \\n=\\n \\n[\\n    \\n{\\n\\'preprocessing__geo__n_clusters\\'\\n:\\n \\n[\\n5\\n,\\n \\n8\\n,\\n \\n10\\n],\\n     \\n\\'random_forest__max_features\\'\\n:\\n \\n[\\n4\\n,\\n \\n6\\n,\\n \\n8\\n]},\\n    \\n{\\n\\'preprocessing__geo__n_clusters\\'\\n:\\n \\n[\\n10\\n,\\n \\n15\\n],\\n     \\n\\'random_forest__max_features\\'\\n:\\n \\n[\\n6\\n,\\n \\n8\\n,\\n \\n10\\n]},\\n]\\ngrid_search\\n \\n=\\n \\nGridSearchCV\\n(\\nfull_pipeline\\n,\\n \\nparam_grid\\n,\\n \\ncv\\n=\\n3\\n,\\n                           \\nscoring\\n=\\n\\'neg_root_mean_squared_error\\'\\n)\\ngrid_search\\n.\\nfit\\n(\\nhousing\\n,\\n \\nhousing_labels\\n)\\nNotice that you can refer to any hyperparameter of any estimator in a\\npipeline, even if this estimator is nested deep inside several pipelines and\\ncolumn transformers. For example, when Scikit-Learn sees\\n\"preprocessing__geo__n_clusters\"\\n, it splits this string at the double\\nunderscores, then it looks for an estimator named \\n\"preprocessing\"\\n in the\\npipeline and finds the preprocessing \\nColumnTransformer\\n. Next, it looks for a\\ntransformer named \\n\"geo\"\\n inside this \\nColumnTransformer\\n and finds the\\nClusterSimilarity\\n transformer we used on the latitude and longitude attributes.\\nThen it finds this transformer’s \\nn_clusters\\n hyperparameter. Similarly,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 158, 'page_label': '159', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='random_forest__max_features\\n refers to the \\nmax_features\\n hyperparameter of\\nthe estimator named \\n\"random_forest\"\\n, which is of course the \\nRandomForest\\nmodel (the \\nmax_features\\n hyperparameter will be explained in \\nChapter 7\\n).\\nTIP\\nWrapping preprocessing steps in a Scikit-Learn pipeline allows you to tune the\\npreprocessing hyperparameters along with the model hyperparameters. This is a good\\nthing since they often interact. For example, perhaps increasing \\nn_clusters\\n requires\\nincreasing \\nmax_features\\n as well. If fitting the pipeline transformers is computationally\\nexpensive, you can set the pipeline’s \\nmemory\\n hyperparameter to the path of a caching\\ndirectory: when you first fit the pipeline, Scikit-Learn will save the fitted transformers to\\nthis directory. If you then fit the pipeline again with the same hyperparameters, Scikit-\\nLearn will just load the cached transformers.\\nThere are two dictionaries in this \\nparam_grid\\n, so \\nGridSearchCV\\n will first\\nevaluate all 3 × 3 = 9 combinations of \\nn_clusters\\n and \\nmax_features\\nhyperparameter values specified in the first \\ndict\\n, then it will try all 2 × 3 = 6\\ncombinations of hyperparameter values in the second \\ndict\\n. So in total the grid\\nsearch will explore 9 + 6 = 15 combinations of hyperparameter values, and it\\nwill train the pipeline 3 times per combination, since we are using 3-fold\\ncross validation. This means there will be a grand total of 15 × 3 = 45 rounds\\nof training! It may take a while, but when it is done you can get the best\\ncombination of parameters like this:\\n>>> \\ngrid_search\\n.\\nbest_params_\\n{\\'preprocessing__geo__n_clusters\\': 15, \\'random_forest__max_features\\': 6}\\nIn this example, the best model is obtained by setting \\nn_clusters\\n to 15 and\\nsetting \\nmax_features\\n to 8.\\nTIP\\nSince 15 is the maximum value that was evaluated for \\nn_clusters\\n, you should probably try\\nsearching again with higher values; the score may continue to improve.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 159, 'page_label': '160', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='You can access the best estimator using \\ngrid_search.best_estimator_\\n. If\\nGridSearchCV\\n is initialized with \\nrefit=True\\n (which is the default), then once\\nit finds the best estimator using cross-validation, it retrains it on the whole\\ntraining set. This is usually a good idea, since feeding it more data will likely\\nimprove its \\nperformance\\n.\\nThe evaluation scores are available using \\ngrid_search.cv_results_\\n. This is a\\ndictionary, but if you wrap it in a DataFrame you get a nice list of all the test\\nscores for each combination of hyperparameters and for each cross-validation\\nsplit, as well as the mean test score across all splits:\\n>>> \\ncv_res\\n \\n=\\n \\npd\\n.\\nDataFrame\\n(\\ngrid_search\\n.\\ncv_results_\\n)\\n>>> \\ncv_res\\n.\\nsort_values\\n(\\nby\\n=\\n\"mean_test_score\"\\n,\\n \\nascending\\n=\\nFalse\\n,\\n \\ninplace\\n=\\nTrue\\n)\\n>>> \\n[\\n...\\n]\\n  \\n# change column names to fit on this page, and show rmse = -score\\n>>> \\ncv_res\\n.\\nhead\\n()\\n  \\n# note: the 1st column is the row ID\\n   n_clusters max_features  split0  split1  split2  mean_test_rmse\\n12         15            6   43460   43919   44748           44042\\n13         15            8   44132   44075   45010           44406\\n14         15           10   44374   44286   45316           44659\\n7          10            6   44683   44655   45657           44999\\n9          10            6   44683   44655   45657           44999\\nThe mean test RMSE score for the best model is 44,042, which is better than\\nthe score you got earlier using the default hyperparameter values (which was\\n47,019). Congratulations, you have successfully fine-tuned your best model!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 160, 'page_label': '161', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"Randomized Search\\nThe grid search\\n approach is fine when you are exploring relatively few\\ncombinations, like in the previous example, but \\nRandomizedSearchCV\\n is\\noften preferable, especially when the hyperparameter search space is large.\\nThis class can be used in much the same way as the \\nGridSearchCV\\n class, but\\ninstead of trying out all possible \\ncombinations\\n it evaluates a fixed number of\\ncombinations, selecting a random value for each hyperparameter at every\\niteration. This may sound surprising, but this approach has several \\nbenefits\\n:\\nIf some of your hyperparameters are continuous (or discrete but with\\nmany possible values), and you let randomized search run for, say, 1,000\\niterations, then it will explore 1,000 different values for each of these\\nhyperparameters, whereas grid search would only explore the few values\\nyou listed for each one.\\nSuppose a hyperparameter does not actually make much difference, but\\nyou don’t know it yet. If it has 10 possible values and you add it to your\\ngrid search, then training will take 10 times longer. But if you add it to a\\nrandom search, it will not make any difference.\\nIf there are 6 hyperparameters to explore, each with 10 possible values,\\nthen grid search offers no other choice than training the model a million\\ntimes, whereas random search can always run for any number of\\niterations you choose.\\nFor each hyperparameter, you must provide either a list of possible values, or\\na probability distribution:\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\nRandomizedSearchCV\\nfrom\\n \\nscipy.stats\\n \\nimport\\n \\nrandint\\nparam_distribs\\n \\n=\\n \\n{\\n'preprocessing__geo__n_clusters'\\n:\\n \\nrandint\\n(\\nlow\\n=\\n3\\n,\\n \\nhigh\\n=\\n50\\n),\\n                  \\n'random_forest__max_features'\\n:\\n \\nrandint\\n(\\nlow\\n=\\n2\\n,\\n \\nhigh\\n=\\n20\\n)}\\nrnd_search\\n \\n=\\n \\nRandomizedSearchCV\\n(\\n    \\nfull_pipeline\\n,\\n \\nparam_distributions\\n=\\nparam_distribs\\n,\\n \\nn_iter\\n=\\n10\\n,\\n \\ncv\\n=\\n3\\n,\\n    \\nscoring\\n=\\n'neg_root_mean_squared_error'\\n,\\n \\nrandom_state\\n=\\n42\\n)\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 161, 'page_label': '162', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='rnd_search\\n.\\nfit\\n(\\nhousing\\n,\\n \\nhousing_labels\\n)\\nScikit-Learn also has \\nHalvingRandomSearchCV\\n and \\nHalvingGridSearchCV\\nhyperparameter search classes. Their goal is to use the computational\\nresources more efficiently, either to train faster or to explore a larger\\nhyperparameter space. Here’s how they work: in the first round, many\\nhyperparameter combinations (called “candidates”) are generated using either\\nthe grid approach or the random approach. These candidates are then used to\\ntrain models that are evaluated using cross-validation, as usual. However,\\ntraining uses limited resources, which speeds up this first round considerably.\\nBy default, “limited resources” means that the models are trained on a small\\npart of the training set. However, other limitations are possible, such as\\nreducing the number of training iterations if the model has a hyperparameter\\nto set it. Once every candidate has been evaluated, only the best ones go on to\\nthe second round, where they are allowed more resources to compete. After\\nseveral rounds, the final candidates are evaluated using full resources. This\\nmay save you some time tuning hyperparameters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 162, 'page_label': '163', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Ensemble Methods\\nAnother way to fine-tune your system is to try to combine the models that\\nperform best. \\nThe group (or “ensemble”) will often perform better than the\\nbest individual model—just like random forests perform better than the\\nindividual decision trees they rely on—especially if the individual models\\nmake very different types of errors. For example, you could train and fine-\\ntune a \\nk\\n-nearest neighbors model, then create an ensemble model that just\\npredicts the mean of the random forest prediction and that model’s\\nprediction. We will cover this topic in more detail in \\nChapter 7\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 163, 'page_label': '164', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Analyzing the Best Models and Their Errors\\nYou will often gain good insights on the problem by inspecting the best\\nmodels. \\nFor example, the \\nRandomForestRegressor\\n can indicate the relative\\nimportance of each attribute for making accurate predictions:\\n>>> \\nfinal_model\\n \\n=\\n \\nrnd_search\\n.\\nbest_estimator_\\n  \\n# includes preprocessing\\n>>> \\nfeature_importances\\n \\n=\\n \\nfinal_model\\n[\\n\"random_forest\"\\n]\\n.\\nfeature_importances_\\n>>> \\nfeature_importances\\n.\\nround\\n(\\n2\\n)\\narray([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, [...], 0.01])\\nLet’s sort these importance scores in descending order and display them next\\nto their corresponding attribute names:\\n>>> \\nsorted\\n(\\nzip\\n(\\nfeature_importances\\n,\\n... \\n           \\nfinal_model\\n[\\n\"preprocessing\"\\n]\\n.\\nget_feature_names_out\\n()),\\n... \\n           \\nreverse\\n=\\nTrue\\n)\\n...\\n[(0.18694559869103852, \\'log__median_income\\'),\\n (0.0748194905715524, \\'cat__ocean_proximity_INLAND\\'),\\n (0.06926417748515576, \\'bedrooms__ratio\\'),\\n (0.05446998753775219, \\'rooms_per_house__ratio\\'),\\n (0.05262301809680712, \\'people_per_house__ratio\\'),\\n (0.03819415873915732, \\'geo__Cluster 0 similarity\\'),\\n [...]\\n (0.00015061247730531558, \\'cat__ocean_proximity_NEAR BAY\\'),\\n (7.301686597099842e-05, \\'cat__ocean_proximity_ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful\\nfeatures (e.g., apparently only one \\nocean_proximity\\n category is really useful,\\nso you could try dropping the others).\\nTIP\\nThe \\nsklearn.feature_selection.SelectFromModel\\n transformer can automatically drop the\\nleast useful features for you: when you fit it, it trains a model (typically a random forest),\\nlooks at its \\nfeature_importances_\\n attribute, and selects the most useful features. Then\\nwhen you call \\ntransform()\\n, it drops the other features.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 164, 'page_label': '165', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='You should also look at the specific errors that your system makes, then try to\\nunderstand why it makes them and what could fix the problem: adding extra\\nfeatures or getting rid of uninformative ones, cleaning up outliers, etc.\\nNow is also a good time to ensure that your model not only works well on\\naverage, but also on all categories of districts, whether they’re rural or urban,\\nrich or poor, northern or southern, minority or not, etc. Creating subsets of\\nyour validation set for each category takes a bit of work, but it’s important: if\\nyour model performs poorly on a whole category of districts, then it should\\nprobably not be deployed until the issue is solved, or at least it should not be\\nused to make predictions for that category, as it may do more harm than\\ngood.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 165, 'page_label': '166', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Evaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that\\nperforms sufficiently well. You are ready to evaluate the final model on the\\ntest set. There is nothing special about this process; just get the predictors and\\nthe labels from your test set and run your \\nfinal_model\\n to transform the data\\nand make predictions, then evaluate these predictions:\\nX_test\\n \\n=\\n \\nstrat_test_set\\n.\\ndrop\\n(\\n\"median_house_value\"\\n,\\n \\naxis\\n=\\n1\\n)\\ny_test\\n \\n=\\n \\nstrat_test_set\\n[\\n\"median_house_value\"\\n]\\n.\\ncopy\\n()\\nfinal_predictions\\n \\n=\\n \\nfinal_model\\n.\\npredict\\n(\\nX_test\\n)\\nfinal_rmse\\n \\n=\\n \\nmean_squared_error\\n(\\ny_test\\n,\\n \\nfinal_predictions\\n,\\n \\nsquared\\n=\\nFalse\\n)\\nprint\\n(\\nfinal_rmse\\n)\\n  \\n# prints 41424.40026462184\\nIn some cases, such a point estimate of the generalization error will not be\\nquite enough to convince you to launch: what if it is just 0.1% better than the\\nmodel currently in production? You might want to have an idea of how\\nprecise this estimate is. \\nFor this, you can compute a 95% \\nconfidence interval\\nfor the generalization error using \\nscipy.stats.t.interval()\\n. You get a fairly large\\ninterval from 39,275 to 43,467, and your previous point estimate of 41,424 is\\nroughly in the middle of it:\\n>>> \\nfrom\\n \\nscipy\\n \\nimport\\n \\nstats\\n>>> \\nconfidence\\n \\n=\\n \\n0.95\\n>>> \\nsquared_errors\\n \\n=\\n \\n(\\nfinal_predictions\\n \\n-\\n \\ny_test\\n)\\n \\n**\\n \\n2\\n>>> \\nnp\\n.\\nsqrt\\n(\\nstats\\n.\\nt\\n.\\ninterval\\n(\\nconfidence\\n,\\n \\nlen\\n(\\nsquared_errors\\n)\\n \\n-\\n \\n1\\n,\\n... \\n                         \\nloc\\n=\\nsquared_errors\\n.\\nmean\\n(),\\n... \\n                         \\nscale\\n=\\nstats\\n.\\nsem\\n(\\nsquared_errors\\n)))\\n...\\narray([39275.40861216, 43467.27680583])\\nIf you did a lot of hyperparameter tuning, the performance will usually be\\nslightly worse than what you measured using cross-validation. \\nThat’s because\\nyour system ends up fine-tuned to perform well on the validation data and\\nwill likely not perform as well on unknown datasets. That’s not the case in\\nthis example since the test RMSE is lower than the validation RMSE, but'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 166, 'page_label': '167', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='when it happens you must resist the temptation to tweak the hyperparameters\\nto make the numbers look good on the test set; the improvements would be\\nunlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution\\n(highlighting what you have learned, what worked and what did not, what\\nassumptions were made, and what your system’s limitations are), document\\neverything, and create nice presentations with clear visualizations and easy-\\nto-remember statements (e.g., “the median income is the number one\\npredictor of housing prices”). In this California housing example, the final\\nperformance of the system is not much better than the experts’ price\\nestimates, which were often off by 30%, but it may still be a good idea to\\nlaunch it, especially if this frees up some time for the experts so they can\\nwork on more interesting and productive tasks.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 167, 'page_label': '168', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Launch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! You now need to get your solution ready\\nfor production (e.g., polish the code, write documentation and tests, and so\\non). Then you can deploy your model to your production environment. \\nThe\\nmost basic way to do this is just to save the best model you trained, transfer\\nthe file to your production environment, and load it. To save the model, you\\ncan use the \\njoblib\\n library like this:\\nimport\\n \\njoblib\\njoblib\\n.\\ndump\\n(\\nfinal_model\\n,\\n \\n\"my_california_housing_model.pkl\"\\n)\\nTIP\\nIt’s often a good idea to save every model you experiment with so that you can come back\\neasily to any model you want. You may also save the cross-validation scores and perhaps\\nthe actual predictions on the validation set. This will allow you to easily compare scores\\nacross model types, and compare the types of errors they make.\\nOnce your model is transferred to production, you can load it and use it. For\\nthis you must first import any custom classes and functions the model relies\\non (which means transferring the code to production), then load the model\\nusing \\njoblib\\n and use it to make predictions:\\nimport\\n \\njoblib\\n[\\n...\\n]\\n  \\n# import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.\\ndef\\n \\ncolumn_ratio\\n(\\nX\\n):\\n \\n[\\n...\\n]\\ndef\\n \\nratio_name\\n(\\nfunction_transformer\\n,\\n \\nfeature_names_in\\n):\\n \\n[\\n...\\n]\\nclass\\n \\nClusterSimilarity\\n(\\nBaseEstimator\\n,\\n \\nTransformerMixin\\n):\\n \\n[\\n...\\n]\\nfinal_model_reloaded\\n \\n=\\n \\njoblib\\n.\\nload\\n(\\n\"my_california_housing_model.pkl\"\\n)\\nnew_data\\n \\n=\\n \\n[\\n...\\n]\\n  \\n# some new districts to make predictions for\\npredictions\\n \\n=\\n \\nfinal_model_reloaded\\n.\\npredict\\n(\\nnew_data\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 168, 'page_label': '169', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='For example, perhaps the model will be used within a website: the user will\\ntype in some data about a new district and click the Estimate Price button.\\nThis will send a query containing the data to the web server, which will\\nforward it to your web application, and finally your code will simply call the\\nmodel’s \\npredict()\\n method (you want to load the model upon server startup,\\nrather than every time the model is used). Alternatively, you can wrap the\\nmodel within a dedicated web service that your web application can query\\nthrough a REST API\\n\\u2060\\n (see \\nFigure 2-20\\n). This makes it easier to upgrade\\nyour model to new versions without interrupting the main application. It also\\nsimplifies scaling, since you can start as many web services as needed and\\nload-balance the requests coming from your web application across these\\nweb services. Moreover, it allows your web application to use any\\nprogramming language, not just Python.\\nFigure 2-20. \\nA model deployed as a web service and used by a web application\\nAnother popular strategy is to deploy your model to the cloud, for example\\non \\nGoogle’s Vertex AI (formerly known as Google Cloud AI Platform and\\nGoogle Cloud ML Engine): just save your model using \\njoblib\\n and upload it to\\nGoogle Cloud Storage (GCS), then head over to Vertex AI and create a new\\nmodel version, pointing it to the GCS file. That’s it! This gives you a simple\\nweb service that takes care of load balancing and scaling for you. It takes\\nJSON requests containing the input data (e.g., of a district) and returns JSON\\nresponses containing the predictions. You can then use this web service in\\nyour website (or whatever production environment you are using). As you\\nwill see in \\nChapter 19\\n, deploying TensorFlow models on Vertex AI is not\\nmuch different from deploying Scikit-Learn models.\\nBut deployment is not the end of the story. You also need to write monitoring\\ncode to check your system’s live performance at regular intervals and trigger\\nalerts when it drops. It may drop very quickly, for example if a component\\nbreaks in your infrastructure, but be aware that it could also decay very\\nslowly, which can easily go unnoticed for a long time. This is quite common\\n13'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 169, 'page_label': '170', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='because of model rot: if the model was trained with last year’s data, it may\\nnot be adapted to today’s data.\\nSo, you need to monitor your model’s live performance. But how do you do\\nthat? Well, it depends. In some cases, the model’s performance can be\\ninferred from downstream metrics. For example, if your model is part of a\\nrecommender system and it suggests products that the users may be interested\\nin, then it’s easy to monitor the number of recommended products sold each\\nday. If this number drops (compared to non-recommended products), then the\\nprime suspect is the model. This may be because the data pipeline is broken,\\nor perhaps the model needs to be retrained on fresh data (as we will discuss\\nshortly).\\nHowever, you may also need human analysis to assess the model’s\\nperformance. For example, suppose you trained an image classification\\nmodel (we’ll look at these in \\nChapter 3\\n) to detect various product defects on a\\nproduction line. How can you get an alert if the model’s performance drops,\\nbefore thousands of defective products get shipped to your clients? One\\nsolution is to send to human raters a sample of all the pictures that the model\\nclassified (especially pictures that the model wasn’t so sure about).\\nDepending on the task, the raters may need to be experts, or they could be\\nnonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon\\nMechanical Turk). In some applications they could even be the users\\nthemselves, responding, for example, via surveys or repurposed captchas.\\n\\u2060\\nEither way, you need to put in place a monitoring system (with or without\\nhuman raters to evaluate the live model), as well as all the relevant processes\\nto define what to do in case of failures and how to prepare for them.\\nUnfortunately, this can be a lot of work. In fact, it is often much more work\\nthan building and training a model.\\nIf the data keeps evolving, you will need to update your datasets and retrain\\nyour model regularly. You should probably automate the whole process as\\nmuch as possible. Here are a few things you can automate:\\nCollect fresh data regularly and label it (e.g., using human raters).\\n14'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 170, 'page_label': '171', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Write a script to train the model and fine-tune the hyperparameters\\nautomatically. This script could run automatically, for example every\\nday or every week, depending on your needs.\\nWrite another script that will evaluate both the new model and the\\nprevious model on the updated test set, and deploy the model to\\nproduction if the performance has not decreased (if it did, make sure you\\ninvestigate why). The script should probably test the performance of\\nyour model on various subsets of the test set, such as poor or rich\\ndistricts, rural or urban districts, etc.\\nYou should also make sure you evaluate the model’s input data quality.\\nSometimes performance will degrade slightly because of a poor-quality\\nsignal (e.g., a malfunctioning sensor sending random values, or another\\nteam’s output becoming stale), but it may take a while before your system’s\\nperformance degrades enough to trigger an alert. If you monitor your model’s\\ninputs, you may catch this earlier. For example, you could trigger an alert if\\nmore and more inputs are missing a feature, or the mean or standard\\ndeviation drifts too far from the training set, or a categorical feature starts\\ncontaining new categories.\\nFinally, make sure you keep backups of every model you create and have the\\nprocess and tools in place to roll back to a previous model quickly, in case\\nthe new model starts failing badly for some reason. Having backups also\\nmakes it possible to easily compare new models with previous ones.\\nSimilarly, you should keep backups of every version of your datasets so that\\nyou can roll back to a previous dataset if the new one ever gets corrupted\\n(e.g., if the fresh data that gets added to it turns out to be full of outliers).\\nHaving backups of your datasets also allows you to evaluate any model\\nagainst any previous dataset.\\nAs you can see, machine learning involves quite a lot of infrastructure.\\nChapter 19\\n discusses some aspects of this, but it’s a very broad topic called\\nML Operations\\n (MLOps), which deserves its own book. \\nSo don’t be\\nsurprised if your first ML project takes a lot of effort and time to build and\\ndeploy to production. Fortunately, once all the infrastructure is in place,'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 171, 'page_label': '172', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='going from idea to production will be much faster.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 172, 'page_label': '173', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Try It Out!\\nHopefully this chapter gave you a good idea of what a machine learning\\nproject looks like as well as showing you some of the tools you can use to\\ntrain a great system. As you can see, much of the work is in the data\\npreparation step: building monitoring tools, setting up human evaluation\\npipelines, and automating regular model training. The machine learning\\nalgorithms are important, of course, but it is probably preferable to be\\ncomfortable with the overall process and know three or four algorithms well\\nrather than to spend all your time exploring advanced algorithms.\\nSo, if you have not already done so, now is a good time to pick up a laptop,\\nselect a dataset that you are interested in, and try to go through the whole\\nprocess from A to Z. A good place to start is on a competition website such\\nas \\nKaggle\\n: you will have a dataset to play with, a clear goal, and people to\\nshare the experience with. Have fun!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 173, 'page_label': '174', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\nThe following exercises are based on this chapter’s housing dataset:\\n1\\n. \\nTry a support vector machine regressor (\\nsklearn.svm.SVR\\n) with various\\nhyperparameters, such as \\nkernel=\"linear\"\\n (with various values for the \\nC\\nhyperparameter)\\n or \\nkernel=\"rbf\"\\n (with various values for the \\nC\\n and\\ngamma\\n hyperparameters). Note that support vector machines don’t scale\\nwell to large datasets, so you should probably train your model on just\\nthe first 5,000 instances of the training set and use only 3-fold cross-\\nvalidation, or else it will take hours. Don’t worry about what the\\nhyperparameters mean for now; we’ll discuss them in \\nChapter 5\\n. How\\ndoes the best \\nSVR\\n predictor perform?\\n2\\n. \\nTry replacing the \\nGridSearchCV\\n with a \\nRandomizedSearchCV\\n.\\n3\\n. \\nTry adding a \\nSelectFromModel\\n transformer in the preparation pipeline\\nto select only the most important attributes.\\n4\\n. \\nTry creating a custom transformer that trains a \\nk\\n-nearest neighbors\\nregressor (\\nsklearn.neighbors.KNeighborsRegressor\\n) in its \\nfit()\\n method,\\nand outputs the model’s predictions in its \\ntransform()\\n method. Then add\\nthis feature to the preprocessing pipeline, using latitude and longitude as\\nthe inputs to this transformer. This will add a feature in the model that\\ncorresponds to the housing median price of the nearest districts.\\n5\\n. \\nAutomatically explore some preparation options using \\nGridSearchCV\\n.\\n6\\n. \\nTry to implement the \\nStandardScalerClone\\n class again from scratch,\\nthen add support for the \\ninverse_transform()\\n method: executing \\nscaler.\\ninverse_transform(scaler.fit_transform(X))\\n should return an array very\\nclose to \\nX\\n. Then add support for feature names: set \\nfeature_names_in_\\nin the \\nfit()\\n method if the input is a DataFrame. This attribute should be a\\nNumPy array of column names. Lastly, implement the\\nget_feature_names_out()\\n method: it should have one optional\\ninput_features=None\\n argument. If passed, the method should check that'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 174, 'page_label': '175', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='its length matches \\nn_features_in_\\n, and it should match\\nfeature_names_in_\\n if it is defined; then \\ninput_features\\n should be\\nreturned. If \\ninput_features\\n is \\nNone\\n, then the method should either return\\nfeature_names_in_\\n if it is defined or \\nnp.array([\"x0\", \"x1\", ...])\\n with\\nlength \\nn_features_in_\\n otherwise.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial\\nAutoregressions”, \\nStatistics & Probability Letters\\n 33, no. 3 (1997): 291–297.\\n2\\n A piece of information fed to a machine learning system is often called a \\nsignal\\n, in reference to\\nClaude Shannon’s information theory, which he developed at Bell Labs to improve\\ntelecommunications. His theory: you want a high signal-to-noise ratio.\\n3\\n Recall that the \\ntranspose operator flips a column vector into a row vector (and vice versa).\\n4\\n You might also need to check legal constraints, such as private fields that should never be\\ncopied to unsafe data stores.\\n5\\n The standard deviation is generally denoted \\nσ\\n (the Greek letter sigma), and it is the square root\\nof the \\nvariance\\n, which is the average of the squared deviation from the mean. When a feature has\\na bell-shaped \\nnormal distribution\\n (also called a \\nGaussian distribution\\n), which is very common,\\nthe “68-95-99.7” rule applies: about 68% of the values fall within 1\\nσ\\n of the mean, 95% within 2\\nσ\\n,\\nand 99.7% within 3\\nσ\\n.\\n6\\n You will often see people set the random seed to 42. This number has no special property, other\\nthan being the Answer to the Ultimate Question of Life, the Universe, and Everything.\\n7\\n The location information is actually quite coarse, and as a result many districts will have the\\nexact same ID, so they will end up in the same set (test or train). This introduces some\\nunfortunate sampling bias.\\n8\\n If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from\\nthe Bay Area down to San Diego (as you might expect). You can add a patch of yellow around\\nSacramento as well.\\n9\\n For more details on the design principles, see Lars Buitinck et al., “API Design for Machine\\nLearning Software: Experiences from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238\\n(2013).\\n10\\n Some predictors also provide methods to measure the confidence of their predictions.\\n11\\n By the time you read these lines, it may be possible to make all transformers output Pandas\\nDataFrames when they receive a DataFrame as input: Pandas in, Pandas out. There will likely be\\na global configuration option for this:\\n \\nsklearn.set_config(pandas_in_out=True)\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 175, 'page_label': '176', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='12\\n See SciPy’s documentation for more details.\\n13\\n In a nutshell, a REST (or RESTful) API is an HTTP-based API that follows some conventions,\\nsuch as using standard HTTP verbs to read, update, create, or delete resources (GET, POST,\\nPUT, and DELETE) and using JSON for the inputs and outputs.\\n14\\n A captcha is a test to ensure a user is not a robot. These tests have often been used as a cheap\\nway to label training data.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 176, 'page_label': '177', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 3. \\nClassification\\nIn \\nChapter 1\\n I mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). \\nIn\\nChapter 2\\n we explored a regression task, predicting housing values, using\\nvarious algorithms such as linear regression, decision trees, and random\\nforests (which will be explained in further detail in later chapters). Now we\\nwill turn our attention to classification systems.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 177, 'page_label': '178', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='MNIST\\nIn this chapter we will be using the MNIST dataset, which is a set of 70,000\\nsmall images of digits handwritten by high school students and employees of\\nthe US Census Bureau. \\nEach image is labeled with the digit it represents.\\nThis set has been studied so much that it is often called the “hello world” of\\nmachine learning: whenever people come up with a new classification\\nalgorithm they are curious to see how it will perform on MNIST, and anyone\\nwho learns machine learning tackles this dataset sooner or later.\\nScikit-Learn provides many helper functions to download popular datasets.\\nMNIST is one of them. The following code fetches the MNIST dataset from\\nOpenML.org:\\n\\u2060\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nfetch_openml\\nmnist\\n \\n=\\n \\nfetch_openml\\n(\\n\\'mnist_784\\'\\n,\\n \\nas_frame\\n=\\nFalse\\n)\\nThe \\nsklearn.datasets\\n package contains mostly three types of functions:\\nfetch_*\\n functions such as \\nfetch_openml()\\n to download real-life datasets,\\nload_*\\n functions to load small toy datasets bundled with Scikit-Learn (so\\nthey don’t need to be downloaded over the internet), and \\nmake_*\\n functions to\\ngenerate fake datasets, useful for tests. Generated datasets are usually\\nreturned as an \\n(X, y)\\n tuple containing the input data and the targets, both as\\nNumPy arrays. Other datasets are returned as \\nsklearn.utils.Bunch\\n objects,\\nwhich are dictionaries whose entries can also be accessed as attributes. \\nThey\\ngenerally contain the following entries:\\n\"DESCR\"\\nA description of the dataset\\n\"data\"\\nThe input data, usually as a 2D NumPy array\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 178, 'page_label': '179', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='\"target\"\\nThe labels, usually as a 1D NumPy array\\nThe \\nfetch_openml()\\n function is a bit unusual since by default it returns the\\ninputs as a Pandas DataFrame and the labels as a Pandas Series (unless the\\ndataset is sparse). But the MNIST dataset contains images, and DataFrames\\naren’t ideal for that, so it’s preferable to set \\nas_frame=False\\n to get the data as\\nNumPy arrays instead. Let’s look at these arrays:\\n>>> \\nX\\n,\\n \\ny\\n \\n=\\n \\nmnist\\n.\\ndata\\n,\\n \\nmnist\\n.\\ntarget\\n>>> \\nX\\narray([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]])\\n>>> \\nX\\n.\\nshape\\n(70000, 784)\\n>>> \\ny\\narray([\\'5\\', \\'0\\', \\'4\\', ..., \\'4\\', \\'5\\', \\'6\\'], dtype=object)\\n>>> \\ny\\n.\\nshape\\n(70000,)\\nThere are 70,000 images, and each image has 784 features. This is because\\neach image is 28 × 28 pixels, and each feature simply represents one pixel’s\\nintensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from\\nthe dataset (\\nFigure 3-1\\n). All we need to do is grab an instance’s feature\\nvector, reshape it to a 28 × 28 array, and display it using Matplotlib’s\\nimshow()\\n function. We use \\ncmap=\"binary\"\\n to get a grayscale color map\\nwhere 0 is white and 255 is black:\\nimport\\n \\nmatplotlib.pyplot\\n \\nas\\n \\nplt\\ndef\\n \\nplot_digit\\n(\\nimage_data\\n):\\n    \\nimage\\n \\n=\\n \\nimage_data\\n.\\nreshape\\n(\\n28\\n,\\n \\n28\\n)\\n    \\nplt\\n.\\nimshow\\n(\\nimage\\n,\\n \\ncmap\\n=\\n\"binary\"\\n)\\n    \\nplt\\n.\\naxis\\n(\\n\"off\"\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 179, 'page_label': '180', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"some_digit\\n \\n=\\n \\nX\\n[\\n0\\n]\\nplot_digit\\n(\\nsome_digit\\n)\\nplt\\n.\\nshow\\n()\\nFigure 3-1. \\nExample of an MNIST image\\nThis looks like a 5, and indeed that’s what the label tells us:\\n>>> \\ny\\n[\\n0\\n]\\n'5'\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 180, 'page_label': '181', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='To give you a feel for the complexity of the classification task, \\nFigure 3-2\\nshows a few more images from the MNIST dataset.\\nBut wait! You should always create a test set and set it aside before\\ninspecting the data closely. The MNIST dataset returned by \\nfetch_openml()\\n is\\nactually already split into a training set (the first 60,000 images) and a test set\\n(the last 10,000 images):\\nX_train\\n,\\n \\nX_test\\n,\\n \\ny_train\\n,\\n \\ny_test\\n \\n=\\n \\nX\\n[:\\n60000\\n],\\n \\nX\\n[\\n60000\\n:],\\n \\ny\\n[:\\n60000\\n],\\n \\ny\\n[\\n60000\\n:]\\nThe training set is already shuffled for us, which is good because this\\nguarantees that all cross-validation folds will be similar (we don’t want one\\nfold to be missing some digits). \\nMoreover, some learning algorithms are\\nsensitive to the order of the training instances, and they perform poorly if\\nthey get many similar instances in a row. Shuffling the dataset ensures that\\nthis won’t happen.\\n\\u2060\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 181, 'page_label': '182', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-2. \\nDigits from the MNIST dataset'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 182, 'page_label': '183', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"Training a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for\\nexample, the number 5. \\nThis “5-detector” will be an example of a \\nbinary\\nclassifier\\n, capable of distinguishing between just two classes, 5 and non-5.\\nFirst we’ll create the target vectors for this classification task:\\ny_train_5\\n \\n=\\n \\n(\\ny_train\\n \\n==\\n \\n'5'\\n)\\n  \\n# True for all 5s, False for all other digits\\ny_test_5\\n \\n=\\n \\n(\\ny_test\\n \\n==\\n \\n'5'\\n)\\nNow let’s pick a classifier and train it. \\nA good place to start is with a\\nstochastic gradient descent\\n (SGD, or stochastic GD) classifier, using Scikit-\\nLearn’s \\nSGDClassifier\\n class. This classifier is capable of handling very large\\ndatasets efficiently. This is in part because SGD deals with training instances\\nindependently, one at a time, which also makes SGD well suited for online\\nlearning, as you will see later. Let’s create an \\nSGDClassifier\\n and train it on\\nthe whole training set:\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nSGDClassifier\\nsgd_clf\\n \\n=\\n \\nSGDClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\nsgd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train_5\\n)\\nNow we can use it to detect images of the number 5:\\n>>> \\nsgd_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([ True])\\nThe classifier guesses that this image represents a 5 (\\nTrue\\n). Looks like it\\nguessed right in this particular case! Now, let’s evaluate this model’s\\nperformance.\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 183, 'page_label': '184', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Performance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a\\nregressor, so we will spend a large part of this chapter on this topic. \\nThere are\\nmany performance measures available, so grab another coffee and get ready\\nto learn a bunch of new concepts and acronyms!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 184, 'page_label': '185', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Measuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in\\nChapter 2\\n. \\nLet’s use the \\ncross_val_score()\\n function to evaluate our\\nSGDClassifier\\n model, using \\nk\\n-fold cross-validation with three folds.\\nRemember that \\nk\\n-fold cross-validation means splitting the training set into \\nk\\nfolds (in this case, three), then training the model \\nk\\n times, holding out a\\ndifferent fold each time for evaluation (see \\nChapter 2\\n):\\n>>> \\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\ncross_val_score\\n>>> \\ncross_val_score\\n(\\nsgd_clf\\n,\\n \\nX_train\\n,\\n \\ny_train_5\\n,\\n \\ncv\\n=\\n3\\n,\\n \\nscoring\\n=\\n\"accuracy\"\\n)\\narray([0.95035, 0.96035, 0.9604 ])\\nWow! Above 95% accuracy (ratio of correct predictions) on all cross-\\nvalidation folds? This looks amazing, doesn’t it? Well, before you get too\\nexcited, let’s look at a dummy classifier that just classifies every single image\\nin the most frequent class, which in this case is the negative class (i.e., \\nnon\\n5):\\nfrom\\n \\nsklearn.dummy\\n \\nimport\\n \\nDummyClassifier\\ndummy_clf\\n \\n=\\n \\nDummyClassifier\\n()\\ndummy_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train_5\\n)\\nprint\\n(\\nany\\n(\\ndummy_clf\\n.\\npredict\\n(\\nX_train\\n)))\\n  \\n# prints False: no 5s detected\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> \\ncross_val_score\\n(\\ndummy_clf\\n,\\n \\nX_train\\n,\\n \\ny_train_5\\n,\\n \\ncv\\n=\\n3\\n,\\n \\nscoring\\n=\\n\"accuracy\"\\n)\\narray([0.90965, 0.90965, 0.90965])\\nThat’s right, it has over 90% accuracy! This is simply because only about\\n10% of the images are 5s, so if you always guess that an image is \\nnot\\n a 5, you\\nwill be right about 90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance\\nmeasure for classifiers, especially when you are dealing with \\nskewed datasets\\n(i.e., when some classes are much more frequent than others). \\nA much better'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 185, 'page_label': '186', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='way to evaluate the performance of a classifier is to look at the \\nconfusion\\nmatrix\\n (CM).\\nIMPLEMENTING CROSS-VALIDATION\\nOccasionally you will need more control over the cross-validation\\nprocess than what Scikit-Learn provides off the shelf. In these cases, you\\ncan implement cross-validation yourself. The following code does\\nroughly the same thing as Scikit-Learn’s \\ncross_val_score()\\n function, and\\nit prints the same result:\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\nStratifiedKFold\\nfrom\\n \\nsklearn.base\\n \\nimport\\n \\nclone\\nskfolds\\n \\n=\\n \\nStratifiedKFold\\n(\\nn_splits\\n=\\n3\\n)\\n  \\n# add shuffle=True if the dataset is\\n                                       \\n# not already shuffled\\nfor\\n \\ntrain_index\\n,\\n \\ntest_index\\n \\nin\\n \\nskfolds\\n.\\nsplit\\n(\\nX_train\\n,\\n \\ny_train_5\\n):\\n    \\nclone_clf\\n \\n=\\n \\nclone\\n(\\nsgd_clf\\n)\\n    \\nX_train_folds\\n \\n=\\n \\nX_train\\n[\\ntrain_index\\n]\\n    \\ny_train_folds\\n \\n=\\n \\ny_train_5\\n[\\ntrain_index\\n]\\n    \\nX_test_fold\\n \\n=\\n \\nX_train\\n[\\ntest_index\\n]\\n    \\ny_test_fold\\n \\n=\\n \\ny_train_5\\n[\\ntest_index\\n]\\n    \\nclone_clf\\n.\\nfit\\n(\\nX_train_folds\\n,\\n \\ny_train_folds\\n)\\n    \\ny_pred\\n \\n=\\n \\nclone_clf\\n.\\npredict\\n(\\nX_test_fold\\n)\\n    \\nn_correct\\n \\n=\\n \\nsum\\n(\\ny_pred\\n \\n==\\n \\ny_test_fold\\n)\\n    \\nprint\\n(\\nn_correct\\n \\n/\\n \\nlen\\n(\\ny_pred\\n))\\n  \\n# prints 0.95035, 0.96035, and 0.9604\\nThe \\nStratifiedKFold\\n class performs stratified sampling (as explained in\\nChapter 2\\n) to produce folds that contain a representative ratio of each\\nclass. \\nAt each iteration the code creates a clone of the classifier, trains\\nthat clone on the training folds, and makes predictions on the test fold.\\nThen it counts the number of correct predictions and outputs the ratio of\\ncorrect predictions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 186, 'page_label': '187', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Confusion Matrices\\nThe general idea of a confusion matrix is to count the number of times\\ninstances of class A are classified as class B, for all A/B pairs. \\nFor example,\\nto know the number of times the classifier confused images of 8s with 0s, you\\nwould look at row #8, column #0 of the confusion matrix.\\nTo compute the confusion matrix, you first need to have a set of predictions\\nso that they can be compared to the actual targets. \\nYou could make\\npredictions on the test set, but it’s best to keep that untouched for now\\n(remember that you want to use the test set only at the very end of your\\nproject, once you have a classifier that you are ready to launch). Instead, you\\ncan use the \\ncross_val_predict()\\n function:\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\ncross_val_predict\\ny_train_pred\\n \\n=\\n \\ncross_val_predict\\n(\\nsgd_clf\\n,\\n \\nX_train\\n,\\n \\ny_train_5\\n,\\n \\ncv\\n=\\n3\\n)\\nJu\\nst like the \\ncross_val_score()\\n function, \\ncross_val_predict()\\n performs \\nk\\n-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the\\npredictions made on each test fold. This means that you get a clean prediction\\nfor each instance in the training set (by “clean” I mean “out-of-sample”: the\\nmodel makes predictions on data that it never saw during training).\\nNow you are ready to get the confusion matrix using the \\nconfusion_matrix()\\nfunction. \\nJust pass it the target classes (\\ny_train_5\\n) and the predicted classes\\n(\\ny_train_pred\\n):\\n>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nconfusion_matrix\\n>>> \\ncm\\n \\n=\\n \\nconfusion_matrix\\n(\\ny_train_5\\n,\\n \\ny_train_pred\\n)\\n>>> \\ncm\\narray([[53892,   687],\\n       [ 1891,  3530]])\\nEach row in a confusion matrix represents an \\nactual class\\n, while each column\\nrepresents a \\npredicted class\\n. \\nThe first row of this matrix considers non-5\\nimages (the \\nnegative class\\n): 53,892 of them were correctly classified as non-'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 187, 'page_label': '188', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='5s (they are called \\ntrue negatives\\n), while the remaining 687 were wrongly\\nclassified as 5s (\\nfalse positives\\n, also called \\ntype I errors\\n). \\nThe second row\\nconsiders the images of 5s (the \\npositive class\\n): 1,891 were wrongly classified\\nas non-5s (\\nfalse negatives\\n, also called \\ntype II errors\\n), while the remaining\\n3,530 were correctly classified as 5s (\\ntrue positives\\n). A perfect classifier\\nwould only have true positives and true negatives, so its confusion matrix\\nwould have nonzero values only on its main diagonal (top left to bottom\\nright):\\n>>> \\ny_train_perfect_predictions\\n \\n=\\n \\ny_train_5\\n  \\n# pretend we reached perfection\\n>>> \\nconfusion_matrix\\n(\\ny_train_5\\n,\\n \\ny_train_perfect_predictions\\n)\\narray([[54579,     0],\\n       [    0,  5421]])\\nThe confusion matrix gives you a lot of information, but sometimes you may\\nprefer a more concise metric. An interesting one to look at is the accuracy of\\nthe positive predictions; this is called the \\nprecision\\n of the classifier (\\nEquation\\n3-1\\n).\\nEquation 3-1. \\nPrecision\\nprecision \\n= \\nTP TP+FP\\nTP\\n is the number of true positives, and \\nFP\\n is the number of false positives.\\nA trivial way to have perfect precision is to create a classifier that always\\nmakes negative predictions, except for one single positive prediction on the\\ninstance it’s most confident about. If this one prediction is correct, then the\\nclassifier has 100% precision (precision = 1/1 = 100%). \\nObviously, such a\\nclassifier would not be very useful, since it would ignore all but one positive\\ninstance. So, precision is typically used along with another metric named\\nrecall\\n, also called \\nsensitivity\\n or the \\ntrue positive rate\\n (TPR): this is the ratio\\nof positive instances that are correctly detected by the classifier (\\nEquation 3-\\n2\\n).\\nEquation 3-2. \\nRecall\\nrecall \\n= \\nTP TP+FN'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 188, 'page_label': '189', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='FN\\n is, of course, the number of false negatives.\\nIf you are confused about the confusion matrix, \\nFigure 3-3\\n may help.\\nFigure 3-3. \\nAn illustrated confusion matrix showing examples of true negatives (top left), false positives\\n(top right), false negatives (lower left), and true positives (lower right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 189, 'page_label': '190', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Precision and Recall\\nScikit-Learn provides several functions to compute classifier metrics,\\nincluding \\nprecision and recall:\\n>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nprecision_score\\n,\\n \\nrecall_score\\n>>> \\nprecision_score\\n(\\ny_train_5\\n,\\n \\ny_train_pred\\n)\\n  \\n# == 3530 / (687 + 3530)\\n0.8370879772350012\\n>>> \\nrecall_score\\n(\\ny_train_5\\n,\\n \\ny_train_pred\\n)\\n  \\n# == 3530 / (1891 + 3530)\\n0.6511713705958311\\nNow our 5-detector does not look as shiny as it did when we looked at its\\naccuracy. When it claims an image represents a 5, it is correct only 83.7% of\\nthe time. Moreover, it only detects 65.1% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric\\ncalled the \\nF\\n score\\n, especially when you need a single metric to compare two\\nclassifiers. \\nThe F\\n score is the \\nharmonic mean\\n of precision and recall\\n(\\nEquation 3-3\\n). Whereas the regular mean treats all values equally, the\\nharmonic mean gives much more weight to low values. As a result, the\\nclassifier will only get a high F\\n score if both recall and precision are high.\\nEquation 3-3. \\nF\\n score\\nF 1 \\n= \\n2 1 precision+1 recall \\n= \\n2 \\n× \\nprecision×recall precision+recall \\n= \\nTP\\nTP+FN+FP 2\\nTo compute the F\\n score, simply call the \\nf1_score()\\n function:\\n>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nf1_score\\n>>> \\nf1_score\\n(\\ny_train_5\\n,\\n \\ny_train_pred\\n)\\n0.7325171197343846\\nThe F\\n score favors classifiers that have similar precision and recall. \\nThis is\\nnot always what you want: in some contexts you mostly care about precision,\\nand in other contexts you really care about recall. For example, if you trained\\na classifier to detect videos that are safe for kids, you would probably prefer a\\nclassifier that rejects many good videos (low recall) but keeps only safe ones\\n1\\n1\\n1\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 190, 'page_label': '191', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='(high precision), rather than a classifier that has a much higher recall but lets\\na few really bad videos show up in your product (in such cases, you may\\neven want to add a human pipeline to check the classifier’s video selection).\\nOn the other hand, suppose you train a classifier to detect shoplifters in\\nsurveillance images: it is probably fine if your classifier only has 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few\\nfalse alerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces\\nrecall, and vice versa. This is called the \\nprecision/recall trade-off\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 191, 'page_label': '192', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The Precision/Recall Trade-off\\nTo understand this trade-off, let’s look at how the \\nSGDClassifier\\n makes its\\nclassification decisions. \\nFor each instance, it computes a score based on a\\ndecision function\\n. If that score is greater than a threshold, it assigns the\\ninstance to the positive class; otherwise it assigns it to the negative class.\\nFigure 3-4\\n shows a few digits positioned from the lowest score on the left to\\nthe highest score on the right. Suppose the \\ndecision threshold\\n is positioned at\\nthe central arrow (between the two 5s): you will find 4 true positives (actual\\n5s) on the right of that threshold, and 1 false positive (actually a 6).\\nTherefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you\\nraise the threshold (move it to the arrow on the right), the false positive (the\\n6) becomes a true negative, thereby increasing the precision (up to 100% in\\nthis case), but one true positive becomes a false negative, decreasing recall\\ndown to 50%. Conversely, lowering the threshold increases recall and\\nreduces precision.\\nFigure 3-4. \\nThe precision/recall trade-off: images are ranked by their classifier score, and those above\\nthe chosen decision threshold are considered positive; the higher the threshold, the lower the recall,\\nbut (in general) the higher the precision\\nScikit-Learn does not let you set the threshold directly, but it does give you\\naccess to the decision scores that it uses to make predictions. \\nInstead of\\ncalling the classifier’s \\npredict()\\n method, you can call its \\ndecision_function()\\nmethod, which returns a score for each instance, and then use any threshold\\nyou want to make predictions based on those scores:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 192, 'page_label': '193', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\ny_scores\\n \\n=\\n \\nsgd_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n>>> \\ny_scores\\narray([2164.22030239])\\n>>> \\nthreshold\\n \\n=\\n \\n0\\n>>> \\ny_some_digit_pred\\n \\n=\\n \\n(\\ny_scores\\n \\n>\\n \\nthreshold\\n)\\narray([ True])\\nThe \\nSGDClassifier\\n uses a threshold equal to 0, so the preceding code returns\\nthe same result as the \\npredict()\\n method (i.e., \\nTrue\\n). Let’s raise the threshold:\\n>>> \\nthreshold\\n \\n=\\n \\n3000\\n>>> \\ny_some_digit_pred\\n \\n=\\n \\n(\\ny_scores\\n \\n>\\n \\nthreshold\\n)\\n>>> \\ny_some_digit_pred\\narray([False])\\nThis confirms that raising the threshold decreases recall. The image actually\\nrepresents a 5, and the classifier detects it when the threshold is 0, but it\\nmisses it when the threshold is increased to 3,000.\\nHow do you decide which threshold to use? \\nFirst, use the \\ncross_val_predict()\\nfunction to get the scores of all instances in the training set, but this time\\nspecify that you want to return decision scores instead of predictions:\\ny_scores\\n \\n=\\n \\ncross_val_predict\\n(\\nsgd_clf\\n,\\n \\nX_train\\n,\\n \\ny_train_5\\n,\\n \\ncv\\n=\\n3\\n,\\n                             \\nmethod\\n=\\n\"decision_function\"\\n)\\nWith these scores, use the \\nprecision_recall_curve()\\n function to compute\\nprecision and recall for all possible thresholds (the function adds a last\\nprecision of 0 and a last recall of 1, corresponding to an infinite threshold):\\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nprecision_recall_curve\\nprecisions\\n,\\n \\nrecalls\\n,\\n \\nthresholds\\n \\n=\\n \\nprecision_recall_curve\\n(\\ny_train_5\\n,\\n \\ny_scores\\n)\\nFinally, use Matplotlib to plot precision and recall as functions of the\\nthreshold value (\\nFigure 3-5\\n). Let’s show the threshold of 3,000 we selected:\\nplt\\n.\\nplot\\n(\\nthresholds\\n,\\n \\nprecisions\\n[:\\n-\\n1\\n],\\n \\n\"b--\"\\n,\\n \\nlabel\\n=\\n\"Precision\"\\n,\\n \\nlinewidth\\n=\\n2\\n)\\nplt\\n.\\nplot\\n(\\nthresholds\\n,\\n \\nrecalls\\n[:\\n-\\n1\\n],\\n \\n\"g-\"\\n,\\n \\nlabel\\n=\\n\"Recall\"\\n,\\n \\nlinewidth\\n=\\n2\\n)\\nplt\\n.\\nvlines\\n(\\nthreshold\\n,\\n \\n0\\n,\\n \\n1.0\\n,\\n \\n\"k\"\\n,\\n \\n\"dotted\"\\n,\\n \\nlabel\\n=\\n\"threshold\"\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 193, 'page_label': '194', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='[\\n...\\n]\\n  \\n# beautify the figure: add grid, legend, axis, labels, and circles\\nplt\\n.\\nshow\\n()\\nFigure 3-5. \\nPrecision and recall versus the decision threshold\\nNOTE\\nYou may wonder why the precision curve is bumpier than the recall curve in \\nFigure 3-5\\n.\\nThe reason is that precision may sometimes go down when you raise the threshold\\n(although in general it will go up). To understand why, look back at \\nFigure 3-4\\n and notice\\nwhat happens when you start from the central threshold and move it just one digit to the\\nright: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can\\nonly go down when the threshold is increased, which explains why its curve looks smooth.\\nAt this threshold value, precision is near 90% and recall is around 50%.\\nAnother way to select a good precision/recall trade-off is to plot precision\\ndirectly against recall, as shown in \\nFigure 3-6\\n (the same threshold is shown):\\nplt\\n.\\nplot\\n(\\nrecalls\\n,\\n \\nprecisions\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n \\nlabel\\n=\\n\"Precision/Recall curve\"\\n)\\n[\\n...\\n]\\n  \\n# beautify the figure: add labels, grid, legend, arrow, and text\\nplt\\n.\\nshow\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 194, 'page_label': '195', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-6. \\nPrecision versus recall\\nYou can see that precision really starts to fall sharply at around 80% recall.\\nYou will probably want to select a precision/recall trade-off just before that\\ndrop—for example, at around 60% recall. But of course, the choice depends\\non your project.\\nSuppose you decide to aim for 90% precision. You could use the first plot to\\nfind the threshold you need to use, but that’s not very precise. Alternatively,\\nyou can search for the lowest threshold that gives you at least 90% precision.\\nFor this, you can use the NumPy array’s \\nargmax()\\n method. This returns the\\nfirst index of the maximum value, which in this case means the first \\nTrue\\nvalue:\\n>>> \\nidx_for_90_precision\\n \\n=\\n \\n(\\nprecisions\\n \\n>=\\n \\n0.90\\n)\\n.\\nargmax\\n()\\n>>> \\nthreshold_for_90_precision\\n \\n=\\n \\nthresholds\\n[\\nidx_for_90_precision\\n]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 195, 'page_label': '196', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\nthreshold_for_90_precision\\n3370.0194991439557\\nTo make predictions (on the training set for now), instead of calling the\\nclassifier’s \\npredict()\\n method, you can run this code:\\ny_train_pred_90\\n \\n=\\n \\n(\\ny_scores\\n \\n>=\\n \\nthreshold_for_90_precision\\n)\\nLet’s check these predictions’ precision and recall:\\n>>> \\nprecision_score\\n(\\ny_train_5\\n,\\n \\ny_train_pred_90\\n)\\n0.9000345901072293\\n>>> \\nrecall_at_90_precision\\n \\n=\\n \\nrecall_score\\n(\\ny_train_5\\n,\\n \\ny_train_pred_90\\n)\\n>>> \\nrecall_at_90_precision\\n0.4799852425751706\\nGreat, you have a 90% precision classifier! As you can see, it is fairly easy to\\ncreate a classifier with virtually any precision you want: just set a high\\nenough threshold, and you’re done. But wait, not so fast–a high-precision\\nclassifier is not very useful if its recall is too low! For many applications,\\n48% recall wouldn’t be great at all.\\nTIP\\nIf someone says, “Let’s reach 99% precision”, you should ask, “At what recall?”'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 196, 'page_label': '197', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The ROC Curve\\nThe \\nreceiver operating characteristic\\n (ROC) curve is another common tool\\nused with binary classifiers. \\nIt is very similar to the precision/recall curve, but\\ninstead of plotting precision versus recall, the ROC curve plots the \\ntrue\\npositive rate\\n (another name for recall) against the \\nfalse positive rate\\n (FPR).\\nThe FPR (also called the \\nfall-out\\n) is the ratio of negative instances that are\\nincorrectly classified as positive. It is equal to 1 – the \\ntrue negative rate\\n(TNR), which is the ratio of negative instances that are correctly classified as\\nnegative. \\nThe TNR is also called \\nspecificity\\n. Hence, the ROC curve plots\\nsensitivity\\n (recall) versus \\n1 – \\nspecificity\\n.\\nTo plot the ROC curve, you first use the \\nroc_curve()\\n function to compute the\\nTPR and FPR for various threshold values:\\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nroc_curve\\nfpr\\n,\\n \\ntpr\\n,\\n \\nthresholds\\n \\n=\\n \\nroc_curve\\n(\\ny_train_5\\n,\\n \\ny_scores\\n)\\nThen you can plot the FPR against the TPR using Matplotlib. The following\\ncode produces the plot in \\nFigure 3-7\\n. To find the point that corresponds to\\n90% precision, we need to look for the index of the desired threshold. Since\\nthresholds are listed in decreasing order in this case, we use \\n<=\\n instead of \\n>=\\non the first line:\\nidx_for_threshold_at_90\\n \\n=\\n \\n(\\nthresholds\\n \\n<=\\n \\nthreshold_for_90_precision\\n)\\n.\\nargmax\\n()\\ntpr_90\\n,\\n \\nfpr_90\\n \\n=\\n \\ntpr\\n[\\nidx_for_threshold_at_90\\n],\\n \\nfpr\\n[\\nidx_for_threshold_at_90\\n]\\nplt\\n.\\nplot\\n(\\nfpr\\n,\\n \\ntpr\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n \\nlabel\\n=\\n\"ROC curve\"\\n)\\nplt\\n.\\nplot\\n([\\n0\\n,\\n \\n1\\n],\\n \\n[\\n0\\n,\\n \\n1\\n],\\n \\n\\'k:\\'\\n,\\n \\nlabel\\n=\\n\"Random classifier\\'s ROC curve\"\\n)\\nplt\\n.\\nplot\\n([\\nfpr_90\\n],\\n \\n[\\ntpr_90\\n],\\n \\n\"ko\"\\n,\\n \\nlabel\\n=\\n\"Threshold for 90% precision\"\\n)\\n[\\n...\\n]\\n  \\n# beautify the figure: add labels, grid, legend, arrow, and text\\nplt\\n.\\nshow\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 197, 'page_label': '198', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-7. \\nA ROC curve plotting the false positive rate against the true positive rate for all possible\\nthresholds; the black circle highlights the chosen ratio (at 90% precision and 48% recall)\\nOnce again there is a trade-off: the higher the recall (TPR), the more false\\npositives (FPR) the classifier produces. The dotted line represents the ROC\\ncurve of a purely random classifier; a good classifier stays as far away from\\nthat line as possible (toward the top-left corner).\\nOne way to compare classifiers is to measure the \\narea under the curve\\n(AUC). \\nA perfect classifier will have a ROC AUC equal to 1, whereas a\\npurely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn\\nprovides a function to estimate the ROC AUC:\\n>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nroc_auc_score\\n>>> \\nroc_auc_score\\n(\\ny_train_5\\n,\\n \\ny_scores\\n)\\n0.9604938554008616'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 198, 'page_label': '199', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='TIP\\nSince the ROC curve is so similar to the precision/recall (PR) curve, you may wonder how\\nto decide which one to use. As a rule of thumb, you should prefer the PR curve whenever\\nthe positive class is rare or when you care more about the false positives than the false\\nnegatives. Otherwise, use the ROC curve. For example, looking at the previous ROC\\ncurve (and the ROC AUC score), you may think that the classifier is really good. But this\\nis mostly because there are few positives (5s) compared to the negatives (non-5s). In\\ncontrast, the PR curve makes it clear that the classifier has room for improvement: the\\ncurve could really be closer to the top-right corner (see \\nFigure 3-6\\n again).\\nLet’s now create a \\nRandomForestClassifier\\n, whose PR curve and F\\n score we\\ncan compare to those of the \\nSGDClassifier\\n:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nRandomForestClassifier\\nforest_clf\\n \\n=\\n \\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\nThe \\nprecision_recall_curve()\\n function expects labels and scores for each\\ninstance, so we need to train the random forest classifier and make it assign a\\nscore to each instance. \\nBut the \\nRandomForestClassifier\\n class does not have a\\ndecision_function()\\n method, due to the way it works (we will cover this in\\nChapter 7\\n). Luckily, it has a \\npredict_proba()\\n method that returns class\\nprobabilities for each instance, and we can just use the probability of the\\npositive class as a score, so it will work fine.\\n \\nWe can call the\\ncross_val_predict()\\n function to train the \\nRandomForestClassifier\\n using cross-\\nvalidation and make it predict class probabilities for every image as \\nfollows:\\ny_probas_forest\\n \\n=\\n \\ncross_val_predict\\n(\\nforest_clf\\n,\\n \\nX_train\\n,\\n \\ny_train_5\\n,\\n \\ncv\\n=\\n3\\n,\\n                                    \\nmethod\\n=\\n\"predict_proba\"\\n)\\nLet’s look at the class probabilities for the first two images in the training set:\\n>>> \\ny_probas_forest\\n[:\\n2\\n]\\narray([[0.11, 0.89],\\n       [0.99, 0.01]])\\nThe model predicts that the first image is positive with 89% probability, and\\n1\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 199, 'page_label': '200', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='it predicts that the second image is negative with 99% probability. Since each\\nimage is either positive or negative, the probabilities in each row add up to\\n100%.\\nWARNING\\nThese are \\nestimated\\n probabilities, not actual probabilities. For example, if you look at all\\nthe images that the model classified as positive with an estimated probability between 50%\\nand 60%, roughly 94% of them are actually positive. \\nSo, the model’s estimated\\nprobabilities were much too low in this case—but models can be overconfident as well.\\nThe \\nsklearn.calibration\\n package contains tools to calibrate the estimated probabilities and\\nmake them much closer to actual probabilities. See the extra material section in \\nthis\\nchapter’s notebook\\n for more details.\\nThe second column contains the estimated probabilities for the positive class,\\nso let’s pass them to the \\nprecision_recall_curve()\\n function:\\ny_scores_forest\\n \\n=\\n \\ny_probas_forest\\n[:,\\n \\n1\\n]\\nprecisions_forest\\n,\\n \\nrecalls_forest\\n,\\n \\nthresholds_forest\\n \\n=\\n \\nprecision_recall_curve\\n(\\n    \\ny_train_5\\n,\\n \\ny_scores_forest\\n)\\nNow we’re ready to plot the PR curve. It is useful to plot the first PR curve as\\nwell to see how they compare (\\nFigure 3-8\\n):\\nplt\\n.\\nplot\\n(\\nrecalls_forest\\n,\\n \\nprecisions_forest\\n,\\n \\n\"b-\"\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n         \\nlabel\\n=\\n\"Random Forest\"\\n)\\nplt\\n.\\nplot\\n(\\nrecalls\\n,\\n \\nprecisions\\n,\\n \\n\"--\"\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n \\nlabel\\n=\\n\"SGD\"\\n)\\n[\\n...\\n]\\n  \\n# beautify the figure: add labels, grid, and legend\\nplt\\n.\\nshow\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 200, 'page_label': '201', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-8. \\nComparing PR curves: the random forest classifier is superior to the SGD classifier\\nbecause its PR curve is much closer to the top-right corner, and it has a greater AUC\\nAs you can see in \\nFigure 3-8\\n, the \\nRandomForestClassifier\\n’s PR curve looks\\nmuch better than the \\nSGDClassifier\\n’s: it comes much closer to the top-right\\ncorner. Its F\\n score and ROC AUC score are also significantly better:\\n>>> \\ny_train_pred_forest\\n \\n=\\n \\ny_probas_forest\\n[:,\\n \\n1\\n]\\n \\n>=\\n \\n0.5\\n  \\n# positive proba ≥ 50%\\n>>> \\nf1_score\\n(\\ny_train_5\\n,\\n \\ny_pred_forest\\n)\\n0.9242275142688446\\n>>> \\nroc_auc_score\\n(\\ny_train_5\\n,\\n \\ny_scores_forest\\n)\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find about 99.1%\\nprecision and 86.6% recall. Not too bad!\\nYou now know how to train binary classifiers, choose the appropriate metric\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 201, 'page_label': '202', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='for your task, evaluate your classifiers using cross-validation, select the\\nprecision/recall trade-off that fits your needs, and use several metrics and\\ncurves to compare various models. You’re ready to try to detect more than\\njust the 5s.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 202, 'page_label': '203', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Multiclass Classification\\nWhereas binary classifiers distinguish between two classes, \\nmulticlass\\nclassifiers\\n (also called \\nmultinomial classifiers\\n) can distinguish between more\\nthan two classes.\\nSome Scikit-Learn classifiers (e.g., \\nLogisticRegression\\n,\\nRandomForestClassifier\\n, and \\nGaussianNB\\n) are capable of handling multiple\\nclasses natively. Others are strictly binary classifiers (e.g., \\nSGDClassifier\\n and\\nSVC\\n). However, there are various strategies that you can use to perform\\nmulticlass classification with multiple binary classifiers.\\nOne way to create a system that can classify the digit images into 10 classes\\n(from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector,\\na 1-detector, a 2-detector, and so on). \\nThen when you want to classify an\\nimage, you get the decision score from each classifier for that image and you\\nselect the class whose classifier outputs the highest score. This is called the\\none-versus-the-rest\\n (OvR) strategy, or sometimes \\none-versus-all\\n (OvA).\\nAnother strategy is to train a binary classifier for every pair of digits: one to\\ndistinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s,\\nand so on. \\nThis is called the \\none-versus-one\\n (OvO) strategy. If there are \\nN\\nclasses, you need to train \\nN\\n × (\\nN\\n – 1) / 2 classifiers. For the MNIST problem,\\nthis means training 45 binary classifiers! When you want to classify an\\nimage, you have to run the image through all 45 classifiers and see which\\nclass wins the most duels. The main advantage of OvO is that each classifier\\nonly needs to be trained on the part of the training set containing the two\\nclasses that it must distinguish.\\nSome algorithms (such as support vector machine classifiers) scale poorly\\nwith the size of the training set. For these algorithms OvO is preferred\\nbecause it is faster to train many classifiers on small training sets than to train\\nfew classifiers on large training sets. For most binary classification\\nalgorithms, however, OvR is preferred.\\nScikit-Learn detects when you try to use a binary classification algorithm for'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 203, 'page_label': '204', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"a multiclass classification task, and it automatically runs OvR or OvO,\\ndepending on the algorithm. \\nLet’s try this with a support vector machine\\nclassifier using the \\nsklearn.svm.SVC\\n class (see \\nChapter 5\\n). We’ll only train\\non the first 2,000 images, or else it will take a very long time:\\nfrom\\n \\nsklearn.svm\\n \\nimport\\n \\nSVC\\nsvm_clf\\n \\n=\\n \\nSVC\\n(\\nrandom_state\\n=\\n42\\n)\\nsvm_clf\\n.\\nfit\\n(\\nX_train\\n[:\\n2000\\n],\\n \\ny_train\\n[:\\n2000\\n])\\n  \\n# y_train, not y_train_5\\nThat was easy! We trained the \\nSVC\\n using the original target classes from 0 to\\n9 (\\ny_train\\n), instead of the 5-versus-the-rest target classes (\\ny_train_5\\n). Since\\nthere are 10 classes (i.e., more than 2), Scikit-Learn used the OvO strategy\\nand trained 45 binary classifiers. Now let’s make a prediction on an image:\\n>>> \\nsvm_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray(['5'], dtype=object)\\nThat’s correct! This code actually made 45 predictions—one per pair of\\nclasses—and it selected the class that won the most duels. If you call the\\ndecision_function()\\n method, you will see that it returns 10 scores per\\ninstance: one per class. Each class gets a score equal to the number of won\\nduels plus or minus a small tweak (max ±0.33) to break ties, based on the\\nclassifier scores:\\n>>> \\nsome_digit_scores\\n \\n=\\n \\nsvm_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n>>> \\nsome_digit_scores\\n.\\nround\\n(\\n2\\n)\\narray([[ 3.79,  0.73,  6.06,  8.3 , -0.29,  9.3 ,  1.75,  2.77,  7.21,\\n         4.82]])\\nThe highest score is 9.3, and it’s indeed the one corresponding to class 5:\\n>>> \\nclass_id\\n \\n=\\n \\nsome_digit_scores\\n.\\nargmax\\n()\\n>>> \\nclass_id\\n5\\nWhen a classifier is trained, it stores the list of target classes in its \\nclasses_\\nattribute, ordered by value. In the case of MNIST, the index of each class in\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 204, 'page_label': '205', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"the \\nclasses_\\n array conveniently matches the class itself (e.g., the class at\\nindex 5 happens to be class \\n'5'\\n), but in general you won’t be so lucky; you\\nwill need to look up the class label like this:\\n>>> \\nsvm_clf\\n.\\nclasses_\\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)\\n>>> \\nsvm_clf\\n.\\nclasses_\\n[\\nclass_id\\n]\\n'5'\\nIf you want to force Scikit-Learn to use one-versus-one or one-versus-the-\\nrest, you can use the \\nOneVsOneClassifier\\n or \\nOneVsRestClassifier\\n classes.\\nSimply create an instance and pass a classifier to its constructor (it doesn’t\\neven have to be a binary classifier). For example, this code creates a\\nmulticlass classifier using the OvR strategy, based on an \\nSVC\\n:\\nfrom\\n \\nsklearn.multiclass\\n \\nimport\\n \\nOneVsRestClassifier\\novr_clf\\n \\n=\\n \\nOneVsRestClassifier\\n(\\nSVC\\n(\\nrandom_state\\n=\\n42\\n))\\novr_clf\\n.\\nfit\\n(\\nX_train\\n[:\\n2000\\n],\\n \\ny_train\\n[:\\n2000\\n])\\nLet’s make a prediction, and check the number of trained classifiers:\\n>>> \\novr_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray(['5'], dtype='<U1')\\n>>> \\nlen\\n(\\novr_clf\\n.\\nestimators_\\n)\\n10\\nTraining an \\nSGDClassifier\\n on a multiclass dataset and using it to make\\npredictions is just as easy:\\n>>> \\nsgd_clf\\n \\n=\\n \\nSGDClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\n>>> \\nsgd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\n>>> \\nsgd_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray(['3'], dtype='<U1')\\nOops, that’s incorrect. Prediction errors do happen! This time Scikit-Learn\\nused the OvR strategy under the hood: since there are 10 classes, it trained 10\\nbinary classifiers. The \\ndecision_function()\\n method now returns one value per\\nclass. Let’s look at the scores that the SGD classifier assigned to each class:\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 205, 'page_label': '206', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\nsgd_clf\\n.\\ndecision_function\\n([\\nsome_digit\\n])\\n.\\nround\\n()\\narray([[-31893., -34420.,  -9531.,   1824., -22320.,  -1386., -26189.,\\n        -16148.,  -4604., -12051.]])\\nYou can see that the classifier is not very confident about its prediction:\\nalmost all scores are very negative, while class 3 has a score of +1,824, and\\nclass 5 is not too far behind at –1,386. Of course, you’ll want to evaluate this\\nclassifier on more than one image. Since there are roughly the same number\\nof images in each class, the accuracy metric is fine. As usual, you can use the\\ncross_val_score()\\n function to evaluate the model:\\n>>> \\ncross_val_score\\n(\\nsgd_clf\\n,\\n \\nX_train\\n,\\n \\ny_train\\n,\\n \\ncv\\n=\\n3\\n,\\n \\nscoring\\n=\\n\"accuracy\"\\n)\\narray([0.87365, 0.85835, 0.8689 ])\\nIt gets over 85.8% on all test folds. If you used a random classifier, you\\nwould get 10% accuracy, so this is not such a bad score, but you can still do\\nmuch better. Simply scaling the inputs (as discussed in \\nChapter 2\\n) increases\\naccuracy above 89.1%:\\n>>> \\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nStandardScaler\\n>>> \\nscaler\\n \\n=\\n \\nStandardScaler\\n()\\n>>> \\nX_train_scaled\\n \\n=\\n \\nscaler\\n.\\nfit_transform\\n(\\nX_train\\n.\\nastype\\n(\\n\"float64\"\\n))\\n>>> \\ncross_val_score\\n(\\nsgd_clf\\n,\\n \\nX_train_scaled\\n,\\n \\ny_train\\n,\\n \\ncv\\n=\\n3\\n,\\n \\nscoring\\n=\\n\"accuracy\"\\n)\\narray([0.8983, 0.891 , 0.9018])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 206, 'page_label': '207', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Error Analysis\\nIf this were a real project, you would now follow the steps in your machine\\nlearning project checklist (see \\nAppendix A\\n). You’d explore data preparation\\noptions, try out multiple models, shortlist the best ones, fine-tune their\\nhyperparameters using \\nGridSearchCV\\n, and automate as much as possible.\\nHere, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors\\nit makes.\\nFirst, look at the confusion matrix. \\nFor this, you first need to make\\npredictions using the \\ncross_val_predict()\\n function; then you can pass the\\nlabels and predictions to the \\nconfusion_matrix()\\n function, just like you did\\nearlier. However, since there are now 10 classes instead of 2, the confusion\\nmatrix will contain quite a lot of numbers, and it may be hard to read.\\nA colored diagram of the confusion matrix is much easier to analyze. \\nTo plot\\nsuch a diagram, use the \\nConfusionMatrixDisplay.from_predictions()\\n function\\nlike this:\\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nConfusionMatrixDisplay\\ny_train_pred\\n \\n=\\n \\ncross_val_predict\\n(\\nsgd_clf\\n,\\n \\nX_train_scaled\\n,\\n \\ny_train\\n,\\n \\ncv\\n=\\n3\\n)\\nConfusionMatrixDisplay\\n.\\nfrom_predictions\\n(\\ny_train\\n,\\n \\ny_train_pred\\n)\\nplt\\n.\\nshow\\n()\\nThis produces the left diagram in \\nFigure 3-9\\n. This confusion matrix looks\\npretty good: most images are on the main diagonal, which means that they\\nwere classified correctly. Notice that the cell on the diagonal in row #5 and\\ncolumn #5 looks slightly darker than the other digits. This could be because\\nthe model made more errors on 5s, or because there are fewer 5s in the\\ndataset than the other digits. That’s why it’s important to normalize the\\nconfusion matrix by dividing each value by the total number of images in the\\ncorresponding (true) class (i.e., divide by the row’s sum). This can be done\\nsimply by setting \\nnormalize=\"true\"\\n. We can also specify the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 207, 'page_label': '208', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='values_format=\".0%\"\\n argument to show percentages with no decimals. The\\nfollowing code produces the diagram on the right in \\nFigure 3-9\\n:\\nConfusionMatrixDisplay\\n.\\nfrom_predictions\\n(\\ny_train\\n,\\n \\ny_train_pred\\n,\\n                                        \\nnormalize\\n=\\n\"true\"\\n,\\n \\nvalues_format\\n=\\n\".0%\"\\n)\\nplt\\n.\\nshow\\n()\\nNow we can easily see that only 82% of the images of 5s were classified\\ncorrectly. The most common error the model made with images of 5s was to\\nmisclassify them as 8s: this happened for 10% of all 5s. But only 2% of 8s\\ngot misclassified as 5s; confusion matrices are generally not symmetrical! If\\nyou look carefully, you will notice that many digits have been misclassified\\nas 8s, but this is not immediately obvious from this diagram. If you want to\\nmake the errors stand out more, you can try putting zero weight on the correct\\npredictions. The following code does just that and produces the diagram on\\nthe left in \\nFigure 3-10\\n:\\nsample_weight\\n \\n=\\n \\n(\\ny_train_pred\\n \\n!=\\n \\ny_train\\n)\\nConfusionMatrixDisplay\\n.\\nfrom_predictions\\n(\\ny_train\\n,\\n \\ny_train_pred\\n,\\n                                        \\nsample_weight\\n=\\nsample_weight\\n,\\n                                        \\nnormalize\\n=\\n\"true\"\\n,\\n \\nvalues_format\\n=\\n\".0%\"\\n)\\nplt\\n.\\nshow\\n()\\nFigure 3-9. \\nConfusion matrix (left) and the same CM normalized by row (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 208, 'page_label': '209', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-10. \\nConfusion matrix with errors only, normalized by row (left) and by column (right)\\nNow you can see much more clearly the kinds of errors the classifier makes.\\nThe column for class 8 is now really bright, which confirms that many\\nimages got misclassified as 8s. In fact this is the most common\\nmisclassification for almost all classes. But be careful how you interpret the\\npercentages in this diagram: remember that we’ve excluded the correct\\npredictions. For example, the 36% in row #7, column #9 does \\nnot\\n mean that\\n36% of all images of 7s were misclassified as 9s. It means that 36% of the\\nerrors\\n the model made on images of 7s were misclassifications as 9s. In\\nreality, only 3% of images of 7s were misclassified as 9s, as you can see in\\nthe diagram on the right in \\nFigure 3-9\\n.\\nIt is also possible to normalize the confusion matrix by column rather than by\\nrow: if you set \\nnormalize=\"pred\"\\n, you get the diagram on the right in\\nFigure 3-10\\n. For example, you can see that 56% of misclassified 7s are\\nactually 9s.\\nAnalyzing the confusion matrix often gives you insights into ways to improve\\nyour classifier. Looking at these plots, it seems that your efforts should be\\nspent on reducing the false 8s. For example, you could try to gather more\\ntraining data for digits that look like 8s (but are not) so that the classifier can\\nlearn to distinguish them from real 8s. Or you could engineer new features\\nthat would help the classifier—for example, writing an algorithm to count the\\nnumber of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you could'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 209, 'page_label': '210', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make\\nsome patterns, such as closed loops, stand out more.\\nAnalyzing individual errors can also be a good way to gain insights into what\\nyour classifier is doing and why it is failing. For example, let’s plot examples\\nof 3s and 5s in a confusion matrix style (\\nFigure 3-11\\n):\\ncl_a\\n,\\n \\ncl_b\\n \\n=\\n \\n'3'\\n,\\n \\n'5'\\nX_aa\\n \\n=\\n \\nX_train\\n[(\\ny_train\\n \\n==\\n \\ncl_a\\n)\\n \\n&\\n \\n(\\ny_train_pred\\n \\n==\\n \\ncl_a\\n)]\\nX_ab\\n \\n=\\n \\nX_train\\n[(\\ny_train\\n \\n==\\n \\ncl_a\\n)\\n \\n&\\n \\n(\\ny_train_pred\\n \\n==\\n \\ncl_b\\n)]\\nX_ba\\n \\n=\\n \\nX_train\\n[(\\ny_train\\n \\n==\\n \\ncl_b\\n)\\n \\n&\\n \\n(\\ny_train_pred\\n \\n==\\n \\ncl_a\\n)]\\nX_bb\\n \\n=\\n \\nX_train\\n[(\\ny_train\\n \\n==\\n \\ncl_b\\n)\\n \\n&\\n \\n(\\ny_train_pred\\n \\n==\\n \\ncl_b\\n)]\\n[\\n...\\n]\\n  \\n# plot all images in X_aa, X_ab, X_ba, X_bb in a confusion matrix style\\nFigure 3-11. \\nSome images of 3s and 5s organized like a confusion matrix\\nAs you can see, some of the digits that the classifier gets wrong (i.e., in the\\nbottom-left and top-right blocks) are so badly written that even a human\\nwould have trouble classifying them. However, most misclassified images\\nseem like obvious errors to us. It may be hard to understand why the\\nclassifier made the mistakes it did, but remember that the human brain is a\\nfantastic pattern recognition system, and our visual system does a lot of\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 210, 'page_label': '211', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='complex preprocessing before any information even reaches our\\nconsciousness. So, the fact that this task feels simple does not mean that it is.\\nRecall that we used a simple \\nSGDClassifier\\n, which is just a linear model: all\\nit does is assign a weight per class to each pixel, and when it sees a new\\nimage it just sums up the weighted pixel intensities to get a score for each\\nclass. Since 3s and 5s differ by only a few pixels, this model will easily\\nconfuse them.\\nThe main difference between 3s and 5s is the position of the small line that\\njoins the top line to the bottom arc. If you draw a 3 with the junction slightly\\nshifted to the left, the classifier might classify it as a 5, and vice versa. In\\nother words, this classifier is quite sensitive to image shifting and rotation.\\nOne way to reduce the 3/5 confusion is to preprocess the images to ensure\\nthat they are well centered and not too rotated. However, this may not be easy\\nsince it requires predicting the correct rotation of each image. A much\\nsimpler approach consists of augmenting the training set with slightly shifted\\nand rotated variants of the training images. This will force the model to learn\\nto be more tolerant to such variations. This is called \\ndata augmentation\\n (we’ll\\ncover this in \\nChapter 14\\n; also see exercise 2 at the end of this chapter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 211, 'page_label': '212', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"Multilabel Classification\\nUntil now, each instance has always been assigned to just one class. But in\\nsome cases you may want your classifier to output multiple classes for each\\ninstance. \\nConsider a face-recognition classifier: what should it do if it\\nrecognizes several people in the same picture? It should attach one tag per\\nperson it recognizes. Say the classifier has been trained to recognize three\\nfaces: Alice, Bob, and Charlie. Then when the classifier is shown a picture of\\nAlice and Charlie, it should output \\n[True, False, True]\\n (meaning “Alice yes,\\nBob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a \\nmultilabel classification\\n system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler\\nexample, just for illustration purposes:\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\nfrom\\n \\nsklearn.neighbors\\n \\nimport\\n \\nKNeighborsClassifier\\ny_train_large\\n \\n=\\n \\n(\\ny_train\\n \\n>=\\n \\n'7'\\n)\\ny_train_odd\\n \\n=\\n \\n(\\ny_train\\n.\\nastype\\n(\\n'int8'\\n)\\n \\n%\\n \\n2\\n \\n==\\n \\n1\\n)\\ny_multilabel\\n \\n=\\n \\nnp\\n.\\nc_\\n[\\ny_train_large\\n,\\n \\ny_train_odd\\n]\\nknn_clf\\n \\n=\\n \\nKNeighborsClassifier\\n()\\nknn_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_multilabel\\n)\\nThis code creates a \\ny_multilabel\\n array containing two target labels for each\\ndigit image: the first indicates whether or not the digit is large (7, 8, or 9), and\\nthe second indicates whether or not it is odd. Then the code creates a\\nKNeighborsClassifier\\n instance, which supports multilabel classification (not\\nall classifiers do), and trains this model using the multiple targets array. Now\\nyou can make a prediction, and notice that it outputs two labels:\\n>>> \\nknn_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large (\\nFalse\\n) and odd (\\nTrue\\n).\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 212, 'page_label': '213', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='There are many ways to evaluate a multilabel classifier, and selecting the\\nright metric really depends on your project. One approach is to measure the\\nF\\n score for each individual label (or any other binary classifier metric\\ndiscussed earlier), then simply compute the average score. \\nThe following\\ncode computes the average F\\n score across all labels:\\n>>> \\ny_train_knn_pred\\n \\n=\\n \\ncross_val_predict\\n(\\nknn_clf\\n,\\n \\nX_train\\n,\\n \\ny_multilabel\\n,\\n \\ncv\\n=\\n3\\n)\\n>>> \\nf1_score\\n(\\ny_multilabel\\n,\\n \\ny_train_knn_pred\\n,\\n \\naverage\\n=\\n\"macro\"\\n)\\n0.976410265560605\\nThis approach assumes that all labels are equally important, which may not\\nbe the case. In particular, if you have many more pictures of Alice than of\\nBob or Charlie, you may want to give more weight to the classifier’s score on\\npictures of Alice. One simple option is to give each label a weight equal to its\\nsupport\\n (i.e., the number of instances with that target label). To do this,\\nsimply set \\naverage=\"weighted\"\\n when calling the \\nf1_score()\\n function.\\n\\u2060\\nIf you wish to use a classifier that does not natively support multilabel\\nclassification, such as \\nSVC\\n, one possible strategy is to train one model per\\nlabel. However, this strategy may have a hard time capturing the\\ndependencies between the labels. For example, a large digit (7, 8, or 9) is\\ntwice more likely to be odd than even, but the classifier for the “odd” label\\ndoes not know what the classifier for the “large” label predicted. To solve this\\nissue, the models can be organized in a chain: when a model makes a\\nprediction, it uses the input features plus all the predictions of the models that\\ncome before it in the chain.\\nThe good news is that Scikit-Learn has a class called \\nChainClassifier\\n that\\ndoes just that! \\nBy default it will use the true labels for training, feeding each\\nmodel the appropriate labels depending on their position in the chain. But if\\nyou set the \\ncv\\n hyperparameter, it will use cross-validation to get “clean” (out-\\nof-sample) predictions from each trained model for every instance in the\\ntraining set, and these predictions will then be used to train all the models\\nlater in the chain. Here’s an example showing how to create and train a\\nChainClassifier\\n using the cross-validation strategy. As earlier, we’ll just use\\nthe first 2,000 images in the training set to speed things up:\\n1\\n1\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 213, 'page_label': '214', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='from\\n \\nsklearn.multioutput\\n \\nimport\\n \\nClassifierChain\\nchain_clf\\n \\n=\\n \\nClassifierChain\\n(\\nSVC\\n(),\\n \\ncv\\n=\\n3\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nchain_clf\\n.\\nfit\\n(\\nX_train\\n[:\\n2000\\n],\\n \\ny_multilabel\\n[:\\n2000\\n])\\nNow we can use this \\nChainClassifier\\n to make predictions:\\n>>> \\nchain_clf\\n.\\npredict\\n([\\nsome_digit\\n])\\narray([[0., 1.]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 214, 'page_label': '215', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Multioutput Classification\\nThe last type of classification task we’ll discuss here is called \\nmultioutput–\\nmulticlass classification\\n (or just \\nmultioutput classification\\n). \\nIt is a\\ngeneralization of multilabel classification where each label can be multiclass\\n(i.e., it can have more than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will\\ntake as input a noisy digit image, and it will (hopefully) output a clean digit\\nimage, represented as an array of pixel intensities, just like the MNIST\\nimages. Notice that the classifier’s output is multilabel (one label per pixel)\\nand each label can have multiple values (pixel intensity ranges from 0 to\\n255). This is thus an example of a multioutput classification system.\\nNOTE\\nThe line between classification and regression is sometimes blurry, such as in this\\nexample. Arguably, predicting pixel intensity is more akin to regression than to\\nclassification. Moreover, multioutput systems are not limited to classification tasks; you\\ncould even have a system that outputs multiple labels per instance, including both class\\nlabels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images\\nand adding noise to their pixel intensities with NumPy’s \\nrandint()\\n function.\\nThe target images will be the original images:\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\n  \\n# to make this code example reproducible\\nnoise\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\n0\\n,\\n \\n100\\n,\\n \\n(\\nlen\\n(\\nX_train\\n),\\n \\n784\\n))\\nX_train_mod\\n \\n=\\n \\nX_train\\n \\n+\\n \\nnoise\\nnoise\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\n0\\n,\\n \\n100\\n,\\n \\n(\\nlen\\n(\\nX_test\\n),\\n \\n784\\n))\\nX_test_mod\\n \\n=\\n \\nX_test\\n \\n+\\n \\nnoise\\ny_train_mod\\n \\n=\\n \\nX_train\\ny_test_mod\\n \\n=\\n \\nX_test\\nLet’s take a peek at the first image from the test set (\\nFigure 3-12\\n). Yes, we’re\\nsnooping on the test data, so you should be frowning right now.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 215, 'page_label': '216', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-12. \\nA noisy image (left) and the target clean image (right)\\nOn the left is the noisy input image, and on the right is the clean target image.\\nNow let’s train the classifier and make it clean up this image (\\nFigure 3-13\\n):\\nknn_clf\\n \\n=\\n \\nKNeighborsClassifier\\n()\\nknn_clf\\n.\\nfit\\n(\\nX_train_mod\\n,\\n \\ny_train_mod\\n)\\nclean_digit\\n \\n=\\n \\nknn_clf\\n.\\npredict\\n([\\nX_test_mod\\n[\\n0\\n]])\\nplot_digit\\n(\\nclean_digit\\n)\\nplt\\n.\\nshow\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 216, 'page_label': '217', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 3-13. \\nThe cleaned-up image\\nLooks close enough to the target! This concludes our tour of classification.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 217, 'page_label': '218', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='You now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall trade-off, compare classifiers, and more generally\\nbuild good classification systems for a variety of tasks. In the next chapters,\\nyou’ll learn how all these machine learning models you’ve been using\\nactually work.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 218, 'page_label': '219', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nTry to build a classifier for the MNIST dataset that achieves over 97%\\naccuracy on the test set. Hint: the \\nKNeighborsClassifier\\n works quite well\\nfor this task; you just need to find good hyperparameter values (try a\\ngrid search on the \\nweights\\n and \\nn_neighbors\\n hyperparameters).\\n2\\n. \\nWrite a function that can shift an MNIST image in any direction (left,\\nright, up, or down) by one pixel.\\n\\u2060\\n Then, for each image in the training\\nset, create four shifted copies (one per direction) and add them to the\\ntraining set. Finally, train your best model on this expanded training set\\nand measure its accuracy on the test set. You should observe that your\\nmodel performs even better now! \\nThis technique of artificially growing\\nthe training set is called \\ndata augmentation\\n or \\ntraining set expansion\\n.\\n3\\n. \\nTackle the Titanic dataset. A great place to start is on \\nKaggle\\n.\\nAlternatively, you can download the data from\\nhttps://homl.info/titanic.tgz\\n and unzip this tarball like you did for the\\nhousing data in \\nChapter 2\\n. This will give you two CSV files, \\ntrain.csv\\nand \\ntest.csv\\n, which you can load using \\npandas.read_csv()\\n. The goal is to\\ntrain a classifier that can predict the \\nSurvived\\n column based on the other\\ncolumns.\\n4\\n. \\nBuild a spam classifier (a more challenging exercise):\\na\\n. \\nDownload examples of spam and ham from \\nApache\\nSpamAssassin’s public datasets\\n.\\nb\\n. \\nUnzip the datasets and familiarize yourself with the data format.\\nc\\n. \\nSplit the data into a training set and a test set.\\nd\\n. \\nWrite a data preparation pipeline to convert each email into a\\nfeature vector. Your preparation pipeline should transform an email\\ninto a (sparse) vector that indicates the presence or absence of each\\npossible word. For example, if all emails only ever contain four\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 219, 'page_label': '220', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='words, “Hello”, “how”, “are”, “you”, then the email “Hello you\\nHello Hello you” would be converted into a vector [1, 0, 0, 1]\\n(meaning [“Hello” is present, “how” is absent, “are” is absent,\\n“you” is present]), or [3, 0, 0, 2] if you prefer to count the number\\nof occurrences of each word.\\nYou may want to add hyperparameters to your preparation pipeline\\nto control whether or not to strip off email headers, convert each\\nemail to lowercase, remove punctuation, replace all URLs with\\n“URL”, replace all numbers with “NUMBER”, or even perform\\nstemming\\n (i.e., trim off word endings; there are Python libraries\\navailable to do this).\\ne\\n. \\nFinally, try out several classifiers and see if you can build a great\\nspam classifier, with both high recall and high precision.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n By default Scikit-Learn caches downloaded datasets in a directory called \\nscikit_learn_data\\n in\\nyour home directory.\\n2\\n Datasets returned by \\nfetch_openml()\\n are not always shuffled or split.\\n3\\n Shuffling may be a bad idea in some contexts—for example, if you are working on time series\\ndata (such as stock market prices or weather conditions). We will explore this in \\nChapter 15\\n.\\n4\\n Scikit-Learn classifiers always have either a \\ndecision_function()\\n method or a \\npredict_proba()\\nmethod, or sometimes both.\\n5\\n Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the\\ndocumentation for more details.\\n6\\n You can use the \\nshift()\\n function from the \\nscipy.ndimage.interpolation\\n module. For example,\\nshift(image, [2, 1], cval=0)\\n shifts the image two pixels down and one pixel to the right.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 220, 'page_label': '221', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 4. \\nTraining Models\\nSo far we have treated machine learning models and their training algorithms\\nmostly like black boxes. \\nIf you went through some of the exercises in the\\nprevious chapters, you may have been surprised by how much you can get\\ndone without knowing anything about what’s under the hood: you optimized\\na regression system, you improved a digit image classifier, and you even built\\na spam classifier from scratch, all without knowing how they actually work.\\nIndeed, in many situations you don’t really need to know the implementation\\ndetails.\\nHowever, having a good understanding of how things work can help you\\nquickly home in on the appropriate model, the right training algorithm to use,\\nand a good set of hyperparameters for your task. Understanding what’s under\\nthe hood will also help you debug issues and perform error analysis more\\nefficiently. Lastly, most of the topics discussed in this chapter will be\\nessential in understanding, building, and training neural networks (discussed\\nin \\nPart II\\n of this book).\\nIn this chapter we will start by looking at the linear regression model, one of\\nthe simplest models there is. \\nWe will discuss two very different ways to train\\nit:\\nUsing a “closed-form” equation\\n\\u2060\\n that directly computes the model\\nparameters that best fit the model to the training set (i.e., the model\\nparameters that minimize the cost function over the training set).\\nUsing an iterative optimization approach called gradient descent (GD)\\nthat gradually tweaks the model parameters to minimize the cost\\nfunction over the training set, eventually converging to the same set of\\nparameters as the first method. We will look at a few variants of gradient\\ndescent that we will use again and again when we study neural networks\\nin \\nPart II\\n: batch GD, mini-batch GD, and stochastic GD.\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 221, 'page_label': '222', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Next we will look at polynomial regression, a more complex model that can\\nfit nonlinear datasets. \\nSince this model has more parameters than linear\\nregression, it is more prone to overfitting the training data. We will explore\\nhow to detect whether or not this is the case using learning curves, and then\\nwe will look at several regularization techniques that can reduce the risk of\\noverfitting the training set.\\nFinally, we will examine two more models that are commonly used for\\nclassification tasks: logistic regression and softmax regression.\\nWARNING\\nThere will be quite a few math equations in this chapter, using basic notions of linear\\nalgebra and calculus. To understand these equations, you will need to know what vectors\\nand matrices are; how to transpose them, multiply them, and inverse them; and what\\npartial derivatives are. If you are unfamiliar with these concepts, please go through the\\nlinear algebra and calculus introductory tutorials available as Jupyter notebooks in the\\nonline supplemental material\\n. For those who are truly allergic to mathematics, you should\\nstill go through this chapter and simply skip the equations; hopefully, the text will be\\nsufficient to help you understand most of the concepts.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 222, 'page_label': '223', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Linear Regression\\nIn \\nChapter 1\\n we looked at a simple regression model of life satisfaction:\\nlife_satisfaction\\n = \\nθ\\n + \\nθ\\n × \\nGDP_per_capita\\nThis model is just a linear function of the input feature \\nGDP_per_capita\\n. \\nθ\\nand \\nθ\\n are the model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a\\nweighted sum of the input features, plus a constant called the \\nbias term\\n (also\\ncalled the \\nintercept term\\n), as shown in \\nEquation 4-1\\n.\\nEquation 4-1. \\nLinear regression model prediction\\ny ^ \\n= \\nθ 0 \\n+ \\nθ 1 \\nx 1 \\n+ \\nθ 2 \\nx 2 \\n+ \\n⋯\\n \\n+ \\nθ n \\nx n\\nIn this equation:\\nŷ\\n is the predicted value.\\nn\\n is the number of features.\\nx\\n is the \\ni\\n feature value.\\nθ\\n is the \\nj\\n model parameter, including the bias term \\nθ\\n and the feature\\nweights \\nθ\\n, \\nθ\\n, \\n⋯\\n, \\nθ\\n.\\nThis can be written much more concisely using a vectorized form, as shown\\nin \\nEquation 4-2\\n.\\nEquation 4-2. \\nLinear regression model prediction (vectorized form)\\ny^=hθ(x)=θ·x\\nIn this equation:\\nh\\n is the hypothesis function, using the model parameters \\nθ\\n.\\nθ\\n is the model’s \\nparameter vector\\n, containing the bias term \\nθ\\n and the\\nfeature weights \\nθ\\n to \\nθ\\n.\\n0\\n1\\n0\\n1\\ni\\nth\\nj\\nth\\n0\\n1\\n2\\nn\\nθ\\n0\\n1\\nn'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 223, 'page_label': '224', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='x\\n is the instance’s \\nfeature vector\\n, containing \\nx\\n to \\nx\\n, with \\nx\\n always\\nequal to 1.\\nθ\\n · \\nx\\n is the dot product of the vectors \\nθ\\n and \\nx\\n, which is equal to \\nθ\\nx\\n +\\nθ\\nx\\n + \\nθ\\nx\\n + ... + \\nθ\\nx\\n.\\nNOTE\\nIn machine learning, vectors are often represented as \\ncolumn vectors\\n, which are 2D arrays\\nwith a single column. If \\nθ\\n and \\nx\\n are column vectors, then the prediction is y^=θ\\n⊺\\nx, where\\nθ\\n⊺\\n is the \\ntranspose\\n of \\nθ\\n (a row vector instead of a column vector) and θ\\n⊺\\nx is the matrix\\nmultiplication of θ\\n⊺\\n and \\nx\\n. It is of course the same prediction, except that it is now\\nrepresented as a single-cell matrix rather than a scalar value. In this book I will use this\\nnotation to avoid switching between dot products and matrix multiplications.\\nOK, that’s the linear regression model—but how do we train it? Well, recall\\nthat training a model means setting its parameters so that the model best fits\\nthe training set. For this purpose, we first need a measure of how well (or\\npoorly) the model fits the training data. In \\nChapter 2\\n we saw that the most\\ncommon performance measure of a regression model is the root mean square\\nerror (\\nEquation 2-1\\n). \\nTherefore, to train a linear regression model, we need to\\nfind the value of \\nθ\\n that minimizes the RMSE. In practice, it is simpler to\\nminimize the mean squared error (MSE) than the RMSE, and it leads to the\\nsame result (because the value that minimizes a positive function also\\nminimizes its square root).\\nWARNING\\nLearning algorithms will often optimize a different loss function during training than the\\nperformance measure used to evaluate the final model. This is generally because the\\nfunction is easier to optimize and/or because it has extra terms needed during training only\\n(e.g., for regularization). A good performance metric is as close as possible to the final\\nbusiness objective. \\nA good training loss is easy to optimize and strongly correlated with\\nthe metric. For example, classifiers are often trained using a cost function such as the log\\nloss (as you will see later in this chapter) but evaluated using precision/recall. The log loss\\nis easy to minimize, and doing so will usually improve precision/recall.\\n0\\nn\\n0\\n0\\n0\\n1\\n1\\n2\\n2\\nn\\nn'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 224, 'page_label': '225', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The MSE of a linear regression hypothesis \\nh\\n on a training set \\nX\\n is calculated\\nusing \\nEquation 4-3\\n.\\nEquation 4-3. \\nMSE cost function for a linear regression model\\nMSE \\n( \\nX \\n, \\nh θ \\n) \\n= \\n1 m \\n∑ i=1 m \\n(θ \\n⊺\\n x (i) -y (i) ) 2\\nMost of these notations were presented in \\nChapter 2\\n (see \\n“Notations”\\n). \\nThe\\nonly difference is that we write \\nh\\n instead of just \\nh\\n to make it clear that the\\nmodel is parametrized by the vector \\nθ\\n. To simplify notations, we will just\\nwrite MSE(\\nθ\\n) instead of MSE(\\nX\\n, \\nh\\n).\\nθ\\nθ\\nθ'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 225, 'page_label': '226', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The Normal Equation\\nTo find the value of \\nθ\\n that minimizes the MSE, there exists a \\nclosed-form\\nsolution\\n—in other words, a mathematical equation that gives the result\\ndirectly. \\nThis is called the \\nNormal equation\\n (\\nEquation 4-4\\n).\\nEquation 4-4. \\nNormal equation\\nθ ^ \\n= \\n(X \\n⊺\\n X) -1 \\n  \\nX \\n⊺\\n \\n  \\ny\\nIn this equation:\\nθ^ is the value of \\nθ\\n that minimizes the cost function.\\ny\\n is the vector of target values containing \\ny\\n to \\ny\\n.\\nLet’s generate some linear-looking data to test this equation on (\\nFigure 4-1\\n):\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\n  \\n# to make this code example reproducible\\nm\\n \\n=\\n \\n100\\n  \\n# number of instances\\nX\\n \\n=\\n \\n2\\n \\n*\\n \\nnp\\n.\\nrandom\\n.\\nrand\\n(\\nm\\n,\\n \\n1\\n)\\n  \\n# column vector\\ny\\n \\n=\\n \\n4\\n \\n+\\n \\n3\\n \\n*\\n \\nX\\n \\n+\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\nm\\n,\\n \\n1\\n)\\n  \\n# column vector\\n(1)\\n(\\nm\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 226, 'page_label': '227', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-1. \\nA randomly generated linear dataset\\nNow let’s compute θ^ using the Normal equation. We will use the \\ninv()\\nfunction from NumPy’s linear algebra module (\\nnp.linalg\\n) to compute the\\ninverse of a matrix, and the \\ndot()\\n method for matrix multiplication:\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nadd_dummy_feature\\nX_b\\n \\n=\\n \\nadd_dummy_feature\\n(\\nX\\n)\\n  \\n# add x0 = 1 to each instance\\ntheta_best\\n \\n=\\n \\nnp\\n.\\nlinalg\\n.\\ninv\\n(\\nX_b\\n.\\nT\\n \\n@\\n \\nX_b\\n)\\n \\n@\\n \\nX_b\\n.\\nT\\n \\n@\\n \\ny\\nNOTE\\nThe \\n@\\n operator performs matrix multiplication. If \\nA\\n and \\nB\\n are NumPy arrays, then \\nA @ B\\nis equivalent to \\nnp.matmul(A, B)\\n. Many other libraries, like TensorFlow, PyTorch, and\\nJAX, support the \\n@\\n operator as well. However, you cannot use \\n@\\n on pure Python arrays\\n(i.e., lists of lists).\\nThe function that we used to generate the data is \\ny\\n = 4 + 3\\nx\\n + Gaussian\\nnoise. Let’s see what the equation found:\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 227, 'page_label': '228', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\ntheta_best\\narray([[4.21509616],\\n       [2.77011339]])\\nWe would have hoped for \\nθ\\n = 4 and \\nθ\\n = 3 instead of \\nθ\\n = 4.215 and \\nθ\\n =\\n2.770. Close enough, but the noise made it impossible to recover the exact\\nparameters of the original function. The smaller and noisier the dataset, the\\nharder it gets.\\nNow we can make predictions using θ^:\\n>>> \\nX_new\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n0\\n],\\n \\n[\\n2\\n]])\\n>>> \\nX_new_b\\n \\n=\\n \\nadd_dummy_feature\\n(\\nX_new\\n)\\n  \\n# add x0 = 1 to each instance\\n>>> \\ny_predict\\n \\n=\\n \\nX_new_b\\n \\n@\\n \\ntheta_best\\n>>> \\ny_predict\\narray([[4.21509616],\\n       [9.75532293]])\\nLet’s plot this model’s predictions (\\nFigure 4-2\\n):\\nimport\\n \\nmatplotlib.pyplot\\n \\nas\\n \\nplt\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n \\ny_predict\\n,\\n \\n\"r-\"\\n,\\n \\nlabel\\n=\\n\"Predictions\"\\n)\\nplt\\n.\\nplot\\n(\\nX\\n,\\n \\ny\\n,\\n \\n\"b.\"\\n)\\n[\\n...\\n]\\n  \\n# beautify the figure: add labels, axis, grid, and legend\\nplt\\n.\\nshow\\n()\\n0\\n1\\n0\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 228, 'page_label': '229', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-2. \\nLinear regression model predictions\\nPerforming linear regression using Scikit-Learn is relatively straightforward:\\n>>> \\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLinearRegression\\n>>> \\nlin_reg\\n \\n=\\n \\nLinearRegression\\n()\\n>>> \\nlin_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\n>>> \\nlin_reg\\n.\\nintercept_\\n,\\n \\nlin_reg\\n.\\ncoef_\\n(array([4.21509616]), array([[2.77011339]]))\\n>>> \\nlin_reg\\n.\\npredict\\n(\\nX_new\\n)\\narray([[4.21509616],\\n       [9.75532293]])\\nNotice that Scikit-Learn separates the bias term (\\nintercept_\\n) from the feature\\nweights (\\ncoef_\\n). \\nThe \\nLinearRegression\\n class is based on the\\nscipy.linalg.lstsq()\\n function (the name stands for “least squares”), which you\\ncould call directly:\\n>>> \\ntheta_best_svd\\n,\\n \\nresiduals\\n,\\n \\nrank\\n,\\n \\ns\\n \\n=\\n \\nnp\\n.\\nlinalg\\n.\\nlstsq\\n(\\nX_b\\n,\\n \\ny\\n,\\n \\nrcond\\n=\\n1e-6\\n)\\n>>> \\ntheta_best_svd\\narray([[4.21509616],\\n       [2.77011339]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 229, 'page_label': '230', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='This function computes θ^=X+y, where X+ is the \\npseudoinverse\\n of \\nX\\n(specifically, \\nthe Moore–Penrose\\n inverse). You can use \\nnp.linalg.pinv()\\n to\\ncompute the \\npseudoinverse\\n directly:\\n>>> \\nnp\\n.\\nlinalg\\n.\\npinv\\n(\\nX_b\\n)\\n \\n@\\n \\ny\\narray([[4.21509616],\\n       [2.77011339]])\\nThe pseudoinverse itself is computed using a standard matrix factorization\\ntechnique called \\nsingular value decomposition\\n (SVD) that can decompose the\\ntraining set matrix \\nX\\n into the matrix multiplication of three matrices \\nU\\n \\nΣ\\n \\nV\\n(see \\nnumpy.linalg.svd()\\n). \\nThe pseudoinverse is computed as X+=VΣ+U\\n⊺\\n. To\\ncompute the matrix Σ+, the algorithm takes \\nΣ\\n and sets to zero all values\\nsmaller than a tiny threshold value, then it replaces all the nonzero values\\nwith their inverse, and finally it transposes the resulting matrix. This\\napproach is more efficient than computing the Normal equation, plus it\\nhandles edge cases nicely: indeed, the Normal equation may not work if the\\nmatrix \\nX\\nX\\n is not invertible (i.e., singular), such as if \\nm\\n < \\nn\\n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\n⊺\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 230, 'page_label': '231', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Computational Complexity\\nThe Normal equation computes the inverse of \\nX\\n \\nX\\n, which is an (\\nn\\n + 1) × (\\nn\\n+ 1) matrix (where \\nn\\n is the number of features). The \\ncomputational\\ncomplexity\\n of inverting such a matrix is typically about \\nO\\n(\\nn\\n) to \\nO\\n(\\nn\\n),\\ndepending on the implementation. In other words, if you double the number\\nof features, you multiply the computation time by roughly 2\\n = 5.3 to 2\\n =\\n8.\\nThe SVD approach used by Scikit-Learn’s \\nLinearRegression\\n class is about\\nO\\n(\\nn\\n). If you double the number of features, you multiply the computation\\ntime by roughly 4.\\nWARNING\\nBoth the Normal equation and the SVD approach get very slow when the number of\\nfeatures grows large (e.g., 100,000). On the positive side, both are linear with regard to the\\nnumber of instances in the training set (they are \\nO\\n(\\nm\\n)), so they handle large training sets\\nefficiently, provided they can fit in memory.\\nAlso, once you have trained your linear regression model (using the Normal\\nequation or any other algorithm), predictions are very fast: the computational\\ncomplexity is linear with regard to both the number of instances you want to\\nmake predictions on and the number of features. In other words, making\\npredictions on twice as many instances (or twice as many features) will take\\nroughly twice as much time.\\nNow we will look at a very different way to train a linear regression model,\\nwhich is better suited for cases where there are a large number of features or\\ntoo many training instances to fit in memory.\\n⊺\\n2.4\\n3\\n2.4\\n3\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 231, 'page_label': '232', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Gradient Descent\\nGradient descent\\n is a generic optimization algorithm capable of finding\\noptimal solutions to a wide range of problems. \\nThe general idea of gradient\\ndescent is to tweak parameters iteratively in order to minimize a cost\\nfunction.\\nSuppose you are lost in the mountains in a dense fog, and you can only feel\\nthe slope of the ground below your feet. A good strategy to get to the bottom\\nof the valley quickly is to go downhill in the direction of the steepest slope.\\nThis is exactly what gradient descent does: it measures the local gradient of\\nthe error function with regard to the parameter vector \\nθ\\n, and it goes in the\\ndirection of descending gradient. Once the gradient is zero, you have reached\\na minimum!\\nIn practice, you start by filling \\nθ\\n with random values (this is called \\nrandom\\ninitialization\\n). \\nThen you improve it gradually, taking one baby step at a time,\\neach step attempting to decrease the cost function (e.g., the MSE), until the\\nalgorithm \\nconverges\\n to a minimum (see \\nFigure 4-3\\n).\\nFigure 4-3. \\nIn this depiction of gradient descent, the model parameters are initialized randomly and get'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 232, 'page_label': '233', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='tweaked repeatedly to minimize the cost function; the learning step size is proportional to the slope of\\nthe cost function, so the steps gradually get smaller as the cost approaches the minimum\\nAn important parameter in gradient descent is the size of the steps,\\ndetermined by the \\nlearning rate\\n hyperparameter. \\nIf the learning rate is too\\nsmall, then the algorithm will have to go through many iterations to\\nconverge, which will take a long time (see \\nFigure 4-4\\n).\\nFigure 4-4. \\nLearning rate too small\\nOn the other hand, if the learning rate is too high, you might jump across the\\nvalley and end up on the other side, possibly even higher up than you were\\nbefore. This might make the algorithm diverge, with larger and larger values,\\nfailing to find a good solution (see \\nFigure 4-5\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 233, 'page_label': '234', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-5. \\nLearning rate too high\\nAdditionally, not all cost functions look like nice, regular bowls. There may\\nbe holes, ridges, plateaus, and all sorts of irregular terrain, making\\nconvergence to the minimum difficult. \\nFigure 4-6\\n shows the two main\\nchallenges with gradient descent. \\nIf the random initialization starts the\\nalgorithm on the left, then it will converge to a \\nlocal minimum\\n, which is not\\nas good as the \\nglobal minimum\\n. If it starts on the right, then it will take a very\\nlong time to cross the plateau. And if you stop too early, you will never reach\\nthe global minimum.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 234, 'page_label': '235', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-6. \\nGradient descent pitfalls\\nFortunately, the MSE cost function for a linear regression model happens to\\nbe a \\nconvex function\\n, which means that if you pick any two points on the\\ncurve, the line segment joining them is never below the curve. \\nThis implies\\nthat there are no local minima, just one global minimum. It is also a\\ncontinuous function with a slope that never changes abruptly.\\n\\u2060\\n These two\\nfacts have a great consequence: gradient descent is guaranteed to approach\\narbitrarily closely the global minimum (if you wait long enough and if the\\nlearning rate is not too high).\\nWhile the cost function has the shape of a bowl, it can be an elongated bowl\\nif the features have very different scales. \\nFigure 4-7\\n shows gradient descent\\non a training set where features 1 and 2 have the same scale (on the left), and\\non a training set where feature 1 has much smaller values than feature 2 (on\\nthe right).\\n\\u2060\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 235, 'page_label': '236', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-7. \\nGradient descent with (left) and without (right) feature scaling\\nAs you can see, on the left the gradient descent algorithm goes straight\\ntoward the minimum, thereby reaching it quickly, whereas on the right it first\\ngoes in a direction almost orthogonal to the direction of the global minimum,\\nand it ends with a long march down an almost flat valley. It will eventually\\nreach the minimum, but it will take a long time.\\nWARNING\\nWhen using gradient descent, you should ensure that all features have a similar scale (e.g.,\\nusing Scikit-Learn’s \\nStandardScaler\\n class), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching\\nfor a combination of model parameters that minimizes a cost function (over\\nthe training set). \\nIt is a search in the model’s \\nparameter space\\n. The more\\nparameters a model has, the more dimensions this space has, and the harder\\nthe search is: searching for a needle in a 300-dimensional haystack is much\\ntrickier than in 3 dimensions. Fortunately, since the cost function is convex in\\nthe case of linear regression, the needle is simply at the bottom of the bowl.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 236, 'page_label': '237', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Batch Gradient Descent\\nTo implement gradient descent, you need to compute the gradient of the cost\\nfunction with regard to each model parameter \\nθ\\n. \\nIn other words, you need to\\ncalculate how much the cost function will change if you change \\nθ\\n just a little\\nbit. This is called a \\npartial derivative\\n. It is like asking, “What is the slope of\\nthe mountain under my feet if I face east”? and then asking the same question\\nfacing north (and so on for all other dimensions, if you can imagine a\\nuniverse with more than three dimensions). \\nEquation 4-5\\n computes the partial\\nderivative of the MSE with regard to parameter \\nθ\\n, noted ∂ MSE(\\nθ\\n) / ∂θ\\n.\\nEquation 4-5. \\nPartial derivatives of the cost function\\n∂ ∂θ j \\nMSE \\n( \\nθ \\n) \\n= \\n2 m \\n∑ i=1 m \\n( \\nθ \\n⊺\\n \\nx (i) \\n- \\ny (i) \\n) \\nx j (i)\\nInstead of computing these partial derivatives individually, you can use\\nEquation 4-6\\n to compute them all in one go. The gradient vector, noted\\n∇\\nMSE(\\nθ\\n), contains all the partial derivatives of the cost function (one for\\neach model parameter).\\nEquation 4-6. \\nGradient vector of the cost function\\n∇\\n θ \\nMSE \\n( \\nθ \\n) \\n= \\n∂ ∂θ 0 \\nMSE \\n( \\nθ \\n) \\n∂ ∂θ 1 \\nMSE \\n( \\nθ \\n) \\n⋮\\n \\n∂ ∂θ n \\nMSE \\n( \\nθ \\n) \\n= \\n2 m\\nX \\n⊺\\n \\n( \\nX \\nθ \\n- \\ny \\n)\\nWARNING\\nNotice that this formula involves calculations over the full training set \\nX\\n, at each gradient\\ndescent step! \\nThis is why the algorithm is called \\nbatch gradient descent\\n: it uses the whole\\nbatch of training data at every step (actually, \\nfull gradient descent\\n would probably be a\\nbetter name). As a result, it is terribly slow on very large training sets (we will look at\\nsome much faster gradient descent algorithms shortly). However, gradient descent scales\\nwell with the number of features; training a linear regression model when there are\\nhundreds of thousands of features is much faster using gradient descent than using the\\nNormal equation or SVD decomposition.\\nOnce you have the gradient vector, which points uphill, just go in the\\nj\\nj\\nj\\nj\\nθ'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 237, 'page_label': '238', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='opposite direction to go downhill. This means subtracting \\n∇\\nMSE(\\nθ\\n) from \\nθ\\n.\\nThis is where the learning rate \\nη\\n comes into play:\\n\\u2060\\n multiply the gradient\\nvector by \\nη\\n to determine the size of the downhill step (\\nEquation 4-7\\n).\\nEquation 4-7. \\nGradient descent step\\nθ(next step)=θ-η\\n∇\\nθ\\u200aMSE(θ)\\nLet’s look at a quick implementation of this algorithm:\\neta\\n \\n=\\n \\n0.1\\n  \\n# learning rate\\nn_epochs\\n \\n=\\n \\n1000\\nm\\n \\n=\\n \\nlen\\n(\\nX_b\\n)\\n  \\n# number of instances\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\ntheta\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n2\\n,\\n \\n1\\n)\\n  \\n# randomly initialized model parameters\\nfor\\n \\nepoch\\n \\nin\\n \\nrange\\n(\\nn_epochs\\n):\\n    \\ngradients\\n \\n=\\n \\n2\\n \\n/\\n \\nm\\n \\n*\\n \\nX_b\\n.\\nT\\n \\n@\\n \\n(\\nX_b\\n \\n@\\n \\ntheta\\n \\n-\\n \\ny\\n)\\n    \\ntheta\\n \\n=\\n \\ntheta\\n \\n-\\n \\neta\\n \\n*\\n \\ngradients\\nThat wasn’t too hard! Each iteration over the training set is called an \\nepoch\\n.\\nLet’s look at the resulting \\ntheta\\n:\\n>>> \\ntheta\\narray([[4.21509616],\\n       [2.77011339]])\\nHey, that’s exactly what the Normal equation found! Gradient descent\\nworked perfectly. \\nBut what if you had used a different learning rate (\\neta\\n)?\\nFigure 4-8\\n shows the first 20 steps of gradient descent using three different\\nlearning rates. The line at the bottom of each plot represents the random\\nstarting point, then each epoch is represented by a darker and darker line.\\nθ\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 238, 'page_label': '239', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-8. \\nGradient descent with various learning rates\\nOn the left, the learning rate is too low: the algorithm will eventually reach\\nthe solution, but it will take a long time. In the middle, the learning rate looks\\npretty good: in just a few epochs, it has already converged to the solution. On\\nthe right, the learning rate is too high: the algorithm diverges, jumping all\\nover the place and actually getting further and further away from the solution\\nat every step.\\nTo find a good learning rate, you can use grid search (see \\nChapter 2\\n).\\nHowever, you may want to limit the number of epochs so that grid search can\\neliminate models that take too long to converge.\\nYou may wonder how to set the number of epochs. \\nIf it is too low, you will\\nstill be far away from the optimal solution when the algorithm stops; but if it\\nis too high, you will waste time while the model parameters do not change\\nanymore. \\nA simple solution is to set a very large number of epochs but to\\ninterrupt the algorithm when the gradient vector becomes tiny—that is, when\\nits norm becomes smaller than a tiny number \\nϵ\\n (called the \\ntolerance\\n)—\\nbecause this happens when gradient descent has (almost) reached the\\nminimum.\\nCONVERGENCE RATE\\nWhen the cost function is convex and its slope does not change abruptly\\n(as is the case for the MSE cost function), batch gradient descent with a\\nfixed learning rate will eventually converge to the optimal solution, but'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 239, 'page_label': '240', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='you may have to wait a while: \\nit can take \\nO\\n(1/\\nϵ\\n) iterations to reach the\\noptimum within a range of \\nϵ\\n, depending on the shape of the cost function.\\nIf you divide the tolerance by 10 to have a more precise solution, then the\\nalgorithm may have to run about 10 times longer.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 240, 'page_label': '241', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Stochastic Gradient Descent\\nThe main problem with batch gradient descent is the fact that it uses the\\nwhole training set to compute the gradients at every step, which makes it very\\nslow when the training set is large. \\nAt the opposite extreme, \\nstochastic\\ngradient descent\\n picks a random instance in the training set at every step and\\ncomputes the gradients based only on that single instance. Obviously,\\nworking on a single instance at a time makes the algorithm much faster\\nbecause it has very little data to manipulate at every iteration. It also makes it\\npossible to train on huge training sets, since only one instance needs to be in\\nmemory at each iteration (stochastic GD can be implemented as an out-of-\\ncore algorithm; see \\nChapter 1\\n).\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is\\nmuch less regular than batch gradient descent: instead of gently decreasing\\nuntil it reaches the minimum, the cost function will bounce up and down,\\ndecreasing only on average. Over time it will end up very close to the\\nminimum, but once it gets there it will continue to bounce around, never\\nsettling down (see \\nFigure 4-9\\n). Once the algorithm stops, the final parameter\\nvalues will be good, but not optimal.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 241, 'page_label': '242', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-9. \\nWith stochastic gradient descent, each training step is much faster but also much more\\nstochastic than when using batch gradient descent\\nWhen the cost function is very irregular (as in \\nFigure 4-6\\n), this can actually\\nhelp the algorithm jump out of local minima, so stochastic gradient descent\\nhas a better chance of finding the global minimum than batch gradient\\ndescent does.\\nTherefore, randomness is good to escape from local optima, but bad because\\nit means that the algorithm can never settle at the minimum. One solution to\\nthis dilemma is to gradually reduce the learning rate. \\nThe steps start out large\\n(which helps make quick progress and escape local minima), then get smaller\\nand smaller, allowing the algorithm to settle at the global minimum. This\\nprocess is akin to \\nsimulated annealing\\n, an algorithm inspired by the process\\nin metallurgy of annealing, where molten metal is slowly cooled down. \\nThe\\nfunction that determines the learning rate at each iteration is called the\\nlearning schedule\\n. If the learning rate is reduced too quickly, you may get\\nstuck in a local minimum, or even end up frozen halfway to the minimum. If'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 242, 'page_label': '243', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='the learning rate is reduced too slowly, you may jump around the minimum\\nfor a long time and end up with a suboptimal solution if you halt training too\\nearly.\\nThis code implements stochastic gradient descent using a simple learning\\nschedule:\\nn_epochs\\n \\n=\\n \\n50\\nt0\\n,\\n \\nt1\\n \\n=\\n \\n5\\n,\\n \\n50\\n  \\n# learning schedule hyperparameters\\ndef\\n \\nlearning_schedule\\n(\\nt\\n):\\n    \\nreturn\\n \\nt0\\n \\n/\\n \\n(\\nt\\n \\n+\\n \\nt1\\n)\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\ntheta\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n2\\n,\\n \\n1\\n)\\n  \\n# random initialization\\nfor\\n \\nepoch\\n \\nin\\n \\nrange\\n(\\nn_epochs\\n):\\n    \\nfor\\n \\niteration\\n \\nin\\n \\nrange\\n(\\nm\\n):\\n        \\nrandom_index\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandint\\n(\\nm\\n)\\n        \\nxi\\n \\n=\\n \\nX_b\\n[\\nrandom_index\\n \\n:\\n \\nrandom_index\\n \\n+\\n \\n1\\n]\\n        \\nyi\\n \\n=\\n \\ny\\n[\\nrandom_index\\n \\n:\\n \\nrandom_index\\n \\n+\\n \\n1\\n]\\n        \\ngradients\\n \\n=\\n \\n2\\n \\n*\\n \\nxi\\n.\\nT\\n \\n@\\n \\n(\\nxi\\n \\n@\\n \\ntheta\\n \\n-\\n \\nyi\\n)\\n  \\n# for SGD, do not divide by m\\n        \\neta\\n \\n=\\n \\nlearning_schedule\\n(\\nepoch\\n \\n*\\n \\nm\\n \\n+\\n \\niteration\\n)\\n        \\ntheta\\n \\n=\\n \\ntheta\\n \\n-\\n \\neta\\n \\n*\\n \\ngradients\\nBy convention we iterate by rounds of \\nm\\n iterations; each round is called an\\nepoch\\n, as earlier. While the batch gradient descent code iterated 1,000 times\\nthrough the whole training set, this code goes through the training set only 50\\ntimes and reaches a pretty good solution:\\n>>> \\ntheta\\narray([[4.21076011],\\n       [2.74856079]])\\nFigure 4-10\\n shows the first 20 steps of training (notice how irregular the steps\\nare).\\nNote that since instances are picked randomly, some instances may be picked\\nseveral times per epoch, while others may not be picked at all. \\nIf you want to\\nbe sure that the algorithm goes through every instance at each epoch, another\\napproach is to shuffle the training set (making sure to shuffle the input'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 243, 'page_label': '244', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='features and the labels jointly), then go through it instance by instance, then\\nshuffle it again, and so on. However, this approach is more complex, and it\\ngenerally does not improve the result.\\nFigure 4-10. \\nThe first 20 steps of stochastic gradient descent\\nWARNING\\nWhen using stochastic gradient descent, the training instances must be independent and\\nidentically distributed (IID) to ensure that the parameters get pulled toward the global\\noptimum, on average. A simple way to ensure this is to shuffle the instances during\\ntraining (e.g., pick each instance randomly, or shuffle the training set at the beginning of\\neach epoch). If you do not shuffle the instances—for example, if the instances are sorted\\nby label—then SGD will start by optimizing for one label, then the next, and so on, and it\\nwill not settle close to the global minimum.\\nTo perform linear regression using stochastic GD with Scikit-Learn, you can\\nuse the \\nSGDRegressor\\n class, which defaults to optimizing the MSE cost\\nfunction. \\nThe following code runs for maximum 1,000 epochs (\\nmax_iter\\n) or\\nuntil the loss drops by less than 10\\n (\\ntol\\n) during 100 epochs\\n–5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 244, 'page_label': '245', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='(\\nn_iter_no_change\\n). It starts with a learning rate of 0.01 (\\neta0\\n), using the\\ndefault learning schedule (different from the one we used). Lastly, it does not\\nuse any regularization (\\npenalty=None\\n; more details on this shortly):\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nSGDRegressor\\nsgd_reg\\n \\n=\\n \\nSGDRegressor\\n(\\nmax_iter\\n=\\n1000\\n,\\n \\ntol\\n=\\n1e-5\\n,\\n \\npenalty\\n=\\nNone\\n,\\n \\neta0\\n=\\n0.01\\n,\\n                       \\nn_iter_no_change\\n=\\n100\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nsgd_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n.\\nravel\\n())\\n  \\n# y.ravel() because fit() expects 1D targets\\nOnce again, you find a solution quite close to the one returned by the Normal\\nequation\\n:\\n>>> \\nsgd_reg\\n.\\nintercept_\\n,\\n \\nsgd_reg\\n.\\ncoef_\\n(array([4.21278812]), array([2.77270267]))\\nTIP\\nAll Scikit-Learn estimators can be trained using the \\nfit()\\n method, but some estimators also\\nhave a \\npartial_fit()\\n method that you can call to run a single round of training on one or\\nmore instances (it ignores hyperparameters like \\nmax_iter\\n or \\ntol\\n). Repeatedly calling\\npartial_fit()\\n will gradually train the model. This is useful when you need more control over\\nthe training process. Other models have a \\nwarm_start\\n hyperparameter instead (and some\\nhave both): if you set \\nwarm_start=True\\n, calling the \\nfit()\\n method on a trained model will\\nnot reset the model; it will just continue training where it left off, respecting\\nhyperparameters like \\nmax_iter\\n and \\ntol\\n. Note that \\nfit()\\n resets the iteration counter used by\\nthe learning schedule, while \\npartial_fit()\\n does not.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 245, 'page_label': '246', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Mini-Batch Gradient Descent\\nThe last gradient descent algorithm we will look at is called \\nmini-batch\\ngradient descent\\n. \\nIt is straightforward once you know batch and stochastic\\ngradient descent: at each step, instead of computing the gradients based on\\nthe full training set (as in batch GD) or based on just one instance (as in\\nstochastic GD), mini-batch GD computes the gradients on small random sets\\nof instances called \\nmini-batches\\n. \\nThe main advantage of mini-batch GD over\\nstochastic GD is that you can get a performance boost from hardware\\noptimization of matrix operations, especially when using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with\\nstochastic GD, especially with fairly large mini-batches. \\nAs a result, mini-\\nbatch GD will end up walking around a bit closer to the minimum than\\nstochastic GD—but it may be harder for it to escape from local minima (in\\nthe case of problems that suffer from local minima, unlike linear regression\\nwith the MSE cost function). \\nFigure 4-11\\n shows the paths taken by the three\\ngradient descent algorithms in parameter space during training. They all end\\nup near the minimum, but batch GD’s path actually stops at the minimum,\\nwhile both stochastic GD and mini-batch GD continue to walk around.\\nHowever, don’t forget that batch GD takes a lot of time to take each step, and\\nstochastic GD and mini-batch GD would also reach the minimum if you used\\na good learning schedule.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 246, 'page_label': '247', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-11. \\nGradient descent paths in parameter space\\nTable 4-1\\n compares the algorithms we’ve discussed so far for linear\\nregression\\n\\u2060\\n (recall that \\nm\\n is the number of training instances and \\nn\\n is the\\nnumber of features).\\nTable 4-1. \\nComparison of algorithms for linear regression\\nAlgorithm\\nLarge \\nm\\nOut-of-core\\nsupport\\nLarge \\nn\\nHyperparams\\nNormal equation\\nFast\\nNo\\nSlow\\n0\\nSVD\\nFast\\nNo\\nSlow\\n0\\nBatch GD\\nSlow\\nNo\\nFast\\n2\\nStochastic GD\\nFast\\nYes\\nFast\\n≥2\\nMini-batch GD\\nFast\\nYes\\nFast\\n≥2\\nThere is almost no difference after training: all these algorithms end up with\\nvery similar models and make predictions in exactly the same way.\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 247, 'page_label': '248', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Polynomial Regression\\nWhat if your data is more complex than a straight line? \\nSurprisingly, you can\\nuse a linear model to fit nonlinear data. A simple way to do this is to add\\npowers of each feature as new features, then train a linear model on this\\nextended set of features. This technique is called \\npolynomial regression\\n.\\nLet’s look at an example. \\nFirst, we’ll generate some nonlinear data (see\\nFigure 4-12\\n), based on a simple \\nquadratic equation\\n—that’s an equation of\\nthe form \\ny\\n = \\nax\\n² + \\nbx\\n + \\nc\\n—plus some noise:\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\nm\\n \\n=\\n \\n100\\nX\\n \\n=\\n \\n6\\n \\n*\\n \\nnp\\n.\\nrandom\\n.\\nrand\\n(\\nm\\n,\\n \\n1\\n)\\n \\n-\\n \\n3\\ny\\n \\n=\\n \\n0.5\\n \\n*\\n \\nX\\n \\n**\\n \\n2\\n \\n+\\n \\nX\\n \\n+\\n \\n2\\n \\n+\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\nm\\n,\\n \\n1\\n)\\nFigure 4-12. \\nGenerated nonlinear and noisy dataset\\nClearly, a straight line will never fit this data properly. \\nSo let’s use Scikit-\\nLearn’s \\nPolynomialFeatures\\n class to transform our training data, adding the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 248, 'page_label': '249', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='square (second-degree polynomial) of each feature in the training set as a new\\nfeature (in this case there is just one feature):\\n>>> \\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nPolynomialFeatures\\n>>> \\npoly_features\\n \\n=\\n \\nPolynomialFeatures\\n(\\ndegree\\n=\\n2\\n,\\n \\ninclude_bias\\n=\\nFalse\\n)\\n>>> \\nX_poly\\n \\n=\\n \\npoly_features\\n.\\nfit_transform\\n(\\nX\\n)\\n>>> \\nX\\n[\\n0\\n]\\narray([-0.75275929])\\n>>> \\nX_poly\\n[\\n0\\n]\\narray([-0.75275929,  0.56664654])\\nX_poly\\n now contains the original feature of \\nX\\n plus the square of this feature.\\nNow we can fit a \\nLinearRegression\\n model to this extended training data\\n(\\nFigure 4-13\\n):\\n>>> \\nlin_reg\\n \\n=\\n \\nLinearRegression\\n()\\n>>> \\nlin_reg\\n.\\nfit\\n(\\nX_poly\\n,\\n \\ny\\n)\\n>>> \\nlin_reg\\n.\\nintercept_\\n,\\n \\nlin_reg\\n.\\ncoef_\\n(array([1.78134581]), array([[0.93366893, 0.56456263]]))\\nFigure 4-13. \\nPolynomial regression model predictions'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 249, 'page_label': '250', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Not bad: the model estimates \\ny ^ \\n= \\n0.56 \\nx 1 \\n2 \\n+ \\n0.93 \\nx 1 \\n+ \\n1.78 \\nwhen in fact\\nthe original function was \\ny \\n= \\n0.5 \\nx 1 \\n2 \\n+ \\n1.0 \\nx 1 \\n+ \\n2.0 \\n+ \\nGaussian noise \\n.\\nNote that when there are multiple features, polynomial regression is capable\\nof finding relationships between features, which is something a plain linear\\nregression model cannot do. This is made possible by the fact that\\nPolynomialFeatures\\n also adds all combinations of features up to the given\\ndegree. For example, if there were two features \\na\\n and \\nb\\n, \\nPolynomialFeatures\\nwith \\ndegree=3\\n would not only add the features \\na\\n, \\na\\n, \\nb\\n, and \\nb\\n, but also the\\ncombinations \\nab\\n, \\na\\nb\\n, and \\nab\\n.\\nWARNING\\nPolynomialFeatures(degree=\\nd\\n)\\n transforms an array containing \\nn\\n features into an array\\ncontaining (\\nn\\n + \\nd\\n)! / \\nd\\n!\\nn\\n! features, where \\nn\\n! is the \\nfactorial\\n of \\nn\\n, equal to 1 × 2 × 3 × \\n⋯\\n × \\nn\\n.\\nBeware of the combinatorial explosion of the number of features!\\n2\\n3\\n2\\n3\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 250, 'page_label': '251', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Learning Curves\\nIf you perform high-degree polynomial regression, you will likely fit the\\ntraining data much better than with plain linear regression. For example,\\nFigure 4-14\\n applies a 300-degree polynomial model to the preceding training\\ndata, and compares the result with a pure linear model and a quadratic model\\n(second-degree polynomial). \\nNotice how the 300-degree polynomial model\\nwiggles around to get as close as possible to the training instances.\\nFigure 4-14. \\nHigh-degree polynomial regression\\nThis high-degree polynomial regression model is severely overfitting the\\ntraining data, while the linear model is underfitting it. \\nThe model that will\\ngeneralize best in this case is the quadratic model, which makes sense\\nbecause the data was generated using a quadratic model. But in general you\\nwon’t know what function generated the data, so how can you decide how\\ncomplex your model should be? How can you tell that your model is\\noverfitting or underfitting the data?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 251, 'page_label': '252', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='In \\nChapter 2\\n you used cross-validation to get an estimate of a model’s\\ngeneralization performance. If a model performs well on the training data but\\ngeneralizes poorly according to the cross-validation metrics, then your model\\nis overfitting. If it performs poorly on both, then it is underfitting. This is one\\nway to tell when a model is too simple or too complex.\\nAnother way to tell is to look at the \\nlearning curves\\n, which are plots of the\\nmodel’s training error and validation error as a function of the training\\niteration: just evaluate the model at regular intervals during training on both\\nthe training set and the validation set, and plot the results. If the model cannot\\nbe trained incrementally (i.e., if it does not support \\npartial_fit()\\n or\\nwarm_start\\n), then you must train it several times on gradually larger subsets\\nof the training set.\\nScikit-Learn has a useful \\nlearning_curve()\\n function to help with this: it trains\\nand evaluates the model using cross-validation. \\nBy default it retrains the\\nmodel on growing subsets of the training set, but if the model supports\\nincremental learning you can set \\nexploit_incremental_learning=True\\n when\\ncalling \\nlearning_curve()\\n and it will train the model incrementally instead. The\\nfunction returns the training set sizes at which it evaluated the model, and the\\ntraining and validation scores it measured for each size and for each cross-\\nvalidation fold. Let’s use this function to look at the learning curves of the\\nplain linear regression model (see \\nFigure 4-15\\n):\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\nlearning_curve\\ntrain_sizes\\n,\\n \\ntrain_scores\\n,\\n \\nvalid_scores\\n \\n=\\n \\nlearning_curve\\n(\\n    \\nLinearRegression\\n(),\\n \\nX\\n,\\n \\ny\\n,\\n \\ntrain_sizes\\n=\\nnp\\n.\\nlinspace\\n(\\n0.01\\n,\\n \\n1.0\\n,\\n \\n40\\n),\\n \\ncv\\n=\\n5\\n,\\n    \\nscoring\\n=\\n\"neg_root_mean_squared_error\"\\n)\\ntrain_errors\\n \\n=\\n \\n-\\ntrain_scores\\n.\\nmean\\n(\\naxis\\n=\\n1\\n)\\nvalid_errors\\n \\n=\\n \\n-\\nvalid_scores\\n.\\nmean\\n(\\naxis\\n=\\n1\\n)\\nplt\\n.\\nplot\\n(\\ntrain_sizes\\n,\\n \\ntrain_errors\\n,\\n \\n\"r-+\"\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n \\nlabel\\n=\\n\"train\"\\n)\\nplt\\n.\\nplot\\n(\\ntrain_sizes\\n,\\n \\nvalid_errors\\n,\\n \\n\"b-\"\\n,\\n \\nlinewidth\\n=\\n3\\n,\\n \\nlabel\\n=\\n\"valid\"\\n)\\n[\\n...\\n]\\n  \\n# beautify the figure: add labels, axis, grid, and legend\\nplt\\n.\\nshow\\n()'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 252, 'page_label': '253', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-15. \\nLearning curves\\nThis model is underfitting. To see why, first let’s look at the training error.\\nWhen there are just one or two instances in the training set, the model can fit\\nthem perfectly, which is why the curve starts at zero. But as new instances are\\nadded to the training set, it becomes impossible for the model to fit the\\ntraining data perfectly, both because the data is noisy and because it is not\\nlinear at all. So the error on the training data goes up until it reaches a\\nplateau, at which point adding new instances to the training set doesn’t make\\nthe average error much better or worse. Now let’s look at the validation error.\\nWhen the model is trained on very few training instances, it is incapable of\\ngeneralizing properly, which is why the validation error is initially quite\\nlarge. Then, as the model is shown more training examples, it learns, and thus\\nthe validation error slowly goes down. However, once again a straight line\\ncannot do a good job of modeling the data, so the error ends up at a plateau,\\nvery close to the other curve.\\nThese learning curves are typical of a model that’s underfitting. Both curves\\nhave reached a plateau; they are close and fairly high.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 253, 'page_label': '254', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='TIP\\nIf your model is underfitting the training data, adding more training examples will not\\nhelp. You need to use a better model or come up with better features.\\nNow let’s look at the learning curves of a 10th-degree polynomial model on\\nthe same data (\\nFigure 4-16\\n):\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nmake_pipeline\\npolynomial_regression\\n \\n=\\n \\nmake_pipeline\\n(\\n    \\nPolynomialFeatures\\n(\\ndegree\\n=\\n10\\n,\\n \\ninclude_bias\\n=\\nFalse\\n),\\n    \\nLinearRegression\\n())\\ntrain_sizes\\n,\\n \\ntrain_scores\\n,\\n \\nvalid_scores\\n \\n=\\n \\nlearning_curve\\n(\\n    \\npolynomial_regression\\n,\\n \\nX\\n,\\n \\ny\\n,\\n \\ntrain_sizes\\n=\\nnp\\n.\\nlinspace\\n(\\n0.01\\n,\\n \\n1.0\\n,\\n \\n40\\n),\\n \\ncv\\n=\\n5\\n,\\n    \\nscoring\\n=\\n\"neg_root_mean_squared_error\"\\n)\\n[\\n...\\n]\\n  \\n# same as earlier\\nFigure 4-16. \\nLearning curves for the 10th-degree polynomial model'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 254, 'page_label': '255', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='These learning curves look a bit like the previous ones, but there are two very\\nimportant differences:\\nThe error on the training data is much lower than before.\\nThere is a gap between the curves. This means that the model performs\\nsignificantly better on the training data than on the validation data,\\nwhich is the hallmark of an overfitting model. If you used a much larger\\ntraining set, however, the two curves would continue to get closer.\\nTIP\\nOne way to improve an overfitting model is to feed it more training data until the\\nvalidation error reaches the training error.\\nTHE BIAS/VARIANCE TRADE-OFF\\nAn important theoretical result of statistics and machine learning is the\\nfact that a model’s generalization error can be expressed as the sum of\\nthree very different errors:\\nBias\\nThis part of the generalization error is due to wrong assumptions,\\nsuch as assuming that the data is linear when it is actually quadratic.\\nA high-bias model is most likely to underfit the training data.\\n\\u2060\\nVariance\\nThis part is due to the model’s excessive sensitivity to small\\nvariations in the training data. \\nA model with many degrees of\\nfreedom (such as a high-degree polynomial model) is likely to have\\nhigh variance and thus overfit the training data.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 255, 'page_label': '256', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='reduce this part of the error is to clean up the data (e.g., fix the data\\nsources, such as broken sensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and\\nreduce its bias. Conversely, reducing a model’s complexity increases its\\nbias and reduces its variance. This is why it is called a trade-off.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 256, 'page_label': '257', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Regularized Linear Models\\nAs you saw in Chapters \\n1\\n and \\n2\\n, a good way to reduce overfitting is to\\nregularize the model (i.e., to constrain it): the fewer degrees of freedom it\\nhas, the harder it will be for it to overfit the data. A simple way to regularize\\na polynomial model is to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the\\nweights of the model. \\nWe will now look at ridge regression, lasso regression,\\nand elastic net regression, which implement three different ways to constrain\\nthe weights.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 257, 'page_label': '258', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Ridge Regression\\nRidge regression\\n (also called \\nTikhonov regularization\\n) is a regularized\\nversion of linear regression: a \\nregularization term\\n equal to αm∑i=1nθi2 is\\nadded to the MSE. This forces the learning algorithm to not only fit the data\\nbut also keep the model weights as small as possible. \\nNote that the\\nregularization term should only be added to the cost function during training.\\nOnce the model is trained, you want to use the unregularized MSE (or the\\nRMSE) to evaluate the model’s performance.\\nThe hyperparameter \\nα\\n controls how much you want to regularize the model.\\nIf \\nα\\n = 0, then ridge regression is just linear regression. If \\nα\\n is very large, then\\nall weights end up very close to zero and the result is a flat line going through\\nthe data’s mean. \\nEquation 4-8\\n presents the ridge regression cost function.\\n\\u2060\\nEquation 4-8. \\nRidge regression cost function\\nJ(θ)=MSE(θ)+αm∑i=1nθi2\\nNote that the bias term \\nθ\\n is not regularized (the sum starts at \\ni\\n = 1, not 0). If\\nwe define \\nw\\n as the vector of feature weights (\\nθ\\n to \\nθ\\n), then the regularization\\nterm is equal to \\nα\\n(\\n∥\\n \\nw\\n \\n∥\\n)\\n / \\nm\\n, where \\n∥\\n \\nw\\n \\n∥\\n represents the ℓ\\n norm of\\nthe weight vector.\\n\\u2060\\n For batch gradient descent, just add 2\\nα\\nw\\n / \\nm\\n to the part\\nof the MSE gradient vector that corresponds to the feature weights, without\\nadding anything to the gradient of the bias term (see \\nEquation 4-6\\n).\\nWARNING\\nIt is important to scale the data (e.g., using a \\nStandardScaler\\n) before performing ridge\\nregression, as it is sensitive to the scale of the input features. This is true of most\\nregularized models.\\nFigure 4-17\\n shows several ridge models that were trained on some very noisy\\nlinear data using different \\nα\\n values. On the left, plain ridge models are used,\\nleading to linear predictions. On the right, the data is first expanded using\\n7\\n0\\n1\\nn\\n2\\n2\\n2\\n2\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 258, 'page_label': '259', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='PolynomialFeatures(degree=10)\\n,\\n then it is scaled using a \\nStandardScaler\\n, and\\nfinally the ridge models are applied to the resulting features: this is\\npolynomial regression with ridge regularization. \\nNote how increasing \\nα\\n leads\\nto flatter (i.e., less extreme, more reasonable) predictions, thus reducing the\\nmodel’s variance but increasing its bias.\\nFigure 4-17. \\nLinear (left) and a polynomial (right) models, both with various levels of ridge\\nregularization\\nAs with linear regression, we can perform ridge regression either by\\ncomputing a closed-form equation or by performing gradient descent. \\nThe\\npros and cons are the same. \\nEquation 4-9\\n shows the closed-form solution,\\nwhere \\nA\\n is the (\\nn\\n + 1) × (\\nn\\n + 1) \\nidentity matrix\\n,\\n\\u2060\\n except with a 0 in the top-\\nleft cell, corresponding to the bias term.\\nEquation 4-9. \\nRidge regression closed-form solution\\nθ ^ \\n= \\n(X \\n⊺\\n X+αA) -1 \\n  \\nX \\n⊺\\n \\n  \\ny\\nHere is how to perform ridge regression with Scikit-Learn using a closed-\\nform solution (a variant of \\nEquation 4-9\\n that uses a matrix factorization\\ntechnique by André-Louis Cholesky):\\n>>> \\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nRidge\\n>>> \\nridge_reg\\n \\n=\\n \\nRidge\\n(\\nalpha\\n=\\n0.1\\n,\\n \\nsolver\\n=\\n\"cholesky\"\\n)\\n>>> \\nridge_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\n>>> \\nridge_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([[1.55325833]])\\n9\\n10'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 259, 'page_label': '260', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='And using stochastic gradient descent:\\n\\u2060\\n>>> \\nsgd_reg\\n \\n=\\n \\nSGDRegressor\\n(\\npenalty\\n=\\n\"l2\"\\n,\\n \\nalpha\\n=\\n0.1\\n \\n/\\n \\nm\\n,\\n \\ntol\\n=\\nNone\\n,\\n... \\n                       \\nmax_iter\\n=\\n1000\\n,\\n \\neta0\\n=\\n0.01\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n...\\n>>> \\nsgd_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n.\\nravel\\n())\\n  \\n# y.ravel() because fit() expects 1D targets\\n>>> \\nsgd_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([1.55302613])\\nThe \\npenalty\\n hyperparameter sets the type of regularization term to use.\\nSpecifying \\n\"l2\"\\n indicates that you want SGD to add a regularization term to\\nthe MSE cost function equal to \\nalpha\\n times the square of the ℓ\\n norm of the\\nweight vector. This is just like ridge regression, except there’s no division by\\nm\\n in this case; that’s why we passed \\nalpha=0.1 / m\\n, to get the same result as\\nRidge(alpha=0.1)\\n.\\nTIP\\nThe \\nRidgeCV\\n class also performs ridge regression, but it automatically tunes\\nhyperparameters using cross-validation. \\nIt’s roughly equivalent to using \\nGridSearchCV\\n,\\nbut it’s optimized for ridge regression and runs \\nmuch\\n faster. Several other estimators\\n(mostly linear) also have efficient CV variants, such as \\nLassoCV\\n and \\nElasticNetCV\\n.\\n10\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 260, 'page_label': '261', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Lasso Regression\\nLeast absolute shrinkage and selection operator regression\\n (usually simply\\ncalled \\nlasso regression\\n) is another regularized version of linear regression:\\njust like ridge regression, it adds a regularization term to the cost function,\\nbut it uses the ℓ\\n norm of the weight vector instead of the square of the ℓ\\nnorm (see \\nEquation 4-10\\n). Notice that the ℓ\\n norm is multiplied by 2\\nα\\n,\\nwhereas the ℓ\\n norm was multiplied by \\nα\\n / \\nm\\n in ridge regression. These\\nfactors were chosen to ensure that the optimal \\nα\\n value is independent from\\nthe training set size: different norms lead to different factors (see \\nScikit-Learn\\nissue #15657\\n for more details).\\nEquation 4-10. \\nLasso regression cost function\\nJ(θ)=MSE(θ)+2α∑i=1nθi\\nFigure 4-18\\n shows the same thing as \\nFigure 4-17\\n but replaces the ridge\\nmodels with lasso models and uses different \\nα\\n values.\\nFigure 4-18. \\nLinear (left) and polynomial (right) models, both using various levels of lasso\\nregularization\\nAn important characteristic of lasso regression is that it tends to eliminate the\\nweights of the least important features (i.e., set them to zero). For example,\\nthe dashed line in the righthand plot in \\nFigure 4-18\\n (with \\nα\\n = 0.01) looks\\nroughly cubic: all the weights for the high-degree polynomial features are\\nequal to zero. \\nIn other words, lasso regression automatically performs feature\\n1\\n2\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 261, 'page_label': '262', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='selection and outputs a \\nsparse model\\n with few nonzero feature weights.\\nYou can get a sense of why this is the case by looking at \\nFigure 4-19\\n: the\\naxes represent two model parameters, and the background contours represent\\ndifferent loss functions. In the top-left plot, the contours represent the ℓ\\n loss\\n(|\\nθ\\n| + |\\nθ\\n|), which drops linearly as you get closer to any axis. For example, if\\nyou initialize the model parameters to \\nθ\\n = 2 and \\nθ\\n = 0.5, running gradient\\ndescent will decrement both parameters equally (as represented by the dashed\\nyellow line); therefore \\nθ\\n will reach 0 first (since it was closer to 0 to begin\\nwith). After that, gradient descent will roll down the gutter until it reaches \\nθ\\n= 0 (with a bit of bouncing around, since the gradients of ℓ\\n never get close to\\n0: they are either –1 or 1 for each parameter). \\nIn the top-right\\n plot, the\\ncontours represent lasso regression’s cost function (i.e., an MSE cost function\\nplus an ℓ\\n loss). The small white circles show the path that gradient descent\\ntakes to optimize some model parameters that were initialized around \\nθ\\n =\\n0.25 and \\nθ\\n = –1:\\n notice once again how the path quickly reaches \\nθ\\n = 0, then\\nrolls down the gutter and ends up bouncing around the global optimum\\n(represented by the red square). If we increased \\nα\\n, the global optimum would\\nmove left along the dashed yellow line, while if we decreased \\nα\\n, the global\\noptimum would move right (in this example, the optimal parameters for the\\nunregularized MSE are \\nθ\\n = 2 and \\nθ\\n = 0.5).\\n1\\n1\\n2\\n1\\n2\\n2\\n1\\n1\\n1\\n1\\n2\\n2\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 262, 'page_label': '263', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-19. \\nLasso versus ridge regularization\\nThe two bottom plots show the same thing but with an ℓ\\n penalty instead. In\\nthe bottom-left plot, you can see that the ℓ\\n loss decreases as we get closer to\\nthe origin, so gradient descent just takes a straight path toward that point. In\\nthe bottom-right plot, the contours represent ridge regression’s cost function\\n(i.e., an MSE cost function plus an ℓ\\n loss). As you can see, the gradients get\\nsmaller as the parameters approach the global optimum, so gradient descent\\nnaturally slows down. This limits the bouncing around, which helps ridge\\nconverge faster than lasso regression. Also note that the optimal parameters\\n(represented by the red square) get closer and closer to the origin when you\\nincrease \\nα\\n, but they never get eliminated entirely.\\nTIP\\nTo keep gradient descent from bouncing around the optimum at the end when using lasso\\n2\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 263, 'page_label': '264', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='regression, you need to gradually reduce the learning rate during training. It will still\\nbounce around the optimum, but the steps will get smaller and smaller, so it will converge.\\nThe lasso cost function is not differentiable at \\nθ\\n = 0 (for \\ni\\n = 1, 2, \\n⋯\\n, \\nn\\n), but\\ngradient descent still works if you use a \\nsubgradient vector\\n \\ng\\n\\u2060\\n instead when\\nany \\nθ\\n = 0. \\nEquation 4-11\\n shows a subgradient vector equation you can use\\nfor gradient descent with the lasso cost function.\\nEquation 4-11. \\nLasso regression subgradient vector\\ng(θ,J)=\\n∇\\nθ\\u200a\\nMSE(θ)+2αsign(θ1)sign(θ2)\\n⋮\\nsign(θn)  where sign(θi)=-1if θi<00if θi=0+1if θi>0\\nHere is a small Scikit-Learn example using the \\nLasso\\n class:\\n>>> \\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLasso\\n>>> \\nlasso_reg\\n \\n=\\n \\nLasso\\n(\\nalpha\\n=\\n0.1\\n)\\n>>> \\nlasso_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\n>>> \\nlasso_reg\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([1.53788174])\\nNote that you could instead use \\nSGDRegressor(penalty=\"l1\", alpha=0.1)\\n.\\ni\\n11\\ni'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 264, 'page_label': '265', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Elastic Net Regression\\nElastic net regression\\n is a middle ground between ridge regression and lasso\\nregression. \\nThe regularization term is a weighted sum of both ridge and\\nlasso’s regularization terms, and you can control the mix ratio \\nr\\n. When \\nr\\n = 0,\\nelastic net is equivalent to ridge regression, and when \\nr\\n = 1, it is equivalent to\\nlasso regression (\\nEquation 4-12\\n).\\nEquation 4-12. \\nElastic net cost function\\nJ(θ)=MSE(θ)+r2α∑i=1nθi+(1-r)αm∑i=1nθi2\\nSo when should you use elastic net regression, or ridge, lasso, or plain linear\\nregression (i.e., without any regularization)? \\nIt is almost always preferable to\\nhave at least a little bit of regularization, so generally you should avoid plain\\nlinear regression. Ridge is a good default, but if you suspect that only a few\\nfeatures are useful, you should prefer lasso or elastic net because they tend to\\nreduce the useless features’ weights down to zero, as discussed earlier. In\\ngeneral, elastic net is preferred over lasso because lasso may behave\\nerratically when the number of features is greater than the number of training\\ninstances or when several features are strongly correlated.\\nHere is a short example that uses Scikit-Learn’s \\nElasticNet\\n (\\nl1_ratio\\ncorresponds to the mix ratio \\nr\\n):\\n>>> \\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nElasticNet\\n>>> \\nelastic_net\\n \\n=\\n \\nElasticNet\\n(\\nalpha\\n=\\n0.1\\n,\\n \\nl1_ratio\\n=\\n0.5\\n)\\n>>> \\nelastic_net\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\n>>> \\nelastic_net\\n.\\npredict\\n([[\\n1.5\\n]])\\narray([1.54333232])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 265, 'page_label': '266', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Early Stopping\\nA very different way to regularize iterative learning algorithms such as\\ngradient descent is to stop training as soon as the validation error reaches a\\nminimum. \\nThis is called \\nearly stopping\\n. \\nFigure 4-20\\n shows a complex model\\n(in this case, a high-degree polynomial regression model) being trained with\\nbatch gradient descent on the quadratic dataset we used earlier. As the epochs\\ngo by, the algorithm learns, and its prediction error (RMSE) on the training\\nset goes down, along with its prediction error on the validation set. After a\\nwhile, though, the validation error stops decreasing and starts to go back up.\\nThis indicates that the model has started to overfit the training data. With\\nearly stopping you just stop training as soon as the validation error reaches\\nthe minimum. It is such a simple and efficient regularization technique that\\nGeoffrey Hinton called it a “beautiful free lunch”.\\nFigure 4-20. \\nEarly stopping regularization\\nTIP'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 266, 'page_label': '267', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"With stochastic and mini-batch gradient descent, the curves are not so smooth, and it may\\nbe hard to know whether you have reached the minimum or not. One solution is to stop\\nonly after the validation error has been above the minimum for some time (when you are\\nconfident that the model will not do any better), then roll back the model parameters to the\\npoint where the validation error was at a minimum.\\nHere is a basic implementation of early stopping:\\nfrom\\n \\ncopy\\n \\nimport\\n \\ndeepcopy\\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nmean_squared_error\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nStandardScaler\\nX_train\\n,\\n \\ny_train\\n,\\n \\nX_valid\\n,\\n \\ny_valid\\n \\n=\\n \\n[\\n...\\n]\\n  \\n# split the quadratic dataset\\npreprocessing\\n \\n=\\n \\nmake_pipeline\\n(\\nPolynomialFeatures\\n(\\ndegree\\n=\\n90\\n,\\n \\ninclude_bias\\n=\\nFalse\\n),\\n                              \\nStandardScaler\\n())\\nX_train_prep\\n \\n=\\n \\npreprocessing\\n.\\nfit_transform\\n(\\nX_train\\n)\\nX_valid_prep\\n \\n=\\n \\npreprocessing\\n.\\ntransform\\n(\\nX_valid\\n)\\nsgd_reg\\n \\n=\\n \\nSGDRegressor\\n(\\npenalty\\n=\\nNone\\n,\\n \\neta0\\n=\\n0.002\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nn_epochs\\n \\n=\\n \\n500\\nbest_valid_rmse\\n \\n=\\n \\nfloat\\n(\\n'inf'\\n)\\nfor\\n \\nepoch\\n \\nin\\n \\nrange\\n(\\nn_epochs\\n):\\n    \\nsgd_reg\\n.\\npartial_fit\\n(\\nX_train_prep\\n,\\n \\ny_train\\n)\\n    \\ny_valid_predict\\n \\n=\\n \\nsgd_reg\\n.\\npredict\\n(\\nX_valid_prep\\n)\\n    \\nval_error\\n \\n=\\n \\nmean_squared_error\\n(\\ny_valid\\n,\\n \\ny_valid_predict\\n,\\n \\nsquared\\n=\\nFalse\\n)\\n    \\nif\\n \\nval_error\\n \\n<\\n \\nbest_valid_rmse\\n:\\n        \\nbest_valid_rmse\\n \\n=\\n \\nval_error\\n        \\nbest_model\\n \\n=\\n \\ndeepcopy\\n(\\nsgd_reg\\n)\\nThis code first adds the polynomial features and scales all the input features,\\nboth for the training set and for the validation set (the code assumes that you\\nhave split the original training set into a smaller training set and a validation\\nset). \\nThen it creates an \\nSGDRegressor\\n model with no regularization and a\\nsmall learning rate. In the training loop, it calls \\npartial_fit()\\n instead of \\nfit()\\n, to\\nperform incremental learning. At each epoch, it measures the RMSE on the\\nvalidation set. \\nIf it is lower than the lowest RMSE seen so far, it saves a copy\\nof the model in the \\nbest_model\\n variable. This implementation does not\\nactually stop training, but it lets you revert to the best model after training.\\nNote that the model is copied using \\ncopy.deepcopy()\\n, because it copies both\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 267, 'page_label': '268', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='the model’s hyperparameters \\nand\\n the learned parameters. \\nIn contrast,\\nsklearn.base.clone()\\n only copies the model’s hyperparameters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 268, 'page_label': '269', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Logistic Regression\\nAs discussed in \\nChapter 1\\n, some regression algorithms can be used for\\nclassification (and vice versa). \\nLogistic regression\\n (also called \\nlogit\\nregression\\n) is commonly used to estimate the probability that an instance\\nbelongs to a particular class (e.g., what is the probability that this email is\\nspam?). \\nIf the estimated probability is greater than a given threshold\\n(typically 50%), then the model predicts that the instance belongs to that class\\n(called the \\npositive class\\n, labeled “1”), and otherwise it predicts that it does\\nnot (i.e., it belongs to the \\nnegative class\\n, labeled “0”). \\nThis makes it a binary\\nclassifier.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 269, 'page_label': '270', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Estimating Probabilities\\nSo how does logistic regression work? \\nJust like a linear regression model, a\\nlogistic regression model computes a weighted sum of the input features (plus\\na bias term), but instead of outputting the result directly like the linear\\nregression model does, it outputs the \\nlogistic\\n of this result (see \\nEquation 4-\\n13\\n).\\nEquation 4-13. \\nLogistic regression model estimated probability (vectorized form)\\np ^ \\n= \\nh θ \\n( \\nx \\n) \\n= \\nσ \\n( \\nθ \\n⊺\\n \\nx \\n)\\nThe logistic—noted \\nσ\\n(·)—is a \\nsigmoid function\\n (i.e., \\nS\\n-shaped) that outputs a\\nnumber between 0 and 1. \\nIt is defined as shown in \\nEquation 4-14\\n and\\nFigure 4-21\\n.\\nEquation 4-14. \\nLogistic function\\nσ \\n( \\nt \\n) \\n= \\n1 1+exp(-t)\\nFigure 4-21. \\nLogistic function\\nOnce the logistic regression model has estimated the probability p^ = \\nh\\n(\\nx\\n)\\nthat an instance \\nx\\n belongs to the positive class, it can make its prediction \\nŷ\\neasily (see \\nEquation 4-15\\n).\\nEquation 4-15. \\nLogistic regression model prediction using a 50% threshold \\nprobability\\ny ^ \\n= \\n0 \\nif \\np ^ \\n< \\n0.5 \\n1 \\nif \\np ^ \\n≥ \\n0.5\\nθ'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 270, 'page_label': '271', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Notice that \\nσ\\n(\\nt\\n) < 0.5 when \\nt\\n < 0, and \\nσ\\n(\\nt\\n) ≥ 0.5 when \\nt\\n ≥ 0, so a logistic\\nregression model using the default threshold of 50% probability predicts 1 if\\nθ\\n \\nx\\n is positive and 0 if it is negative.\\nNOTE\\nThe score \\nt\\n is often called the \\nlogit\\n. \\nThe name comes from the fact that the logit function,\\ndefined as logit(\\np\\n) = log(\\np\\n / (1 – \\np\\n)), is the inverse of the logistic function. Indeed, if you\\ncompute the logit of the estimated probability \\np\\n, you will find that the result is \\nt\\n. The logit\\nis also called the \\nlog-odds\\n, since it is the log of the ratio between the estimated probability\\nfor the positive class and the estimated probability for the negative class.\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 271, 'page_label': '272', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Training and Cost Function\\nNow you know how a logistic regression model estimates probabilities and\\nmakes predictions. \\nBut how is it trained? The objective of training is to set\\nthe parameter vector \\nθ\\n so that the model estimates high probabilities for\\npositive instances (\\ny\\n = 1) and low probabilities for negative instances (\\ny\\n = 0).\\nThis idea is captured by the cost function shown in \\nEquation 4-16\\n for a single\\ntraining instance \\nx\\n.\\nEquation 4-16. \\nCost function of a single training instance\\nc(θ)=-log(p^)if y=1-log(1-p^)if y=0\\nThis cost function makes sense because \\n–log(\\nt\\n) grows very large when \\nt\\napproaches 0, so the cost will be large if the model estimates a probability\\nclose to 0 for a positive instance, and it will also be large if the model\\nestimates a probability close to 1 for a negative instance. On the other hand, –\\nlog(\\nt\\n) is close to 0 when \\nt\\n is close to 1, so the cost will be close to 0 if the\\nestimated probability is close to 0 for a negative instance or close to 1 for a\\npositive instance, which is precisely what we want.\\nThe cost function over the whole training set is the average cost over all\\ntraining instances. \\nIt can be written in a single expression called the \\nlog loss\\n,\\nshown in \\nEquation 4-17\\n.\\nEquation 4-17. \\nLogistic regression cost function (log loss)\\nJ(θ)=-1m∑i=1my(i)logp^(i)+(1-y(i))log1-p^(i)\\nWARNING\\nThe log loss was not just pulled out of a hat. It can be shown mathematically (using\\nBayesian inference) that minimizing this loss will result in the model with the \\nmaximum\\nlikelihood\\n of being optimal, assuming that the instances follow a Gaussian distribution\\naround the mean of their class. \\nWhen you use the log loss, this is the implicit assumption\\nyou are making. The more wrong this assumption is, the more biased the model will be.\\nSimilarly, when we used the MSE to train linear regression models, we were implicitly\\nassuming that the data was purely linear, plus some Gaussian noise. So, if the data is not\\nlinear (e.g., if it’s quadratic) or if the noise is not Gaussian (e.g., if outliers are not'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 272, 'page_label': '273', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='exponentially rare), then the model will be biased.\\nThe bad news is that there is no known closed-form equation to compute the\\nvalue of \\nθ\\n that minimizes this cost function (there is no equivalent of the\\nNormal equation). \\nBut the good news is that this cost function is convex, so\\ngradient descent (or any other optimization algorithm) is guaranteed to find\\nthe global minimum (if the learning rate is not too large and you wait long\\nenough). \\nThe partial derivatives of the cost function with regard to the \\nj\\nmodel parameter \\nθ\\n are given by \\nEquation 4-18\\n.\\nEquation 4-18. \\nLogistic cost function partial derivatives\\n∂ ∂θ j \\nJ \\n( \\nθ \\n) \\n= \\n1 m \\n∑ i=1 m \\nσ \\n( \\nθ \\n⊺\\n \\nx (i) \\n) \\n- \\ny (i) \\nx j (i)\\nThis equation looks very much like \\nEquation 4-5\\n: for each instance it\\ncomputes the prediction error and multiplies it by the \\nj\\n feature value, and\\nthen it computes the average over all training instances. Once you have the\\ngradient vector containing all the partial derivatives, you can use it in the\\nbatch gradient descent algorithm. That’s it: you now know how to train a\\nlogistic regression model. For stochastic GD you would take one instance at a\\ntime, and for mini-batch GD you would use a mini-batch at a time.\\nth\\nj\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 273, 'page_label': '274', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"Decision Boundaries\\nWe can use the iris dataset to illustrate logistic regression. \\nThis is a famous\\ndataset that contains the sepal and petal length and width of 150 iris flowers\\nof three different species: \\nIris setosa\\n, \\nIris versicolor\\n, and \\nIris virginica\\n (see\\nFigure 4-22\\n).\\nFigure 4-22. \\nFlowers of three iris plant species\\n\\u2060\\nLet’s try to build a classifier to detect the \\nIris virginica\\n type based only on the\\npetal width feature. The first step is to load the data and take a quick peek:\\n>>> \\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nload_iris\\n>>> \\niris\\n \\n=\\n \\nload_iris\\n(\\nas_frame\\n=\\nTrue\\n)\\n>>> \\nlist\\n(\\niris\\n)\\n['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names',\\n 'filename', 'data_module']\\n>>> \\niris\\n.\\ndata\\n.\\nhead\\n(\\n3\\n)\\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\\n0                5.1               3.5                1.4               0.2\\n1                4.9               3.0                1.4               0.2\\n2                4.7               3.2                1.3               0.2\\n>>> \\niris\\n.\\ntarget\\n.\\nhead\\n(\\n3\\n)\\n  \\n# note that the instances are not shuffled\\n0    0\\n1    0\\n12\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 274, 'page_label': '275', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='2    0\\nName: target, dtype: int64\\n>>> \\niris\\n.\\ntarget_names\\narray([\\'setosa\\', \\'versicolor\\', \\'virginica\\'], dtype=\\'<U10\\')\\nNext we’ll split the data and train a logistic regression model on the training\\nset:\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLogisticRegression\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\ntrain_test_split\\nX\\n \\n=\\n \\niris\\n.\\ndata\\n[[\\n\"petal width (cm)\"\\n]]\\n.\\nvalues\\ny\\n \\n=\\n \\niris\\n.\\ntarget_names\\n[\\niris\\n.\\ntarget\\n]\\n \\n==\\n \\n\\'virginica\\'\\nX_train\\n,\\n \\nX_test\\n,\\n \\ny_train\\n,\\n \\ny_test\\n \\n=\\n \\ntrain_test_split\\n(\\nX\\n,\\n \\ny\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nlog_reg\\n \\n=\\n \\nLogisticRegression\\n(\\nrandom_state\\n=\\n42\\n)\\nlog_reg\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\nLet’s look at the model’s estimated probabilities for flowers with petal widths\\nvarying from 0 cm to 3 cm (\\nFigure 4-23\\n):\\n\\u2060\\nX_new\\n \\n=\\n \\nnp\\n.\\nlinspace\\n(\\n0\\n,\\n \\n3\\n,\\n \\n1000\\n)\\n.\\nreshape\\n(\\n-\\n1\\n,\\n \\n1\\n)\\n  \\n# reshape to get a column vector\\ny_proba\\n \\n=\\n \\nlog_reg\\n.\\npredict_proba\\n(\\nX_new\\n)\\ndecision_boundary\\n \\n=\\n \\nX_new\\n[\\ny_proba\\n[:,\\n \\n1\\n]\\n \\n>=\\n \\n0.5\\n][\\n0\\n,\\n \\n0\\n]\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n \\ny_proba\\n[:,\\n \\n0\\n],\\n \\n\"b--\"\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n         \\nlabel\\n=\\n\"Not Iris virginica proba\"\\n)\\nplt\\n.\\nplot\\n(\\nX_new\\n,\\n \\ny_proba\\n[:,\\n \\n1\\n],\\n \\n\"g-\"\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n \\nlabel\\n=\\n\"Iris virginica proba\"\\n)\\nplt\\n.\\nplot\\n([\\ndecision_boundary\\n,\\n \\ndecision_boundary\\n],\\n \\n[\\n0\\n,\\n \\n1\\n],\\n \\n\"k:\"\\n,\\n \\nlinewidth\\n=\\n2\\n,\\n         \\nlabel\\n=\\n\"Decision boundary\"\\n)\\n[\\n...\\n]\\n \\n# beautify the figure: add grid, labels, axis, legend, arrows, and samples\\nplt\\n.\\nshow\\n()\\n13'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 275, 'page_label': '276', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-23. \\nEstimated probabilities and decision boundary\\nThe petal width of \\nIris virginica\\n flowers (represented as triangles) ranges\\nfrom 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares)\\ngenerally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice\\nthat there is a bit of overlap. Above about 2 cm the classifier is highly\\nconfident that the flower is an \\nIris virginica\\n (it outputs a high probability for\\nthat class), while below 1 cm it is highly confident that it is not an \\nIris\\nvirginica\\n (high probability for the “Not Iris virginica” class). In between\\nthese extremes, the classifier is unsure. However, if you ask it to predict the\\nclass (using the \\npredict()\\n method rather than the \\npredict_proba()\\n method), it\\nwill return whichever class is the most likely. Therefore, there is a \\ndecision\\nboundary\\n at around 1.6 cm where both probabilities are equal to 50%: if the\\npetal width is greater than 1.6 cm the classifier will predict that the flower is\\nan \\nIris virginica\\n, and otherwise it will predict that it is not (even if it is not\\nvery confident):\\n>>> \\ndecision_boundary\\n1.6516516516516517\\n>>> \\nlog_reg\\n.\\npredict\\n([[\\n1.7\\n],\\n \\n[\\n1.5\\n]])\\narray([ True, False])\\nFigure 4-24\\n shows the same dataset, but this time displaying two features:\\npetal width and length. Once trained, the logistic regression classifier can,\\nbased on these two features, estimate the probability that a new flower is an\\nIris virginica\\n. The dashed line represents the points where the model\\nestimates a 50% probability: this is the model’s decision boundary. Note that\\n14'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 276, 'page_label': '277', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='it is a linear boundary.\\n\\u2060\\n Each parallel line represents the points where the\\nmodel outputs a specific probability, from 15% (bottom left) to 90% (top\\nright). All the flowers beyond the top-right line have over 90% chance of\\nbeing \\nIris virginica\\n, according to the model.\\nFigure 4-24. \\nLinear decision boundary\\nNOTE\\nThe hyperparameter controlling the regularization strength of a Scikit-Learn\\nLogisticRegression\\n model is not \\nalpha\\n (as in other linear models), but its inverse: \\nC\\n. The\\nhigher the value of \\nC\\n, the \\nless\\n the model is regularized.\\nJust like the other linear models, logistic regression models can be\\nregularized using ℓ\\n or ℓ\\n penalties. Scikit-Learn actually adds an ℓ\\n penalty\\nby default.\\n14\\n1\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 277, 'page_label': '278', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Softmax Regression\\nThe logistic regression model can be generalized to support multiple classes\\ndirectly, without having to train and combine multiple binary classifiers (as\\ndiscussed in \\nChapter 3\\n). \\nThis is called \\nsoftmax regression\\n, or \\nmultinomial\\nlogistic regression\\n.\\nThe idea is simple: when given an instance \\nx\\n, the softmax regression model\\nfirst computes a score \\ns\\n(\\nx\\n) for each class \\nk\\n, then estimates the probability of\\neach class by applying the \\nsoftmax function\\n (also called the \\nnormalized\\nexponential\\n) to the scores. \\nThe equation to compute \\ns\\n(\\nx\\n) should look\\nfamiliar, as it is just like the equation for linear regression prediction (see\\nEquation 4-19\\n).\\nEquation 4-19. \\nSoftmax score for class k\\ns k \\n( \\nx \\n) \\n= \\n(θ (k) ) \\n⊺\\n \\nx\\nNote that each class has its own dedicated parameter vector \\nθ\\n. All these\\nvectors are typically stored as rows in a \\nparameter matrix\\n \\nΘ\\n.\\nOnce you have computed the score of every class for the instance \\nx\\n, you can\\nestimate the probability p^k that the instance belongs to class \\nk\\n by running\\nthe scores through the softmax function (\\nEquation 4-20\\n). The function\\ncomputes the exponential of every score, then normalizes them (dividing by\\nthe sum of all the exponentials). The scores are generally called logits or log-\\nodds (although they are actually unnormalized log-odds).\\nEquation 4-20. \\nSoftmax function\\np ^ k \\n= \\nσ \\ns(x) k \\n= \\nexps k (x) ∑ j=1 K exps j (x)\\nIn this equation:\\nK\\n is the number of classes.\\ns\\n(\\nx\\n) is a vector containing the scores of each class for the instance \\nx\\n.\\nσ\\n(\\ns\\n(\\nx\\n))\\n is the estimated probability that the instance \\nx\\n belongs to class \\nk\\n,\\nk\\nk\\n(\\nk\\n)\\nk'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 278, 'page_label': '279', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='given the scores of each class for that instance.\\nJust like the logistic regression classifier, by default the softmax regression\\nclassifier predicts the class with the highest estimated probability (which is\\nsimply the class with the highest score), as shown in \\nEquation 4-21\\n.\\nEquation 4-21. \\nSoftmax regression classifier prediction\\ny ^ \\n= \\nargmax k \\nσ \\ns(x) k \\n= \\nargmax k \\ns k \\n( \\nx \\n) \\n= \\nargmax k \\n(θ (k) ) \\n⊺\\n \\nx\\nThe \\nargmax\\n operator returns the value of a variable that maximizes a\\nfunction. \\nIn this equation, it returns the value of \\nk\\n that maximizes the\\nestimated probability \\nσ\\n(\\ns\\n(\\nx\\n))\\n.\\nTIP\\nThe softmax regression classifier predicts only one class at a time (i.e., it is multiclass, not\\nmultioutput), so it should be used only with mutually exclusive classes, such as different\\nspecies of plants. You cannot use it to recognize multiple people in one picture.\\nNow that you know how the model estimates probabilities and makes\\npredictions, let’s take a look at training. The objective is to have a model that\\nestimates a high probability for the target class (and consequently a low\\nprobability for the other classes). \\nMinimizing the cost function shown in\\nEquation 4-22\\n, called the \\ncross entropy\\n, should lead to this objective because\\nit penalizes the model when it estimates a low probability for a target class.\\nCross entropy is frequently used to measure how well a set of estimated class\\nprobabilities matches the target classes.\\nEquation 4-22. \\nCross entropy cost function\\nJ(Θ)=-1m∑i=1m∑k=1Kyk(i)logp^k(i)\\nIn this equation, yk(i) is the target probability that the \\ni\\n instance belongs to\\nclass \\nk\\n. In general, it is either equal to 1 or 0, depending on whether the\\ninstance belongs to the class or not.\\nNotice that when there are just two classes (\\nK\\n = 2), this cost function is\\nk\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 279, 'page_label': '280', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='equivalent to the logistic regression cost function (log loss; see \\nEquation 4-\\n17\\n).\\nCROSS ENTROPY\\nCross entropy originated from Claude Shannon’s \\ninformation theory\\n.\\nSuppose you want to efficiently transmit information about the weather\\nevery day. \\nIf there are eight options (sunny, rainy, etc.), you could encode\\neach option using 3 bits, because 2\\n = 8. However, if you think it will be\\nsunny almost every day, it would be much more efficient to code “sunny”\\non just one bit (0) and the other seven options on four bits (starting with a\\n1). Cross entropy measures the average number of bits you actually send\\nper option. \\nIf your assumption about the weather is perfect, cross entropy\\nwill be equal to the entropy of the weather itself (i.e., its intrinsic\\nunpredictability). But if your assumption is wrong (e.g., if it rains often),\\ncross entropy will be greater by an amount called the \\nKullback–Leibler\\n(KL) divergence\\n.\\nThe cross entropy between two probability distributions \\np\\n and \\nq\\n is\\ndefined as \\nH\\n(\\np\\n,\\nq\\n) = –Σ\\n\\u2009\\np\\n(\\nx\\n) log \\nq\\n(\\nx\\n) (at least when the distributions are\\ndiscrete). For more details, check out \\nmy video on the subject\\n.\\nThe gradient vector of this cost function with regard to \\nθ\\n is given by\\nEquation 4-23\\n.\\nEquation 4-23. \\nCross entropy gradient vector for class k\\n∇\\n θ (k) \\nJ \\n( \\nΘ \\n) \\n= \\n1 m \\n∑ i=1 m \\np ^ k (i) \\n- \\ny k (i) \\nx (i)\\nNow you can compute the gradient vector for every class, then use gradient\\ndescent (or any other optimization algorithm) to find the parameter matrix \\nΘ\\nthat minimizes the cost function.\\nLet’s use softmax regression to classify the iris plants into all three classes.\\nScikit-Learn’s \\nLogisticRegression\\n classifier uses softmax regression\\nautomatically when you train it on more than two classes (assuming you use\\nsolver=\"lbfgs\"\\n, which is the default). It also applies ℓ\\n regularization by\\n3\\nx\\n(\\nk\\n)\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 280, 'page_label': '281', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='default, which you can control using the hyperparameter \\nC\\n, as mentioned\\nearlier:\\nX\\n \\n=\\n \\niris\\n.\\ndata\\n[[\\n\"petal length (cm)\"\\n,\\n \\n\"petal width (cm)\"\\n]]\\n.\\nvalues\\ny\\n \\n=\\n \\niris\\n[\\n\"target\"\\n]\\nX_train\\n,\\n \\nX_test\\n,\\n \\ny_train\\n,\\n \\ny_test\\n \\n=\\n \\ntrain_test_split\\n(\\nX\\n,\\n \\ny\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nsoftmax_reg\\n \\n=\\n \\nLogisticRegression\\n(\\nC\\n=\\n30\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nsoftmax_reg\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\nSo the next time you find an iris with petals that are 5 cm long and 2 cm\\nwide, you can ask your model to tell you what type of iris it is, and it will\\nanswer \\nIris virginica\\n (class 2) with 96% probability (or \\nIris versicolor\\n with\\n4% probability):\\n>>> \\nsoftmax_reg\\n.\\npredict\\n([[\\n5\\n,\\n \\n2\\n]])\\narray([2])\\n>>> \\nsoftmax_reg\\n.\\npredict_proba\\n([[\\n5\\n,\\n \\n2\\n]])\\n.\\nround\\n(\\n2\\n)\\narray([[0.  , 0.04, 0.96]])\\nFigure 4-25\\n shows the resulting decision boundaries, represented by the\\nbackground colors. Notice that the decision boundaries between any two\\nclasses are linear. \\nThe figure also shows the probabilities for the \\nIris\\nversicolor\\n class, represented by the curved lines (e.g., the line labeled with\\n0.30 represents the 30% probability boundary). Notice that the model can\\npredict a class that has an estimated probability below 50%. For example, at\\nthe point where all decision boundaries meet, all classes have an equal\\nestimated probability of 33%.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 281, 'page_label': '282', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 4-25. \\nSoftmax regression decision boundaries\\nIn this chapter, you learned various ways to train linear models, both for\\nregression and for classification. You used a closed-form equation to solve\\nlinear regression, as well as gradient descent, and you learned how various\\npenalties can be added to the cost function during training to regularize the\\nmodel. Along the way, you also learned how to plot learning curves and\\nanalyze them, and how to implement early stopping. Finally, you learned\\nhow logistic regression and softmax regression work. We’ve opened up the\\nfirst machine learning black boxes! In the next chapters we will open many\\nmore, starting with support vector machines.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 282, 'page_label': '283', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nWhich linear regression training algorithm can you use if you have a\\ntraining set with millions of features?\\n2\\n. \\nSuppose the features in your training set have very different scales.\\nWhich algorithms might suffer from this, and how? What can you do\\nabout it?\\n3\\n. \\nCan gradient descent get stuck in a local minimum when training a\\nlogistic regression model?\\n4\\n. \\nDo all gradient descent algorithms lead to the same model, provided you\\nlet them run long enough?\\n5\\n. \\nSuppose you use batch gradient descent and you plot the validation error\\nat every epoch. If you notice that the validation error consistently goes\\nup, what is likely going on? How can you fix this?\\n6\\n. \\nIs it a good idea to stop mini-batch gradient descent immediately when\\nthe validation error goes up?\\n7\\n. \\nWhich gradient descent algorithm (among those we discussed) will\\nreach the vicinity of the optimal solution the fastest? Which will actually\\nconverge? How can you make the others converge as well?\\n8\\n. \\nSuppose you are using polynomial regression. You plot the learning\\ncurves and you notice that there is a large gap between the training error\\nand the validation error. What is happening? What are three ways to\\nsolve this?\\n9\\n. \\nSuppose you are using ridge regression and you notice that the training\\nerror and the validation error are almost equal and fairly high. Would\\nyou say that the model suffers from high bias or high variance? Should\\nyou increase the regularization hyperparameter \\nα\\n or reduce it?\\n10\\n. \\nWhy would you want to use:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 283, 'page_label': '284', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='a\\n. \\nRidge regression instead of plain linear regression (i.e., without any\\nregularization)?\\nb\\n. \\nLasso instead of ridge regression?\\nc\\n. \\nElastic net instead of lasso regression?\\n11\\n. \\nSuppose you want to classify pictures as outdoor/indoor and\\ndaytime/nighttime. Should you implement two logistic regression\\nclassifiers or one softmax regression classifier?\\n12\\n. \\nImplement batch gradient descent with early stopping for softmax\\nregression without using Scikit-Learn, only NumPy. Use it on a\\nclassification task such as the iris dataset.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n A closed-form equation is only composed of a finite number of constants, variables, and\\nstandard operations: for example, \\na\\n = sin(\\nb\\n – \\nc\\n). No infinite sums, no limits, no integrals, etc.\\n2\\n Technically speaking, its \\nderivative is \\nLipschitz continuous\\n.\\n3\\n Since feature 1 is smaller, it takes a larger change in \\nθ\\n to affect the cost function, which is why\\nthe bowl is elongated along the \\nθ\\n axis.\\n4\\n Eta (\\nη\\n) is the seventh letter of the Greek alphabet.\\n5\\n While the Normal equation can only perform linear regression, the gradient descent algorithms\\ncan be used to train many other models, as you’ll see.\\n6\\n This notion of bias is not to be confused with the bias term of linear models.\\n7\\n It is common to use the notation \\nJ\\n(\\nθ\\n) for cost functions that don’t have a short name; I’ll often\\nuse this notation throughout the rest of this book. The context will make it clear which cost\\nfunction is being discussed.\\n8\\n Norms are discussed in \\nChapter 2\\n.\\n9\\n A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).\\n10\\n Alternatively, you can use the \\nRidge\\n class with the \\n\"sag\"\\n solver. Stochastic average GD is a\\nvariant of stochastic GD. For more details, see the presentation \\n“Minimizing Finite Sums with\\nthe Stochastic Average Gradient Algorithm”\\n by Mark Schmidt et al. from the University of\\nBritish Columbia.\\n11\\n You can think of a subgradient vector at a nondifferentiable point as an intermediate vector\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 284, 'page_label': '285', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='between the gradient vectors around that point.\\n12\\n Photos reproduced from the corresponding Wikipedia pages. \\nIris virginica\\n photo by Frank\\nMayfield (\\nCreative Commons BY-SA 2.0\\n), \\nIris versicolor\\n photo by D. Gordon E. Robertson\\n(\\nCreative Commons BY-SA 3.0\\n), \\nIris setosa\\n photo public domain.\\n13\\n NumPy’s \\nreshape()\\n function allows one dimension to be –1, which means “automatic”: the\\nvalue is inferred from the length of the array and the remaining dimensions.\\n14\\n It is the set of points \\nx\\n such that \\nθ\\n + \\nθ\\nx\\n + \\nθ\\nx\\n = 0, which defines a straight line.\\n0\\n1\\n1\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 285, 'page_label': '286', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 5. \\nSupport Vector\\nMachines\\nA \\nsupport vector machine\\n (SVM) is a powerful and versatile machine\\nlearning model, capable of performing linear or nonlinear classification,\\nregression, and even novelty detection. \\nSVMs shine with small to medium-\\nsized nonlinear datasets (i.e., hundreds to thousands of instances), especially\\nfor classification tasks. However, they don’t scale very well to very large\\ndatasets, as you will see.\\nThis chapter will explain the core concepts of SVMs, how to use them, and\\nhow they work. Let’s jump right in!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 286, 'page_label': '287', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Linear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some visuals.\\nFigure 5-1\\n shows part of the iris dataset that was introduced at the end of\\nChapter 4\\n. \\nThe two classes can clearly be separated easily with a straight line\\n(they are \\nlinearly separable\\n). The left plot shows the decision boundaries of\\nthree possible linear classifiers. The model whose decision boundary is\\nrepresented by the dashed line is so bad that it does not even separate the\\nclasses properly. The other two models work perfectly on this training set,\\nbut their decision boundaries come so close to the instances that these models\\nwill probably not perform as well on new instances. In contrast, the solid line\\nin the plot on the right represents the decision boundary of an SVM classifier;\\nthis line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. You can think of an SVM classifier as\\nfitting the widest possible street (represented by the parallel dashed lines)\\nbetween the classes. \\nThis is called \\nlarge margin classification\\n.\\nFigure 5-1. \\nLarge margin classification\\nNotice that adding more training instances “off the street” will not affect the\\ndecision boundary at all: it is fully determined (or “supported”) by the\\ninstances located on the edge of the street. \\nThese instances are called the\\nsupport vectors\\n (they are circled in \\nFigure 5-1\\n).\\nWARNING\\nSVMs are sensitive to the feature scales, as you can see in \\nFigure 5-2\\n. In the left plot, the\\nvertical scale is much larger than the horizontal scale, so the widest possible street is close\\nto horizontal. \\nAfter feature scaling (e.g., using Scikit-Learn’s \\nStandardScaler\\n), the decision'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 287, 'page_label': '288', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='boundary in the right plot looks much better.\\nFigure 5-2. \\nSensitivity to feature scales'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 288, 'page_label': '289', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Soft Margin Classification\\nIf we strictly impose that all instances must be off the street and on the\\ncorrect side, this is called \\nhard margin classification\\n. \\nThere are two main\\nissues with hard margin classification. First, it only works if the data is\\nlinearly separable. Second, it is sensitive to outliers. \\nFigure 5-3\\n shows the iris\\ndataset with just one additional outlier: on the left, it is impossible to find a\\nhard margin; on the right, the decision boundary ends up very different from\\nthe one we saw in \\nFigure 5-1\\n without the outlier, and the model will probably\\nnot generalize as well.\\nFigure 5-3. \\nHard margin sensitivity to outliers\\nTo avoid these issues, we need to use a more flexible model. \\nThe objective is\\nto find a good balance between keeping the street as large as possible and\\nlimiting the \\nmargin violations\\n (i.e., instances that end up in the middle of the\\nstreet or even on the wrong side). \\nThis is called \\nsoft margin classification\\n.\\nWhen creating an SVM model using Scikit-Learn, you can specify several\\nhyperparameters, including the regularization hyperparameter \\nC\\n. If you set it\\nto a low value, then you end up with the model on the left of \\nFigure 5-4\\n. With\\na high value, you get the model on the right. As you can see, reducing \\nC\\nmakes the street larger, but it also leads to more margin violations. In other\\nwords, reducing \\nC\\n results in more instances supporting the street, so there’s\\nless risk of overfitting. But if you reduce it too much, then the model ends up\\nunderfitting, as seems to be the case here: the model with \\nC=100\\n looks like it\\nwill generalize better than the one with \\nC=1\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 289, 'page_label': '290', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 5-4. \\nLarge margin (left) versus fewer margin violations (right)\\nTIP\\nIf your SVM model is overfitting, you can try regularizing it by reducing \\nC\\n.\\nThe following Scikit-Learn code loads the iris dataset and trains a linear\\nSVM classifier to detect \\nIris virginica\\n flowers. \\nThe pipeline first scales the\\nfeatures, then uses a \\nLinearSVC\\n with \\nC=1\\n:\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nload_iris\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nmake_pipeline\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nStandardScaler\\nfrom\\n \\nsklearn.svm\\n \\nimport\\n \\nLinearSVC\\niris\\n \\n=\\n \\nload_iris\\n(\\nas_frame\\n=\\nTrue\\n)\\nX\\n \\n=\\n \\niris\\n.\\ndata\\n[[\\n\"petal length (cm)\"\\n,\\n \\n\"petal width (cm)\"\\n]]\\n.\\nvalues\\ny\\n \\n=\\n \\n(\\niris\\n.\\ntarget\\n \\n==\\n \\n2\\n)\\n  \\n# Iris virginica\\nsvm_clf\\n \\n=\\n \\nmake_pipeline\\n(\\nStandardScaler\\n(),\\n                        \\nLinearSVC\\n(\\nC\\n=\\n1\\n,\\n \\nrandom_state\\n=\\n42\\n))\\nsvm_clf\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nThe resulting model is represented on the left in \\nFigure 5-4\\n.\\nThen, as usual, you can use the model to make predictions:\\n>>> \\nX_new\\n \\n=\\n \\n[[\\n5.5\\n,\\n \\n1.7\\n],\\n \\n[\\n5.0\\n,\\n \\n1.5\\n]]\\n>>> \\nsvm_clf\\n.\\npredict\\n(\\nX_new\\n)\\narray([ True, False])\\nThe first plant is classified as an \\nIris virginica\\n, while the second is not. Let’s'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 290, 'page_label': '291', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='look at the scores that the SVM used to make these predictions. These\\nmeasure the signed distance between each instance and the decision\\nboundary:\\n>>> \\nsvm_clf\\n.\\ndecision_function\\n(\\nX_new\\n)\\narray([ 0.66163411, -0.22036063])\\nUnlike \\nLogisticRegression\\n, \\nLinearSVC\\n doesn’t have a \\npredict_proba()\\nmethod to estimate the class probabilities. That said, if you use the \\nSVC\\n class\\n(discussed shortly) instead of \\nLinearSVC\\n, and if you set its \\nprobability\\nhyperparameter to \\nTrue\\n, then the model will fit an extra model at the end of\\ntraining to map the SVM decision function scores to estimated probabilities.\\nUnder the hood, this requires using 5-fold cross-validation to generate out-of-\\nsample predictions for every instance in the training set, then training a\\nLogisticRegression\\n model, so it will slow down training considerably. After\\nthat, the \\npredict_proba()\\n and \\npredict_log_proba()\\n methods will be available.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 291, 'page_label': '292', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and often work surprisingly\\nwell, many datasets are not even close to being linearly separable. \\nOne\\napproach to handling nonlinear datasets is to add more features, such as\\npolynomial features (as we did in \\nChapter 4\\n); in some cases this can result in\\na linearly separable dataset. Consider the lefthand plot in \\nFigure 5-5\\n: it\\nrepresents a simple dataset with just one feature, \\nx\\n. This dataset is not\\nlinearly separable, as you can see. But if you add a second feature \\nx\\n = (\\nx\\n)\\n,\\nthe resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. \\nAdding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a pipeline\\ncontaining a \\nPolynomialFeatures\\n transformer (discussed in \\n“Polynomial\\nRegression”\\n), followed by a \\nStandardScaler\\n and a \\nLinearSVC\\n classifier. \\nLet’s\\ntest this on the moons dataset, \\na toy dataset for binary classification in which\\nthe data points are shaped as two interleaving crescent moons (see \\nFigure 5-\\n6\\n). You can generate this dataset using the \\nmake_moons()\\n function:\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nmake_moons\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nPolynomialFeatures\\nX\\n,\\n \\ny\\n \\n=\\n \\nmake_moons\\n(\\nn_samples\\n=\\n100\\n,\\n \\nnoise\\n=\\n0.15\\n,\\n \\nrandom_state\\n=\\n42\\n)\\npolynomial_svm_clf\\n \\n=\\n \\nmake_pipeline\\n(\\n    \\nPolynomialFeatures\\n(\\ndegree\\n=\\n3\\n),\\n    \\nStandardScaler\\n(),\\n    \\nLinearSVC\\n(\\nC\\n=\\n10\\n,\\n \\nmax_iter\\n=\\n10_000\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n1\\n2\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 292, 'page_label': '293', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=')\\npolynomial_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nFigure 5-6. \\nLinear SVM classifier using polynomial features'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 293, 'page_label': '294', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Polynomial Kernel\\nAdding polynomial features is simple to implement and can work great with\\nall sorts of machine learning algorithms (not just SVMs). \\nThat said, at a low\\npolynomial degree this method cannot deal with very complex datasets, and\\nwith a high polynomial degree it creates a huge number of features, making\\nthe model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous\\nmathematical technique called the \\nkernel trick\\n (which is explained later in\\nthis chapter). \\nThe kernel trick makes it possible to get the same result as if\\nyou had added many polynomial features, even with a very high degree,\\nwithout actually having to add them. \\nThis means there’s no combinatorial\\nexplosion of the number of features. This trick is implemented by the \\nSVC\\nclass. Let’s test it on the moons dataset:\\nfrom\\n \\nsklearn.svm\\n \\nimport\\n \\nSVC\\npoly_kernel_svm_clf\\n \\n=\\n \\nmake_pipeline\\n(\\nStandardScaler\\n(),\\n                                    \\nSVC\\n(\\nkernel\\n=\\n\"poly\"\\n,\\n \\ndegree\\n=\\n3\\n,\\n \\ncoef0\\n=\\n1\\n,\\n \\nC\\n=\\n5\\n))\\npoly_kernel_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nThis code trains an SVM classifier using a third-degree polynomial kernel,\\nrepresented on the left in \\nFigure 5-7\\n. On the right is another SVM classifier\\nusing a 10th-degree polynomial kernel. \\nObviously, if your model is\\noverfitting, you might want to reduce the polynomial degree. Conversely, if it\\nis underfitting, you can try increasing it. The hyperparameter \\ncoef0\\n controls\\nhow much the model is influenced by high-degree terms versus low-degree\\nterms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 294, 'page_label': '295', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 5-7. \\nSVM classifiers with a polynomial kernel\\nTIP\\nAlthough hyperparameters will generally be tuned automatically (e.g., using randomized\\nsearch), it’s good to have a sense of what each hyperparameter actually does and how it\\nmay interact with other hyperparameters: this way, you can narrow the search to a much\\nsmaller space.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 295, 'page_label': '296', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Similarity Features\\nAnother technique\\n to tackle nonlinear problems is to add features computed\\nusing a similarity function, which measures how much each instance\\nresembles a particular \\nlandmark\\n, as we did in \\nChapter 2\\n when we added the\\ngeographic similarity features. For example, let’s take the 1D dataset from\\nearlier and add two landmarks to it at \\nx\\n = –2 and \\nx\\n = 1 (see the left plot in\\nFigure 5-8\\n). Next, we’ll define the similarity function to be the Gaussian RBF\\nwith \\nγ\\n = 0.3. This is a bell-shaped function varying from 0 (very far away\\nfrom the landmark) to 1 (at the landmark).\\nNow we are ready to compute the new features. For example, let’s look at the\\ninstance \\nx\\n = –1: it is located at a distance of 1 from the first landmark and 2\\nfrom the second landmark. Therefore, its new features are \\nx\\n = exp(–0.3 × 1\\n)\\n≈ 0.74 and \\nx\\n = exp(–0.3 × 2\\n) ≈ 0.30. The plot on the right in \\nFigure 5-8\\nshows the transformed dataset (dropping the original features). As you can\\nsee, it is now linearly \\nseparable\\n.\\nFigure 5-8. \\nSimilarity features using the Gaussian RBF\\nYou may wonder how to select the landmarks. \\nThe simplest approach is to\\ncreate a landmark at the location of each and every instance in the dataset.\\nDoing that creates many dimensions and thus increases the chances that the\\ntransformed training set will be linearly separable. The downside is that a\\ntraining set with \\nm\\n instances and \\nn\\n features gets transformed into a training\\nset with \\nm\\n instances and \\nm\\n features (assuming you drop the original features).\\n1\\n1\\n1\\n2\\n2\\n3\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 296, 'page_label': '297', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='If your training set is very large, you end up with an equally large number of\\nfeatures.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 297, 'page_label': '298', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Gaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can\\nbe useful with any machine learning algorithm, but it may be computationally\\nexpensive to compute all the additional features (especially on large training\\nsets). \\nOnce again the kernel trick does its SVM magic, making it possible to\\nobtain a similar result as if you had added many similarity features, but\\nwithout actually doing so. Let’s try the \\nSVC\\n class with the Gaussian RBF\\nkernel\\n:\\nrbf_kernel_svm_clf\\n \\n=\\n \\nmake_pipeline\\n(\\nStandardScaler\\n(),\\n                                   \\nSVC\\n(\\nkernel\\n=\\n\"rbf\"\\n,\\n \\ngamma\\n=\\n5\\n,\\n \\nC\\n=\\n0.001\\n))\\nrbf_kernel_svm_clf\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nThis \\nmodel is represented at the bottom left in \\nFigure 5-9\\n. The other plots\\nshow models trained with different values of hyperparameters \\ngamma\\n (\\nγ\\n) and\\nC\\n. Increasing \\ngamma\\n makes the bell-shaped curve narrower (see the lefthand\\nplots in \\nFigure 5-8\\n). As a result, each instance’s range of influence is smaller:\\nthe decision boundary ends up being more irregular, wiggling around\\nindividual instances. Conversely, a small \\ngamma\\n value makes the bell-shaped\\ncurve wider: instances have a larger range of influence, and the decision\\nboundary ends up smoother. \\nSo \\nγ\\n acts like a regularization \\nhyperparameter\\n: if\\nyour model is overfitting, you should reduce \\nγ\\n; if it is underfitting, you should\\nincrease \\nγ\\n (similar to the \\nC\\n hyperparameter).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 298, 'page_label': '299', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 5-9. \\nSVM classifiers using an RBF kernel\\nOther kernels exist but are used much more rarely. \\nSome kernels are\\nspecialized for specific data structures. \\nString kernels\\n are sometimes used\\nwhen classifying text documents or DNA sequences (e.g., using the string\\nsubsequence kernel or kernels based on the Levenshtein distance).\\nTIP\\nWith so many kernels to choose from, how can you decide which one to use? As a rule of\\nthumb, you should always try the \\nlinear\\n kernel first. The \\nLinearSVC\\n class is much faster\\nthan \\nSVC(kernel=\"linear\")\\n, especially if the training set is very large. If it is not too large,\\nyou should also try kernelized SVMs, starting with the Gaussian RBF kernel; it often\\nworks really well. Then, if you have spare time and computing power, you can experiment\\nwith a few other kernels using hyperparameter search. If there are kernels specialized for\\nyour training set’s data structure, make sure to give them a try too.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 299, 'page_label': '300', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='SVM Classes and Computational Complexity\\nThe \\nLinearSVC\\n class is based on the \\nliblinear\\n library, which implements an\\noptimized algorithm\\n for linear SVMs.\\n\\u2060\\n \\nIt does not support the kernel trick,\\nbut it scales almost linearly with the number of training instances and the\\nnumber of features. Its training time complexity is roughly \\nO\\n(\\nm\\n × \\nn\\n). \\nThe\\nalgorithm takes longer if you require very high precision. This is controlled\\nby the tolerance hyperparameter \\nϵ\\n (called \\ntol\\n in Scikit-Learn). In most\\nclassification tasks, the default tolerance is fine.\\nThe \\nSVC\\n class is based on the \\nlibsvm\\n library, which implements an \\nalgorithm\\nthat supports the kernel trick\\n.\\n\\u2060\\n The training time complexity is usually\\nbetween \\nO\\n(\\nm\\n × \\nn\\n) and \\nO\\n(\\nm\\n × \\nn\\n). \\nUnfortunately, this means that it gets\\ndreadfully slow when the number of training instances gets large (e.g.,\\nhundreds of thousands of instances), so this algorithm is best for small or\\nmedium-sized nonlinear training sets. \\nIt scales well with the number of\\nfeatures, especially with sparse features (i.e., when each instance has few\\nnonzero features). In this case, the algorithm scales roughly with the average\\nnumber of nonzero features per instance.\\nThe \\nSGDClassifier\\n class also performs large margin classification by default,\\nand its hyperparameters–especially the regularization hyperparameters (\\nalpha\\nand \\npenalty\\n) and the \\nlearning_rate\\n–can be adjusted to produce similar results\\nas the linear SVMs. \\nFor training it uses stochastic gradient descent (see\\nChapter 4\\n), which allows incremental learning and uses little memory, so you\\ncan use it to train a model on a large dataset that does not fit in RAM (i.e., for\\nout-of-core learning). Moreover, it scales very well, as its computational\\ncomplexity is \\nO\\n(\\nm\\n × \\nn\\n). \\nTable 5-1\\n compares Scikit-Learn’s SVM\\nclassification classes.\\nTable 5-1. \\nComparison of Scikit-Learn classes for SVM classification\\nClass\\nTime complexity\\nOut-of-core\\nsupport\\nScaling required\\nKernel trick\\nLinearSVC\\nO\\n(\\nm\\n × \\nn\\n)\\nNo\\nYes\\nNo\\n1\\n2\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 300, 'page_label': '301', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='SVC\\nO\\n(\\nm\\n² × \\nn\\n) to \\nO\\n(\\nm\\n³\\n× \\nn\\n)\\nNo\\nYes\\nYes\\nSGDClassifier\\nO\\n(\\nm\\n × \\nn\\n)\\nYes\\nYes\\nNo\\nNow let’s see how the SVM algorithms can also be used for linear and\\nnonlinear regression.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 301, 'page_label': '302', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='SVM Regression\\nTo use SVMs for regression instead of classification, the trick is to tweak the\\nobjective: instead of trying to fit the largest possible street between two\\nclasses while limiting margin violations, SVM regression tries to fit as many\\ninstances as possible \\non\\n the street while limiting margin violations (i.e.,\\ninstances \\noff\\n the street). The width of the street is controlled by a\\nhyperparameter, \\nϵ\\n. \\nFigure 5-10\\n shows two linear SVM regression models\\ntrained on some linear data, one with a small margin (\\nϵ\\n = 0.5) and the other\\nwith a larger margin (\\nϵ\\n = 1.2).\\nFigure 5-10. \\nSVM regression\\nReducing \\nϵ\\n increases the number of support vectors, which regularizes the\\nmodel. \\nMoreover, if you add more training instances within the margin, it\\nwill not affect the model’s predictions; thus, the model is said to be \\nϵ-\\ninsensitive\\n.\\nYou can use Scikit-Learn’s \\nLinearSVR\\n class to perform linear SVM\\nregression. The following code produces the model represented on the left in\\nFigure 5-10\\n:\\nfrom\\n \\nsklearn.svm\\n \\nimport\\n \\nLinearSVR\\nX\\n,\\n \\ny\\n \\n=\\n \\n[\\n...\\n]\\n  \\n# a linear dataset\\nsvm_reg\\n \\n=\\n \\nmake_pipeline\\n(\\nStandardScaler\\n(),'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 302, 'page_label': '303', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='LinearSVR\\n(\\nepsilon\\n=\\n0.5\\n,\\n \\nrandom_state\\n=\\n42\\n))\\nsvm_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nTo tackle nonlinear regression tasks, you can use a kernelized SVM model.\\nFigure 5-11\\n shows SVM regression on a random quadratic training set, using\\na second-degree polynomial kernel. There is some regularization in the left\\nplot (i.e., a small \\nC\\n value), and much less in the right plot (i.e., a large \\nC\\nvalue).\\nFigure 5-11. \\nSVM regression using a second-degree polynomial kernel\\nThe following code uses Scikit-Learn’s \\nSVR\\n class (which supports the kernel\\ntrick) to produce the model represented on the left in \\nFigure 5-11\\n:\\nfrom\\n \\nsklearn.svm\\n \\nimport\\n \\nSVR\\nX\\n,\\n \\ny\\n \\n=\\n \\n[\\n...\\n]\\n  \\n# a quadratic dataset\\nsvm_poly_reg\\n \\n=\\n \\nmake_pipeline\\n(\\nStandardScaler\\n(),\\n                             \\nSVR\\n(\\nkernel\\n=\\n\"poly\"\\n,\\n \\ndegree\\n=\\n2\\n,\\n \\nC\\n=\\n0.01\\n,\\n \\nepsilon\\n=\\n0.1\\n))\\nsvm_poly_reg\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nThe \\nSVR\\n class is the regression equivalent of the \\nSVC\\n class, and the\\nLinearSVR\\n class is the regression equivalent of the \\nLinearSVC\\n class. The\\nLinearSVR\\n class scales linearly with the size of the training set (just like the\\nLinearSVC\\n class), while the \\nSVR\\n class gets much too slow when the training\\nset grows very large (just like the \\nSVC\\n class).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 303, 'page_label': '304', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='NOTE\\nSVMs can also be used for novelty detection, as you will see in \\nChapter 9\\n.\\nThe rest of this chapter explains how SVMs make predictions and how their\\ntraining algorithms work, starting with linear SVM classifiers. If you are just\\ngetting started with machine learning, you can safely skip this and go straight\\nto the exercises at the end of this chapter, and come back later when you want\\nto get a deeper understanding of SVMs.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 304, 'page_label': '305', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Under the Hood of Linear SVM Classifiers\\nA linear SVM classifier predicts the class of a new instance \\nx\\n by first\\ncomputing the decision function \\nθ\\n \\nx\\n = \\nθ\\n \\nx\\n + \\n⋯\\n + \\nθ\\n \\nx\\n, where \\nx\\n is the bias\\nfeature (always equal to 1). If the result is positive, then the predicted class \\nŷ\\nis the positive class (1); otherwise it is the negative class (0). This is exactly\\nlike \\nLogisticRegression\\n (discussed in \\nChapter 4\\n).\\nNOTE\\nUp to now, I have used the convention of putting all the model parameters in one vector \\nθ\\n,\\nincluding the bias term \\nθ\\n and the input feature weights \\nθ\\n to \\nθ\\n. \\nThis required adding a\\nbias input \\nx\\n = 1 to all instances. Another very common convention is to separate the bias\\nterm \\nb\\n (equal to \\nθ\\n) and the feature weights vector \\nw\\n (containing \\nθ\\n to \\nθ\\n). \\nIn this case, no\\nbias feature needs to be added to the input feature vectors, and the linear SVM’s decision\\nfunction is equal to \\nw\\n \\nx\\n + \\nb\\n = \\nw\\n \\nx\\n + \\n⋯\\n + \\nw\\n \\nx\\n + \\nb\\n. I will use this convention throughout\\nthe rest of this book.\\nSo, making predictions with a linear SVM classifier is quite straightforward.\\nHow about training? This requires finding the weights vector \\nw\\n and the bias\\nterm \\nb\\n that make the street, or margin, as wide as possible while limiting the\\nnumber of margin violations. Let’s start with the width of the street: to make\\nit larger, we need to make \\nw\\n smaller. This may be easier to visualize in 2D,\\nas shown in \\nFigure 5-12\\n. Let’s define the borders of the street as the points\\nwhere the decision function is equal to –1 or +1. In the left plot the weight \\nw\\nis 1, so the points at which \\nw\\n \\nx\\n = –1 or +1 are \\nx\\n = –1 and +1: therefore the\\nmargin’s size is 2. In the right plot the weight is 0.5, so the points at which \\nw\\nx\\n = –1 or +1 are \\nx\\n = –2 and +2: the margin’s size is 4. So, we need to keep\\nw\\n as small as possible. Note that the bias term \\nb\\n has no influence on the size\\nof the margin: tweaking it just shifts the margin around, without affecting its\\nsize.\\n⊺\\n0\\n0\\nn\\nn\\n0\\n0\\n1\\nn\\n0\\n0\\n1\\nn\\n⊺\\n1\\n1\\nn\\nn\\n1\\n1\\n1\\n1\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 305, 'page_label': '306', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 5-12. \\nA smaller weight vector results in a larger margin\\nWe also want to avoid margin violations, so we need the decision function to\\nbe greater than 1 for all positive training instances and lower than –1 for\\nnegative training instances. \\nIf we define \\nt\\n = –1 for negative instances (when\\ny\\n = 0) and \\nt\\n = 1 for positive instances (when \\ny\\n = 1), then we can write\\nthis constraint as \\nt\\n(\\nw\\n \\nx\\n + \\nb\\n) ≥ 1 for all instances.\\nWe can therefore express the hard margin\\n linear SVM classifier objective as\\nthe constrained optimization problem in \\nEquation 5-1\\n.\\nEquation 5-1. \\nHard margin linear SVM classifier objective\\nminimize w,b \\n1 2 \\nw \\n⊺\\n \\nw \\nsubject \\nto \\nt (i) \\n( \\nw \\n⊺\\n \\nx (i) \\n+ \\nb \\n) \\n≥ \\n1 \\nfor \\ni \\n= \\n1 \\n, \\n2 \\n, \\n⋯\\n \\n, \\nm\\nNOTE\\nWe are minimizing ½ \\nw\\n \\nw\\n, which is equal to ½\\n∥\\n \\nw\\n \\n∥\\n, rather than minimizing \\n∥\\n \\nw\\n \\n∥\\n(the norm of \\nw\\n). Indeed, ½\\n∥\\n \\nw\\n \\n∥\\n has a nice, simple derivative (it is just \\nw\\n), while \\n∥\\n \\nw\\n∥\\n is not differentiable at \\nw\\n = 0. Optimization algorithms often work much better on\\ndifferentiable functions.\\nTo get the soft margin objective, we need to introduce a \\nslack variable\\n \\nζ\\n ≥ 0\\nfor each instance:\\n\\u2060\\n \\nζ\\n measures how much the \\ni\\n instance is allowed to\\nviolate the margin. \\nWe now have two conflicting objectives: make the slack\\nvariables as small as possible to reduce the margin violations, and make ½ \\nw\\nw\\n as small as possible to increase the margin. This is where the \\nC\\nhyperparameter comes in: it allows us to define the trade-off between these\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n⊺\\n(\\ni\\n)\\n⊺\\n2\\n2\\n(\\ni\\n)\\n3\\n(\\ni\\n)\\nth\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 306, 'page_label': '307', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='two objectives. This gives us the constrained optimization problem in\\nEquation 5-2\\n.\\nEquation 5-2. \\nSoft margin linear SVM classifier objective\\nminimize w,b,ζ \\n1 2 \\nw \\n⊺\\n \\nw \\n+ \\nC \\n∑ i=1 m \\nζ (i) \\nsubject \\nto \\nt (i) \\n( \\nw \\n⊺\\n \\nx (i) \\n+ \\nb \\n) \\n≥\\n1 \\n- \\nζ (i) \\nand \\nζ (i) \\n≥ \\n0 \\nfor \\ni \\n= \\n1 \\n, \\n2 \\n, \\n⋯\\n \\n, \\nm\\nThe hard margin and soft margin problems are both convex quadratic\\noptimization problems with linear constraints. \\nSuch problems are known as\\nquadratic programming\\n (QP) problems. Many off-the-shelf solvers are\\navailable to solve QP problems by using a variety of techniques that are\\noutside the scope of this book.\\n\\u2060\\nUsing a QP solver is one way to train an SVM. \\nAnother is to use gradient\\ndescent to minimize the \\nhinge loss\\n or the \\nsquared hinge loss\\n (see \\nFigure 5-\\n13\\n). \\nGiven an instance \\nx\\n of the positive class (i.e., with \\nt\\n = 1), the loss is 0 if\\nthe output \\ns\\n of the decision function (\\ns\\n = \\nw\\n \\nx\\n + \\nb\\n) is greater than or equal to\\n1. This happens when the instance is off the street and on the positive side.\\nGiven an instance of the negative class (i.e., with \\nt\\n = –1), the loss is 0 if \\ns\\n ≤ –\\n1. This happens when the instance is off the street and on the negative side.\\nThe further away an instance is from the correct side of the margin, the\\nhigher the loss: it grows linearly for the hinge loss, and quadratically for the\\nsquared hinge loss. This makes the squared hinge loss more sensitive to\\noutliers. However, if the dataset is clean, it tends to converge faster. \\nBy\\ndefault, \\nLinearSVC\\n uses the squared hinge loss, while \\nSGDClassifier\\n uses the\\nhinge loss. Both classes let you choose the loss by setting the \\nloss\\nhyperparameter to \\n\"hinge\"\\n or \\n\"squared_hinge\"\\n. The \\nSVC\\n class’s optimization\\nalgorithm finds a similar solution as minimizing the hinge loss.\\n4\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 307, 'page_label': '308', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 5-13. \\nThe hinge loss (left) and the squared hinge loss (right)\\nNext, we’ll look at yet another way to train a linear SVM classifier: solving\\nthe dual problem.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 308, 'page_label': '309', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The Dual Problem\\nGiven a constrained optimization problem, known as the \\nprimal problem\\n, it is\\npossible to express a different but closely related problem, called its \\ndual\\nproblem\\n. \\nThe \\nsolution\\n to the dual problem typically gives a lower bound to\\nthe solution of the primal problem, but under some conditions it can have the\\nsame solution as the primal problem. Luckily, the SVM problem happens to\\nmeet these conditions,\\n\\u2060\\n so you can choose to solve the primal problem or\\nthe dual problem; both will have the same solution. \\nEquation 5-3\\n shows the\\ndual form of the linear SVM objective. If you are interested in knowing how\\nto derive the dual problem from the primal problem, see the extra material\\nsection in \\nthis chapter’s notebook\\n.\\nEquation 5-3. \\nDual form of the linear SVM objective\\nminimize α 12∑i=1m\\u200a\\n∑j=1mα(i)α(j)t(i)t(j)x(i)\\n⊺\\nx(j)  -  ∑i=1mα(i)subject to α(i)≥0 for all i=1,2,\\n…,m and ∑i=1mα(i)t(i)=0\\nOnce you find the vector \\nα ^ \\nthat minimizes this equation (using a QP\\nsolver), use \\nEquation 5-4\\n to compute the \\nw ^ \\nand b^ that minimize the primal\\nproblem. In this equation, \\nn\\n represents the number of support vectors.\\nEquation 5-4. \\nFrom the dual solution to the primal solution\\nw ^ \\n= \\n∑ i=1 m \\nα ^ (i) \\nt (i) \\nx (i) \\nb ^ \\n= \\n1 n s \\n∑ i=1 α ^ (i) >0 m \\nt (i) \\n- \\nw ^ \\n⊺\\n \\nx (i)\\nThe dual problem is faster to solve than the primal one when the number of\\ntraining instances is smaller than the number of features. More importantly,\\nthe dual problem makes the kernel trick possible, while the primal problem\\ndoes not. So what is this kernel trick, anyway?\\n5\\ns'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 309, 'page_label': '310', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Kernelized SVMs\\nSuppose you want to apply a second-degree polynomial transformation to a\\ntwo-dimensional training set (such as the moons training set), then train a\\nlinear SVM classifier on the transformed training set. \\nEquation 5-5\\n shows the\\nsecond-degree polynomial mapping function \\nϕ\\n that you want to apply.\\nEquation 5-5. \\nSecond-degree polynomial mapping\\nϕ \\nx \\n= \\nϕ \\nx 1 \\nx 2 \\n= \\nx 1 \\n2 \\n2 \\nx 1 \\nx 2 \\nx 2 \\n2\\nNotice that the transformed vector is 3D instead of 2D. Now let’s look at\\nwhat happens to a couple of 2D vectors, \\na\\n and \\nb\\n, if we apply this second-\\ndegree polynomial mapping and then compute the dot product\\n\\u2060\\n of the\\ntransformed vectors (see \\nEquation 5-6\\n).\\nEquation 5-6. \\nKernel trick for a second-degree polynomial mapping\\nϕ \\n(a) \\n⊺\\n \\nϕ \\n( \\nb \\n) \\n= \\na 1 \\n2 2a 1 a 2 a 2 \\n2 \\n⊺\\n \\nb 1 \\n2 \\n2 \\nb 1 \\nb 2 \\nb 2 \\n2 \\n= \\na 1 \\n2 \\nb 1 \\n2 \\n+ \\n2 \\na 1\\nb 1 \\na 2 \\nb 2 \\n+ \\na 2 \\n2 \\nb 2 \\n2 \\n= \\na 1 b 1 +a 2 b 2 \\n2 \\n= \\na 1 a 2 \\n⊺\\n b 1 b 2 \\n2 \\n= \\n(a \\n⊺\\n b) 2\\nHow about that? The dot product of the transformed vectors is equal to the\\nsquare of the dot product of the original vectors: \\nϕ\\n(\\na\\n)\\n \\nϕ\\n(\\nb\\n) = (\\na\\n \\nb\\n)\\n.\\nHere is the key insight: if you apply the transformation \\nϕ\\n to all training\\ninstances, then the dual problem (see \\nEquation 5-3\\n) will contain the dot\\nproduct \\nϕ\\n(\\nx\\n)\\n \\nϕ\\n(\\nx\\n). But if \\nϕ\\n is the second-degree polynomial\\ntransformation defined in \\nEquation 5-5\\n, then you can replace this dot product\\nof transformed vectors simply by \\n(x (i) \\n⊺\\n x (j) ) 2 \\n. So, you don’t need to\\ntransform the training instances at all; just replace the dot product by its\\nsquare in \\nEquation 5-3\\n. The result will be strictly the same as if you had gone\\nthrough the trouble of transforming the training set and then fitting a linear\\nSVM algorithm, but this trick makes the whole process much more\\ncomputationally efficient.\\nThe function \\nK\\n(\\na\\n, \\nb\\n) = (\\na\\n \\nb\\n)\\n is a second-degree polynomial kernel. In\\nmachine learning, a \\nkernel\\n is a function capable of computing the dot product\\nϕ\\n(\\na\\n)\\n \\nϕ\\n(\\nb\\n), based only on the original vectors \\na\\n and \\nb\\n, without having to\\n6\\n⊺\\n⊺\\n2\\n(\\ni\\n)\\n⊺\\n(\\nj\\n)\\n⊺\\n2\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 310, 'page_label': '311', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='compute (or even to know about) the transformation \\nϕ\\n. \\nEquation 5-7\\n lists\\nsome of the most commonly used kernels.\\nEquation 5-7. \\nCommon kernels\\nLinear: \\nK \\n( \\na \\n, \\nb \\n) \\n= \\na \\n⊺\\n \\nb \\nPolynomial: \\nK \\n( \\na \\n, \\nb \\n) \\n= \\nγa \\n⊺\\n b+r d \\nGaussian \\nRBF:\\nK \\n( \\na \\n, \\nb \\n) \\n= \\nexp \\n( \\n- \\nγ \\na-b 2 \\n) \\nSigmoid: \\nK \\n( \\na \\n, \\nb \\n) \\n= \\ntanh \\nγ \\na \\n⊺\\n \\nb \\n+ \\nr\\nMERCER’S THEOREM\\nAccording to \\nMercer’s theorem\\n, if a function \\nK\\n(\\na\\n, \\nb\\n) respects a few\\nmathematical conditions called \\nMercer’s conditions\\n (e.g., \\nK\\n must be\\ncontinuous and symmetric in its arguments so that \\nK\\n(\\na\\n, \\nb\\n) = \\nK\\n(\\nb\\n, \\na\\n),\\netc.), then there exists a function \\nϕ\\n that maps \\na\\n and \\nb\\n into another space\\n(possibly with much higher dimensions) such that \\nK\\n(\\na\\n, \\nb\\n) = \\nϕ\\n(\\na\\n)\\n \\nϕ\\n(\\nb\\n).\\nYou can use \\nK\\n as a kernel because you know \\nϕ\\n exists, even if you don’t\\nknow what \\nϕ\\n is. \\nIn the case of the Gaussian RBF kernel, it can be shown\\nthat \\nϕ\\n maps each training instance to an infinite-dimensional space, so it’s\\na good thing you don’t need to actually perform the mapping!\\nNote that some frequently used kernels (such as the sigmoid kernel) don’t\\nrespect all of Mercer’s conditions, yet they generally work well in\\npractice.\\nThere is still one loose end we must tie up. \\nEquation 5-4\\n shows how to go\\nfrom the dual solution to the primal solution in the case of a linear SVM\\nclassifier. But if you apply the kernel trick, you end up with equations that\\ninclude \\nϕ\\n(\\nx\\n). In fact, \\nw ^ \\nmust have the same number of dimensions as\\nϕ\\n(\\nx\\n), which may be huge or even infinite, so you can’t compute it. But how\\ncan you make predictions without knowing \\nw ^ ? Well, the good news is that\\nyou can plug the formula for \\nw ^ \\nfrom \\nEquation 5-4\\n into the decision\\nfunction for a new instance \\nx\\n, and you get an equation with only dot\\nproducts between input vectors. This makes it possible to use the kernel trick\\n(\\nEquation 5-8\\n).\\nEquation 5-8. \\nMaking predictions with a kernelized SVM\\n⊺\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\nn\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 311, 'page_label': '312', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='h w ^,b ^ \\nϕ \\n( \\nx (n) \\n) \\n= \\nw ^ \\n⊺\\n \\nϕ \\n( \\nx (n) \\n) \\n+ \\nb ^ \\n= \\n∑ i=1 m α ^ (i) t (i) ϕ(x (i) ) \\n⊺\\n \\nϕ\\n( \\nx (n) \\n) \\n+ \\nb ^ \\n= \\n∑ i=1 m \\nα ^ (i) \\nt (i) \\nϕ \\n(x (i) ) \\n⊺\\n \\nϕ \\n( \\nx (n) \\n) \\n+ \\nb ^ \\n= \\n∑ i=1 α ^ (i)\\n>0 m \\nα ^ (i) \\nt (i) \\nK \\n( \\nx (i) \\n, \\nx (n) \\n) \\n+ \\nb ^\\nNote that since \\nα\\n ≠ 0 only for support vectors, making predictions involves\\ncomputing the dot product of the new input vector \\nx\\n with only the support\\nvectors, not all the training instances. Of course, you need to use the same\\ntrick to compute the bias term b^ (\\nEquation 5-9\\n).\\nEquation 5-9. \\nUsing the kernel trick to compute the bias term\\nb ^ \\n= \\n1 n s \\n∑ i=1 α ^ (i) >0 m \\nt (i) \\n- \\nw ^ \\n⊺\\n \\nϕ \\n( \\nx (i) \\n) \\n= \\n1 n s \\n∑ i=1 α ^ (i) >0 m\\nt (i) \\n- \\n∑ j=1 m α ^ (j) t (j) ϕ(x (j) ) \\n⊺\\n \\nϕ \\n( \\nx (i) \\n) \\n= \\n1 n s \\n∑ i=1 α ^ (i) >0 m \\nt (i) \\n-\\n∑ j=1 α ^ (j) >0 m \\nα ^ (j) \\nt (j) \\nK \\n( \\nx (i) \\n, \\nx (j) \\n)\\nIf you are starting to get a headache, that’s perfectly normal: it’s an\\nunfortunate side effect of the kernel trick.\\nNOTE\\nIt is also possible to implement online kernelized SVMs, capable of incremental learning,\\nas described in the papers \\n“Incremental and Decremental Support Vector Machine\\nLearning”\\n\\u2060\\n and \\n“Fast Kernel Classifiers with Online and Active Learning”\\n.\\n\\u2060\\n These\\nkernelized SVMs are implemented in Matlab and C++. But for large-scale nonlinear\\nproblems, you may want to consider using random forests (see \\nChapter 7\\n) or neural\\nnetworks (see \\nPart II\\n).\\n(\\ni\\n)\\n(\\nn\\n)\\n7\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 312, 'page_label': '313', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nWhat is the fundamental idea behind support vector machines?\\n2\\n. \\nWhat is a support vector?\\n3\\n. \\nWhy is it important to scale the inputs when using SVMs?\\n4\\n. \\nCan an SVM classifier output a confidence score when it classifies an\\ninstance? What about a probability?\\n5\\n. \\nHow can you choose between \\nLinearSVC\\n, \\nSVC\\n, and \\nSGDClassifier\\n?\\n6\\n. \\nSay you’ve trained an SVM classifier with an RBF kernel, but it seems\\nto underfit the training set. Should you increase or decrease \\nγ\\n (\\ngamma\\n)?\\nWhat about \\nC\\n?\\n7\\n. \\nWhat does it mean for a model to be \\nϵ-insensitive\\n?\\n8\\n. \\nWhat is the point of using the kernel trick?\\n9\\n. \\nTrain a \\nLinearSVC\\n on a linearly separable dataset. Then train an \\nSVC\\nand a \\nSGDClassifier\\n on the same dataset. See if you can get them to\\nproduce roughly the same model.\\n10\\n. \\nTrain an SVM classifier on the wine dataset, which you can load using\\nsklearn.datasets.load_wine()\\n. This dataset contains the chemical\\nanalyses of 178 wine samples produced by 3 different cultivators: the\\ngoal is to train a classification model capable of predicting the cultivator\\nbased on the wine’s chemical analysis. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all three\\nclasses. What accuracy can you reach?\\n11\\n. \\nTrain and fine-tune an SVM regressor on the California housing dataset.\\nYou can use the original dataset rather than the tweaked version we used\\nin \\nChapter 2\\n, which you can load using\\nsklearn.datasets.fetch_california_housing()\\n.\\n The targets represent'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 313, 'page_label': '314', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='hundreds of thousands of dollars. Since there are over 20,000 instances,\\nSVMs can be slow, so for hyperparameter tuning you should use far\\nfewer instances (e.g., 2,000) to test many more hyperparameter\\ncombinations. What is your best model’s RMSE?\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n Chih-Jen Lin et al., “A Dual Coordinate Descent Method for Large-Scale Linear SVM”,\\nProceedings of the 25th International Conference on Machine Learning\\n (2008): 408–415.\\n2\\n John Platt, “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector\\nMachines” (Microsoft Research technical report, April 21, 1998).\\n3\\n Zeta (\\nζ\\n) is the sixth letter of the Greek alphabet.\\n4\\n To learn more about quadratic programming, you can start by reading Stephen Boyd and Lieven\\nVandenberghe’s book \\nConvex Optimization\\n (Cambridge University Press) or watching Richard\\nBrown’s \\nseries of video lectures\\n.\\n5\\n The objective function is convex, and the inequality constraints are continuously differentiable\\nand convex functions.\\n6\\n As explained in \\nChapter 4\\n, the dot product of two vectors \\na\\n and \\nb\\n is normally noted \\na\\n · \\nb\\n.\\nHowever, in machine learning, vectors are frequently represented as column vectors (i.e., single-\\ncolumn matrices), so the dot product is achieved by computing \\na\\nb\\n. To remain consistent with\\nthe rest of the book, we will use this notation here, ignoring the fact that this technically results in\\na single-cell matrix rather than a scalar value.\\n7\\n Gert Cauwenberghs and Tomaso Poggio, “Incremental and Decremental Support Vector\\nMachine Learning”, \\nProceedings of the 13th International Conference on Neural Information\\nProcessing Systems\\n (2000): 388–394.\\n8\\n Antoine Bordes et al., “Fast Kernel Classifiers with Online and Active Learning”, \\nJournal of\\nMachine Learning Research\\n 6 (2005): 1579–1619.\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 314, 'page_label': '315', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 6. \\nDecision Trees\\nDecision trees\\n are versatile machine learning algorithms that can perform\\nboth classification and regression tasks, and even multioutput tasks. They are\\npowerful algorithms, capable of fitting complex datasets. For example, in\\nChapter 2\\n you trained a \\nDecisionTreeRegressor\\n model on the California\\nhousing dataset, fitting it perfectly (actually, overfitting it).\\nDecision trees are also the fundamental components of random forests (see\\nChapter 7\\n), which are among the most powerful machine learning algorithms\\navailable today.\\nIn this chapter we will start by discussing how to train, visualize, and make\\npredictions with decision trees. Then we will go through the CART training\\nalgorithm used by Scikit-Learn, and we will explore how to regularize trees\\nand use them for regression tasks. Finally, we will discuss some of the\\nlimitations of decision trees.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 315, 'page_label': '316', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Training and Visualizing a Decision Tree\\nTo understand decision trees, let’s build one and take a look at how it makes\\npredictions. \\nThe following code trains a \\nDecisionTreeClassifier\\n on the iris\\ndataset (see \\nChapter 4\\n):\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nload_iris\\nfrom\\n \\nsklearn.tree\\n \\nimport\\n \\nDecisionTreeClassifier\\niris\\n \\n=\\n \\nload_iris\\n(\\nas_frame\\n=\\nTrue\\n)\\nX_iris\\n \\n=\\n \\niris\\n.\\ndata\\n[[\\n\"petal length (cm)\"\\n,\\n \\n\"petal width (cm)\"\\n]]\\n.\\nvalues\\ny_iris\\n \\n=\\n \\niris\\n.\\ntarget\\ntree_clf\\n \\n=\\n \\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ntree_clf\\n.\\nfit\\n(\\nX_iris\\n,\\n \\ny_iris\\n)\\nYou can visualize the trained decision tree by first using the\\nexport_graphviz()\\n function to output a graph definition file called\\niris_tree.dot\\n:\\nfrom\\n \\nsklearn.tree\\n \\nimport\\n \\nexport_graphviz\\nexport_graphviz\\n(\\n        \\ntree_clf\\n,\\n        \\nout_file\\n=\\n\"iris_tree.dot\"\\n,\\n        \\nfeature_names\\n=\\n[\\n\"petal length (cm)\"\\n,\\n \\n\"petal width (cm)\"\\n],\\n        \\nclass_names\\n=\\niris\\n.\\ntarget_names\\n,\\n        \\nrounded\\n=\\nTrue\\n,\\n        \\nfilled\\n=\\nTrue\\n    \\n)\\nThen you can use \\ngraphviz.Source.from_file()\\n to load and display the file in a\\nJupyter notebook:\\nfrom\\n \\ngraphviz\\n \\nimport\\n \\nSource\\nSource\\n.\\nfrom_file\\n(\\n\"iris_tree.dot\"\\n)\\nGraphviz\\n is an open source graph visualization software package. It also'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 316, 'page_label': '317', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='includes a \\ndot\\n command-line tool to convert \\n.dot\\n files to a variety of formats,\\nsuch as PDF or PNG.\\nYour first decision tree looks like \\nFigure 6-1\\n.\\nFigure 6-1. \\nIris decision tree'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 317, 'page_label': '318', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Making Predictions\\nLet’s see how the tree represented in \\nFigure 6-1\\n makes predictions. Suppose\\nyou find an iris flower and you want to classify it based on its petals. \\nYou\\nstart at the \\nroot node\\n (depth 0, at the top): this node asks whether the flower’s\\npetal length is smaller than 2.45 cm. If it is, then you move down to the root’s\\nleft child node (depth 1, left). \\nIn this case, it is a \\nleaf node\\n (i.e., it does not\\nhave any child nodes), so it does not ask any questions: simply look at the\\npredicted class for that node, and the decision tree predicts that your flower is\\nan \\nIris setosa\\n (\\nclass=setosa\\n).\\nNow suppose you find another flower, and this time the petal length is greater\\nthan 2.45 cm. You again start at the root but now move down to its right child\\nnode (depth 1, right). \\nThis is not a leaf node, it’s a \\nsplit node\\n, so it asks\\nanother question: is the petal width smaller than 1.75 cm? If it is, then your\\nflower is most likely an \\nIris versicolor\\n (depth 2, left). If not, it is likely an \\nIris\\nvirginica\\n (depth 2, right). It’s really that simple.\\nNOTE\\nOne of the many qualities of decision trees is that they require very little data preparation.\\nIn fact, they don’t require feature scaling or centering at all.\\nA node’s \\nsamples\\n attribute counts how many training instances it applies to.\\nFor example, 100 training instances have a petal length greater than 2.45 cm\\n(depth 1, right), and of those 100, 54 have a petal width smaller than 1.75 cm\\n(depth 2, left). A node’s \\nvalue\\n attribute tells you how many training instances\\nof each class this node applies to: for example, the bottom-right node applies\\nto 0 \\nIris setosa\\n, 1 \\nIris versicolor\\n, and 45 \\nIris virginica\\n. \\nFinally, a node’s \\ngini\\nattribute measures its \\nGini impurity\\n: a node is “pure” (\\ngini=0\\n) if all training\\ninstances it applies to belong to the same class. For example, since the depth-\\n1 left node applies only to \\nIris setosa\\n training instances, it is pure and its Gini\\nimpurity is 0. \\nEquation 6-1\\n shows how the training algorithm computes the\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 318, 'page_label': '319', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Gini impurity \\nG\\n of the \\ni\\n node. The depth-2 left node has a Gini impurity\\nequal to 1 – (0/54)\\n – (49/54)\\n – (5/54)\\n ≈ 0.168.\\nEquation 6-1. \\nGini impurity\\nG i \\n= \\n1 \\n- \\n∑ k=1 n \\np i,k \\n2\\nIn this equation:\\nG\\n is the Gini impurity of the \\ni\\n node.\\np\\n is the ratio of class \\nk\\n instances among the training instances in the \\ni\\nnode.\\nNOTE\\nScikit-Learn uses the CART algorithm, which produces only \\nbinary trees\\n, meaning trees\\nwhere split nodes always have exactly two children (i.e., questions only have yes/no\\nanswers). However, other algorithms, such as ID3, can produce decision trees with nodes\\nthat have more than two \\nchildren\\n.\\nFigure 6-2\\n shows this decision tree’s decision boundaries. \\nThe thick vertical\\nline represents the decision boundary of the root node (depth 0): petal length\\n= 2.45 cm. Since the lefthand area is pure (only \\nIris setosa\\n), it cannot be split\\nany further. However, the righthand area is impure, so the depth-1 right node\\nsplits it at petal width = 1.75 cm (represented by the dashed line). Since\\nmax_depth\\n was set to 2, the decision tree stops right there. If you set\\nmax_depth\\n to 3, then the two depth-2 nodes would each add another decision\\nboundary (represented by the two vertical dotted lines).\\ni\\nth\\n2\\n2\\n2\\ni\\nth\\ni\\n,\\nk\\nth'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 319, 'page_label': '320', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 6-2. \\nDecision tree decision boundaries\\nTIP\\nThe tree structure, including all the information shown in \\nFigure 6-1\\n, is available via the\\nclassifier’s \\ntree_\\n attribute. Type \\nhelp(tree_clf.tree_)\\n for details, and see the \\nthis chapter’s\\nnotebook\\n for an example.\\nMODEL INTERPRETATION: WHITE BOX VERSUS BLACK\\nBOX\\nDecision trees are intuitive, and their decisions are easy to interpret. Such\\nmodels are often called \\nwhite box models\\n. \\nIn contrast, as you will see,\\nrandom forests and neural networks are generally considered \\nblack box\\nmodels\\n. They make great predictions, and you can easily check the\\ncalculations that they performed to make these predictions; nevertheless,\\nit is usually hard to explain in simple terms why the predictions were\\nmade. For example, if a neural network says that a particular person\\nappears in a picture, it is hard to know what contributed to this prediction:\\nDid the model recognize that person’s eyes? Their mouth? Their nose?\\nTheir shoes? Or even the couch that they were sitting on? Conversely,\\ndecision trees provide nice, simple classification rules that can even be'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 320, 'page_label': '321', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='applied manually if need be (e.g., for flower classification). \\nThe field of\\ninterpretable ML\\n aims at creating ML systems that can explain their\\ndecisions in a way humans can understand. This is important in many\\ndomains—for example, to ensure the system does not make unfair\\ndecisions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 321, 'page_label': '322', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Estimating Class Probabilities\\nA decision tree can also estimate the probability that an instance belongs to a\\nparticular class \\nk\\n. \\nFirst it traverses the tree to find the leaf node for this\\ninstance, and then it returns the ratio of training instances of class \\nk\\n in this\\nnode. For example, suppose you have found a flower whose petals are 5 cm\\nlong and 1.5 cm wide. The \\ncorresponding\\n leaf node is the depth-2 left node,\\nso the decision tree outputs the following probabilities: 0% for \\nIris setosa\\n(0/54), 90.7% for \\nIris versicolor\\n (49/54), and 9.3% for \\nIris virginica\\n (5/54).\\nAnd if you ask it to predict the class, it outputs \\nIris versicolor\\n (class 1)\\nbecause it has the highest probability. Let’s check this:\\n>>> \\ntree_clf\\n.\\npredict_proba\\n([[\\n5\\n,\\n \\n1.5\\n]])\\n.\\nround\\n(\\n3\\n)\\narray([[0.   , 0.907, 0.093]])\\n>>> \\ntree_clf\\n.\\npredict\\n([[\\n5\\n,\\n \\n1.5\\n]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere\\nelse in the bottom-right rectangle of \\nFigure 6-2\\n—for example, if the petals\\nwere 6 cm long and 1.5 cm wide (even though it seems obvious that it would\\nmost likely be an \\nIris virginica\\n in this case).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 322, 'page_label': '323', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The CART Training Algorithm\\nScikit-Learn uses the \\nClassification and Regression Tree\\n (CART) algorithm\\nto train decision trees (also called “growing” trees). \\nThe algorithm works by\\nfirst splitting the training set into two subsets using a single feature \\nk\\n and a\\nthreshold \\nt\\n (e.g., “petal length ≤ 2.45 cm”). How does it choose \\nk\\n and \\nt\\n? It\\nsearches for the pair (\\nk\\n, \\nt\\n) that produces the purest subsets, weighted by their\\nsize. \\nEquation 6-2\\n gives the cost function that the algorithm tries to minimize.\\nEquation 6-2. \\nCART cost function for classification\\nJ \\n( \\nk \\n, \\nt k \\n) \\n= \\nm left \\nm \\nG left \\n+ \\nm right \\nm \\nG right \\nwhere \\nG left/right \\nmeasures\\nthe \\nimpurity \\nof \\nthe \\nleft/right \\nsubset \\nm left/right \\nis \\nthe \\nnumber \\nof \\ninstances \\nin\\nthe \\nleft/right \\nsubset\\nOnce the CART algorithm has successfully split the training set in two, it\\nsplits the subsets using the same logic, then the sub-subsets, and so on,\\nrecursively. It stops recursing once it reaches the maximum depth (defined by\\nthe \\nmax_depth\\n hyperparameter), or if it cannot find a split that will reduce\\nimpurity. \\nA few other hyperparameters (described in a moment) control\\nadditional stopping conditions: \\nmin_samples_split\\n, \\nmin_samples_leaf\\n,\\nmin_weight_fraction_leaf\\n, and \\nmax_leaf_nodes\\n.\\nWARNING\\nAs you can see, the CART algorithm is a \\ngreedy algorithm\\n: it greedily searches for an\\noptimum split at the top level, then repeats the process at each subsequent level. \\nIt does\\nnot check whether or not the split will lead to the lowest possible impurity several levels\\ndown. A greedy algorithm often produces a solution that’s reasonably good but not\\nguaranteed to be optimal.\\nUnfortunately, finding the optimal tree is known to be an \\nNP-complete\\n problem.\\n\\u2060\\n It\\nrequires \\nO\\n(exp(\\nm\\n)) time, making the problem intractable even for small training sets. This\\nis why we must settle for a “reasonably good” solution when training decision trees.\\nk\\nk\\nk\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 323, 'page_label': '324', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Computational Complexity\\nMaking predictions requires traversing the decision tree from the root to a\\nleaf. \\nDecision trees generally are approximately balanced, so traversing the\\ndecision tree requires going through roughly \\nO\\n(log\\n(\\nm\\n)) nodes, where\\nlog\\n(\\nm\\n) is the \\nbinary logarithm\\n of \\nm\\n, equal to log(\\nm\\n) / log(2). Since each\\nnode only requires checking the value of one feature, the overall prediction\\ncomplexity is \\nO\\n(log\\n(\\nm\\n)), independent of the number of features. So\\npredictions are very fast, even when dealing with large training sets.\\nThe training algorithm compares all features (or less if \\nmax_features\\n is set)\\non all samples at each node. Comparing all features on all samples at each\\nnode results in a training complexity of \\nO\\n(\\nn\\n × \\nm\\n log\\n(\\nm\\n)).\\n2\\n2\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 324, 'page_label': '325', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Gini Impurity or Entropy?\\nBy default, the \\nDecisionTreeClassifier\\n class uses the Gini impurity measure,\\nbut you can select the \\nentropy\\n impurity measure instead by setting the\\ncriterion\\n hyperparameter to \\n\"entropy\"\\n. \\nThe concept of entropy originated in\\nthermodynamics as a measure of molecular disorder: entropy approaches zero\\nwhen molecules are still and well ordered. Entropy later spread to a wide\\nvariety of domains, including in Shannon’s information theory, where it\\nmeasures the average information content of a message, as we saw in\\nChapter 4\\n. Entropy is zero when all messages are identical. In machine\\nlearning, entropy is frequently used as an \\nimpurity\\n measure: a set’s entropy is\\nzero when it contains instances of only one class. \\nEquation 6-3\\n shows the\\ndefinition of the entropy of the \\ni\\n node. For example, the depth-2 left node in\\nFigure 6-1\\n has an entropy equal to –(49/54) log\\n (49/54) – (5/54) log\\n (5/54)\\n≈ 0.445.\\nEquation 6-3. \\nEntropy\\nH i \\n= \\n- \\n∑ k=1 p i,k ≠0 n \\np i,k \\nlog 2 \\n( \\np i,k \\n)\\nSo, should you use Gini impurity or entropy? The truth is, most of the time it\\ndoes not make a big difference: they lead to similar trees. Gini impurity is\\nslightly faster to compute, so it is a good default. However, when they differ,\\nGini impurity tends to isolate the most frequent class in its own branch of the\\ntree, while entropy tends to produce slightly more balanced trees.\\n\\u2060\\nth\\n2\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 325, 'page_label': '326', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Regularization Hyperparameters\\nDecision trees make very few assumptions about the training data (as\\nopposed to linear models, which assume that the data is linear, for example).\\nIf left unconstrained, the tree structure will adapt itself to the training data,\\nfitting it very closely—indeed, most likely overfitting it. \\nSuch a model is\\noften called a \\nnonparametric model\\n, not because it does not have any\\nparameters (it often has a lot) but because the number of parameters is not\\ndetermined prior to training, so the model structure is free to stick closely to\\nthe data. \\nIn contrast, a \\nparametric model\\n, such as a linear model, has a\\npredetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the decision tree’s\\nfreedom during training. As you know by now, this is called regularization.\\nThe regularization hyperparameters depend on the algorithm used, but\\ngenerally you can at least restrict the maximum depth of the decision tree. In\\nScikit-Learn, this is controlled by the \\nmax_depth\\n hyperparameter. The default\\nvalue is \\nNone\\n, which means unlimited. Reducing \\nmax_depth\\n will regularize\\nthe model and thus reduce the risk of overfitting.\\nThe \\nDecisionTreeClassifier\\n class has a few other parameters that similarly\\nrestrict the shape of the decision tree:\\nmax_features\\nMaximum number of features that are evaluated for splitting at each node\\nmax_leaf_nodes\\nMaximum number of leaf nodes\\nmin_samples_split\\nMinimum number of samples a node must have before it can be split\\nmin_samples_leaf'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 326, 'page_label': '327', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Minimum number of samples a leaf node must have to be created\\nmin_weight_fraction_leaf\\nSame as \\nmin_samples_leaf\\n but expressed as a fraction of the total number\\nof weighted instances\\nIncreasing \\nmin_*\\n hyperparameters or reducing \\nmax_*\\n hyperparameters will\\nregularize the model.\\nNOTE\\nOther algorithms work by first training the decision tree without restrictions, then \\npruning\\n(deleting) unnecessary nodes. \\nA node whose children are all leaf nodes is considered\\nunnecessary if the purity improvement it provides is not statistically significant. \\nStandard\\nstatistical tests, such as the \\nχ\\n \\ntest\\n (chi-squared test), are used to estimate the probability\\nthat the improvement is purely the result of chance (which is called the \\nnull hypothesis\\n). If\\nthis probability, called the \\np-value\\n, is higher than a given threshold (typically 5%,\\ncontrolled by a hyperparameter), then the node is considered unnecessary and its children\\nare deleted. The pruning continues until all unnecessary nodes have been pruned.\\nLet’s test regularization on the moons dataset, introduced in \\nChapter 5\\n. \\nWe’ll\\ntrain one decision tree without regularization, and another with\\nmin_samples_leaf=5\\n. Here’s the code; \\nFigure 6-3\\n shows the decision\\nboundaries of each tree:\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nmake_moons\\nX_moons\\n,\\n \\ny_moons\\n \\n=\\n \\nmake_moons\\n(\\nn_samples\\n=\\n150\\n,\\n \\nnoise\\n=\\n0.2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ntree_clf1\\n \\n=\\n \\nDecisionTreeClassifier\\n(\\nrandom_state\\n=\\n42\\n)\\ntree_clf2\\n \\n=\\n \\nDecisionTreeClassifier\\n(\\nmin_samples_leaf\\n=\\n5\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ntree_clf1\\n.\\nfit\\n(\\nX_moons\\n,\\n \\ny_moons\\n)\\ntree_clf2\\n.\\nfit\\n(\\nX_moons\\n,\\n \\ny_moons\\n)\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 327, 'page_label': '328', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 6-3. \\nDecision boundaries of an unregularized tree (left) and a regularized tree (right)\\nThe unregularized model on the left is clearly overfitting, and the regularized\\nmodel on the right will probably generalize better. We can verify this by\\nevaluating both trees on a test set generated using a different random seed:\\n>>> \\nX_moons_test\\n,\\n \\ny_moons_test\\n \\n=\\n \\nmake_moons\\n(\\nn_samples\\n=\\n1000\\n,\\n \\nnoise\\n=\\n0.2\\n,\\n... \\n                                        \\nrandom_state\\n=\\n43\\n)\\n...\\n>>> \\ntree_clf1\\n.\\nscore\\n(\\nX_moons_test\\n,\\n \\ny_moons_test\\n)\\n0.898\\n>>> \\ntree_clf2\\n.\\nscore\\n(\\nX_moons_test\\n,\\n \\ny_moons_test\\n)\\n0.92\\nIndeed, the second tree has a better accuracy on the test set.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 328, 'page_label': '329', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Regression\\nDecision trees are also capable of performing regression tasks. \\nLet’s build a\\nregression tree using Scikit-Learn’s \\nDecisionTreeRegressor\\n class, training it\\non a noisy quadratic dataset with \\nmax_depth=2\\n:\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\nfrom\\n \\nsklearn.tree\\n \\nimport\\n \\nDecisionTreeRegressor\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\nX_quad\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrand\\n(\\n200\\n,\\n \\n1\\n)\\n \\n-\\n \\n0.5\\n  \\n# a single random input feature\\ny_quad\\n \\n=\\n \\nX_quad\\n \\n**\\n \\n2\\n \\n+\\n \\n0.025\\n \\n*\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n200\\n,\\n \\n1\\n)\\ntree_reg\\n \\n=\\n \\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ntree_reg\\n.\\nfit\\n(\\nX_quad\\n,\\n \\ny_quad\\n)\\nThe resulting tree is represented in \\nFigure 6-4\\n.\\nFigure 6-4. \\nA decision tree for regression\\nThis tree looks very similar to the classification tree you built earlier. The\\nmain difference is that instead of predicting a class in each node, it predicts a\\nvalue. For example, suppose you want to make a prediction for a new'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 329, 'page_label': '330', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='instance with \\nx\\n = 0.2. The root node asks whether \\nx\\n ≤ 0.197. Since it is not,\\nthe algorithm goes to the right child node, which asks whether \\nx\\n ≤ 0.772.\\nSince it is, the algorithm goes to the left child node. This is a leaf node, and it\\npredicts \\nvalue=0.111\\n. This prediction is the average target value of the 110\\ntraining instances associated with this leaf node, and it results in a mean\\nsquared error equal to 0.015 over these 110 instances.\\nThis model’s predictions are represented on the left in \\nFigure 6-5\\n. If you set\\nmax_depth=3\\n, you get the predictions represented on the right. Notice how\\nthe predicted value for each region is always the average target value of the\\ninstances in that region. The algorithm splits each region in a way that makes\\nmost training instances as close as possible to that predicted value.\\nFigure 6-5. \\nPredictions of two decision tree regression models\\nThe CART algorithm\\n \\nworks as described earlier, except that instead of trying\\nto split the training set in a way that minimizes impurity, it now tries to split\\nthe training set in a way that minimizes the MSE. \\nEquation 6-4\\n shows the\\ncost function that the algorithm tries to minimize.\\nEquation 6-4. \\nCART cost function for regression\\nJ(k,tk)=mleftmMSEleft+mrightmMSErightwhereMSEnode=∑i\\n∈\\nnode(y^node-\\ny(i))2mnodey^node=∑i\\n∈\\nnodey(i)mnode\\nJust like for classification tasks, decision trees are prone to overfitting when\\ndealing with regression tasks. Without any regularization (i.e., using the\\ndefault hyperparameters), you get the predictions on the left in \\nFigure 6-6\\n.\\n1\\n1\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 330, 'page_label': '331', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='These predictions are obviously overfitting the training set very badly. Just\\nsetting \\nmin_samples_leaf=10\\n results in a much more reasonable model,\\nrepresented on the right in \\nFigure 6-6\\n.\\nFigure 6-6. \\nPredictions of an unregularized regression tree (left) and a regularized tree (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 331, 'page_label': '332', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Sensitivity to Axis Orientation\\nHopefully by now you are convinced that decision trees have a lot going for\\nthem: they are relatively easy to understand and interpret, simple to use,\\nversatile, and powerful. \\nHowever, they do have a few limitations. First, as\\nyou may have noticed, decision trees love orthogonal decision boundaries (all\\nsplits are perpendicular to an axis), which makes them sensitive to the data’s\\norientation. For example, \\nFigure 6-7\\n shows a simple linearly separable\\ndataset: on the left, a decision tree can split it easily, while on the right, after\\nthe dataset is rotated by 45°, the decision boundary looks unnecessarily\\nconvoluted. Although both decision trees fit the training set perfectly, it is\\nvery likely that the model on the right will not generalize well.\\nFigure 6-7. \\nSensitivity to training set rotation\\nOne way to limit this problem is to scale the data, then apply a principal\\ncomponent analysis transformation. \\nWe will look at PCA in detail in\\nChapter 8\\n, but for now you only need to know that it rotates the data in a way\\nthat reduces the correlation between the features, which often (not always)\\nmakes things easier for trees.\\nLet’s create a small pipeline that scales the data and rotates it using PCA,\\nthen train a \\nDecisionTreeClassifier\\n on that data. \\nFigure 6-8\\n shows the\\ndecision boundaries of that tree: as you can see, the rotation makes it possible\\nto fit the dataset pretty well using only one feature, \\nz\\n, which is a linear\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 332, 'page_label': '333', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='function of the original petal length and width.\\n Here’s the code:\\nfrom\\n \\nsklearn.decomposition\\n \\nimport\\n \\nPCA\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nmake_pipeline\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nStandardScaler\\npca_pipeline\\n \\n=\\n \\nmake_pipeline\\n(\\nStandardScaler\\n(),\\n \\nPCA\\n())\\nX_iris_rotated\\n \\n=\\n \\npca_pipeline\\n.\\nfit_transform\\n(\\nX_iris\\n)\\ntree_clf_pca\\n \\n=\\n \\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ntree_clf_pca\\n.\\nfit\\n(\\nX_iris_rotated\\n,\\n \\ny_iris\\n)\\nFigure 6-8. \\nA tree’s decision boundaries on the scaled and PCA-rotated iris dataset'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 333, 'page_label': '334', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Decision Trees Have a High Variance\\nMore generally, the main issue with decision trees is that they have quite a\\nhigh variance: small changes to the hyperparameters or to the data may\\nproduce very different models. \\nIn fact, since the training algorithm used by\\nScikit-Learn is stochastic—it randomly selects the set of features to evaluate\\nat each node—even retraining the same decision tree on the exact same data\\nmay produce a very different model, such as the one represented in \\nFigure 6-\\n9\\n (unless you set the \\nrandom_state\\n hyperparameter). As you can see, it looks\\nvery different from the previous decision tree (\\nFigure 6-2\\n).\\nFigure 6-9. \\nRetraining the same model on the same data may produce a very different model\\nLuckily, by averaging predictions over many trees, it’s possible to reduce\\nvariance significantly. Such an \\nensemble\\n of trees is called a \\nrandom forest\\n,\\nand it’s one of the most powerful types of models available today, as you will\\nsee in the next chapter.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 334, 'page_label': '335', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nWhat is the approximate depth of a decision tree trained (without\\nrestrictions) on a training set with one million instances?\\n2\\n. \\nIs a node’s Gini impurity generally lower or higher than its parent’s? Is\\nit \\ngenerally\\n lower/higher, or \\nalways\\n lower/higher?\\n3\\n. \\nIf a decision tree is overfitting the training set, is it a good idea to try\\ndecreasing \\nmax_depth\\n?\\n4\\n. \\nIf a decision tree is underfitting the training set, is it a good idea to try\\nscaling the input features?\\n5\\n. \\nIf it takes one hour to train a decision tree on a training set containing\\none million instances, roughly how much time will it take to train\\nanother decision tree on a training set containing ten million instances?\\nHint: consider the CART algorithm’s computational complexity.\\n6\\n. \\nIf it takes one hour to train a decision tree on a given training set,\\nroughly how much time will it take if you double the number of\\nfeatures?\\n7\\n. \\nTrain and fine-tune a decision tree for the moons dataset by following\\nthese steps:\\na\\n. \\nUse \\nmake_moons(n_samples=10000, noise=0.4)\\n to generate a\\nmoons dataset.\\nb\\n. \\nUse \\ntrain_test_split()\\n to split the dataset into a training set and a test\\nset.\\nc\\n. \\nUse grid search with cross-validation (with the help of the\\nGridSearchCV\\n class) to find good hyperparameter values for a\\nDecisionTreeClassifier\\n. Hint: try various values for\\nmax_leaf_nodes\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 335, 'page_label': '336', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='d\\n. \\nTrain it on the full training set using these hyperparameters, and\\nmeasure your model’s performance on the test set. You should get\\nroughly 85% to 87% accuracy.\\n8\\n. \\nGrow a forest by following these steps:\\na\\n. \\nContinuing the previous exercise, generate 1,000 subsets of the\\ntraining set, each containing 100 instances selected randomly. Hint:\\nyou can use Scikit-Learn’s \\nShuffleSplit\\n class for this.\\nb\\n. \\nTrain one decision tree on each subset, using the best\\nhyperparameter values found in the previous exercise. Evaluate\\nthese 1,000 decision trees on the test set. Since they were trained on\\nsmaller sets, these decision trees will likely perform worse than the\\nfirst decision tree, achieving only about 80% \\naccuracy\\n.\\nc\\n. \\nNow comes the magic. \\nFor each test set instance, generate the\\npredictions of the 1,000 decision trees, and keep only the most\\nfrequent prediction (you can use SciPy’s \\nmode()\\n function for this).\\nThis approach gives you \\nmajority-vote predictions\\n over the test set.\\nd\\n. \\nEvaluate these predictions on the test set: you should obtain a\\nslightly higher accuracy than your first model (about 0.5 to 1.5%\\nhigher). Congratulations, you have trained a random forest\\nclassifier!\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n P is the set of problems that can be solved in \\npolynomial time\\n (i.e., a polynomial of the dataset\\nsize). \\nNP is the set of problems whose solutions can be verified in polynomial time. An NP-hard\\nproblem is a problem that can be reduced to a known NP-hard problem in polynomial time. An\\nNP-complete problem is both NP and NP-hard. A major open mathematical question is whether\\nor not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be found\\nfor any NP-complete problem (except perhaps one day on a quantum computer).\\n2\\n See Sebastian Raschka’s \\ninteresting analysis\\n for more details.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 336, 'page_label': '337', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 7. \\nEnsemble Learning and\\nRandom Forests\\nSuppose you pose a complex question to thousands of random people, then\\naggregate their answers. \\nIn many cases you will find that this aggregated\\nanswer is better than an expert’s answer. \\nThis is called the \\nwisdom of the\\ncrowd\\n. Similarly, if you aggregate the predictions of a group of predictors\\n(such as classifiers or regressors), you will often get better predictions than\\nwith the best individual predictor. A group of predictors is called an\\nensemble\\n; thus, this technique is called \\nensemble learning\\n, and an ensemble\\nlearning algorithm is called an \\nensemble method\\n.\\nAs an example of an ensemble method, you can train a group of decision tree\\nclassifiers, each on a different random subset of the training set. You can then\\nobtain the predictions of all the individual trees, and the class that gets the\\nmost votes is the ensemble’s prediction (see the last exercise in \\nChapter 6\\n).\\nSuch an ensemble of decision trees is called a \\nrandom forest\\n, and despite its\\nsimplicity, this is one of the most powerful machine learning algorithms\\navailable today.\\nAs discussed in \\nChapter 2\\n, you will often use ensemble methods near the end\\nof a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in machine\\nlearning competitions often involve several ensemble methods—most\\nfamously in the \\nNetflix Prize competition\\n.\\nIn this chapter we will examine the most popular ensemble methods,\\nincluding voting classifiers, bagging and pasting ensembles, random forests,\\nand boosting, and stacking ensembles.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 337, 'page_label': '338', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80%\\naccuracy. \\nYou may have a logistic regression classifier, an SVM classifier, a\\nrandom forest classifier, a \\nk\\n-nearest neighbors classifier, and perhaps a few\\nmore (see \\nFigure 7-1\\n).\\nFigure 7-1. \\nTraining diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the\\npredictions of each classifier: \\nthe class that gets the most votes is the\\nensemble’s prediction. This majority-vote classifier is called a \\nhard voting\\nclassifier (see \\nFigure 7-2\\n).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 338, 'page_label': '339', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 7-2. \\nHard voting classifier predictions\\nSomewhat surprisingly, this voting classifier often achieves a higher accuracy\\nthan the best classifier in the ensemble. \\nIn fact, even if each classifier is a\\nweak learner\\n (meaning it does only slightly better than random guessing), the\\nensemble can still be a \\nstrong learner\\n (achieving high accuracy), provided\\nthere are a sufficient number of weak learners in the ensemble and they are\\nsufficiently diverse.\\nHow is this possible? The following analogy can help shed some light on this\\nmystery. Suppose you have a slightly biased coin that has a 51% chance of\\ncoming up heads and 49% chance of coming up tails. If you toss it 1,000\\ntimes, you will generally get more or less 510 heads and 490 tails, and hence\\na majority of heads. If you do the math, you will find that the probability of\\nobtaining a majority of heads after 1,000 tosses is close to 75%. \\nThe more\\nyou toss the coin, the higher the probability (e.g., with 10,000 tosses, the\\nprobability climbs over 97%). This is due to the \\nlaw of large numbers\\n: as you\\nkeep tossing the coin, the ratio of heads gets closer and closer to the\\nprobability of heads (51%). \\nFigure 7-3\\n shows 10 series of biased coin tosses.\\nYou can see that as the number of tosses increases, the ratio of heads'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 339, 'page_label': '340', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='approaches 51%. Eventually all 10 series end up so close to 51% that they are\\nconsistently above 50%.\\nFigure 7-3. \\nThe law of large numbers\\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are\\nindividually correct only 51% of the time (barely better than random\\nguessing). If you predict the majority voted class, you can hope for up to 75%\\naccuracy! However, this is only true if all classifiers are perfectly\\nindependent, making uncorrelated errors, which is clearly not the case\\nbecause they are trained on the same data. They are likely to make the same\\ntypes of errors, so there will be many majority votes for the wrong class,\\nreducing the ensemble’s accuracy.\\nTIP\\nEnsemble methods work best when the predictors are as independent from one another as\\npossible. One way to get diverse classifiers is to train them using very different\\nalgorithms. This increases the chance that they will make very different types of errors,\\nimproving the ensemble’s accuracy.\\nScikit-Learn provides a \\nVotingClassifier\\n class that’s quite easy to use: just\\ngive it a list of name/predictor pairs, and use it like a normal classifier. Let’s\\ntry it on the moons dataset (introduced in \\nChapter 5\\n). \\nWe will load and split\\nthe moons dataset into a training set and a test set, then we’ll create and train'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 340, 'page_label': '341', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='a voting classifier composed of three diverse classifiers:\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nmake_moons\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nRandomForestClassifier\\n,\\n \\nVotingClassifier\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLogisticRegression\\nfrom\\n \\nsklearn.model_selection\\n \\nimport\\n \\ntrain_test_split\\nfrom\\n \\nsklearn.svm\\n \\nimport\\n \\nSVC\\nX\\n,\\n \\ny\\n \\n=\\n \\nmake_moons\\n(\\nn_samples\\n=\\n500\\n,\\n \\nnoise\\n=\\n0.30\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nX_train\\n,\\n \\nX_test\\n,\\n \\ny_train\\n,\\n \\ny_test\\n \\n=\\n \\ntrain_test_split\\n(\\nX\\n,\\n \\ny\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nvoting_clf\\n \\n=\\n \\nVotingClassifier\\n(\\n    \\nestimators\\n=\\n[\\n        \\n(\\n\\'lr\\'\\n,\\n \\nLogisticRegression\\n(\\nrandom_state\\n=\\n42\\n)),\\n        \\n(\\n\\'rf\\'\\n,\\n \\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n42\\n)),\\n        \\n(\\n\\'svc\\'\\n,\\n \\nSVC\\n(\\nrandom_state\\n=\\n42\\n))\\n    \\n]\\n)\\nvoting_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\nWhen you fit a \\nVotingClassifier\\n, it clones every estimator and fits the clones.\\nThe original estimators are available via the \\nestimators\\n attribute, while the\\nfitted clones are available via the \\nestimators_\\n attribute. If you prefer a dict\\nrather than a list, you can use \\nnamed_estimators\\n or \\nnamed_estimators_\\ninstead. To begin, let’s look at each fitted classifier’s accuracy on the test set:\\n>>> \\nfor\\n \\nname\\n,\\n \\nclf\\n \\nin\\n \\nvoting_clf\\n.\\nnamed_estimators_\\n.\\nitems\\n():\\n... \\n    \\nprint\\n(\\nname\\n,\\n \\n\"=\"\\n,\\n \\nclf\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n))\\n...\\nlr = 0.864\\nrf = 0.896\\nsvc = 0.896\\nWhen you call the voting classifier’s \\npredict()\\n method, it performs hard\\nvoting. For example, the voting classifier predicts class 1 for the first instance\\nof the test set, because two out of three classifiers predict that class:\\n>>> \\nvoting_clf\\n.\\npredict\\n(\\nX_test\\n[:\\n1\\n])\\narray([1])\\n>>> \\n[\\nclf\\n.\\npredict\\n(\\nX_test\\n[:\\n1\\n])\\n \\nfor\\n \\nclf\\n \\nin\\n \\nvoting_clf\\n.\\nestimators_\\n]\\n[array([1]), array([1]), array([0])]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 341, 'page_label': '342', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Now let’s look at the performance of the voting classifier on the test set:\\n>>> \\nvoting_clf\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n)\\n0.912\\nThere you have it! The voting classifier outperforms all the individual\\nclassifiers\\n.\\nIf all classifiers are able to estimate class probabilities (i.e., if they all have a\\npredict_proba()\\n method), then you can tell Scikit-Learn to predict the class\\nwith the highest class probability, averaged over all the individual classifiers.\\nThis is called \\nsoft voting\\n. It often achieves higher performance than hard\\nvoting because it gives more weight to highly confident votes. All you need\\nto do is set the voting classifier’s \\nvoting\\n hyperparameter to \\n\"soft\"\\n, and ensure\\nthat all classifiers can estimate class probabilities. This is not the case for the\\nSVC\\n class by default, so you need to set its \\nprobability\\n hyperparameter to\\nTrue\\n (this will make the \\nSVC\\n class use cross-validation to estimate class\\nprobabilities, slowing down training, and it will add a \\npredict_proba()\\nmethod). Let’s try that:\\n>>> \\nvoting_clf\\n.\\nvoting\\n \\n=\\n \\n\"soft\"\\n>>> \\nvoting_clf\\n.\\nnamed_estimators\\n[\\n\"svc\"\\n]\\n.\\nprobability\\n \\n=\\n \\nTrue\\n>>> \\nvoting_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\n>>> \\nvoting_clf\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n)\\n0.92\\nWe reach 92% accuracy simply by using soft voting—not bad!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 342, 'page_label': '343', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Bagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training\\nalgorithms, as just discussed. \\nAnother approach is to use the same training\\nalgorithm for every predictor but train them on different random subsets of\\nthe training set. When sampling is performed \\nwith\\n replacement,\\n\\u2060\\n this\\nmethod is called \\nbagging\\n\\u2060\\n (short for \\nbootstrap aggregating\\n\\u2060\\n). When sampling\\nis performed \\nwithout\\n replacement, it is called \\npasting\\n.\\n\\u2060\\nIn other words, both bagging and pasting allow training instances to be\\nsampled several times across multiple predictors, but only bagging allows\\ntraining instances to be sampled several times for the same predictor. This\\nsampling and training process is represented in \\nFigure 7-4\\n.\\nFigure 7-4. \\nBagging and pasting involve training several predictors on different random samples of the\\ntraining set\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. \\nThe\\n1\\n2\\n3\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 343, 'page_label': '344', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='aggregation function is typically the \\nstatistical mode\\n for classification (i.e.,\\nthe most frequent prediction, just like with a hard voting classifier), or the\\naverage for regression. Each individual predictor has a higher bias than if it\\nwere trained on the original training set, but aggregation reduces both bias\\nand variance.\\n\\u2060\\n Generally, the net result is that the ensemble has a similar\\nbias but a lower variance than a single predictor trained on the original\\ntraining set.\\nAs you can see in \\nFigure 7-4\\n, predictors can all be trained in parallel, via\\ndifferent CPU cores or even different servers. Similarly, predictions can be\\nmade in parallel. This is one of the reasons bagging and pasting are such\\npopular methods: they scale very well.\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 344, 'page_label': '345', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Bagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting:\\nBaggingClassifier\\n class (or \\nBaggingRegressor\\n for regression). \\nThe following\\ncode trains an ensemble of 500 decision tree classifiers:\\n\\u2060\\n each is trained on\\n100 training instances randomly sampled from the training set with\\nreplacement (this is an example of bagging, but if you want to use pasting\\ninstead, just set \\nbootstrap=False\\n). The \\nn_jobs\\n parameter tells Scikit-Learn the\\nnumber of CPU cores to use for training and predictions, and \\n–1\\n tells\\n Scikit-\\nLearn to use all available cores:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nBaggingClassifier\\nfrom\\n \\nsklearn.tree\\n \\nimport\\n \\nDecisionTreeClassifier\\nbag_clf\\n \\n=\\n \\nBaggingClassifier\\n(\\nDecisionTreeClassifier\\n(),\\n \\nn_estimators\\n=\\n500\\n,\\n                            \\nmax_samples\\n=\\n100\\n,\\n \\nn_jobs\\n=-\\n1\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nbag_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\nNOTE\\nA \\nBaggingClassifier\\n automatically performs soft voting instead of hard voting if the base\\nclassifier can estimate class probabilities (i.e., if it has a \\npredict_proba()\\n method), which is\\nthe case with decision tree classifiers.\\nFigure 7-5\\n compares the decision boundary of a single decision tree with the\\ndecision boundary of a bagging ensemble of 500 trees (from the preceding\\ncode), both trained on the moons dataset. As you can see, the ensemble’s\\npredictions will likely generalize much better than the single decision tree’s\\npredictions: the ensemble has a comparable bias but a smaller variance (it\\nmakes roughly the same number of errors on the training set, but the decision\\nboundary is less irregular).\\nBagging introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting; but\\nthe extra diversity also means that the predictors end up being less correlated,\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 345, 'page_label': '346', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='so the ensemble’s variance is reduced. Overall, bagging often results in better\\nmodels, which explains why it’s generally preferred. But if you have spare\\ntime and CPU power, you can use cross-validation to evaluate both bagging\\nand pasting and select the one that works best.\\nFigure 7-5. \\nA single decision tree (left) versus a bagging ensemble of 500 trees (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 346, 'page_label': '347', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Out-of-Bag Evaluation\\nWith bagging, some training instances may be sampled several times for any\\ngiven predictor, while others may not be sampled at all. \\nBy default a\\nBaggingClassifier\\n samples \\nm\\n training instances with replacement\\n(\\nbootstrap=True\\n), where \\nm\\n is the size of the training set. With this process, it\\ncan be shown mathematically that only about 63% of the training instances\\nare sampled on average for each predictor.\\n\\u2060\\n The remaining 37% of the\\ntraining instances that are not sampled are called \\nout-of-bag\\n (OOB) instances.\\nNote that they are not the same 37% for all predictors.\\nA bagging ensemble can be evaluated using OOB instances, without the need\\nfor a separate validation set: indeed, if there are enough estimators, then each\\ninstance in the training set will likely be an OOB instance of several\\nestimators, so these estimators can be used to make a fair ensemble prediction\\nfor that instance. Once you have a prediction for each instance, you can\\ncompute the ensemble’s prediction accuracy (or any other metric).\\nIn Scikit-Learn, you can set \\noob_score=True\\n when creating a\\nBaggingClassifier\\n to request an automatic OOB evaluation after training. The\\nfollowing code demonstrates this. The resulting evaluation score is available\\nin the \\noob_score_\\n attribute:\\n>>> \\nbag_clf\\n \\n=\\n \\nBaggingClassifier\\n(\\nDecisionTreeClassifier\\n(),\\n \\nn_estimators\\n=\\n500\\n,\\n... \\n                            \\noob_score\\n=\\nTrue\\n,\\n \\nn_jobs\\n=-\\n1\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n...\\n>>> \\nbag_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\n>>> \\nbag_clf\\n.\\noob_score_\\n0.896\\nAccording to this OOB evaluation, this \\nBaggingClassifier\\n is likely to achieve\\nabout 89.6% accuracy on the test set. Let’s verify this:\\n>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\naccuracy_score\\n>>> \\ny_pred\\n \\n=\\n \\nbag_clf\\n.\\npredict\\n(\\nX_test\\n)\\n>>> \\naccuracy_score\\n(\\ny_test\\n,\\n \\ny_pred\\n)\\n0.92\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 347, 'page_label': '348', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='We get 92% accuracy on the test. The OOB evaluation was a bit too\\npessimistic, just over 2% too low.\\nThe OOB decision function for each training instance is also available\\nthrough the \\noob_decision_function_\\n attribute. Since the base estimator has a\\npredict_proba()\\n method, the decision function returns the class probabilities\\nfor each training instance. For example, the OOB evaluation estimates that\\nthe first training instance has a 67.6% probability of belonging to the positive\\nclass and a 32.4% probability of belonging to the negative class:\\n>>> \\nbag_clf\\n.\\noob_decision_function_\\n[:\\n3\\n]\\n  \\n# probas for the first 3 instances\\narray([[0.32352941, 0.67647059],\\n       [0.3375    , 0.6625    ],\\n       [1.        , 0.        ]])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 348, 'page_label': '349', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Random Patches and Random Subspaces\\nThe \\nBaggingClassifier\\n class supports sampling the features as well. Sampling\\nis controlled by two hyperparameters: \\nmax_features\\n and \\nbootstrap_features\\n.\\nThey work the same way as \\nmax_samples\\n and \\nbootstrap\\n, but for feature\\nsampling instead of instance sampling. Thus, each predictor will be trained\\non a random subset of the input features.\\nThis technique is particularly useful when you are dealing with high-\\ndimensional inputs (such as images), as it can considerably speed up training.\\nSampling both training instances and features is called the \\nrandom patches\\nmethod\\n.\\n\\u2060\\n Keeping all training instances (by setting \\nbootstrap=False\\n and\\nmax_samples=1.0\\n) but sampling features (by setting \\nbootstrap_features\\n to\\nTrue\\n and/or \\nmax_features\\n to a value smaller than \\n1.0\\n) is called the \\nrandom\\nsubspaces\\n method\\n.\\n\\u2060\\nSampling features results in even more predictor diversity, trading a bit more\\nbias for a lower variance.\\n8\\n9'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 349, 'page_label': '350', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Random Forests\\nAs we have discussed, a \\nrandom forest\\n\\u2060\\n is an ensemble of decision trees,\\ngenerally trained via the bagging method (or sometimes pasting), typically\\nwith \\nmax_samples\\n set to the size of the training set. \\nInstead of building a\\nBaggingClassifier\\n and passing it a \\nDecisionTreeClassifier\\n, you can use the\\nRandomForestClassifier\\n class, which is more convenient and optimized for\\ndecision trees\\n\\u2060\\n (similarly, there is a \\nRandomForestRegressor\\n class for\\nregression tasks). The following code trains a random forest classifier with\\n500 trees, each limited to maximum 16 leaf nodes, using all available CPU\\ncores:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nRandomForestClassifier\\nrnd_clf\\n \\n=\\n \\nRandomForestClassifier\\n(\\nn_estimators\\n=\\n500\\n,\\n \\nmax_leaf_nodes\\n=\\n16\\n,\\n                                 \\nn_jobs\\n=-\\n1\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nrnd_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\ny_pred_rf\\n \\n=\\n \\nrnd_clf\\n.\\npredict\\n(\\nX_test\\n)\\nWith a few exceptions, a \\nRandomForestClassifier\\n has all the hyperparameters\\nof a \\nDecisionTreeClassifier\\n (to control how trees are grown), plus all the\\nhyperparameters of a \\nBaggingClassifier\\n to control the ensemble itself.\\nThe random forest algorithm introduces extra randomness when growing\\ntrees; instead of searching for the very best feature when splitting a node (see\\nChapter 6\\n), it searches for the best feature among a random subset of features.\\nBy default, it samples n features (where \\nn\\n is the total number of features).\\nThe algorithm results in greater tree diversity, which (again) trades a higher\\nbias for a lower variance, generally yielding an overall better model. So, the\\nfollowing \\nBaggingClassifier\\n is equivalent to the previous\\nRandomForestClassifier\\n:\\nbag_clf\\n \\n=\\n \\nBaggingClassifier\\n(\\n    \\nDecisionTreeClassifier\\n(\\nmax_features\\n=\\n\"sqrt\"\\n,\\n \\nmax_leaf_nodes\\n=\\n16\\n),\\n    \\nn_estimators\\n=\\n500\\n,\\n \\nn_jobs\\n=-\\n1\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n10\\n11'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 350, 'page_label': '351', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Extra-Trees\\nWhen you are growing a tree in a random forest, at each node only a random\\nsubset of the features is considered for splitting (as discussed earlier). \\nIt is\\npossible to make trees even more random by also using random thresholds\\nfor each feature rather than searching for the best possible thresholds (like\\nregular decision trees do). For this, simply set \\nsplitter=\"random\"\\n when\\ncreating a \\nDecisionTreeClassifier\\n.\\nA forest of such extremely random trees is called an \\nextremely randomized\\ntrees\\n\\u2060\\n (or \\nextra-trees\\n for short) ensemble. Once again, this technique trades\\nmore bias for a lower variance. \\nIt also makes extra-trees classifiers much\\nfaster to train than regular random forests, because finding the best possible\\nthreshold for each feature at every node is one of the most time-consuming\\ntasks of growing a tree.\\nYou can create an extra-trees classifier using Scikit-Learn’s\\nExtraTreesClassifier\\n class. \\nIts API is identical to the \\nRandomForestClassifier\\nclass, except \\nbootstrap\\n defaults to \\nFalse\\n. Similarly, the \\nExtraTreesRegressor\\nclass has the same API as the \\nRandomForestRegressor\\n class, except \\nbootstrap\\ndefaults to \\nFalse\\n.\\nTIP\\nIt is hard to tell in advance whether a \\nRandomForestClassifier\\n will perform better or worse\\nthan an \\nExtraTreesClassifier\\n. Generally, the only way to know is to try both and compare\\nthem using cross-validation.\\n12'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 351, 'page_label': '352', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Feature Importance\\nYet another great quality of random forests is that they make it easy to\\nmeasure the relative importance of each feature. \\nScikit-Learn measures a\\nfeature’s importance by looking at how much the tree nodes that use that\\nfeature reduce impurity on average, across all trees in the forest. More\\nprecisely, it is a weighted average, where each node’s weight is equal to the\\nnumber of training samples that are associated with it (see \\nChapter 6\\n).\\nScikit-Learn computes this score automatically for each feature after training,\\nthen it scales the results so that the sum of all importances is equal to 1. You\\ncan access the result using the \\nfeature_importances_\\n variable. \\nFor example,\\nthe following code trains a \\nRandomForestClassifier\\n on the iris dataset\\n(introduced in \\nChapter 4\\n) and outputs each feature’s importance. It seems that\\nthe most important features are the petal length (44%) and width (42%),\\nwhile sepal length and width are rather unimportant in comparison (11% and\\n2%, respectively):\\n>>> \\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nload_iris\\n>>> \\niris\\n \\n=\\n \\nload_iris\\n(\\nas_frame\\n=\\nTrue\\n)\\n>>> \\nrnd_clf\\n \\n=\\n \\nRandomForestClassifier\\n(\\nn_estimators\\n=\\n500\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n>>> \\nrnd_clf\\n.\\nfit\\n(\\niris\\n.\\ndata\\n,\\n \\niris\\n.\\ntarget\\n)\\n>>> \\nfor\\n \\nscore\\n,\\n \\nname\\n \\nin\\n \\nzip\\n(\\nrnd_clf\\n.\\nfeature_importances_\\n,\\n \\niris\\n.\\ndata\\n.\\ncolumns\\n):\\n... \\n    \\nprint\\n(\\nround\\n(\\nscore\\n,\\n \\n2\\n),\\n \\nname\\n)\\n...\\n0.11 sepal length (cm)\\n0.02 sepal width (cm)\\n0.44 petal length (cm)\\n0.42 petal width (cm)\\nSimilarly, if you train a random forest classifier on the MNIST dataset\\n(introduced in \\nChapter 3\\n) and plot each pixel’s importance, you get the image\\nrepresented in \\nFigure 7-6\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 352, 'page_label': '353', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 7-6. \\nMNIST pixel importance (according to a random forest classifier)\\nRandom forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 353, 'page_label': '354', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Boosting\\nBoosting\\n (originally called \\nhypothesis boosting\\n) refers to any ensemble\\nmethod that can combine several weak learners into a strong learner. \\nThe\\ngeneral idea of most boosting methods is to train predictors sequentially, each\\ntrying to correct its predecessor. \\nThere are many boosting methods available,\\nbut by far the most popular are \\nAdaBoost\\n\\u2060\\n (short for \\nadaptive boosting\\n) and\\ngradient boosting\\n. Let’s start with AdaBoost.\\n13'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 354, 'page_label': '355', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='AdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more\\nattention to the training instances that the predecessor underfit. \\nThis results in\\nnew predictors focusing more and more on the hard cases. This is the\\ntechnique used by \\nAdaBoost\\n.\\nFor example, when training an AdaBoost classifier, the algorithm first trains\\na base classifier (such as a decision tree) and uses it to make predictions on\\nthe training set. \\nThe algorithm then increases the relative weight of\\nmisclassified training instances. Then it trains a second classifier, using the\\nupdated weights, and again makes predictions on the training set, updates the\\ninstance weights, and so on (see \\nFigure 7-7\\n).\\nFigure 7-8\\n shows the decision boundaries of five consecutive predictors on\\nthe moons dataset (in this example, each predictor is a highly regularized\\nSVM classifier with an RBF kernel).\\n\\u2060\\n The first classifier gets many\\ninstances wrong, so their weights get boosted. The second classifier therefore\\ndoes a better job on these instances, and so on. The plot on the right\\nrepresents the same sequence of predictors, except that the learning rate is\\nhalved (i.e., the misclassified instance weights are boosted much less at every\\niteration). \\nAs you can see, this sequential learning technique has some\\nsimilarities with gradient descent, except that instead of tweaking a single\\npredictor’s parameters to minimize a cost function, AdaBoost adds predictors\\nto the ensemble, gradually making it better.\\n14'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 355, 'page_label': '356', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 7-7. \\nAdaBoost sequential training with instance weight updates\\nOnce all predictors are trained, the ensemble makes predictions very much\\nlike bagging or pasting, except that predictors have different weights\\ndepending on their overall accuracy on the weighted training set.\\nFigure 7-8. \\nDecision boundaries of consecutive predictors\\nWARNING'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 356, 'page_label': '357', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='There is one important drawback to this sequential learning technique: training cannot be\\nparallelized since each predictor can only be trained after the previous predictor has been\\ntrained and evaluated. As a result, it does not scale as well as bagging or pasting.\\nLet’s take a closer look at the AdaBoost algorithm. Each instance weight \\nw\\nis initially set to 1/\\nm\\n. A first predictor is trained, and its weighted error rate \\nr\\nis computed on the training set; see \\nEquation 7-1\\n.\\nEquation 7-1. \\nWeighted error rate of the j\\n predictor\\nrj \\n= \\n∑ \\ni=1 \\ny^ \\nj \\n(i) \\n≠ \\ny \\n(i) \\nm \\nw(i) \\nwhere \\ny^ \\nj(i) \\nis \\nthe \\njth \\npredictor’s\\npredictionfor \\nthe \\ni \\nth \\ninstance\\nThe predictor’s weight \\nα\\n is then computed using \\nEquation 7-2\\n, where \\nη\\n is the\\nlearning rate hyperparameter (defaults to 1).\\n\\u2060\\n The more accurate the\\npredictor is, the higher its weight will be. If it is just guessing randomly, then\\nits weight will be close to zero. However, if it is most often wrong (i.e., less\\naccurate than random guessing), then its weight will be negative.\\nEquation 7-2. \\nPredictor weight\\nα j \\n= \\nη \\nlog \\n1-r j \\nr j\\nNext, the AdaBoost algorithm updates the instance weights, using \\nEquation\\n7-3\\n, which boosts the weights of the misclassified instances.\\nEquation 7-3. \\nWeight update rule\\nfor \\ni \\n= \\n1 \\n, \\n2 \\n, \\n⋯\\n \\n, \\nm \\nw (i) \\n← \\nw (i) \\nif \\ny j \\n^ (i) \\n= \\ny (i) \\nw (i) \\nexp \\n( \\nα j \\n) \\nif \\ny j \\n^ (i) \\n≠\\ny (i)\\nThen all the instance weights are normalized (i.e., divided by ∑i=1mw(i)).\\nFinally, a new predictor is trained using the updated weights, and the whole\\nprocess is repeated: the new predictor’s weight is computed, the instance\\nweights are updated, then another predictor is trained, and so on. The\\nalgorithm stops when the desired number of predictors is reached, or when a\\nperfect predictor is found.\\nTo make predictions, AdaBoost simply computes the predictions of all the\\n(\\ni\\n)\\n1\\nth\\nj\\n15'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 357, 'page_label': '358', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='predictors and weighs them using the predictor weights \\nα\\n. The predicted\\nclass is the one that receives the majority of weighted votes (see \\nEquation 7-\\n4\\n).\\nEquation 7-4. \\nAdaBoost predictions\\ny ^ \\n( \\nx \\n) \\n= \\nargmax k \\n∑ j=1 y ^ j (x)=k N \\nα j \\nwhere \\nN \\nis \\nthe \\nnumber \\nof\\npredictors\\nScikit-Learn uses a multiclass version of AdaBoost called \\nSAMME\\n\\u2060\\n (which\\nstands for \\nStagewise Additive Modeling using a Multiclass Exponential loss\\nfunction\\n). \\nWhen there are just two classes, SAMME is equivalent to\\nAdaBoost. If the predictors can estimate class probabilities (i.e., if they have\\na \\npredict_proba()\\n method), Scikit-Learn can use a variant of SAMME called\\nSAMME.R\\n (the \\nR\\n stands for “Real”), which relies on class probabilities rather\\nthan predictions and generally performs better.\\nThe following code trains an AdaBoost classifier based on 30 \\ndecision\\nstumps\\n using Scikit-Learn’s \\nAdaBoostClassifier\\n class (as you might expect,\\nthere is also an \\nAdaBoostRegressor\\n class). \\nA decision stump is a decision tree\\nwith \\nmax_depth=1\\n—in other words, a tree composed of a single decision\\nnode plus two leaf nodes. This is the default base estimator for the\\nAdaBoostClassifier\\n class:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nAdaBoostClassifier\\nada_clf\\n \\n=\\n \\nAdaBoostClassifier\\n(\\n    \\nDecisionTreeClassifier\\n(\\nmax_depth\\n=\\n1\\n),\\n \\nn_estimators\\n=\\n30\\n,\\n    \\nlearning_rate\\n=\\n0.5\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nada_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\nTIP\\nIf your AdaBoost ensemble is overfitting the training set, you can try reducing the number\\nof estimators or more strongly regularizing the base estimator.\\nj\\n16'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 358, 'page_label': '359', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Gradient Boosting\\nAnother very popular boosting algorithm is \\ngradient boosting\\n.\\n\\u2060\\n \\nJust like\\nAdaBoost, gradient boosting works by sequentially adding predictors to an\\nensemble, each one correcting its predecessor. \\nHowever, instead of tweaking\\nthe instance weights at every iteration like AdaBoost does, this method tries\\nto fit the new predictor to the \\nresidual errors\\n made by the previous predictor.\\nLet’s go through a simple \\nregression example, using decision trees as the\\nbase predictors; this is called \\ngradient tree boosting\\n, or \\ngradient boosted\\nregression trees\\n (GBRT). \\nFirst, let’s generate a noisy quadratic dataset and fit\\na \\nDecisionTreeRegressor\\n to it:\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\nfrom\\n \\nsklearn.tree\\n \\nimport\\n \\nDecisionTreeRegressor\\nnp\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\nX\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrand\\n(\\n100\\n,\\n \\n1\\n)\\n \\n-\\n \\n0.5\\ny\\n \\n=\\n \\n3\\n \\n*\\n \\nX\\n[:,\\n \\n0\\n]\\n \\n**\\n \\n2\\n \\n+\\n \\n0.05\\n \\n*\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n100\\n)\\n  \\n# y = 3x² + Gaussian noise\\ntree_reg1\\n \\n=\\n \\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ntree_reg1\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nNext, we’ll train a second \\nDecisionTreeRegressor\\n on the residual errors made\\nby the first predictor:\\ny2\\n \\n=\\n \\ny\\n \\n-\\n \\ntree_reg1\\n.\\npredict\\n(\\nX\\n)\\ntree_reg2\\n \\n=\\n \\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nrandom_state\\n=\\n43\\n)\\ntree_reg2\\n.\\nfit\\n(\\nX\\n,\\n \\ny2\\n)\\nAnd then we’ll train a third regressor on the residual errors made by the\\nsecond predictor:\\ny3\\n \\n=\\n \\ny2\\n \\n-\\n \\ntree_reg2\\n.\\npredict\\n(\\nX\\n)\\ntree_reg3\\n \\n=\\n \\nDecisionTreeRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nrandom_state\\n=\\n44\\n)\\ntree_reg3\\n.\\nfit\\n(\\nX\\n,\\n \\ny3\\n)\\nNow we have an ensemble containing three trees. It can make predictions on\\n17'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 359, 'page_label': '360', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='a new instance simply by adding up the predictions of all the trees:\\n>>> \\nX_new\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n-\\n0.4\\n],\\n \\n[\\n0.\\n],\\n \\n[\\n0.5\\n]])\\n>>> \\nsum\\n(\\ntree\\n.\\npredict\\n(\\nX_new\\n)\\n \\nfor\\n \\ntree\\n \\nin\\n \\n(\\ntree_reg1\\n,\\n \\ntree_reg2\\n,\\n \\ntree_reg3\\n))\\narray([0.49484029, 0.04021166, 0.75026781])\\nFigure 7-9\\n represents the predictions of these three trees in the left column,\\nand the ensemble’s predictions in the right column. In the first row, the\\nensemble has just one tree, so its predictions are exactly the same as the first\\ntree’s predictions. In the second row, a new tree is trained on the residual\\nerrors of the first tree. On the right you can see that the ensemble’s\\npredictions are equal to the sum of the predictions of the first two trees.\\nSimilarly, in the third row another tree is trained on the residual errors of the\\nsecond tree. You can see that the ensemble’s predictions gradually get better\\nas trees are added to the ensemble.\\nYou can use Scikit-Learn’s \\nGradientBoostingRegressor\\n class to train GBRT\\nensembles more easily (there’s also a \\nGradientBoostingClassifier\\n class for\\nclassification). \\nMuch like the \\nRandomForestRegressor\\n class, it has\\nhyperparameters to control the growth of decision trees (e.g., \\nmax_depth\\n,\\nmin_samples_leaf\\n), as well as \\nhyperparameters\\n to control the ensemble\\ntraining, such as the number of trees (\\nn_estimators\\n). The following code\\ncreates the same ensemble as the previous one:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nGradientBoostingRegressor\\ngbrt\\n \\n=\\n \\nGradientBoostingRegressor\\n(\\nmax_depth\\n=\\n2\\n,\\n \\nn_estimators\\n=\\n3\\n,\\n                                 \\nlearning_rate\\n=\\n1.0\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ngbrt\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 360, 'page_label': '361', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 7-9. \\nIn this depiction of gradient boosting, the first predictor (top left) is trained normally, then\\neach consecutive predictor (middle left and lower left) is trained on the previous predictor’s residuals;\\nthe right column shows the resulting ensemble’s predictions\\nThe \\nlearning_rate\\n hyperparameter scales the contribution of each tree. \\nIf you\\nset it to a low value, such as \\n0.05\\n, you will need more trees in the ensemble to\\nfit the training set, but the predictions will usually generalize better. This is a\\nregularization technique called \\nshrinkage\\n. \\nFigure 7-10\\n shows two GBRT\\nensembles trained with different hyperparameters: the one on the left does not\\nhave enough trees to fit the training set, while the one on the right has about\\nthe right amount. If we added more trees, the GBRT would start to overfit the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 361, 'page_label': '362', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='training set.\\nFigure 7-10. \\nGBRT ensembles with not enough predictors (left) and just enough (right)\\nTo find the optimal number of trees, you could perform cross-validation\\nusing \\nGridSearchCV\\n or \\nRandomizedSearchCV\\n, as usual, but there’s a\\nsimpler way: if you set the \\nn_iter_no_change\\n hyperparameter to an integer\\nvalue, say 10, then the \\nGradientBoostingRegressor\\n will automatically stop\\nadding more trees during training if it sees that the last 10 trees didn’t help.\\nThis is simply early stopping (introduced in \\nChapter 4\\n), but with a little bit of\\npatience: it tolerates having no progress for a few iterations before it stops.\\nLet’s train the ensemble using early stopping:\\ngbrt_best\\n \\n=\\n \\nGradientBoostingRegressor\\n(\\n    \\nmax_depth\\n=\\n2\\n,\\n \\nlearning_rate\\n=\\n0.05\\n,\\n \\nn_estimators\\n=\\n500\\n,\\n    \\nn_iter_no_change\\n=\\n10\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ngbrt_best\\n.\\nfit\\n(\\nX\\n,\\n \\ny\\n)\\nIf you set \\nn_iter_no_change\\n too low, training may stop too early and the\\nmodel will underfit. But if you set it too high, it will overfit instead. We also\\nset a fairly small learning rate and a high number of estimators, but the actual\\nnumber of estimators in the trained ensemble is much lower, thanks to early\\nstopping:\\n>>> \\ngbrt_best\\n.\\nn_estimators_\\n92\\nWhen \\nn_iter_no_change\\n is set, the \\nfit()\\n method automatically splits the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 362, 'page_label': '363', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='training set into a smaller training set and a validation set: this allows it to\\nevaluate the model’s performance each time it adds a new tree. The size of\\nthe validation set is controlled by the \\nvalidation_fraction\\n hyperparameter,\\nwhich is 10% by default. The \\ntol\\n hyperparameter determines the maximum\\nperformance improvement that still counts as negligible. It defaults to 0.0001.\\nThe \\nGradientBoostingRegressor\\n class also supports a \\nsubsample\\nhyperparameter, which specifies the fraction of training instances to be used\\nfor training each tree. \\nFor example, if \\nsubsample=0.25\\n, then each tree is\\ntrained on 25% of the training instances, selected randomly. As you can\\nprobably guess by now, this technique trades a higher bias for a lower\\nvariance. It also speeds up training considerably. \\nThis is called \\nstochastic\\ngradient boosting\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 363, 'page_label': '364', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Histogram-Based Gradient Boosting\\nScikit-Learn also provides another GBRT implementation, optimized for\\nlarge datasets: \\nhistogram-based gradient boosting\\n (HGB). \\nIt works by\\nbinning the input features, replacing them with integers. The number of bins\\nis controlled by the \\nmax_bins\\n hyperparameter, which defaults to 255 and\\ncannot be set any higher than this. Binning can greatly reduce the number of\\npossible thresholds that the training algorithm needs to evaluate. Moreover,\\nworking with integers makes it possible to use faster and more memory-\\nefficient data structures. And the way the bins are built removes the need for\\nsorting the features when training each tree.\\nAs a result, this \\nimplementation has a computational complexity of \\nO\\n(\\nb\\n×\\nm\\n)\\ninstead of \\nO\\n(\\nn\\n×\\nm\\n×log(\\nm\\n)), where \\nb\\n is the number of bins, \\nm\\n is the number of\\ntraining instances, and \\nn\\n is the number of features. In practice, this means that\\nHGB can train hundreds of times faster than regular GBRT on large datasets.\\nHowever, binning causes a precision loss, which acts as a regularizer:\\ndepending on the dataset, this may help reduce overfitting, or it may cause\\nunderfitting.\\nScikit-Learn provides two classes for HGB: \\nHistGradientBoostingRegressor\\nand \\nHistGradientBoostingClassifier\\n. They’re similar to\\nGradientBoostingRegressor\\n and \\nGradientBoostingClassifier\\n, with a few\\nnotable differences:\\nEarly stopping is automatically activated if the number of instances is\\ngreater than 10,000. You can turn early stopping always on or always\\noff by setting the \\nearly_stopping\\n hyperparameter to \\nTrue\\n or \\nFalse\\n.\\nSubsampling is not supported.\\nn_estimators\\n is renamed to \\nmax_iter\\n.\\nThe only decision tree hyperparameters that can be tweaked are\\nmax_leaf_nodes\\n, \\nmin_samples_leaf\\n, and \\nmax_depth\\n.\\nThe HGB classes also have two nice features: \\nthey support both categorical'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 364, 'page_label': '365', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='features and missing values. This simplifies preprocessing quite a bit.\\nHowever, the categorical features must be represented as integers ranging\\nfrom 0 to a number lower than \\nmax_bins\\n. You can use an \\nOrdinalEncoder\\n for\\nthis. For example, here’s how to build and train a complete pipeline for the\\nCalifornia housing dataset introduced in \\nChapter 2\\n:\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nmake_pipeline\\nfrom\\n \\nsklearn.compose\\n \\nimport\\n \\nmake_column_transformer\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nHistGradientBoostingRegressor\\nfrom\\n \\nsklearn.preprocessing\\n \\nimport\\n \\nOrdinalEncoder\\nhgb_reg\\n \\n=\\n \\nmake_pipeline\\n(\\n    \\nmake_column_transformer\\n((\\nOrdinalEncoder\\n(),\\n \\n[\\n\"ocean_proximity\"\\n]),\\n                            \\nremainder\\n=\\n\"passthrough\"\\n),\\n    \\nHistGradientBoostingRegressor\\n(\\ncategorical_features\\n=\\n[\\n0\\n],\\n \\nrandom_state\\n=\\n42\\n)\\n)\\nhgb_reg\\n.\\nfit\\n(\\nhousing\\n,\\n \\nhousing_labels\\n)\\nThe whole pipeline is just as short as the imports! No need for an imputer,\\nscaler, or a one-hot encoder, so it’s really convenient. Note that\\ncategorical_features\\n must be set to the categorical column indices (or a\\nBoolean array). Without any hyperparameter tuning, this model yields an\\nRMSE of about 47,600, which is not too bad.\\nTIP\\nSeveral other optimized implementations of gradient boosting are available in the Python\\nML ecosystem: in particular, \\nXGBoost\\n, \\nCatBoost\\n, and \\nLightGBM\\n. These libraries have\\nbeen around for several years. They are all specialized for gradient boosting, their APIs\\nare very similar to Scikit-Learn’s, and they provide many additional features, including\\nGPU acceleration; you should definitely check them out! Moreover, the \\nTensorFlow\\nRandom Forests library\\n provides optimized implementations of a variety of random forest\\nalgorithms, including plain random forests, extra-trees, GBRT, and several more.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 365, 'page_label': '366', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Stacking\\nThe last ensemble method we will discuss in this chapter is called \\nstacking\\n(short for \\nstacked generalization\\n).\\n\\u2060\\n \\nIt is based on a simple idea: instead of\\nusing trivial functions (such as hard voting) to aggregate the predictions of all\\npredictors in an ensemble, why don’t we train a model to perform this\\naggregation? \\nFigure 7-11\\n shows such an ensemble performing a regression\\ntask on a new instance. Each of the bottom three predictors predicts a\\ndifferent value (3.1, 2.7, and 2.9), and then the final predictor (called a\\nblender\\n, or a \\nmeta learner\\n) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\nFigure 7-11. \\nAggregating predictions using a blending predictor\\n18'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 366, 'page_label': '367', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='To train the blender, you first need to build the blending training set. You can\\nuse \\ncross_val_predict()\\n on every predictor in the ensemble to get out-of-\\nsample predictions for each instance in the original training set (\\nFigure 7-12\\n),\\nand use these can be used as the input features to train the blender; and the\\ntargets can simply be copied from the original training set. Note that\\nregardless of the number of features in the original training set (just one in\\nthis example), the blending training set will contain one input feature per\\npredictor (three in this example). Once the blender is trained, the base\\npredictors are retrained one last time on the full original training set.\\nFigure 7-12. \\nTraining the blender in a stacking ensemble\\nIt is actually possible to train several different blenders this way (e.g., one\\nusing linear regression, another using random forest regression) to get a'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 367, 'page_label': '368', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"whole layer of blenders, and then add another blender on top of that to\\nproduce the final prediction, as shown in \\nFigure 7-13\\n. You may be able to\\nsqueeze out a few more drops of performance by doing this, but it will cost\\nyou in both training time and system complexity.\\nFigure 7-13. \\nPredictions in a multilayer stacking ensemble\\nScikit-Learn provides two classes for stacking ensembles: \\nStackingClassifier\\nand \\nStackingRegressor\\n. \\nFor example, we can replace the \\nVotingClassifier\\n we\\nused at the beginning of this chapter on the moons dataset with a\\nStackingClassifier\\n:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nStackingClassifier\\nstacking_clf\\n \\n=\\n \\nStackingClassifier\\n(\\n    \\nestimators\\n=\\n[\\n        \\n(\\n'lr'\\n,\\n \\nLogisticRegression\\n(\\nrandom_state\\n=\\n42\\n)),\\n        \\n(\\n'rf'\\n,\\n \\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n42\\n)),\\n        \\n(\\n'svc'\\n,\\n \\nSVC\\n(\\nprobability\\n=\\nTrue\\n,\\n \\nrandom_state\\n=\\n42\\n))\\n    \\n],\\n    \\nfinal_estimator\\n=\\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n43\\n),\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 368, 'page_label': '369', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='cv\\n=\\n5\\n  \\n# number of cross-validation folds\\n)\\nstacking_clf\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train\\n)\\nFor each predictor, the stacking classifier will call \\npredict_proba()\\n if\\navailable; if not it will fall back to \\ndecision_function()\\n or, as a last resort, call\\npredict()\\n. If you don’t provide a final estimator, \\nStackingClassifier\\n will use\\nLogisticRegression\\n and \\nStackingRegressor\\n will use \\nRidgeCV\\n.\\nIf you evaluate this stacking model on the test set, you will find 92.8%\\naccuracy, which is a bit better than the voting classifier using soft voting,\\nwhich got 92%.\\nIn conclusion, ensemble methods are versatile, powerful, and fairly simple to\\nuse. Random forests, AdaBoost, and GBRT are among the first models you\\nshould test for most machine learning tasks, and they particularly shine with\\nheterogeneous tabular data. Moreover, as they require very little\\npreprocessing, they’re great for getting a prototype up and running quickly.\\nLastly, ensemble methods like voting classifiers and stacking classifiers can\\nhelp push your system’s performance to its limits.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 369, 'page_label': '370', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nIf you have trained five different models on the exact same training data,\\nand they all achieve 95% precision, is there any chance that you can\\ncombine these models to get better results? If so, how? If not, why?\\n2\\n. \\nWhat is the difference between hard and soft voting classifiers?\\n3\\n. \\nIs it possible to speed up training of a bagging ensemble by distributing\\nit across multiple servers? What about pasting ensembles, boosting\\nensembles, random forests, or stacking ensembles?\\n4\\n. \\nWhat is the benefit of out-of-bag evaluation?\\n5\\n. \\nWhat makes extra-trees ensembles more random than regular random\\nforests? How can this extra randomness help? Are extra-trees classifiers\\nslower or faster than regular random forests?\\n6\\n. \\nIf your AdaBoost ensemble underfits the training data, which\\nhyperparameters should you tweak, and how?\\n7\\n. \\nIf your gradient boosting ensemble overfits the training set, should you\\nincrease or decrease the learning rate?\\n8\\n. \\nLoad the MNIST dataset (introduced in \\nChapter 3\\n), and split it into a\\ntraining set, a validation set, and a test set (e.g., use 50,000 instances for\\ntraining, 10,000 for validation, and 10,000 for testing). Then train\\nvarious classifiers, such as a random forest classifier, an extra-trees\\nclassifier, and an SVM classifier. Next, try to combine them into an\\nensemble that outperforms each individual classifier on the validation\\nset, using soft or hard voting. Once you have found one, try it on the test\\nset. How much better does it perform compared to the individual\\nclassifiers?\\n9\\n. \\nRun the individual classifiers from the previous exercise to make\\npredictions on the validation set, and create a new training set with the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 370, 'page_label': '371', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='resulting predictions: each training instance is a vector containing the set\\nof predictions from all your classifiers for an image, and the target is the\\nimage’s class. Train a classifier on this new training set. Congratulations\\n—you have just trained a blender, and together with the classifiers it\\nforms a stacking ensemble! Now evaluate the ensemble on the test set.\\nFor each image in the test set, make predictions with all your classifiers,\\nthen feed the predictions to the blender to get the ensemble’s\\npredictions. How does it compare to the voting classifier you trained\\nearlier? Now try again using a \\nStackingClassifier\\n instead. Do you get\\nbetter performance? If so, why?\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n Imagine picking a card randomly from a deck of cards, writing it down, then placing it back in\\nthe deck before picking the next card: the same card could be sampled multiple times.\\n2\\n Leo Breiman, “Bagging Predictors”, \\nMachine Learning\\n 24, no. 2 (1996): 123–140.\\n3\\n In statistics, \\nresampling with replacement is called \\nbootstrapping\\n.\\n4\\n Leo Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line”,\\nMachine Learning\\n 36, no. 1–2 (1999): 85–103.\\n5\\n Bias and variance were introduced in \\nChapter 4\\n.\\n6\\n \\nmax_samples\\n can alternatively be set to a float between 0.0 and 1.0, in which case the max\\nnumber of sampled instances is equal to the size of the training set times \\nmax_samples\\n.\\n7\\n As \\nm\\n grows, this ratio approaches 1 – exp(–1) ≈ 63%.\\n8\\n Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches”, \\nLecture Notes in Computer\\nScience\\n 7523 (2012): 346–361.\\n9\\n Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests”, \\nIEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n 20, no. 8 (1998): 832–844.\\n10\\n Tin Kam Ho, “Random Decision Forests”, \\nProceedings of the Third International Conference\\non Document Analysis and Recognition\\n 1 (1995): 278.\\n11\\n The \\nBaggingClassifier\\n class remains useful if you want a bag of something other than decision\\ntrees.\\n12\\n Pierre Geurts et al., “Extremely Randomized Trees”, \\nMachine Learning\\n 63, no. 1 (2006): 3–42.\\n13\\n Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line\\nLearning and an Application to Boosting”, \\nJournal of Computer and System Sciences\\n 55, no. 1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 371, 'page_label': '372', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='(1997): 119–139.\\n14\\n This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost;\\nthey are slow and tend to be unstable with it.\\n15\\n The original AdaBoost algorithm does not use a learning rate hyperparameter.\\n16\\n For more details, see Ji Zhu et al., “Multi-Class AdaBoost”, \\nStatistics and Its Interface\\n 2, no. 3\\n(2009): 349–360.\\n17\\n Gradient boosting was first introduced in Leo Breiman’s \\n1997 paper\\n “Arcing the Edge” and was\\nfurther developed in the \\n1999 paper\\n “Greedy Function Approximation: A Gradient Boosting\\nMachine” by Jerome H. Friedman.\\n18\\n David H. Wolpert, “Stacked Generalization”, \\nNeural Networks\\n 5, no. 2 (1992): 241–259.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 372, 'page_label': '373', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 8. \\nDimensionality\\nReduction\\nMany machine learning problems involve thousands or even millions of\\nfeatures for each training instance. \\nNot only do all these features make\\ntraining extremely slow, but they can also make it much harder to find a good\\nsolution, as you will see. This problem is often referred to as the \\ncurse of\\ndimensionality\\n.\\nFortunately, in real-world problems, it is often possible to reduce the number\\nof features considerably, turning an intractable problem into a tractable one.\\nFor example, consider the MNIST images (introduced in \\nChapter 3\\n): the\\npixels on the image borders are almost always white, so you could\\ncompletely drop these pixels from the training set without losing much\\ninformation. \\nAs we saw in the previous chapter, (\\nFigure 7-6\\n) confirms that\\nthese pixels are utterly unimportant for the classification task. Additionally,\\ntwo neighboring pixels are often highly correlated: if you merge them into a\\nsingle pixel (e.g., by taking the mean of the two pixel intensities), you will\\nnot lose much information.\\nWARNING\\nReducing dimensionality does cause some information loss, just like compressing an\\nimage to JPEG can degrade its quality, so even though it will speed up training, it may\\nmake your system perform slightly worse. It also makes your pipelines a bit more complex\\nand thus harder to maintain. Therefore, I recommend you first try to train your system with\\nthe original data before considering using dimensionality reduction. In some cases,\\nreducing the dimensionality of the training data may filter out some noise and unnecessary\\ndetails and thus result in higher performance, but in general it won’t; it will just speed up\\ntraining.\\nApart from speeding up training, dimensionality reduction is also extremely'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 373, 'page_label': '374', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='useful for data visualization. \\nReducing the number of dimensions down to\\ntwo (or three) makes it possible to plot a condensed view of a high-\\ndimensional training set on a graph and often gain some important insights by\\nvisually detecting patterns, such as clusters. Moreover, data visualization is\\nessential to communicate your conclusions to people who are not data\\nscientists—in particular, decision makers who will use your results.\\nIn this chapter we will first discuss the curse of dimensionality and get a\\nsense of what goes on in high-dimensional space. Then we will consider the\\ntwo main approaches to dimensionality reduction (projection and manifold\\nlearning), and we will go through three of the most popular dimensionality\\nreduction techniques: PCA, random projection, and locally linear embedding\\n(LLE).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 374, 'page_label': '375', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The Curse of Dimensionality\\nWe are so used to living in three dimensions\\n\\u2060\\n that our intuition fails us\\nwhen we try to imagine a high-dimensional space. \\nEven a basic 4D\\nhypercube is incredibly hard to picture in our minds (see \\nFigure 8-1\\n), let\\nalone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.\\nFigure 8-1. \\nPoint, segment, square, cube, and tesseract (0D to 4D hypercubes)\\n\\u2060\\nIt turns out that many things behave very differently in high-dimensional\\nspace. For example, if you pick a random point in a unit square (a 1 × 1\\nsquare), it will have only about a 0.4% chance of being located less than\\n0.001 from a border (in other words, it is very unlikely that a random point\\nwill be “extreme” along any dimension). But in a 10,000-dimensional unit\\nhypercube, this probability is greater than 99.999999%. Most points in a\\nhigh-dimensional hypercube are very close to the border.\\n\\u2060\\nHere is a more troublesome difference: if you pick two points randomly in a\\nunit square, the distance between these two points will be, on average,\\nroughly 0.52. If you pick two random points in a 3D unit cube, the average\\ndistance will be roughly 0.66. But what about two points picked randomly in\\na 1,000,000-dimensional unit hypercube? The average distance, believe it or\\nnot, will be about 408.25 (roughly 1,000,0006)! This is counterintuitive: how\\ncan two points be so far apart when they both lie within the same unit\\nhypercube? Well, there’s just plenty of space in high dimensions. As a result,\\nhigh-dimensional datasets are at risk of being very sparse: most training\\n1\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 375, 'page_label': '376', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='instances are likely to be far away from each other. This also means that a\\nnew instance will likely be far away from any training instance, making\\npredictions much less reliable than in lower dimensions, since they will be\\nbased on much larger extrapolations. In short, the more dimensions the\\ntraining set has, the greater the risk of overfitting it.\\nIn theory, one solution to the curse of dimensionality could be to increase the\\nsize of the training set to reach a sufficient density of training instances.\\nUnfortunately, in practice, the number of training instances required to reach\\na given density grows exponentially with the number of dimensions. With\\njust 100 features—significantly fewer than in the MNIST problem—all\\nranging from 0 to 1, you would need more training instances than atoms in\\nthe observable universe in order for training instances to be within 0.1 of\\neach other on average, assuming they were spread out uniformly across all\\ndimensions.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 376, 'page_label': '377', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Main Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a\\nlook at the two main approaches to reducing dimensionality: projection and\\nmanifold learning.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 377, 'page_label': '378', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Projection\\nIn most real-world problems, training instances are \\nnot\\n spread out uniformly\\nacross all dimensions. \\nMany features are almost constant, while others are\\nhighly correlated (as discussed earlier for MNIST). As a result, all training\\ninstances lie within (or close to) a much lower-dimensional \\nsubspace\\n of the\\nhigh-dimensional space. This sounds very abstract, so let’s look at an\\nexample. In \\nFigure 8-2\\n you can see a 3D dataset represented by small\\nspheres.\\nFigure 8-2. \\nA 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 378, 'page_label': '379', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='dimensional (2D) subspace of the higher-dimensional (3D) space. If we\\nproject every training instance perpendicularly onto this subspace (as\\nrepresented by the short dashed lines connecting the instances to the plane),\\nwe get the new 2D dataset shown in \\nFigure 8-3\\n. Ta-da! We have just reduced\\nthe dataset’s dimensionality from 3D to 2D.\\n Note that the axes correspond to\\nnew features \\nz\\n and \\nz\\n: they are the coordinates of the projections on the\\nplane.\\nFigure 8-3. \\nThe new 2D dataset after projection\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 379, 'page_label': '380', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Manifold Learning\\nHowever, projection\\n is not always the best approach to dimensionality\\nreduction. \\nIn many cases the subspace may twist and turn, such as in the\\nfamous Swiss roll toy dataset represented in \\nFigure 8-4\\n.\\nFigure 8-4. \\nSwiss roll dataset\\nSimply projecting onto a plane (e.g., by dropping \\nx\\n) would squash different\\nlayers of the Swiss roll together, as shown on the left side of \\nFigure 8-5\\n.\\nWhat you probably want instead is to unroll the Swiss roll to obtain the 2D\\ndataset on the right side of \\nFigure 8-5\\n.\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 380, 'page_label': '381', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 8-5. \\nSquashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)\\nThe Swiss roll is an example of a 2D \\nmanifold\\n. Put simply, a 2D manifold is\\na 2D shape that can be bent and twisted in a higher-dimensional space. More\\ngenerally, a \\nd\\n-dimensional manifold is a part of an \\nn\\n-dimensional space\\n(where \\nd\\n < \\nn\\n) that locally resembles a \\nd\\n-dimensional hyperplane. In the case\\nof the Swiss roll, \\nd\\n = 2 and \\nn\\n = 3: it locally resembles a 2D plane, but it is\\nrolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold\\non which the training instances lie; this is called \\nmanifold learning\\n. It relies\\non the \\nmanifold assumption\\n, also called the \\nmanifold hypothesis\\n, which holds\\nthat most real-world high-dimensional datasets lie close to a much lower-\\ndimensional manifold. \\nThis assumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images\\nhave some similarities. They are made of connected lines, the borders are\\nwhite, and they are more or less centered. If you randomly generated images,\\nonly a ridiculously tiny fraction of them would look like handwritten digits.\\nIn other words, the degrees of freedom available to you if you try to create a\\ndigit image are dramatically lower than the degrees of freedom you have if\\nyou are allowed to generate any image you want. These constraints tend to\\nsqueeze the dataset into a lower-dimensional manifold.\\nThe manifold assumption is often accompanied by another implicit\\nassumption: that the task at hand (e.g., classification or regression) will be\\nsimpler if expressed in the lower-dimensional space of the manifold. For'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 381, 'page_label': '382', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='example, in the top row of \\nFigure 8-6\\n the Swiss roll is split into two classes:\\nin the 3D space (on the left) the decision boundary would be fairly complex,\\nbut in the 2D unrolled manifold space (on the right) the decision boundary is\\na straight line.\\nHowever, this implicit assumption does not always hold. For example, in the\\nbottom row of \\nFigure 8-6\\n, the decision boundary is located at \\nx\\n = 5. This\\ndecision boundary looks very simple in the original 3D space (a vertical\\nplane), but it looks more complex in the unrolled manifold (a collection of\\nfour independent line segments).\\nIn short, reducing the dimensionality of your training set before training a\\nmodel will usually speed up training, but it may not always lead to a better or\\nsimpler solution; it all depends on the dataset.\\nHopefully you now have a good sense of what the curse of dimensionality is\\nand how dimensionality reduction algorithms can fight it, especially when the\\nmanifold assumption holds. The rest of this chapter will go through some of\\nthe most popular algorithms for dimensionality reduction.\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 382, 'page_label': '383', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 8-6. \\nThe decision boundary may not always be simpler with lower dimensions'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 383, 'page_label': '384', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='PCA\\nPrincipal component analysis\\n (PCA) is by far the most popular\\ndimensionality reduction algorithm. \\nFirst it identifies the hyperplane that lies\\nclosest to the data, and then it projects the data onto it, just like in \\nFigure 8-2\\n.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 384, 'page_label': '385', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Preserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane,\\nyou first need to choose the right hyperplane. \\nFor example, a simple 2D\\ndataset is represented on the left in \\nFigure 8-7\\n, along with three different axes\\n(i.e., 1D hyperplanes). On the right is the result of the projection of the\\ndataset onto each of these axes. As you can see, the projection onto the solid\\nline preserves the maximum variance (top), while the projection onto the\\ndotted line preserves very little variance (bottom) and the projection onto the\\ndashed line preserves an intermediate amount of variance (middle).\\nFigure 8-7. \\nSelecting the subspace on which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of\\nvariance, as it will most likely lose less information than the other\\nprojections. Another way to justify this choice is that it is the axis that\\nminimizes the mean squared distance between the original dataset and its\\nprojection onto that axis. This is the rather simple idea behind \\nPCA\\n.\\n\\u2060\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 385, 'page_label': '386', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the\\ntraining set. In \\nFigure 8-7\\n, it is the solid line. It also finds a second axis,\\northogonal to the first one, that accounts for the largest amount of the\\nremaining variance. In this 2D example there is no choice: it is the dotted\\nline. If it were a higher-dimensional dataset, PCA would also find a third\\naxis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as\\nmany axes as the number of dimensions in the dataset.\\nThe \\ni\\n axis is called the \\ni\\n \\nprincipal component\\n (PC) of the data. \\nIn \\nFigure 8-\\n7\\n, the first PC is the axis on which vector \\nc\\n lies, and the second PC is the\\naxis on which vector \\nc\\n lies. In \\nFigure 8-2\\n the first two PCs are on the\\nprojection plane, and the third PC is the axis orthogonal to that plane. After\\nthe projection, in \\nFigure 8-3\\n, the first PC corresponds to the \\nz\\n axis, and the\\nsecond PC corresponds to the \\nz\\n axis.\\nNOTE\\nFor each principal component, PCA finds a zero-centered unit vector pointing in the\\ndirection of the PC. Since two opposing unit vectors lie on the same axis, the direction of\\nthe unit vectors returned by PCA is not stable: if you perturb the training set slightly and\\nrun PCA again, the unit vectors may point in the opposite direction as the original vectors.\\nHowever, they will generally still lie on the same axes. In some cases, a pair of unit\\nvectors may even rotate or swap (if the variances along these two axes are very close), but\\nthe plane they define will generally remain the same.\\nSo how can you find the principal components of a training set? \\nLuckily,\\nthere is a standard matrix factorization technique called \\nsingular value\\ndecomposition\\n (SVD) that can decompose the training set matrix \\nX\\n into the\\nmatrix multiplication of three matrices \\nU\\n \\nΣ\\n \\nV\\n, where \\nV\\n contains the unit\\nvectors that define all the principal components that you are looking for, as\\nshown in \\nEquation 8-1\\n.\\nEquation 8-1. \\nPrincipal components matrix\\nth\\nth\\n1\\n2\\n1\\n2\\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 386, 'page_label': '387', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='V \\n= \\n∣\\n \\n∣\\n \\n∣\\n \\nc 1 \\nc 2 \\n⋯\\n \\nc n \\n∣\\n \\n∣\\n \\n∣\\nThe following Python code uses NumPy’s \\nsvd()\\n function to obtain all the\\nprincipal components of the 3D training set represented in \\nFigure 8-2\\n, then it\\nextracts the two unit vectors that define the first two PCs:\\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\nX\\n \\n=\\n \\n[\\n...\\n]\\n  \\n# create a small 3D dataset\\nX_centered\\n \\n=\\n \\nX\\n \\n-\\n \\nX\\n.\\nmean\\n(\\naxis\\n=\\n0\\n)\\nU\\n,\\n \\ns\\n,\\n \\nVt\\n \\n=\\n \\nnp\\n.\\nlinalg\\n.\\nsvd\\n(\\nX_centered\\n)\\nc1\\n \\n=\\n \\nVt\\n[\\n0\\n]\\nc2\\n \\n=\\n \\nVt\\n[\\n1\\n]\\nWARNING\\nPCA assumes that the dataset is centered around the origin. As you will see, Scikit-Learn’s\\nPCA classes take care of centering the data for you. If you implement PCA yourself (as in\\nthe preceding example), or if you use other libraries, don’t forget to center the data first.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 387, 'page_label': '388', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Projecting Down to d Dimensions\\nOnce\\n you have identified all the principal components, you can reduce the\\ndimensionality of the dataset down to \\nd\\n dimensions by projecting it onto the\\nhyperplane defined by the first \\nd\\n principal components. Selecting this\\nhyperplane ensures that the projection will preserve as much variance as\\npossible. For example, in \\nFigure 8-2\\n the 3D dataset is projected down to the\\n2D plane defined by the first two principal \\ncomponents\\n, preserving a large\\npart of the dataset’s variance. As a result, the 2D projection looks very much\\nlike the original 3D dataset.\\nTo project the training set onto the hyperplane and obtain a reduced dataset\\nX\\n of dimensionality \\nd\\n, compute the matrix multiplication of the training\\nset matrix \\nX\\n by the matrix \\nW\\n, defined as the matrix containing the first \\nd\\ncolumns of \\nV\\n, as shown in \\nEquation 8-2\\n.\\nEquation 8-2. \\nProjecting the training set down to \\nd\\n dimensions\\nX d-proj \\n= \\nX \\nW d\\nThe following Python code projects the training set onto the plane defined by\\nthe first two principal components:\\nW2\\n \\n=\\n \\nVt\\n[:\\n2\\n]\\n.\\nT\\nX2D\\n \\n=\\n \\nX_centered\\n \\n@\\n \\nW2\\nThere you have it! You now know how to reduce the dimensionality of any\\ndataset by projecting it down to any number of dimensions, while preserving\\nas much variance as possible.\\nd\\n-proj\\nd'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 388, 'page_label': '389', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Using Scikit-Learn\\nScikit-Learn’s \\nPCA\\n class uses SVD to implement PCA, just like we did\\nearlier in this chapter. \\nThe following code applies PCA to reduce the\\ndimensionality of the dataset down to two dimensions (note that it\\nautomatically takes care of centering the data):\\nfrom\\n \\nsklearn.decomposition\\n \\nimport\\n \\nPCA\\npca\\n \\n=\\n \\nPCA\\n(\\nn_components\\n=\\n2\\n)\\nX2D\\n \\n=\\n \\npca\\n.\\nfit_transform\\n(\\nX\\n)\\nAfter fitting the \\nPCA\\n transformer to the dataset, its \\ncomponents_\\n attribute\\nholds the transpose of \\nW\\n: it contains one row for each of the first \\nd\\n principal\\ncomponents.\\nd'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 389, 'page_label': '390', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Explained Variance Ratio\\nAnother useful piece of information is the \\nexplained variance ratio\\n of each\\nprincipal component, available via the \\nexplained_variance_ratio_\\n variable.\\nThe ratio indicates the proportion of the dataset’s variance that lies along\\neach principal component. For example, let’s look at the explained variance\\nratios of the first two components of the 3D dataset represented in \\nFigure 8-2\\n:\\n>>> \\npca\\n.\\nexplained_variance_ratio_\\narray([0.7578477 , 0.15186921])\\nThis output tells us that about 76% of the dataset’s variance lies along the\\nfirst PC, and about 15% lies along the second PC. This leaves about 9% for\\nthe third PC, so it is reasonable to assume that the third PC probably carries\\nlittle information.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 390, 'page_label': '391', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"Choosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to,\\nit is simpler to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance—say, 95% (An exception to this rule, of course,\\nis if \\nyou are reducing dimensionality for data visualization, in which case you\\nwill want to reduce the dimensionality down to 2 or 3).\\nThe following code loads and splits the MNIST dataset (introduced in\\nChapter 3\\n) and performs PCA without reducing dimensionality, then\\ncomputes the minimum number of dimensions required to preserve 95% of\\nthe training set’s variance:\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nfetch_openml\\nmnist\\n \\n=\\n \\nfetch_openml\\n(\\n'mnist_784'\\n,\\n \\nas_frame\\n=\\nFalse\\n)\\nX_train\\n,\\n \\ny_train\\n \\n=\\n \\nmnist\\n.\\ndata\\n[:\\n60_000\\n],\\n \\nmnist\\n.\\ntarget\\n[:\\n60_000\\n]\\nX_test\\n,\\n \\ny_test\\n \\n=\\n \\nmnist\\n.\\ndata\\n[\\n60_000\\n:],\\n \\nmnist\\n.\\ntarget\\n[\\n60_000\\n:]\\npca\\n \\n=\\n \\nPCA\\n()\\npca\\n.\\nfit\\n(\\nX_train\\n)\\ncumsum\\n \\n=\\n \\nnp\\n.\\ncumsum\\n(\\npca\\n.\\nexplained_variance_ratio_\\n)\\nd\\n \\n=\\n \\nnp\\n.\\nargmax\\n(\\ncumsum\\n \\n>=\\n \\n0.95\\n)\\n \\n+\\n \\n1\\n  \\n# d equals 154\\nYou could then set \\nn_components=d\\n and run PCA again, but there’s a better\\noption. Instead of specifying the number of principal components you want to\\npreserve, you can set \\nn_components\\n to be a float between 0.0 and 1.0,\\nindicating the ratio of variance you wish to preserve:\\npca\\n \\n=\\n \\nPCA\\n(\\nn_components\\n=\\n0.95\\n)\\nX_reduced\\n \\n=\\n \\npca\\n.\\nfit_transform\\n(\\nX_train\\n)\\nThe actual number of components is determined during training, and it is\\nstored in the \\nn_components_\\n attribute:\\n>>> \\npca\\n.\\nn_components_\\n154\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 391, 'page_label': '392', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Yet another option is to plot the explained variance as a function of the\\nnumber of dimensions (simply plot \\ncumsum\\n; see \\nFigure 8-8\\n). There will\\nusually be an elbow in the curve, where the explained variance stops growing\\nfast. In this case, you can see that reducing the dimensionality down to about\\n100 dimensions wouldn’t lose too much explained variance.\\nFigure 8-8. \\nExplained variance as a function of the number of dimensions\\nLastly, if you are using dimensionality reduction as a preprocessing step for a\\nsupervised learning task (e.g., classification), then you can tune the number\\nof dimensions as you would any other hyperparameter (see \\nChapter 2\\n). \\nFor\\nexample, the following code example creates a two-step pipeline, first\\nreducing dimensionality using PCA, then classifying using a random forest.\\nNext, it uses \\nRandomizedSearchCV\\n to find a good combination of\\nhyperparameters for both PCA and the random forest classifier. This example\\ndoes a quick search, tuning only 2 hyperparameters, training on just 1,000\\ninstances, and running for just 10 iterations, but feel free to do a more\\nthorough search if you have the time:\\nfrom\\n \\nsklearn.ensemble\\n \\nimport\\n \\nRandomForestClassifier'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 392, 'page_label': '393', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='from\\n \\nsklearn.model_selection\\n \\nimport\\n \\nRandomizedSearchCV\\nfrom\\n \\nsklearn.pipeline\\n \\nimport\\n \\nmake_pipeline\\nclf\\n \\n=\\n \\nmake_pipeline\\n(\\nPCA\\n(\\nrandom_state\\n=\\n42\\n),\\n                    \\nRandomForestClassifier\\n(\\nrandom_state\\n=\\n42\\n))\\nparam_distrib\\n \\n=\\n \\n{\\n    \\n\"pca__n_components\"\\n:\\n \\nnp\\n.\\narange\\n(\\n10\\n,\\n \\n80\\n),\\n    \\n\"randomforestclassifier__n_estimators\"\\n:\\n \\nnp\\n.\\narange\\n(\\n50\\n,\\n \\n500\\n)\\n}\\nrnd_search\\n \\n=\\n \\nRandomizedSearchCV\\n(\\nclf\\n,\\n \\nparam_distrib\\n,\\n \\nn_iter\\n=\\n10\\n,\\n \\ncv\\n=\\n3\\n,\\n                                \\nrandom_state\\n=\\n42\\n)\\nrnd_search\\n.\\nfit\\n(\\nX_train\\n[:\\n1000\\n],\\n \\ny_train\\n[:\\n1000\\n])\\nLet’s look at the best hyperparameters found:\\n>>> \\nprint\\n(\\nrnd_search\\n.\\nbest_params_\\n)\\n{\\'randomforestclassifier__n_estimators\\': 465, \\'pca__n_components\\': 23}\\nIt’s interesting to note how low the optimal number of components is: we\\nreduced a 784-dimensional dataset to just 23 dimensions! This is tied to the\\nfact that we used a random forest, which is a pretty powerful model. If we\\nused a linear model instead, such as an \\nSGDClassifier\\n, the search would find\\nthat we need to preserve more dimensions (about 70).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 393, 'page_label': '394', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='PCA for Compression\\nAfter dimensionality reduction, the training set takes up much less space. \\nFor\\nexample, after applying PCA to the MNIST dataset while preserving 95% of\\nits variance, we are left with 154 features, instead of the original 784 features.\\nSo the dataset is now less than 20% of its original size, and we only lost 5%\\nof its variance! This is a reasonable compression ratio, and it’s easy to see\\nhow such a size reduction would speed up a classification algorithm\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions\\nby applying the inverse transformation of the PCA projection. This won’t\\ngive you back the original data, since the projection lost a bit of information\\n(within the 5% variance that was dropped), but it will likely be close to the\\noriginal data. \\nThe mean squared distance between the original data and the\\nreconstructed data (compressed and then decompressed) is called the\\nreconstruction error\\n.\\nThe \\ninverse_transform()\\n method lets us decompress the reduced MNIST\\ndataset back to 784 dimensions:\\nX_recovered\\n \\n=\\n \\npca\\n.\\ninverse_transform\\n(\\nX_reduced\\n)\\nFigure 8-9\\n shows a few digits from the original training set (on the left), and\\nthe corresponding digits after compression and decompression. You can see\\nthat there is a slight image quality loss, but the digits are still mostly intact.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 394, 'page_label': '395', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 8-9. \\nMNIST compression that preserves 95% of the variance\\nThe equation for the inverse transformation is shown in \\nEquation 8-3\\n.\\nEquation 8-3. \\nPCA inverse transformation, back to the original number of dimensions\\nX recovered \\n= \\nX d-proj \\nW d \\n⊺'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 395, 'page_label': '396', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Randomized PCA\\nIf you set the \\nsvd_solver\\n hyperparameter to \\n\"randomized\"\\n, Scikit-Learn uses\\na stochastic algorithm called \\nrandomized PCA\\n that quickly finds an\\napproximation of the first \\nd\\n principal components. \\nIts computational\\ncomplexity is \\nO\\n(\\nm\\n × \\nd\\n) + \\nO\\n(\\nd\\n), instead of \\nO\\n(\\nm\\n × \\nn\\n) + \\nO\\n(\\nn\\n) for the full\\nSVD approach, so it is dramatically faster than full SVD when \\nd\\n is much\\nsmaller than \\nn\\n:\\nrnd_pca\\n \\n=\\n \\nPCA\\n(\\nn_components\\n=\\n154\\n,\\n \\nsvd_solver\\n=\\n\"randomized\"\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nX_reduced\\n \\n=\\n \\nrnd_pca\\n.\\nfit_transform\\n(\\nX_train\\n)\\nTIP\\nBy default, \\nsvd_solver\\n is actually set to \\n\"auto\"\\n: Scikit-Learn automatically uses the\\nrandomized PCA algorithm if max(\\nm\\n, \\nn\\n) > 500 and \\nn_components\\n is an integer smaller\\nthan 80% of min(\\nm\\n, \\nn\\n), or else it uses the full SVD approach. So the preceding code would\\nuse the randomized PCA algorithm even if you removed the \\nsvd_solver=\"randomized\"\\nargument, since 154 < 0.8 × 784. If you want to force Scikit-Learn to use full SVD for a\\nslightly more precise result, you can set the \\nsvd_solver\\n hyperparameter to \\n\"full\"\\n.\\n2\\n3\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 396, 'page_label': '397', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Incremental PCA\\nOne problem with the preceding implementations of PCA is that they require\\nthe whole training set to fit in memory in order for the algorithm to run.\\nFortunately, \\nincremental PCA\\n (IPCA) algorithms have been developed that\\nallow you to split the training set into mini-batches and feed these in one\\nmini-batch at a time. This is useful for large training sets and for applying\\nPCA online (i.e., on the fly, as new instances arrive).\\nThe following code splits the MNIST training set into 100 mini-batches\\n(using NumPy’s \\narray_split()\\n function) and feeds them to Scikit-Learn’s\\nIncrementalPCA\\n class\\n\\u2060\\n to reduce the dimensionality of the MNIST dataset\\ndown to 154 dimensions, just like before. Note that you must call the\\npartial_fit()\\n method with each mini-batch, rather than the \\nfit()\\n method with\\nthe whole training set:\\nfrom\\n \\nsklearn.decomposition\\n \\nimport\\n \\nIncrementalPCA\\nn_batches\\n \\n=\\n \\n100\\ninc_pca\\n \\n=\\n \\nIncrementalPCA\\n(\\nn_components\\n=\\n154\\n)\\nfor\\n \\nX_batch\\n \\nin\\n \\nnp\\n.\\narray_split\\n(\\nX_train\\n,\\n \\nn_batches\\n):\\n    \\ninc_pca\\n.\\npartial_fit\\n(\\nX_batch\\n)\\nX_reduced\\n \\n=\\n \\ninc_pca\\n.\\ntransform\\n(\\nX_train\\n)\\nAlternatively, you can use NumPy’s \\nmemmap\\n class, which allows you to\\nmanipulate a large array stored in a binary file on disk as if it were entirely in\\nmemory; the class loads only the data it needs in memory, when it needs it.\\nTo demonstrate this, let’s first create a memory-mapped (memmap) file and\\ncopy the MNIST training set to it, then call \\nflush()\\n to ensure that any data still\\nin the cache gets saved to disk. In real life, \\nX_train\\n would typically not fit in\\nmemory, so you would load it chunk by chunk and save each chunk to the\\nright part of the memmap array:\\nfilename\\n \\n=\\n \\n\"my_mnist.mmap\"\\nX_mmap\\n \\n=\\n \\nnp\\n.\\nmemmap\\n(\\nfilename\\n,\\n \\ndtype\\n=\\n\\'float32\\'\\n,\\n \\nmode\\n=\\n\\'write\\'\\n,\\n \\nshape\\n=\\nX_train\\n.\\nshape\\n)\\nX_mmap\\n[:]\\n \\n=\\n \\nX_train\\n  \\n# could be a loop instead, saving the data chunk by chunk\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 397, 'page_label': '398', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='X_mmap\\n.\\nflush\\n()\\nNext, we can load the memmap file and use it like a regular NumPy array.\\nLet’s use the \\nIncrementalPCA\\n class to reduce its dimensionality. Since this\\nalgorithm uses only a small part of the array at any given time, memory usage\\nremains under control. This makes it possible to call the usual \\nfit()\\n method\\ninstead of \\npartial_fit()\\n, which is quite convenient:\\nX_mmap\\n \\n=\\n \\nnp\\n.\\nmemmap\\n(\\nfilename\\n,\\n \\ndtype\\n=\\n\"float32\"\\n,\\n \\nmode\\n=\\n\"readonly\"\\n)\\n.\\nreshape\\n(\\n-\\n1\\n,\\n \\n784\\n)\\nbatch_size\\n \\n=\\n \\nX_mmap\\n.\\nshape\\n[\\n0\\n]\\n \\n//\\n \\nn_batches\\ninc_pca\\n \\n=\\n \\nIncrementalPCA\\n(\\nn_components\\n=\\n154\\n,\\n \\nbatch_size\\n=\\nbatch_size\\n)\\ninc_pca\\n.\\nfit\\n(\\nX_mmap\\n)\\nWARNING\\nOnly the raw binary data is saved to disk, so you need to specify the data type and shape\\nof the array when you load it. If you omit the shape, \\nnp.memmap()\\n returns a 1D array.\\nFor very high-dimensional datasets, PCA can be too slow. As you saw\\nearlier, even if you use randomized PCA its computational complexity is still\\nO\\n(\\nm\\n × \\nd\\n) + \\nO\\n(\\nd\\n), so the target number of dimensions \\nd\\n must not be too\\nlarge. If you are dealing with a dataset with tens of thousands of features or\\nmore (e.g., images), then training may become much too slow: in this case,\\nyou should consider using random projection instead.\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 398, 'page_label': '399', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Random Projection\\nAs its name suggests, the random projection algorithm projects the data to a\\nlower-dimensional space using a random linear projection. \\nThis may sound\\ncrazy, but it turns out that such a random projection is actually very likely to\\npreserve distances fairly well, as was demonstrated mathematically by\\nWilliam B. Johnson and Joram Lindenstrauss in a famous lemma. So, two\\nsimilar instances will remain similar after the projection, and two very\\ndifferent instances will remain very different.\\nObviously, the more dimensions you drop, the more information is lost, and\\nthe more distances get distorted. So how can you choose the optimal number\\nof dimensions? Well, Johnson and Lindenstrauss came up with an equation\\nthat determines the minimum number of dimensions to preserve in order to\\nensure—with high probability—that distances won’t change by more than a\\ngiven tolerance. For example, if you have a dataset containing \\nm\\n = 5,000\\ninstances with \\nn\\n = 20,000 features each, and you don’t want the squared\\ndistance between any two instances to change by more than \\nε\\n = 10%,\\n then\\nyou should project the data down to \\nd\\n dimensions, with \\nd\\n ≥ 4 log(\\nm\\n) / (½ \\nε\\n² -\\n⅓ \\nε\\n³), which is 7,300 dimensions. That’s quite a significant dimensionality\\nreduction! Notice that the equation does not use \\nn\\n, it only relies on \\nm\\n and \\nε\\n.\\nThis equation is implemented by the \\njohnson_lindenstrauss_min_dim()\\nfunction:\\n>>> \\nfrom\\n \\nsklearn.random_projection\\n \\nimport\\n \\njohnson_lindenstrauss_min_dim\\n>>> \\nm\\n,\\n \\nε\\n \\n=\\n \\n5_000\\n,\\n \\n0.1\\n>>> \\nd\\n \\n=\\n \\njohnson_lindenstrauss_min_dim\\n(\\nm\\n,\\n \\neps\\n=\\nε\\n)\\n>>> \\nd\\n7300\\nNow we can just generate a random matrix \\nP\\n of shape [\\nd\\n, \\nn\\n], where each item\\nis sampled randomly from a Gaussian distribution with mean 0 and variance\\n1 / \\nd\\n, and use it to project a dataset from \\nn\\n dimensions down to \\nd\\n:\\nn\\n \\n=\\n \\n20_000\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 399, 'page_label': '400', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='np\\n.\\nrandom\\n.\\nseed\\n(\\n42\\n)\\nP\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\nd\\n,\\n \\nn\\n)\\n \\n/\\n \\nnp\\n.\\nsqrt\\n(\\nd\\n)\\n  \\n# std dev = square root of variance\\nX\\n \\n=\\n \\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\nm\\n,\\n \\nn\\n)\\n  \\n# generate a fake dataset\\nX_reduced\\n \\n=\\n \\nX\\n \\n@\\n \\nP\\n.\\nT\\nThat’s all there is to it! It’s simple and efficient, and no training is required:\\nthe only thing the algorithm needs to create the random matrix is the dataset’s\\nshape. The data itself is not used at all.\\nScikit-Learn offers a \\nGaussianRandomProjection\\n class to do exactly what we\\njust did: when you call its \\nfit()\\n method, it uses\\njohnson_lindenstrauss_min_dim()\\n to determine the output dimensionality,\\nthen it generates a random matrix, which it stores in the \\ncomponents_\\nattribute. \\nThen when you call \\ntransform()\\n, it uses this matrix to perform the\\nprojection. When creating the transformer, you can set \\neps\\n if you want to\\ntweak \\nε\\n (it defaults to 0.1), and \\nn_components\\n if you want to force a specific\\ntarget dimensionality \\nd\\n. The following code example gives the same result as\\nthe preceding code (you can also verify that \\ngaussian_rnd_proj.components_\\nis equal to \\nP\\n):\\nfrom\\n \\nsklearn.random_projection\\n \\nimport\\n \\nGaussianRandomProjection\\ngaussian_rnd_proj\\n \\n=\\n \\nGaussianRandomProjection\\n(\\neps\\n=\\nε\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nX_reduced\\n \\n=\\n \\ngaussian_rnd_proj\\n.\\nfit_transform\\n(\\nX\\n)\\n  \\n# same result as above\\nScikit-Learn also provides a second random projection transformer, known as\\nSparseRandomProjection\\n.\\n \\nIt determines the target dimensionality in the same\\nway, generates a random matrix of the same shape, and performs the\\nprojection identically. The main difference is that the random matrix is\\nsparse. This means it uses much less memory: about 25 MB instead of almost\\n1.2 GB in the preceding example! And it’s also much faster, both to generate\\nthe random matrix and to reduce dimensionality: about 50% faster in this\\ncase. Moreover, if the input is sparse, the transformation keeps it sparse\\n(unless you set \\ndense_output=True\\n). Lastly, it enjoys the same distance-\\npreserving property as the previous approach, and the quality of the\\ndimensionality reduction is comparable. In short, it’s usually preferable to'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 400, 'page_label': '401', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='use this transformer instead of the first one, especially for large or sparse\\ndatasets.\\nThe ratio \\nr\\n of nonzero items in the sparse random matrix is called its \\ndensity\\n.\\nBy default, it is equal to 1/n. With 20,000 features, this means that only 1 in\\n~141 cells in the random matrix is nonzero: that’s quite sparse! You can set\\nthe \\ndensity\\n hyperparameter to another value if you prefer. Each cell in the\\nsparse random matrix has a probability \\nr\\n of being nonzero, and each nonzero\\nvalue is either –\\nv\\n or +\\nv\\n (both equally likely), where \\nv\\n = 1/dr.\\nIf you want to perform the inverse transform, you first need to compute the\\npseudo-inverse of the components matrix using SciPy’s \\npinv()\\n function, then\\nmultiply the reduced data by the transpose of the pseudo-inverse:\\ncomponents_pinv\\n \\n=\\n \\nnp\\n.\\nlinalg\\n.\\npinv\\n(\\ngaussian_rnd_proj\\n.\\ncomponents_\\n)\\nX_recovered\\n \\n=\\n \\nX_reduced\\n \\n@\\n \\ncomponents_pinv\\n.\\nT\\nWARNING\\nComputing the pseudo-inverse may take a very long time if the components matrix is\\nlarge, as the computational complexity of \\npinv()\\n is \\nO\\n(\\ndn\\n²) if \\nd\\n < \\nn\\n, or \\nO\\n(\\nnd\\n²) otherwise.\\nIn summary, random projection is a simple, fast, memory-efficient, and\\nsurprisingly powerful dimensionality reduction algorithm that you should\\nkeep in mind, especially when you deal with high-dimensional datasets.\\nNOTE\\nRandom projection is not always used to reduce the dimensionality of large datasets. For\\nexample, a \\n2017 paper\\n\\u2060\\n by Sanjoy Dasgupta et al. showed that the brain of a fruit fly\\nimplements an analog of random projection to map dense low-dimensional olfactory\\ninputs to sparse high-dimensional binary outputs: for each odor, only a small fraction of\\nthe output neurons get activated, but similar odors activate many of the same neurons.\\nThis is similar to a well-known algorithm called \\nlocality sensitive hashing\\n (LSH), which is\\ntypically used in search engines to group similar documents.\\n7'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 401, 'page_label': '402', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='LLE\\nLocally linear embedding\\n (LLE)\\n\\u2060\\n is a \\nnonlinear dimensionality reduction\\n(NLDR) technique. \\nIt is a manifold learning technique that does not rely on\\nprojections, unlike PCA and random projection. In a nutshell, LLE works by\\nfirst measuring how each training instance linearly relates to its nearest\\nneighbors, and then looking for a low-dimensional representation of the\\ntraining set where these local relationships are best preserved (more details\\nshortly). This approach makes it particularly good at unrolling twisted\\nmanifolds, especially when there is not too much noise.\\nThe following code makes a Swiss roll, then uses Scikit-Learn’s\\nLocallyLinearEmbedding\\n class to unroll it:\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nmake_swiss_roll\\nfrom\\n \\nsklearn.manifold\\n \\nimport\\n \\nLocallyLinearEmbedding\\nX_swiss\\n,\\n \\nt\\n \\n=\\n \\nmake_swiss_roll\\n(\\nn_samples\\n=\\n1000\\n,\\n \\nnoise\\n=\\n0.2\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nlle\\n \\n=\\n \\nLocallyLinearEmbedding\\n(\\nn_components\\n=\\n2\\n,\\n \\nn_neighbors\\n=\\n10\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nX_unrolled\\n \\n=\\n \\nlle\\n.\\nfit_transform\\n(\\nX_swiss\\n)\\nThe variable \\nt\\n is a 1D NumPy array containing the position of each instance\\nalong the rolled axis of the Swiss roll. We don’t use it in this example, but it\\ncan be used as a target for a nonlinear regression task.\\nThe resulting 2D dataset is shown in \\nFigure 8-10\\n. As you can see, the Swiss\\nroll is completely unrolled, and the distances between instances are locally\\nwell preserved. However, distances are not preserved on a larger scale: the\\nunrolled Swiss roll should be a rectangle, not this kind of stretched and\\ntwisted band. Nevertheless, LLE did a pretty good job of modeling the\\nmanifold.\\n8'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 402, 'page_label': '403', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 8-10. \\nUnrolled Swiss roll using LLE\\nHere’s how LLE works: for each training instance \\nx\\n, the algorithm\\nidentifies its \\nk\\n-nearest neighbors (in the preceding code \\nk\\n = 10), then tries to\\nreconstruct \\nx\\n as a linear function of these neighbors. More specifically, it\\ntries to find the weights \\nw\\n such that the squared distance between \\nx\\n and \\n∑\\nj=1 m \\nw i,j \\nx (j) \\nis as small as possible, assuming \\nw\\n = 0 if \\nx\\n is not one of\\nthe \\nk\\n-nearest neighbors of \\nx\\n. Thus the first step of LLE is the constrained\\noptimization problem described in \\nEquation 8-4\\n, where \\nW\\n is the weight\\nmatrix containing all the weights \\nw\\n. The second constraint simply\\nnormalizes the weights for each training instance \\nx\\n.\\nEquation 8-4. \\nLLE step 1: linearly modeling local relationships\\nW ^ \\n= \\nargmin W \\n∑ i=1 m \\nx (i) -∑ j=1 m w i,j x (j) \\n2 \\nsubject \\nto \\nw i,j \\n= \\n0 \\nif \\nx\\n(j) \\nis \\nnot \\none \\nof \\nthe \\nk \\nn.n. \\nof \\nx (i) \\n∑ j=1 m \\nw i,j \\n= \\n1 \\nfor \\ni \\n= \\n1 \\n, \\n2 \\n, \\n⋯\\n \\n, \\nm\\nAfter this step, the weight matrix \\nW ^ \\n(containing the weights w^i,j) encodes\\nthe local linear relationships between the training instances. The second step\\nis to map the training instances into a \\nd\\n-dimensional space (where \\nd\\n < \\nn\\n)\\nwhile preserving these local relationships as much as possible. If \\nz\\n is the\\nimage of \\nx\\n in this \\nd\\n-dimensional space, then we want the squared distance\\nbetween \\nz\\n and \\n∑ j=1 m \\nw ^ i,j \\nz (j) \\nto be as small as possible. This idea\\n(\\ni\\n)\\n(\\ni\\n)\\ni,j\\n(\\ni\\n)\\ni,j\\n(\\nj\\n)\\n(\\ni\\n)\\ni,j\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 403, 'page_label': '404', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='leads to the unconstrained optimization problem described in \\nEquation 8-5\\n. It\\nlooks very similar to the first step, but instead of keeping the instances fixed\\nand finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the\\nlow-dimensional space. Note that \\nZ\\n is the matrix containing all \\nz\\n.\\nEquation 8-5. \\nLLE step 2: reducing dimensionality while preserving relationships\\nZ ^ \\n= \\nargmin Z \\n∑ i=1 m \\nz (i) -∑ j=1 m w ^ i,j z (j) \\n2\\nScikit-Learn’s LLE implementation has the following computational\\ncomplexity: \\nO\\n(\\nm\\n log(\\nm\\n)\\nn\\n log(\\nk\\n))\\n for finding the \\nk\\n-nearest neighbors, \\nO\\n(\\nmnk\\n)\\nfor optimizing the weights, and \\nO\\n(\\ndm\\n) for constructing the low-dimensional\\nrepresentations. Unfortunately, the \\nm\\n in the last term makes this algorithm\\nscale poorly to very large datasets.\\nAs you can see, LLE is quite different from the projection techniques, and it’s\\nsignificantly more complex, but it can also construct much better low-\\ndimensional representations, especially if the data is nonlinear.\\n(\\ni\\n)\\n3\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 404, 'page_label': '405', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Other Dimensionality Reduction Techniques\\nBefore we conclude this chapter, \\nlet’s take a quick look at a few other\\npopular dimensionality reduction techniques available in Scikit-Learn:\\nsklearn.manifold.MDS\\nMultidimensional scaling\\n (MDS) reduces dimensionality while trying to\\npreserve the distances between the instances. \\nRandom projection does\\nthat for high-dimensional data, but it doesn’t work well on low-\\ndimensional data.\\nsklearn.manifold.Isomap\\nIsomap\\n creates a graph by connecting each instance to its nearest\\nneighbors, then reduces dimensionality while trying to preserve the\\ngeodesic distances\\n between the instances. \\nThe geodesic distance between\\ntwo nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.\\nsklearn.manifold.TSNE\\nt-distributed stochastic neighbor embedding\\n (t-SNE) reduces\\ndimensionality while trying to keep similar instances close and dissimilar\\ninstances apart. \\nIt is mostly used for visualization, in particular to\\nvisualize clusters of instances in high-dimensional space. For example, in\\nthe exercises at the end of this chapter you will use t-SNE to visualize a\\n2D map of the MNIST images.\\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis\\nLinear discriminant analysis\\n (LDA) is a linear classification algorithm\\nthat, during training, learns the most discriminative axes between the\\nclasses. \\nThese axes can then be used to define a hyperplane onto which to\\nproject the data. The benefit of this approach is that the projection will\\nkeep classes as far apart as possible, so LDA is a good technique to'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 405, 'page_label': '406', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='reduce dimensionality before running another classification algorithm\\n(unless LDA alone is sufficient).\\nFigure 8-11\\n shows the results of MDS, Isomap, and t-SNE on the Swiss roll.\\nMDS manages to flatten the Swiss roll without losing its global curvature,\\nwhile Isomap drops it entirely. Depending on the downstream task,\\npreserving the large-scale structure may be good or bad. t-SNE does a\\nreasonable job of flattening the Swiss roll, preserving a bit of curvature, and\\nit also amplifies clusters, tearing the roll apart.\\n Again, this might be good or\\nbad, depending on the downstream task.\\nFigure 8-11. \\nUsing various techniques to reduce the Swiss roll to 2D'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 406, 'page_label': '407', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nWhat are the main motivations for reducing a dataset’s dimensionality?\\nWhat are the main drawbacks?\\n2\\n. \\nWhat is the curse of dimensionality?\\n3\\n. \\nOnce a dataset’s dimensionality has been reduced, is it possible to\\nreverse the operation? If so, how? If not, why?\\n4\\n. \\nCan PCA be used to reduce the dimensionality of a highly nonlinear\\ndataset?\\n5\\n. \\nSuppose you perform PCA on a 1,000-dimensional dataset, setting the\\nexplained variance ratio to 95%. How many dimensions will the\\nresulting dataset have?\\n6\\n. \\nIn what cases would you use regular PCA, incremental PCA,\\nrandomized PCA, or random projection?\\n7\\n. \\nHow can you evaluate the performance of a dimensionality reduction\\nalgorithm on your dataset?\\n8\\n. \\nDoes it make any sense to chain two different dimensionality reduction\\nalgorithms?\\n9\\n. \\nLoad the MNIST dataset (introduced in \\nChapter 3\\n) and split it into a\\ntraining set and a test set (take the first 60,000 instances for training, and\\nthe remaining 10,000 for testing). Train a random forest classifier on the\\ndataset and time how long it takes, then evaluate the resulting model on\\nthe test set. Next, use PCA to reduce the dataset’s dimensionality, with\\nan explained variance ratio of 95%. Train a new random forest classifier\\non the reduced dataset and see how long it takes. Was training much\\nfaster? Next, evaluate the classifier on the test set. How does it compare\\nto the previous classifier? Try again with an \\nSGDClassifier\\n. How much\\ndoes PCA help now?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 407, 'page_label': '408', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='10\\n. \\nUse t-SNE to reduce the first 5,000 images of the MNIST dataset down\\nto 2 dimensions and plot the result using Matplotlib. You can use a\\nscatterplot using 10 different colors to represent each image’s target\\nclass. Alternatively, you can replace each dot in the scatterplot with the\\ncorresponding instance’s class (a digit from 0 to 9), or even plot scaled-\\ndown versions of the digit images themselves (if you plot all digits the\\nvisualization will be too cluttered, so you should either draw a random\\nsample or plot an instance only if no other instance has already been\\nplotted at a close distance). You should get a nice visualization with\\nwell-separated clusters of digits. Try using other dimensionality\\nreduction algorithms, such as PCA, LLE, or MDS, and compare the\\nresulting visualizations.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n Well, four dimensions if you count time, and a few more if you are a string theorist.\\n2\\n Watch a rotating tesseract projected into 3D space at \\nhttps://homl.info/30\\n. Image by Wikipedia\\nuser NerdBoy1392 (\\nCreative Commons BY-SA 3.0\\n). Reproduced from\\nhttps://en.wikipedia.org/wiki/Tesseract\\n.\\n3\\n Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much\\nsugar they put in their coffee), if you consider enough dimensions.\\n4\\n Karl Pearson, “On Lines and Planes of Closest Fit to Systems of Points in Space”, \\nThe London,\\nEdinburgh, and Dublin Philosophical Magazine and Journal of Science\\n 2, no. 11 (1901): 559–\\n572.\\n5\\n Scikit-Learn uses the \\nalgorithm\\n described in David A. Ross et al., “Incremental Learning for\\nRobust Visual Tracking”, \\nInternational Journal of Computer Vision\\n 77, no. 1–3 (2008): 125–\\n141.\\n6\\n \\nε\\n is the Greek letter epsilon, often used for tiny values.\\n7\\n Sanjoy Dasgupta et al., “A neural algorithm for a fundamental computing problem”, \\nScience\\n358, no. 6364 (2017): 793–796.\\n8\\n Sam T. Roweis and Lawrence K. Saul, “Nonlinear Dimensionality Reduction by Locally Linear\\nEmbedding”, \\nScience\\n 290, no. 5500 (2000): 2323–2326.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 408, 'page_label': '409', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 9. \\nUnsupervised Learning\\nTechniques\\nAlthough most of the applications of machine learning today are based on\\nsupervised learning (and as a result, this is where most of the investments go\\nto), the vast majority of the available data is unlabeled: we have the input\\nfeatures \\nX\\n, but we do not have the labels \\ny\\n. \\nThe computer scientist Yann\\nLeCun famously said that “if intelligence was a cake, unsupervised learning\\nwould be the cake, supervised learning would be the icing on the cake, and\\nreinforcement learning would be the cherry on the cake.” In other words,\\nthere is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nSay you want to create a system that will take a few pictures of each item on\\na manufacturing production line and detect which items are defective. You\\ncan fairly easily create a system that will take pictures automatically, and this\\nmight give you thousands of pictures every day. You can then build a\\nreasonably large dataset in just a few weeks. But wait, there are no labels! If\\nyou want to train a regular binary classifier that will predict whether an item\\nis defective or not, you will need to label every single picture as “defective”\\nor “normal”. This will generally require human experts to sit down and\\nmanually go through all the pictures. This is a long, costly, and tedious task,\\nso it will usually only be done on a small subset of the available pictures. As\\na result, the labeled dataset will be quite small, and the classifier’s\\nperformance will be disappointing. Moreover, every time the company makes\\nany change to its products, the whole process will need to be started over\\nfrom scratch. Wouldn’t it be great if the algorithm could just exploit the\\nunlabeled data without needing humans to label every picture? Enter\\nunsupervised learning.\\nIn \\nChapter 8\\n we looked at the most common unsupervised learning task:\\ndimensionality reduction. In this chapter we will look at a few more'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 409, 'page_label': '410', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='unsupervised tasks:\\nClustering\\nThe goal is to group similar instances together into \\nclusters\\n. \\nClustering is\\na great tool for data analysis, customer segmentation, recommender\\nsystems, search engines, image segmentation, semi-supervised learning,\\ndimensionality reduction, and more.\\nAnomaly detection (also called \\noutlier detection\\n)\\nThe objective is to learn what “normal” data looks like, and then use that\\nto detect abnormal instances. \\nThese instances are called \\nanomalies\\n, or\\noutliers\\n, while the normal instances are called \\ninliers\\n. Anomaly detection\\nis useful in a wide variety of applications, such as fraud detection,\\ndetecting defective products in manufacturing, identifying new trends in\\ntime series, or removing outliers from a dataset before training another\\nmodel, which can significantly improve the performance of the resulting\\nmodel.\\nDensity estimation\\nThis is the task of estimating the \\nprobability density function\\n (PDF) of the\\nrandom process that generated the dataset. \\nDensity estimation is\\ncommonly used for anomaly detection: instances located in very low-\\ndensity regions are likely to be anomalies. It is also useful for data\\nanalysis and visualization.\\nReady for some cake? We will start with two clustering algorithms, \\nk\\n-means\\nand DBSCAN, then we’ll discuss Gaussian mixture models and see how they\\ncan be used for density estimation, clustering, and anomaly detection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 410, 'page_label': '411', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Clustering Algorithms: k-means and DBSCAN\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have\\nnever seen before. You look around and you notice a few more. They are not\\nidentical, yet they are sufficiently similar for you to know that they most\\nlikely belong to the same species (or at least the same genus). You may need\\na botanist to tell you what species that is, but you certainly don’t need an\\nexpert to identify groups of similar-looking objects. This is called \\nclustering\\n:\\nit is the task of identifying similar instances and assigning them to \\nclusters\\n, or\\ngroups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However,\\nunlike classification, clustering is an unsupervised task. Consider \\nFigure 9-1\\n:\\non the left is the iris dataset (introduced in \\nChapter 4\\n), where each instance’s\\nspecies (i.e., its class) is represented with a different marker. It is a labeled\\ndataset, for which classification algorithms such as logistic regression,\\nSVMs, or random forest classifiers are well suited. On the right is the same\\ndataset, but without the labels, so you cannot use a classification algorithm\\nanymore. This is where clustering algorithms step in: many of them can\\neasily detect the lower-left cluster. It is also quite easy to see with our own\\neyes, but it is not so obvious that the upper-right cluster is composed of two\\ndistinct subclusters. That said, the dataset has two additional features (sepal\\nlength and width) that are not represented here, and clustering algorithms can\\nmake good use of all features, so in fact they identify the three clusters fairly\\nwell (e.g., using a Gaussian mixture model, only 5 instances out of 150 are\\nassigned to the wrong cluster).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 411, 'page_label': '412', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-1. \\nClassification (left) versus clustering (right)\\nClustering is used in a wide variety of applications, including:\\nCustomer segmentation\\nYou can cluster your customers based on their purchases and their\\nactivity on your website. \\nThis is useful to understand who your customers\\nare and what they need, so you can adapt your products and marketing\\ncampaigns to each segment. For example, customer segmentation can be\\nuseful in \\nrecommender systems\\n to suggest content that other users in the\\nsame cluster enjoyed.\\nData analysis\\nWhen you analyze a new dataset, it can be helpful to run a clustering\\nalgorithm, and then analyze each cluster separately.\\nDimensionality reduction\\nOnce a dataset has been clustered, it is usually possible to measure each\\ninstance’s \\naffinity\\n with each cluster; \\naffinity is any measure of how well\\nan instance fits into a cluster. Each instance’s feature vector \\nx\\n can then be\\nreplaced with the vector of its cluster affinities. If there are \\nk\\n clusters,\\nthen this vector is \\nk\\n-dimensional. The new vector is typically much\\nlower-dimensional than the original feature vector, but it can preserve\\nenough information for further processing.\\nFeature engineering'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 412, 'page_label': '413', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='The cluster affinities can often be useful as extra features. \\nFor example,\\nwe used \\nk\\n-means in \\nChapter 2\\n to add geographic cluster affinity features\\nto the California housing dataset, and they helped us get better\\nperformance.\\nAnomaly detection\\n (also called \\noutlier detection\\n)\\nAny instance that has a low affinity to all the clusters is likely to be an\\nanomaly. \\nFor example, if you have clustered the users of your website\\nbased on their behavior, you can detect users with unusual behavior, such\\nas an unusual number of requests per second.\\nSemi-supervised learning\\nIf you only have a few labels, you could perform clustering and propagate\\nthe labels to all the instances in the same cluster. \\nThis technique can\\ngreatly increase the number of labels available for a subsequent\\nsupervised learning algorithm, and thus improve its performance.\\nSearch engines\\nSome search engines let you search for images that are similar to a\\nreference image. \\nTo build such a system, you would first apply a\\nclustering algorithm to all the images in your database; similar images\\nwould end up in the same cluster. Then when a user provides a reference\\nimage, all you’d need to do is use the trained clustering model to find this\\nimage’s cluster, and you could then simply return all the images from this\\ncluster.\\nImage segmentation\\nBy clustering pixels according to their color, then replacing each pixel’s\\ncolor with the mean color of its cluster, it is possible to considerably\\nreduce the number of different colors in an image. \\nImage segmentation is\\nused in many object detection and tracking systems, as it makes it easier\\nto detect the contour of each object.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 413, 'page_label': '414', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='There is no universal definition of what a cluster is: it really depends on the\\ncontext, and different algorithms will capture different kinds of clusters.\\nSome algorithms look for instances centered around a particular point, called\\na \\ncentroid\\n. Others look for continuous regions of densely packed instances:\\nthese clusters can take on any shape. Some algorithms are hierarchical,\\nlooking for clusters of clusters. And the list goes on.\\nIn this section, we will look at two popular clustering algorithms, \\nk\\n-means\\nand DBSCAN, and explore some of their applications, such as nonlinear\\ndimensionality reduction, semi-supervised learning, and anomaly detection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 414, 'page_label': '415', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content=\"k-means\\nConsider the unlabeled dataset represented in \\nFigure 9-2\\n: you can clearly see\\nfive blobs of instances. \\nThe \\nk\\n-means algorithm is a simple algorithm capable\\nof clustering this kind of dataset very quickly and efficiently, often in just a\\nfew iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957 as a\\ntechnique for pulse-code \\nmodulation,\\n but it was only \\npublished\\n outside of the\\ncompany in 1982.\\n\\u2060\\n In 1965, Edward W. Forgy had published virtually the\\nsame algorithm, so \\nk\\n-means is sometimes referred to as the Lloyd–Forgy\\nalgorithm.\\nFigure 9-2. \\nAn unlabeled dataset composed of five blobs of instances\\nLet’s train a \\nk\\n-means clusterer on this dataset. It will try to find each blob’s\\ncenter and assign each instance to the closest blob:\\nfrom\\n \\nsklearn.cluster\\n \\nimport\\n \\nKMeans\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nmake_blobs\\nX\\n,\\n \\ny\\n \\n=\\n \\nmake_blobs\\n([\\n...\\n])\\n  \\n# make the blobs: y contains the cluster IDs, but we\\n                          \\n# will not use them; that's what we want to predict\\nk\\n \\n=\\n \\n5\\nkmeans\\n \\n=\\n \\nKMeans\\n(\\nn_clusters\\n=\\nk\\n,\\n \\nrandom_state\\n=\\n42\\n)\\ny_pred\\n \\n=\\n \\nkmeans\\n.\\nfit_predict\\n(\\nX\\n)\\n1\"),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 415, 'page_label': '416', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Note that you have to specify the number of clusters \\nk\\n that the algorithm must\\nfind. In this example, it is pretty obvious from looking at the data that \\nk\\nshould be set to 5, but in general it is not that easy. We will discuss this\\nshortly.\\nEach instance will be assigned to one of the five clusters. \\nIn the context of\\nclustering, an instance’s \\nlabel\\n is the index of the cluster to which the\\nalgorithm assigns this instance; this is not to be confused with the class labels\\nin classification, which are used as targets (remember that clustering is an\\nunsupervised learning task). The \\nKMeans\\n instance preserves the predicted\\nlabels of the instances it was trained on, available via the \\nlabels_\\n instance\\nvariable:\\n>>> \\ny_pred\\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\\n>>> \\ny_pred\\n \\nis\\n \\nkmeans\\n.\\nlabels_\\nTrue\\nWe can also take a look at the five centroids that the algorithm found:\\n>>> \\nkmeans\\n.\\ncluster_centers_\\narray([[-2.80389616,  1.80117999],\\n       [ 0.20876306,  2.25551336],\\n       [-2.79290307,  2.79641063],\\n       [-1.46679593,  2.28585348],\\n       [-2.80037642,  1.30082566]])\\nYou can easily assign new instances to the cluster whose centroid is closest:\\n>>> \\nimport\\n \\nnumpy\\n \\nas\\n \\nnp\\n>>> \\nX_new\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n0\\n,\\n \\n2\\n],\\n \\n[\\n3\\n,\\n \\n2\\n],\\n \\n[\\n-\\n3\\n,\\n \\n3\\n],\\n \\n[\\n-\\n3\\n,\\n \\n2.5\\n]])\\n>>> \\nkmeans\\n.\\npredict\\n(\\nX_new\\n)\\narray([1, 1, 2, 2], dtype=int32)\\nIf you plot the cluster’s decision boundaries, you get a Voronoi tessellation:\\nsee \\nFigure 9-3\\n, where each centroid is represented with an X.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 416, 'page_label': '417', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-3. \\nk-means decision boundaries (Voronoi tessellation)\\nThe vast majority of the instances were clearly assigned to the appropriate\\ncluster, but a few instances were probably mislabeled, especially near the\\nboundary between the top-left cluster and the central cluster. Indeed, the \\nk\\n-\\nmeans algorithm does not behave very well when the blobs have very\\ndifferent diameters because all it cares about when assigning an instance to a\\ncluster is the distance to the centroid.\\nInstead of assigning each instance to a single cluster, which is called \\nhard\\nclustering\\n, it can be useful to give each instance a score per cluster, which is\\ncalled \\nsoft clustering\\n. \\nThe score can be the distance between the instance and\\nthe centroid or a similarity score (or affinity), such as the Gaussian radial\\nbasis function we used in \\nChapter 2\\n. In the \\nKMeans\\n class, the \\ntransform()\\nmethod measures the distance from each instance to every centroid:\\n>>> \\nkmeans\\n.\\ntransform\\n(\\nX_new\\n)\\n.\\nround\\n(\\n2\\n)\\narray([[2.81, 0.33, 2.9 , 1.49, 2.89],\\n       [5.81, 2.8 , 5.85, 4.48, 5.84],\\n       [1.21, 3.29, 0.29, 1.69, 1.71],\\n       [0.73, 3.22, 0.36, 1.55, 1.22]])\\nIn this example, the first instance in \\nX_new\\n is located at a distance of about\\n2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the\\nthird centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 417, 'page_label': '418', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='If you have a high-dimensional dataset and you transform it this way, you\\nend up with a \\nk\\n-dimensional dataset: this transformation can be a very\\nefficient nonlinear dimensionality reduction technique. Alternatively, you can\\nuse these distances as extra features to train another model, as in \\nChapter 2\\n.\\nThe k-means algorithm\\nSo, how does the algorithm work? Well, suppose you were given the\\ncentroids. \\nYou could easily label all the instances in the dataset by assigning\\neach of them to the cluster whose centroid is closest. Conversely, if you were\\ngiven all the instance labels, you could easily locate each cluster’s centroid\\nby computing the mean of the instances in that cluster. But you are given\\nneither the labels nor the centroids, so how can you proceed? Start by placing\\nthe centroids randomly (e.g., by picking \\nk\\n instances at random from the\\ndataset and using their locations as centroids). Then label the instances,\\nupdate the centroids, label the instances, update the centroids, and so on until\\nthe centroids stop moving. The algorithm is guaranteed to converge in a finite\\nnumber of steps (usually quite small). That’s because the mean squared\\ndistance between the instances and their closest centroids can only go down\\nat each step, and since it cannot be negative, it’s guaranteed to converge.\\nYou can see the algorithm in action in \\nFigure 9-4\\n: the centroids are initialized\\nrandomly (top left), then the instances are labeled (top right), then the\\ncentroids are updated (center left), the instances are relabeled (center right),\\nand so on. As you can see, in just three iterations the algorithm has reached a\\nclustering that seems close to optimal.\\nNOTE\\nThe computational \\ncomplexity of the algorithm is generally linear with regard to the\\nnumber of instances \\nm\\n, the number of clusters \\nk\\n, and the number of dimensions \\nn\\n.\\nHowever, this is only true when the data has a clustering structure. If it does not, then in\\nthe worst-case scenario the complexity can increase exponentially with the number of\\ninstances. In practice, this rarely happens, and \\nk\\n-means is generally one of the fastest\\nclustering algorithms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 418, 'page_label': '419', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-4. \\nThe \\nk\\n-means algorithm\\nAlthough the algorithm is guaranteed to converge, it may not converge to the\\nright solution (i.e., it may converge to a local optimum): whether it does or\\nnot depends on the centroid initialization. \\nFigure 9-5\\n shows two suboptimal\\nsolutions that the algorithm can converge to if you are not lucky with the\\nrandom initialization step.\\nFigure 9-5. \\nSuboptimal solutions due to unlucky centroid initializations'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 419, 'page_label': '420', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Let’s take a look at a few ways you can mitigate this risk by improving the\\ncentroid \\ninitialization\\n.\\nCentroid initialization methods\\nIf you happen to know approximately where the centroids should be (e.g., if\\nyou ran another clustering algorithm earlier), then you can set the \\ninit\\nhyperparameter to a NumPy array containing the list of centroids, and set\\nn_init\\n to \\n1\\n:\\ngood_init\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n-\\n3\\n,\\n \\n3\\n],\\n \\n[\\n-\\n3\\n,\\n \\n2\\n],\\n \\n[\\n-\\n3\\n,\\n \\n1\\n],\\n \\n[\\n-\\n1\\n,\\n \\n2\\n],\\n \\n[\\n0\\n,\\n \\n2\\n]])\\nkmeans\\n \\n=\\n \\nKMeans\\n(\\nn_clusters\\n=\\n5\\n,\\n \\ninit\\n=\\ngood_init\\n,\\n \\nn_init\\n=\\n1\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nkmeans\\n.\\nfit\\n(\\nX\\n)\\nAnother solution is to run the algorithm multiple times with different random\\ninitializations and keep the best solution. The number of random\\ninitializations is controlled by the \\nn_init\\n hyperparameter: by default it is equal\\nto \\n10\\n, which means that the whole algorithm described earlier runs 10 times\\nwhen you call \\nfit()\\n, and Scikit-Learn keeps the best solution. But how exactly\\ndoes it know which solution is the best? It uses a performance metric! \\nThat\\nmetric is called the model’s \\ninertia\\n, which is the sum of the squared distances\\nbetween the instances and their closest centroids. It is roughly equal to 219.4\\nfor the model on the left in \\nFigure 9-5\\n, 258.6 for the model on the right in\\nFigure 9-5\\n, and only 211.6 for the model in \\nFigure 9-3\\n. The \\nKMeans\\n class\\nruns the algorithm \\nn_init\\n times and keeps the model with the lowest inertia.\\nIn this example, the model in \\nFigure 9-3\\n will be selected (unless we are very\\nunlucky with \\nn_init\\n consecutive random initializations). If you are curious, a\\nmodel’s inertia is accessible via the \\ninertia_\\n instance variable:\\n>>> \\nkmeans\\n.\\ninertia_\\n211.59853725816836\\nThe \\nscore()\\n method returns the negative inertia (it’s negative because a\\npredictor’s \\nscore()\\n method must always respect Scikit-Learn’s “greater is\\nbetter” rule: if a predictor is better than another, its \\nscore()\\n method should\\nreturn a greater score):'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 420, 'page_label': '421', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\nkmeans\\n.\\nscore\\n(\\nX\\n)\\n-211.5985372581684\\nAn important improvement to the \\nk\\n-means algorithm, \\nk-means++\\n, was\\nproposed in a \\n2006 paper\\n by David Arthur and Sergei Vassilvitskii.\\n\\u2060\\n They\\nintroduced a smarter initialization step that tends to select centroids that are\\ndistant from one another, and this improvement makes the \\nk\\n-means algorithm\\nmuch less likely to converge to a suboptimal solution. The paper showed that\\nthe additional computation required for the smarter initialization step is well\\nworth it because it makes it possible to drastically reduce the number of times\\nthe algorithm needs to be run to find the optimal solution. \\nThe \\nk\\n-means++\\ninitialization algorithm works like this:\\n1\\n. \\nTake one centroid \\nc\\n, chosen uniformly at random from the dataset.\\n2\\n. \\nTake a new centroid \\nc\\n, choosing an instance \\nx\\n with probability\\nDx(i)2 / ∑j=1mDx(j)2, where D(\\nx\\n) is the distance between the\\ninstance \\nx\\n and the closest centroid that was already chosen. This\\nprobability distribution ensures that instances farther away from already\\nchosen centroids are much more likely to be selected as centroids.\\n3\\n. \\nRepeat the previous step until all \\nk\\n centroids have been chosen.\\nThe \\nKMeans\\n class uses this initialization method by default.\\nAccelerated k-means and mini-batch k-means\\nAnother improvement to the \\nk\\n-means algorithm was proposed in a \\n2003\\npaper\\n by Charles Elkan.\\n\\u2060\\n \\nOn some large datasets with many clusters, the\\nalgorithm can be accelerated by avoiding many unnecessary distance\\ncalculations. Elkan achieved this by exploiting the triangle inequality (i.e.,\\nthat a straight line is always the shortest distance between two points\\n\\u2060\\n) and\\nby keeping track of lower and upper bounds for distances between instances\\nand centroids. However, Elkan’s algorithm does not always accelerate\\ntraining, and sometimes it can even slow down training significantly; it\\ndepends on the dataset. Still, if you want to give it a try, set\\nalgorithm=\"elkan\"\\n.\\n2\\n(1)\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\ni\\n)\\n3\\n4'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 421, 'page_label': '422', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Yet another important variant of the \\nk\\n-means algorithm was proposed in a\\n2010 paper\\n by David Sculley.\\n\\u2060\\n \\nInstead of using the full dataset at each\\niteration, the algorithm is capable of using mini-batches, moving the\\ncentroids just slightly at each iteration. This speeds up the algorithm\\n(typically by a factor of three to four) and makes it possible to cluster huge\\ndatasets that do not fit in memory. Scikit-Learn implements this algorithm in\\nthe \\nMiniBatchKMeans\\n class, which you can use just like the \\nKMeans\\n class:\\nfrom\\n \\nsklearn.cluster\\n \\nimport\\n \\nMiniBatchKMeans\\nminibatch_kmeans\\n \\n=\\n \\nMiniBatchKMeans\\n(\\nn_clusters\\n=\\n5\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nminibatch_kmeans\\n.\\nfit\\n(\\nX\\n)\\nIf the dataset does not fit in memory, the simplest option is to use the\\nmemmap\\n class, as we did for incremental PCA in \\nChapter 8\\n. Alternatively,\\nyou can pass one mini-batch at a time to the \\npartial_fit()\\n method, but this will\\nrequire much more work, since you will need to perform multiple\\ninitializations and select the best one yourself.\\nAlthough the mini-batch \\nk\\n-means algorithm is much faster than the regular \\nk\\n-\\nmeans algorithm, its inertia is generally slightly worse. You can see this in\\nFigure 9-6\\n: the plot on the left compares the inertias of mini-batch \\nk\\n-means\\nand regular \\nk\\n-means models trained on the previous five-blobs dataset using\\nvarious numbers of clusters \\nk\\n. \\nThe difference between the two curves is\\nsmall, but visible. In the plot on the right, you can see that mini-batch \\nk\\n-\\nmeans is roughly 3.5 times faster than regular \\nk\\n-means on this dataset.\\n5'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 422, 'page_label': '423', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-6. \\nMini-batch \\nk\\n-means has a higher inertia than \\nk\\n-means (left) but it is much faster (right),\\nespecially as \\nk\\n increases\\nFinding the optimal number of clusters\\nSo far, we’ve set the number of clusters \\nk\\n to 5 because it was obvious by\\nlooking at the data that this was the correct number of clusters. \\nBut in\\ngeneral, it won’t be so easy to know how to set \\nk\\n, and the result might be\\nquite bad if you set it to the wrong value. As you can see in \\nFigure 9-7\\n, for\\nthis dataset setting \\nk\\n to 3 or 8 results in fairly bad models.\\nYou might be thinking that you could just pick the model with the lowest\\ninertia. Unfortunately, it is not that simple. The inertia for \\nk\\n=3 is about 653.2,\\nwhich is much higher than for \\nk\\n=5 (211.6). But with \\nk\\n=8, the inertia is just\\n119.1. The inertia is not a good performance metric when trying to choose \\nk\\nbecause it keeps getting lower as we increase \\nk\\n. Indeed, the more clusters\\nthere are, the closer each instance will be to its closest centroid, and therefore\\nthe lower the inertia will be. Let’s plot the inertia as a function of \\nk\\n. When we\\ndo this, the curve often contains an inflexion point called the \\nelbow\\n (see\\nFigure 9-8\\n).\\nFigure 9-7. \\nBad choices for the number of clusters: when k is too small, separate clusters get merged\\n(left), and when k is too large, some clusters get chopped into multiple pieces (right)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 423, 'page_label': '424', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-8. \\nPlotting the inertia as a function of the number of clusters \\nk\\nAs you can see, the inertia drops very quickly as we increase \\nk\\n up to 4, but\\nthen it decreases much more slowly as we keep increasing \\nk\\n. This curve has\\nroughly the shape of an arm, and there is an elbow at \\nk\\n = 4. So, if we did not\\nknow better, we might think 4 was a good choice: any lower value would be\\ndramatic, while any higher value would not help much, and we might just be\\nsplitting perfectly good clusters in half for no good reason.\\nThis technique for choosing the best value for the number of clusters is rather\\ncoarse. A more precise (but also more computationally expensive) approach\\nis to use the \\nsilhouette score\\n, which is the mean \\nsilhouette coefficient\\n over all\\nthe instances. \\nAn \\ninstance’s\\n silhouette coefficient is equal to \\n(\\nb\\n – \\na\\n) / max(\\na\\n,\\nb\\n), where \\na\\n is the mean distance to the other instances in the same cluster\\n(i.e., the mean intra-cluster distance) and \\nb\\n is the mean nearest-cluster\\ndistance (i.e., the mean distance to the instances of the next closest cluster,\\ndefined as the one that minimizes \\nb\\n, excluding the instance’s own cluster).\\nThe silhouette coefficient can vary between –1 and +1. A coefficient close to\\n+1 means that the instance is well inside its own cluster and far from other\\nclusters, while a coefficient close to 0 means that it is close to a cluster\\nboundary; finally, a coefficient close to –1 means that the instance may have\\nbeen assigned to the wrong cluster.\\nTo compute the silhouette score, you can use Scikit-Learn’s\\nsilhouette_score()\\n function, giving it all the instances in the dataset and the\\nlabels they were assigned:'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 424, 'page_label': '425', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\nfrom\\n \\nsklearn.metrics\\n \\nimport\\n \\nsilhouette_score\\n>>> \\nsilhouette_score\\n(\\nX\\n,\\n \\nkmeans\\n.\\nlabels_\\n)\\n0.655517642572828\\nLet’s compare the silhouette scores for different numbers of clusters (see\\nFigure 9-9\\n).\\nFigure 9-9. \\nSelecting the number of clusters \\nk\\n using the silhouette score\\nAs you can see, this visualization is much richer than the previous one:\\nalthough it confirms that \\nk\\n = 4 is a very good choice, it also highlights the\\nfact that \\nk\\n = 5 is quite good as well, and much better than \\nk\\n = 6 or 7. This was\\nnot visible when comparing inertias.\\nAn even more informative visualization is obtained when we plot every\\ninstance’s silhouette coefficient, sorted by the clusters they are assigned to\\nand by the value of the coefficient. \\nThis is called a \\nsilhouette diagram\\n (see\\nFigure 9-10\\n). Each diagram contains one knife shape per cluster. The shape’s\\nheight indicates the number of instances in the cluster, and its width\\nrepresents the sorted silhouette coefficients of the instances in the cluster\\n(wider is better).\\nThe vertical dashed lines represent the mean silhouette score for each number\\nof clusters. When most of the instances in a cluster have a lower coefficient\\nthan this score (i.e., if many of the instances stop short of the dashed line,\\nending to the left of it), then the cluster is rather bad since this means its\\ninstances are much too close to other clusters. Here we can see that when \\nk\\n =\\n3 or 6, we get bad clusters. But when \\nk\\n = 4 or 5, the clusters look pretty good:\\nmost instances extend beyond the dashed line, to the right and closer to 1.0.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 425, 'page_label': '426', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='When \\nk\\n = 4, the cluster at index 1 (the second from the bottom) is rather big.\\nWhen \\nk\\n = 5, all clusters have similar sizes. So, even though the overall\\nsilhouette score from \\nk\\n = 4 is slightly greater than for \\nk\\n = 5, it seems like a\\ngood idea to use \\nk\\n = 5 to get clusters of similar sizes.\\nFigure 9-10. \\nAnalyzing the silhouette diagrams for various values of \\nk'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 426, 'page_label': '427', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Limits of k-means\\nDespite its many merits, most notably being fast and scalable, \\nk\\n-means is not\\nperfect. \\nAs we saw, it is necessary to run the algorithm several times to avoid\\nsuboptimal solutions, plus you need to specify the number of clusters, which\\ncan be quite a hassle. Moreover, \\nk\\n-means does not behave very well when the\\nclusters have varying sizes, different densities, or nonspherical shapes. For\\nexample, \\nFigure 9-11\\n shows how \\nk\\n-means clusters a dataset containing three\\nellipsoidal clusters of different dimensions, densities, and orientations.\\nAs you can see, neither of these solutions is any good. The solution on the\\nleft is better, but it still chops off 25% of the middle cluster and assigns it to\\nthe cluster on the right. The solution on the right is just terrible, even though\\nits inertia is lower. So, depending on the data, different clustering algorithms\\nmay perform better. \\nOn these types of elliptical clusters, Gaussian mixture\\nmodels work great.\\nFigure 9-11. \\nk-means fails to cluster these ellipsoidal blobs properly\\nTIP\\nIt is important to scale the input features (see \\nChapter 2\\n) before you run \\nk\\n-means, or the\\nclusters may be very stretched and \\nk\\n-means will perform poorly. Scaling the features does\\nnot guarantee that all the clusters will be nice and spherical, but it generally helps \\nk\\n-means.\\nNow let’s look at a few ways we can benefit from clustering. We will use \\nk\\n-\\nmeans, but feel free to experiment with other clustering algorithms.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 427, 'page_label': '428', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Using Clustering for Image Segmentation\\nImage segmentation\\n is the task of partitioning an image into multiple\\nsegments. \\nThere are several variants:\\nIn \\ncolor segmentation\\n, pixels with a similar color get assigned to the\\nsame segment. \\nThis is sufficient in many applications. For example, if\\nyou want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nIn \\nsemantic segmentation\\n, all pixels that are part of the same object type\\nget assigned to the same segment. \\nFor example, in a self-driving car’s\\nvision system, all pixels that are part of a pedestrian’s image might be\\nassigned to the “pedestrian” segment (there would be one segment\\ncontaining all the pedestrians).\\nIn \\ninstance segmentation\\n, all pixels that are part of the same individual\\nobject are assigned to the same segment. \\nIn this case there would be a\\ndifferent segment for each pedestrian.\\nThe state of the art in semantic or instance segmentation today is achieved\\nusing complex architectures based on convolutional neural networks (see\\nChapter 14\\n). In this chapter we are going to focus on the (much simpler)\\ncolor segmentation task, using \\nk\\n-means.\\nWe’ll start by importing the Pillow package (successor to the Python Imaging\\nLibrary, PIL), which we’ll then use to load the \\nladybug.png\\n image (see the\\nupper-left image in \\nFigure 9-12\\n), assuming it’s located at \\nfilepath\\n:\\n>>> \\nimport\\n \\nPIL\\n>>> \\nimage\\n \\n=\\n \\nnp\\n.\\nasarray\\n(\\nPIL\\n.\\nImage\\n.\\nopen\\n(\\nfilepath\\n))\\n>>> \\nimage\\n.\\nshape\\n(533, 800, 3)\\nThe image is represented as a 3D array. The first dimension’s size is the\\nheight; the second is the width; and the third is the number of color channels,\\nin this case red, green, and blue (RGB). In other words, for each pixel there is'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 428, 'page_label': '429', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='a 3D vector containing the intensities of red, green, and blue as unsigned 8-\\nbit integers between 0 and 255. Some images may have fewer channels (such\\nas grayscale images, which only have one), and some images may have more\\nchannels (such as images with an additional \\nalpha channel\\n for transparency,\\nor satellite images, which often contain channels for additional light\\nfrequencies (like infrared).\\nThe following code reshapes the array to get a long list of RGB colors, then it\\nclusters these colors using \\nk\\n-means with eight clusters. It creates a\\nsegmented_img\\n array containing the nearest cluster center for each pixel (i.e.,\\nthe mean color of each pixel’s cluster), and lastly it reshapes this array to the\\noriginal image shape. The third line uses advanced NumPy indexing; for\\nexample, if the first 10 labels in \\nkmeans_.labels_\\n are equal to 1, then the first\\n10 colors in \\nsegmented_img\\n are equal to \\nkmeans.cluster_centers_[1]\\n:\\nX\\n \\n=\\n \\nimage\\n.\\nreshape\\n(\\n-\\n1\\n,\\n \\n3\\n)\\nkmeans\\n \\n=\\n \\nKMeans\\n(\\nn_clusters\\n=\\n8\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n.\\nfit\\n(\\nX\\n)\\nsegmented_img\\n \\n=\\n \\nkmeans\\n.\\ncluster_centers_\\n[\\nkmeans\\n.\\nlabels_\\n]\\nsegmented_img\\n \\n=\\n \\nsegmented_img\\n.\\nreshape\\n(\\nimage\\n.\\nshape\\n)\\nThis outputs the image shown in the upper right of \\nFigure 9-12\\n. You can\\nexperiment with various numbers of clusters, as shown in the figure. When\\nyou use fewer than eight clusters, notice that the ladybug’s flashy red color\\nfails to get a cluster of its own: it gets merged with colors from the\\nenvironment. This is because \\nk\\n-means prefers clusters of similar sizes. The\\nladybug is small—much smaller than the rest of the image—so even though\\nits color is flashy, \\nk\\n-means fails to dedicate a cluster to it.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 429, 'page_label': '430', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-12. \\nImage segmentation using \\nk\\n-means with various numbers of color clusters\\nThat wasn’t too hard, was it? Now let’s look at another application of\\nclustering.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 430, 'page_label': '431', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have\\nplenty of unlabeled instances and very few labeled instances. \\nIn this section,\\nwe’ll use the digits dataset, which is a simple MNIST-like dataset containing\\n1,797 grayscale 8 × 8 images representing the digits 0 to 9. First, let’s load\\nand split the dataset (it’s already shuffled):\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nload_digits\\nX_digits\\n,\\n \\ny_digits\\n \\n=\\n \\nload_digits\\n(\\nreturn_X_y\\n=\\nTrue\\n)\\nX_train\\n,\\n \\ny_train\\n \\n=\\n \\nX_digits\\n[:\\n1400\\n],\\n \\ny_digits\\n[:\\n1400\\n]\\nX_test\\n,\\n \\ny_test\\n \\n=\\n \\nX_digits\\n[\\n1400\\n:],\\n \\ny_digits\\n[\\n1400\\n:]\\nWe will pretend we only have labels for 50 instances. To get a baseline\\nperformance, let’s train a logistic regression model on these 50 labeled\\ninstances:\\nfrom\\n \\nsklearn.linear_model\\n \\nimport\\n \\nLogisticRegression\\nn_labeled\\n \\n=\\n \\n50\\nlog_reg\\n \\n=\\n \\nLogisticRegression\\n(\\nmax_iter\\n=\\n10_000\\n)\\nlog_reg\\n.\\nfit\\n(\\nX_train\\n[:\\nn_labeled\\n],\\n \\ny_train\\n[:\\nn_labeled\\n])\\nWe can then measure the accuracy of this model on the test set (note that the\\ntest set must be labeled):\\n>>> \\nlog_reg\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n)\\n0.7481108312342569\\nThe model’s accuracy is just 74.8%. That’s not great: indeed, if you try\\ntraining the model on the full training set, you will find that it will reach\\nabout 90.7% accuracy. Let’s see how we can do better. First, let’s cluster the\\ntraining set into 50 clusters. Then, for each cluster, we’ll find the image\\nclosest to the centroid. \\nWe’ll call these images the \\nrepresentative images\\n:\\nk\\n \\n=\\n \\n50'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 431, 'page_label': '432', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='kmeans\\n \\n=\\n \\nKMeans\\n(\\nn_clusters\\n=\\nk\\n,\\n \\nrandom_state\\n=\\n42\\n)\\nX_digits_dist\\n \\n=\\n \\nkmeans\\n.\\nfit_transform\\n(\\nX_train\\n)\\nrepresentative_digit_idx\\n \\n=\\n \\nnp\\n.\\nargmin\\n(\\nX_digits_dist\\n,\\n \\naxis\\n=\\n0\\n)\\nX_representative_digits\\n \\n=\\n \\nX_train\\n[\\nrepresentative_digit_idx\\n]\\nFigure 9-13\\n shows the 50 representative images.\\nFigure 9-13. \\nFifty representative digit images (one per cluster)\\nLet’s look at each image and manually label them:\\ny_representative_digits\\n \\n=\\n \\nnp\\n.\\narray\\n([\\n1\\n,\\n \\n3\\n,\\n \\n6\\n,\\n \\n0\\n,\\n \\n7\\n,\\n \\n9\\n,\\n \\n2\\n,\\n \\n...\\n,\\n \\n5\\n,\\n \\n1\\n,\\n \\n9\\n,\\n \\n9\\n,\\n \\n3\\n,\\n \\n7\\n])\\nNow we have a dataset with just 50 labeled instances, but instead of being\\nrandom instances, each of them is a representative image of its cluster. Let’s\\nsee if the performance is any better:\\n>>> \\nlog_reg\\n \\n=\\n \\nLogisticRegression\\n(\\nmax_iter\\n=\\n10_000\\n)\\n>>> \\nlog_reg\\n.\\nfit\\n(\\nX_representative_digits\\n,\\n \\ny_representative_digits\\n)\\n>>> \\nlog_reg\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n)\\n0.8488664987405542\\nWow! We jumped from 74.8% accuracy to 84.9%, although we are still only\\ntraining the model on 50 instances. Since it is often costly and painful to label\\ninstances, especially when it has to be done manually by experts, it is a good\\nidea to label representative instances rather than just random instances.\\nBut perhaps we can go one step further: what if we propagated the labels to\\nall the other instances in the same cluster? \\nThis is called \\nlabel propagation\\n:\\ny_train_propagated\\n \\n=\\n \\nnp\\n.\\nempty\\n(\\nlen\\n(\\nX_train\\n),\\n \\ndtype\\n=\\nnp\\n.\\nint64\\n)\\nfor\\n \\ni\\n \\nin\\n \\nrange\\n(\\nk\\n):\\n    \\ny_train_propagated\\n[\\nkmeans\\n.\\nlabels_\\n \\n==\\n \\ni\\n]\\n \\n=\\n \\ny_representative_digits\\n[\\ni\\n]'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 432, 'page_label': '433', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Now let’s train the model again and look at its performance:\\n>>> \\nlog_reg\\n \\n=\\n \\nLogisticRegression\\n()\\n>>> \\nlog_reg\\n.\\nfit\\n(\\nX_train\\n,\\n \\ny_train_propagated\\n)\\n>>> \\nlog_reg\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n)\\n0.8942065491183879\\nWe got another significant accuracy boost! Let’s see if we can do even better\\nby ignoring the 1% of instances that are farthest from their cluster center: this\\nshould eliminate some outliers. The following code first computes the\\ndistance from each instance to its closest cluster center, then for each cluster\\nit sets the 1% largest distances to –1. Lastly, it creates a set without these\\ninstances marked with a –1 distance:\\npercentile_closest\\n \\n=\\n \\n99\\nX_cluster_dist\\n \\n=\\n \\nX_digits_dist\\n[\\nnp\\n.\\narange\\n(\\nlen\\n(\\nX_train\\n)),\\n \\nkmeans\\n.\\nlabels_\\n]\\nfor\\n \\ni\\n \\nin\\n \\nrange\\n(\\nk\\n):\\n    \\nin_cluster\\n \\n=\\n \\n(\\nkmeans\\n.\\nlabels_\\n \\n==\\n \\ni\\n)\\n    \\ncluster_dist\\n \\n=\\n \\nX_cluster_dist\\n[\\nin_cluster\\n]\\n    \\ncutoff_distance\\n \\n=\\n \\nnp\\n.\\npercentile\\n(\\ncluster_dist\\n,\\n \\npercentile_closest\\n)\\n    \\nabove_cutoff\\n \\n=\\n \\n(\\nX_cluster_dist\\n \\n>\\n \\ncutoff_distance\\n)\\n    \\nX_cluster_dist\\n[\\nin_cluster\\n \\n&\\n \\nabove_cutoff\\n]\\n \\n=\\n \\n-\\n1\\npartially_propagated\\n \\n=\\n \\n(\\nX_cluster_dist\\n \\n!=\\n \\n-\\n1\\n)\\nX_train_partially_propagated\\n \\n=\\n \\nX_train\\n[\\npartially_propagated\\n]\\ny_train_partially_propagated\\n \\n=\\n \\ny_train_propagated\\n[\\npartially_propagated\\n]\\nNow let’s train the model again on this partially propagated dataset and see\\nwhat accuracy we get:\\n>>> \\nlog_reg\\n \\n=\\n \\nLogisticRegression\\n(\\nmax_iter\\n=\\n10_000\\n)\\n>>> \\nlog_reg\\n.\\nfit\\n(\\nX_train_partially_propagated\\n,\\n \\ny_train_partially_propagated\\n)\\n>>> \\nlog_reg\\n.\\nscore\\n(\\nX_test\\n,\\n \\ny_test\\n)\\n0.9093198992443325\\nNice! With just 50 labeled instances (only 5 examples per class on average!)\\nwe got 90.9% accuracy, which is actually slightly higher than the\\nperformance we got on the fully labeled digits dataset (90.7%). This is partly\\nthanks to the fact that we dropped some outliers, and partly because the'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 433, 'page_label': '434', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='propagated labels are actually pretty good—their accuracy is about 97.5%, as\\nthe following code shows:\\n>>> \\n(\\ny_train_partially_propagated\\n \\n==\\n \\ny_train\\n[\\npartially_propagated\\n])\\n.\\nmean\\n()\\n0.9755555555555555\\nTIP\\nScikit-Learn also offers two classes that can propagate labels automatically:\\nLabelSpreading\\n and \\nLabelPropagation\\n in the \\nsklearn.semi_supervised\\n package. Both\\nclasses construct a similarity matrix between all the instances, and iteratively propagate\\nlabels from labeled instances to similar unlabeled instances. \\nThere’s also a very different\\nclass called \\nSelfTrainingClassifier\\n in the same package: you give it a base classifier (such\\nas a \\nRandomForestClassifier\\n) and it trains it on the labeled instances, then uses it to\\npredict labels for the unlabeled samples. It then updates the training set with the labels it is\\nmost confident about, and repeats this process of training and labeling until it cannot add\\nlabels anymore. These techniques are not magic bullets, but they can occasionally give\\nyour model a little boost.\\nACTIVE LEARNING\\nTo continue improving your model and your training set, the next step\\ncould be to do a few rounds of \\nactive learning\\n, which is when a human\\nexpert interacts with the learning algorithm, providing labels for specific\\ninstances when the algorithm requests them. \\nThere are many different\\nstrategies for active learning, but one of the most common ones is called\\nuncertainty sampling\\n. Here is how it works:\\n1\\n. \\nThe model is trained on the labeled instances gathered so far, and\\nthis model is used to make predictions on all the unlabeled instances.\\n2\\n. \\nThe instances for which the model is most uncertain (i.e., where its\\nestimated probability is lowest) are given to the expert for labeling.\\n3\\n. \\nYou iterate this process until the performance improvement stops\\nbeing worth the labeling effort.\\nOther active learning strategies include labeling the instances that would'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 434, 'page_label': '435', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='result in the largest model change or the largest drop in the model’s\\nvalidation error, or the instances that different models disagree on (e.g.,\\nan SVM and a random forest).\\nBefore we move on to Gaussian mixture models, let’s take a look at\\nDBSCAN, another popular clustering algorithm that illustrates a very\\ndifferent approach based on local density estimation. This approach allows\\nthe algorithm to identify clusters of arbitrary shapes.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 435, 'page_label': '436', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='DBSCAN\\nThe \\ndensity-based spatial clustering of applications with noise\\n (DBSCAN)\\nalgorithm defines clusters as continuous regions of high density. Here is how\\nit works:\\nFor each instance, the algorithm counts how many instances are located\\nwithin a small distance ε (epsilon) from it. \\nThis region is called the\\ninstance’s \\nε-neighborhood\\n.\\nIf an instance has at least \\nmin_samples\\n instances in its ε-neighborhood\\n(including itself), then it is considered a \\ncore instance\\n. In other words,\\ncore instances are those that are located in dense regions.\\nAll instances in the neighborhood of a core instance belong to the same\\ncluster. This neighborhood may include other core instances; therefore, a\\nlong sequence of neighboring core instances forms a single cluster.\\nAny instance that is not a core instance and does not have one in its\\nneighborhood is considered an anomaly.\\nThis algorithm works well if all the clusters are well separated by low-density\\nregions. \\nThe \\nDBSCAN\\n class in Scikit-Learn is as simple to use as you might\\nexpect. Let’s test it on the moons dataset, introduced in \\nChapter 5\\n:\\nfrom\\n \\nsklearn.cluster\\n \\nimport\\n \\nDBSCAN\\nfrom\\n \\nsklearn.datasets\\n \\nimport\\n \\nmake_moons\\nX\\n,\\n \\ny\\n \\n=\\n \\nmake_moons\\n(\\nn_samples\\n=\\n1000\\n,\\n \\nnoise\\n=\\n0.05\\n)\\ndbscan\\n \\n=\\n \\nDBSCAN\\n(\\neps\\n=\\n0.05\\n,\\n \\nmin_samples\\n=\\n5\\n)\\ndbscan\\n.\\nfit\\n(\\nX\\n)\\nThe labels of all the instances are now available in the \\nlabels_\\n instance\\nvariable:\\n>>> \\ndbscan\\n.\\nlabels_\\narray([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5, [...], 3,  3,  4,  2,  6,  3])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 436, 'page_label': '437', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Notice that some instances have a cluster index equal to –1, which means that\\nthey are considered as anomalies by the algorithm. The indices of the core\\ninstances are available in the \\ncore_sample_indices_\\n instance variable, and the\\ncore instances themselves are available in the \\ncomponents_\\n instance variable:\\n>>> \\ndbscan\\n.\\ncore_sample_indices_\\narray([  0,   4,   5,   6,   7,   8,  10,  11, [...], 993, 995, 997, 998, 999])\\n>>> \\ndbscan\\n.\\ncomponents_\\narray([[-0.02137124,  0.40618608],\\n       [-0.84192557,  0.53058695],\\n       [...],\\n       [ 0.79419406,  0.60777171]])\\nThis clustering is represented in the lefthand plot of \\nFigure 9-14\\n. As you can\\nsee, it identified quite a lot of anomalies, plus seven different clusters. How\\ndisappointing! Fortunately, if we widen each instance’s neighborhood by\\nincreasing \\neps\\n to 0.2, we get the clustering on the right, which looks perfect.\\nLet’s continue with this model.\\nFigure 9-14. \\nDBSCAN clustering using two different neighborhood radiuses\\nSurprisingly, the \\nDBSCAN\\n class does not have a \\npredict()\\n method, although\\nit has a \\nfit_predict()\\n method. In other words, it cannot predict which cluster a\\nnew instance belongs to. This decision was made because different\\nclassification algorithms can be better for different tasks, so the authors\\ndecided to let the user choose which one to use. Moreover, it’s not hard to\\nimplement. For example, let’s train a \\nKNeighborsClassifier\\n:\\nfrom\\n \\nsklearn.neighbors\\n \\nimport\\n \\nKNeighborsClassifier'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 437, 'page_label': '438', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='knn\\n \\n=\\n \\nKNeighborsClassifier\\n(\\nn_neighbors\\n=\\n50\\n)\\nknn\\n.\\nfit\\n(\\ndbscan\\n.\\ncomponents_\\n,\\n \\ndbscan\\n.\\nlabels_\\n[\\ndbscan\\n.\\ncore_sample_indices_\\n])\\nNow, given a few new instances, we can predict which clusters they most\\nlikely belong to and even estimate a probability for each cluster:\\n>>> \\nX_new\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n-\\n0.5\\n,\\n \\n0\\n],\\n \\n[\\n0\\n,\\n \\n0.5\\n],\\n \\n[\\n1\\n,\\n \\n-\\n0.1\\n],\\n \\n[\\n2\\n,\\n \\n1\\n]])\\n>>> \\nknn\\n.\\npredict\\n(\\nX_new\\n)\\narray([1, 0, 1, 0])\\n>>> \\nknn\\n.\\npredict_proba\\n(\\nX_new\\n)\\narray([[0.18, 0.82],\\n       [1.  , 0.  ],\\n       [0.12, 0.88],\\n       [1.  , 0.  ]])\\nNote that we only trained the classifier on the core instances, but we could\\nalso have chosen to train it on all the instances, or all but the anomalies: this\\nchoice depends on the final task.\\nThe decision boundary is represented in \\nFigure 9-15\\n (the crosses represent\\nthe four instances in \\nX_new\\n). Notice that since there is no anomaly in the\\ntraining set, the classifier always chooses a cluster, even when that cluster is\\nfar away. It is fairly straightforward to introduce a maximum distance, in\\nwhich case the two instances that are far away from both clusters are\\nclassified as anomalies. To do this, use the \\nkneighbors()\\n method of the\\nKNeighborsClassifier\\n. Given a set of instances, it returns the distances and\\nthe indices of the \\nk\\n-nearest neighbors in the training set (two matrices, each\\nwith \\nk\\n columns):\\n>>> \\ny_dist\\n,\\n \\ny_pred_idx\\n \\n=\\n \\nknn\\n.\\nkneighbors\\n(\\nX_new\\n,\\n \\nn_neighbors\\n=\\n1\\n)\\n>>> \\ny_pred\\n \\n=\\n \\ndbscan\\n.\\nlabels_\\n[\\ndbscan\\n.\\ncore_sample_indices_\\n][\\ny_pred_idx\\n]\\n>>> \\ny_pred\\n[\\ny_dist\\n \\n>\\n \\n0.2\\n]\\n \\n=\\n \\n-\\n1\\n>>> \\ny_pred\\n.\\nravel\\n()\\narray([-1,  0,  1, -1])'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 438, 'page_label': '439', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-15. \\nDecision boundary between two clusters\\nIn short, DBSCAN is a very simple yet powerful algorithm capable of\\nidentifying any number of clusters of any shape. It is robust to outliers, and it\\nhas just two hyperparameters (\\neps\\n and \\nmin_samples\\n). If the density varies\\nsignificantly across the clusters, however, or if there’s no sufficiently low-\\ndensity region around some clusters, DBSCAN can struggle to capture all the\\nclusters properly.\\n Moreover, its computational complexity is roughly \\nO\\n(\\nm\\nn\\n),\\nso it does not scale well to large \\ndatasets.\\nTIP\\nYou may also want to try \\nhierarchical DBSCAN\\n (HDBSCAN), \\nwhich is implemented in\\nthe \\nscikit-learn-contrib\\n project\\n, as it is usually better than DBSCAN at finding clusters of\\nvarying densities.\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 439, 'page_label': '440', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Other Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should\\ntake a look at. I cannot cover them all in detail here, but here is a brief\\noverview:\\nAgglomerative clustering\\nA hierarchy of clusters is built from the bottom up. Think of many tiny\\nbubbles floating on water and gradually attaching to each other until\\nthere’s one big group of bubbles. \\nSimilarly, at each iteration,\\nagglomerative clustering connects the nearest pair of clusters (starting\\nwith individual instances). \\nIf you drew a tree with a branch for every pair\\nof clusters that merged, you would get a binary tree of clusters, where the\\nleaves are the individual instances. This approach can capture clusters of\\nvarious shapes; it also produces a flexible and informative cluster tree\\ninstead of forcing you to choose a particular cluster scale, and it can be\\nused with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix, which is a sparse \\nm\\n × \\nm\\nmatrix that indicates which pairs of instances are neighbors (e.g., returned\\nby \\nsklearn.neighbors.kneighbors_graph()\\n). Without a connectivity matrix,\\nthe algorithm does not scale well to large datasets.\\nBIRCH\\nThe balanced iterative reducing and clustering using hierarchies (BIRCH)\\nalgorithm was designed specifically for very large datasets, and it can be\\nfaster than batch \\nk\\n-means, with similar results, as long as the number of\\nfeatures is not too large (<20). \\nDuring training, it builds a tree structure\\ncontaining just enough information to quickly assign each new instance to\\na cluster, without having to store all the instances in the tree: this\\napproach allows it to use limited memory while handling huge datasets.\\nMean-shift\\nThis algorithm\\n starts by placing a circle centered on each instance; then'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 440, 'page_label': '441', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates\\nthis mean-shifting step until all the circles stop moving (i.e., until each of\\nthem is centered on the mean of the instances it contains). Mean-shift\\nshifts the circles in the direction of higher density, until each of them has\\nfound a local density maximum. Finally, all the instances whose circles\\nhave settled in the same place (or close enough) are assigned to the same\\ncluster. Mean-shift has some of the same features as DBSCAN, like how\\nit can find any number of clusters of any shape, it has very few\\nhyperparameters (just one—the radius of the circles, called the\\nbandwidth\\n), and it relies on local density estimation. But unlike\\nDBSCAN, mean-shift tends to chop clusters into pieces when they have\\ninternal density variations. Unfortunately, its computational complexity is\\nO\\n(\\nm\\nn\\n), so it is not suited for large datasets.\\nAffinity propagation\\nIn this algorithm, instances repeatedly exchange messages between one\\nanother until every instance has elected another instance (or itself) to\\nrepresent it. \\nThese elected instances are called \\nexemplars\\n. Each exemplar\\nand all the instances that elected it form one cluster. In real-life politics,\\nyou typically want to vote for a candidate whose opinions are similar to\\nyours, but you also want them to win the election, so you might choose a\\ncandidate you don’t fully agree with, but who is more popular. You\\ntypically evaluate popularity through polls. Affinity propagation works in\\na similar way, and it tends to choose exemplars located near the center of\\nclusters, similar to \\nk\\n-means. But unlike with \\nk\\n-means, you don’t have to\\npick a number of clusters ahead of time: it is determined during training.\\nMoreover, affinity propagation can deal nicely with clusters of different\\nsizes. Sadly, this algorithm has a computational complexity of \\nO\\n(\\nm\\n), so\\nit is not suited for large datasets.\\nSpectral clustering\\nThis algorithm\\n takes a similarity matrix between the instances and creates\\na low-dimensional embedding from it (i.e., it reduces the matrix’s\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 441, 'page_label': '442', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='dimensionality), then it uses another clustering algorithm in this low-\\ndimensional space (Scikit-Learn’s implementation uses \\nk\\n-means).\\nSpectral clustering can capture complex cluster structures, and it can also\\nbe used to cut graphs (e.g., to identify clusters of friends on a social\\nnetwork). It does not scale well to large numbers of instances, and it does\\nnot behave well when the clusters have very different sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density\\nestimation, clustering, and anomaly detection.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 442, 'page_label': '443', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Gaussian Mixtures\\nA \\nGaussian mixture model\\n (GMM) is a probabilistic model that assumes that\\nthe instances were generated from a mixture of several Gaussian distributions\\nwhose parameters are unknown. \\nAll the instances generated from a single\\nGaussian distribution form a cluster that typically looks like an ellipsoid.\\nEach cluster can have a different ellipsoidal shape, size, density, and\\norientation, just like in \\nFigure 9-11\\n. When you observe an instance, you know\\nit was generated from one of the Gaussian distributions, but you are not told\\nwhich one, and you do not know what the parameters of these distributions\\nare.\\nThere are several GMM variants. In the simplest variant, implemented in the\\nGaussianMixture\\n class, you must know in advance the number \\nk\\n of Gaussian\\ndistributions. \\nThe dataset \\nX\\n is assumed to have been generated through the\\nfollowing probabilistic process:\\nFor each instance, a cluster is picked randomly from among \\nk\\n clusters.\\nThe probability of choosing the \\nj\\n cluster is the cluster’s weight \\nϕ\\n.\\n\\u2060\\nThe index of the cluster chosen for the \\ni\\n instance is noted \\nz\\n.\\nIf the \\ni\\n instance was assigned to the \\nj\\n cluster (i.e., \\nz\\n = \\nj\\n), then the\\nlocation \\nx\\n of this instance is sampled randomly from the Gaussian\\ndistribution with mean \\nμ\\n and covariance matrix \\nΣ\\n. This is noted \\nx\\n~ \\n᷒\\n(\\nμ\\n, \\nΣ\\n).\\nSo what can you do with such a model? Well, given the dataset \\nX\\n, you\\ntypically want to start by estimating the weights \\nϕ\\n and all the distribution\\nparameters \\nμ\\n to \\nμ\\n and \\nΣ\\n to \\nΣ\\n. Scikit-Learn’s \\nGaussianMixture\\n class\\nmakes this super easy:\\nfrom\\n \\nsklearn.mixture\\n \\nimport\\n \\nGaussianMixture\\ngm\\n \\n=\\n \\nGaussianMixture\\n(\\nn_components\\n=\\n3\\n,\\n \\nn_init\\n=\\n10\\n)\\ngm\\n.\\nfit\\n(\\nX\\n)\\nth\\n(\\nj\\n)\\n6\\nth\\n(\\ni\\n)\\nth\\nth\\n(\\ni\\n)\\n(\\ni\\n)\\n(\\nj\\n)\\n(\\nj\\n)\\n(\\ni\\n)\\n(\\nj\\n)\\n(\\nj\\n)\\n(1)\\n(\\nk\\n)\\n(1)\\n(\\nk\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 443, 'page_label': '444', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Let’s look at the parameters that the algorithm estimated:\\n>>> \\ngm\\n.\\nweights_\\narray([0.39025715, 0.40007391, 0.20966893])\\n>>> \\ngm\\n.\\nmeans_\\narray([[ 0.05131611,  0.07521837],\\n       [-1.40763156,  1.42708225],\\n       [ 3.39893794,  1.05928897]])\\n>>> \\ngm\\n.\\ncovariances_\\narray([[[ 0.68799922,  0.79606357],\\n        [ 0.79606357,  1.21236106]],\\n       [[ 0.63479409,  0.72970799],\\n        [ 0.72970799,  1.1610351 ]],\\n       [[ 1.14833585, -0.03256179],\\n        [-0.03256179,  0.95490931]]])\\nGreat, it worked fine! Indeed, two of the three clusters were generated with\\n500 instances each, while the third cluster only contains 250 instances. So the\\ntrue cluster weights are 0.4, 0.4, and 0.2, respectively, and that’s roughly\\nwhat the algorithm found. Similarly, the true means and covariance matrices\\nare quite close to those found by the algorithm. But how? \\nThis class relies on\\nthe \\nexpectation-maximization\\n (EM) algorithm, which has many similarities\\nwith the \\nk\\n-means algorithm: it also initializes the cluster parameters\\nrandomly, then it repeats two steps until convergence, first assigning\\ninstances to clusters (this is called the \\nexpectation step\\n) and then updating the\\nclusters (this is called the \\nmaximization step\\n). Sounds familiar, right? In the\\ncontext of clustering, you can think of EM as a generalization of \\nk\\n-means that\\nnot only finds the cluster centers (\\nμ\\n to \\nμ\\n), but also their size, shape, and\\norientation (\\nΣ\\n to \\nΣ\\n), as well as their relative weights (\\nϕ\\n to \\nϕ\\n). Unlike\\nk\\n-means, though, EM uses soft cluster assignments, not hard assignments.\\nFor each instance, during the expectation step, the algorithm estimates the\\nprobability that it belongs to each cluster (based on the current cluster\\nparameters). \\nThen, during the maximization step, each cluster is updated\\nusing \\nall\\n the instances in the dataset, with each instance weighted by the\\nestimated probability that it belongs to that cluster. These probabilities are\\ncalled the \\nresponsibilities\\n of the clusters for the instances. \\nDuring\\n the\\n(1)\\n(\\nk\\n)\\n(1)\\n(\\nk\\n)\\n(1)\\n(\\nk\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 444, 'page_label': '445', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='maximization step, each cluster’s update will mostly be impacted by the\\ninstances it is most responsible for.\\nWARNING\\nUnfortunately, just like \\nk\\n-means, EM can end up converging to poor solutions, so it needs\\nto be run several times, keeping only the best solution. This is why we set \\nn_init\\n to 10. Be\\ncareful: by default \\nn_init\\n is set to 1.\\nYou can check whether or not the algorithm converged and how many\\niterations it took:\\n>>> \\ngm\\n.\\nconverged_\\nTrue\\n>>> \\ngm\\n.\\nn_iter_\\n4\\nNow that you have an estimate of the location, size, shape, orientation, and\\nrelative weight of each cluster, the model can easily assign each instance to\\nthe most likely cluster (hard clustering) or estimate the probability that it\\nbelongs to a particular cluster (soft clustering). Just use the \\npredict()\\n method\\nfor hard clustering, or the \\npredict_proba()\\n method for soft clustering:\\n>>> \\ngm\\n.\\npredict\\n(\\nX\\n)\\narray([0, 0, 1, ..., 2, 2, 2])\\n>>> \\ngm\\n.\\npredict_proba\\n(\\nX\\n)\\n.\\nround\\n(\\n3\\n)\\narray([[0.977, 0.   , 0.023],\\n       [0.983, 0.001, 0.016],\\n       [0.   , 1.   , 0.   ],\\n       ...,\\n       [0.   , 0.   , 1.   ],\\n       [0.   , 0.   , 1.   ],\\n       [0.   , 0.   , 1.   ]])\\nA Gaussian mixture model is a \\ngenerative model\\n, meaning you can sample\\nnew instances from it (note that they are ordered by cluster index):\\n>>> \\nX_new\\n,\\n \\ny_new\\n \\n=\\n \\ngm\\n.\\nsample\\n(\\n6\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 445, 'page_label': '446', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='>>> \\nX_new\\narray([[-0.86944074, -0.32767626],\\n       [ 0.29836051,  0.28297011],\\n       [-2.8014927 , -0.09047309],\\n       [ 3.98203732,  1.49951491],\\n       [ 3.81677148,  0.53095244],\\n       [ 2.84104923, -0.73858639]])\\n>>> \\ny_new\\narray([0, 0, 1, 2, 2, 2])\\nIt is also possible to estimate the density of the model at any given location.\\nThis is achieved using the \\nscore_samples()\\n method: for each instance it is\\ngiven, this method estimates the log of the \\nprobability density function\\n (PDF)\\nat that location. The greater the score, the higher the density:\\n>>> \\ngm\\n.\\nscore_samples\\n(\\nX\\n)\\n.\\nround\\n(\\n2\\n)\\narray([-2.61, -3.57, -3.33, ..., -3.51, -4.4 , -3.81])\\nIf you compute the exponential of these scores, you get the value of the PDF\\nat the location of the given instances. These are not probabilities, but\\nprobability \\ndensities\\n: they can take on any positive value, not just a value\\nbetween 0 and 1. To estimate the probability that an instance will fall within a\\nparticular region, you would have to integrate the PDF over that region (if\\nyou do so over the entire space of possible instance locations, the result will\\nbe 1).\\nFigure 9-16\\n shows the cluster means, the decision boundaries (dashed lines),\\nand the density contours of this model.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 446, 'page_label': '447', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-16. \\nCluster means, decision boundaries, and density contours of a trained Gaussian mixture\\nmodel\\nNice! The algorithm clearly found an excellent solution. Of course, we made\\nits task easy by generating the data using a set of 2D Gaussian distributions\\n(unfortunately, real-life data is not always so Gaussian and low-dimensional).\\nWe also gave the algorithm the correct number of clusters. When there are\\nmany dimensions, or many clusters, or few instances, EM can struggle to\\nconverge to the optimal solution. You might need to reduce the difficulty of\\nthe task by limiting the number of parameters that the algorithm has to learn.\\nOne way to do this is to limit the range of shapes and orientations that the\\nclusters can have. This can be achieved by imposing constraints on the\\ncovariance matrices. To do this, set the \\ncovariance_type\\n hyperparameter to\\none of the following values:\\n\"spherical\"\\nAll clusters must be spherical, but they can have different diameters (i.e.,\\ndifferent variances).\\n\"diag\"\\nClusters can take on any ellipsoidal shape of any size, but the ellipsoid’s\\naxes must be parallel to the coordinate axes (i.e., the covariance matrices\\nmust be diagonal).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 447, 'page_label': '448', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='\"tied\"\\nAll clusters must have the same ellipsoidal shape, size, and orientation\\n(i.e., all clusters share the same covariance matrix).\\nBy default, \\ncovariance_type\\n is equal to \\n\"full\"\\n, which means that each cluster\\ncan take on any shape, size, and orientation (it has its own unconstrained\\ncovariance matrix). \\nFigure 9-17\\n plots the solutions found by the EM\\nalgorithm when \\ncovariance_type\\n is set to \\n\"tied\"\\n or \\n\"spherical\"\\n.\\nFigure 9-17. \\nGaussian mixtures for tied clusters (left) and spherical clusters (right)\\nNOTE\\nThe computational complexity of training a \\nGaussianMixture\\n model depends on the\\nnumber of instances \\nm\\n, the number of dimensions \\nn\\n, the number of clusters \\nk\\n, and the\\nconstraints on the covariance matrices. \\nIf \\ncovariance_type\\n is \\n\"spherical\"\\n or \\n\"diag\"\\n, it is\\nO\\n(\\nkmn\\n), assuming the data has a clustering structure. If \\ncovariance_type\\n is \\n\"tied\"\\n or \\n\"full\"\\n,\\nit is \\nO\\n(\\nkmn\\n + \\nkn\\n), so it will not scale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. We’ll see\\nhow in the next section.\\n2\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 448, 'page_label': '449', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Using Gaussian Mixtures for Anomaly Detection\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any\\ninstance located in a low-density region can be considered an anomaly. \\nYou\\nmust define what density threshold you want to use. For example, in a\\nmanufacturing company that tries to detect defective products, the ratio of\\ndefective products is usually well known. Say it is equal to 2%. You then set\\nthe density threshold to be the value that results in having 2% of the instances\\nlocated in areas below that threshold density. If you notice that you get too\\nmany false positives (i.e., perfectly good products that are flagged as\\ndefective), you can lower the threshold. Conversely, if you have too many\\nfalse negatives (i.e., defective products that the system does not flag as\\ndefective), you can increase the threshold. This is the usual precision/recall\\ntrade-off (see \\nChapter 3\\n). Here is how you would identify the outliers using\\nthe fourth percentile lowest density as the threshold (i.e., approximately 4%\\nof the instances will be flagged as anomalies):\\ndensities\\n \\n=\\n \\ngm\\n.\\nscore_samples\\n(\\nX\\n)\\ndensity_threshold\\n \\n=\\n \\nnp\\n.\\npercentile\\n(\\ndensities\\n,\\n \\n2\\n)\\nanomalies\\n \\n=\\n \\nX\\n[\\ndensities\\n \\n<\\n \\ndensity_threshold\\n]\\nFigure 9-18\\n represents these anomalies as stars.\\nA closely related task is \\nnovelty detection\\n: it differs from anomaly detection\\nin that the algorithm is assumed to be trained on a “clean” dataset,\\nuncontaminated by outliers, whereas anomaly detection does not make this\\nassumption. \\nIndeed, outlier detection is often used to clean up a dataset.\\nTIP\\nGaussian mixture models try to fit all the data, including the outliers; if you have too many\\nof them this will bias the model’s view of “normality”, and some outliers may wrongly be\\nconsidered as normal. If this happens, you can try to fit the model once, use it to detect and\\nremove the most extreme outliers, then fit the model again on the cleaned-up dataset.\\nAnother approach is to use robust covariance estimation methods (see the \\nEllipticEnvelope\\nclass).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 449, 'page_label': '450', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-18. \\nAnomaly detection using a Gaussian mixture model\\nJust like \\nk\\n-means, the \\nGaussianMixture\\n algorithm requires you to specify the\\nnumber of clusters. So how can you find that number?'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 450, 'page_label': '451', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Selecting the Number of Clusters\\nWith \\nk\\n-means, you can use the inertia or the silhouette score to select the\\nappropriate number of clusters. \\nBut with Gaussian mixtures, it is not possible\\nto use these metrics because they are not reliable when the clusters are not\\nspherical or have different sizes. \\nInstead, you can try to find the model that\\nminimizes a \\ntheoretical information criterion\\n, such as the \\nBayesian\\ninformation criterion\\n (BIC) or the \\nAkaike information criterion\\n (AIC),\\ndefined in \\nEquation 9-1\\n.\\nEquation 9-1. \\nBayesian information criterion (BIC) and Akaike information criterion (AIC)\\nB \\nI \\nC \\n= \\nlog \\n( \\nm \\n) \\np \\n- \\n2 \\nlog \\n( \\nL ^ \\n) \\nA \\nI \\nC \\n= \\n2 \\np \\n- \\n2 \\nlog \\n( \\nL ^ \\n)\\nIn these equations:\\nm\\n is the number of instances, as always.\\np\\n is the number of parameters learned by the model.\\nL^ is the maximized value of the \\nlikelihood function\\n of the model.\\nBoth the BIC and the AIC penalize models that have more parameters to\\nlearn (e.g., more clusters) and reward models that fit the data well. They often\\nend up selecting the same model. When they differ, the model selected by the\\nBIC tends to be simpler (fewer parameters) than the one selected by the AIC,\\nbut tends to not fit the data quite as well (this is especially true for larger\\ndatasets).\\nLIKELIHOOD FUNCTION\\nThe terms “probability” and “likelihood” are often used interchangeably\\nin everyday language, but they have very different meanings in statistics.\\nGiven a statistical model with some parameters \\nθ\\n, the word “probability”\\nis used to describe how plausible a future outcome \\nx\\n is (knowing the\\nparameter values \\nθ\\n), while the word “likelihood” is used to describe how\\nplausible a particular set of parameter values \\nθ\\n are, after the outcome \\nx\\n is'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 451, 'page_label': '452', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='known.\\nConsider a 1D mixture model of two Gaussian distributions centered at –\\n4 and +1. For simplicity, this toy model has a single parameter \\nθ\\n that\\ncontrols the standard deviations of both distributions. The top-left contour\\nplot in \\nFigure 9-19\\n shows the entire model \\nf\\n(\\nx\\n; \\nθ\\n) as a function of both \\nx\\nand \\nθ\\n. To estimate the probability distribution of a future outcome \\nx\\n, you\\nneed to set the model parameter \\nθ\\n. For example, if you set \\nθ\\n to 1.3 (the\\nhorizontal line), you get the probability density function \\nf\\n(\\nx\\n; \\nθ\\n=1.3)\\nshown in the lower-left plot. Say you want to estimate the probability that\\nx\\n will fall between –2 and +2. You must calculate the integral of the PDF\\non this range (i.e., the surface of the shaded region). But what if you\\ndon’t know \\nθ\\n, and instead if you have observed a single instance \\nx\\n=2.5\\n(the vertical line in the upper-left plot)? In this case, you get the\\nlikelihood function \\nℒ\\n(\\nθ\\n|\\nx\\n=2.5)=f(\\nx\\n=2.5; \\nθ\\n), represented in the upper-right\\nplot.\\nFigure 9-19. \\nA model’s parametric function (top left), and some derived functions: a PDF (lower\\nleft), a likelihood function (top right), and a log likelihood function (lower right)\\nIn short, the PDF is a function of \\nx\\n (with \\nθ\\n fixed), while the likelihood\\nfunction is a function of \\nθ\\n (with \\nx\\n fixed). \\nIt is important to understand\\nthat the likelihood function is \\nnot\\n a probability distribution: if you'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 452, 'page_label': '453', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='integrate a probability distribution over all possible values of \\nx\\n, you\\nalways get 1, but if you integrate the likelihood function over all possible\\nvalues of \\nθ\\n the result can be any positive value.\\nGiven a dataset \\nX\\n, a common task is to try to estimate the most likely\\nvalues for the model parameters. To do this, you must find the values that\\nmaximize the likelihood function, given \\nX\\n. \\nIn this example, if you have\\nobserved a single instance \\nx\\n=2.5, the \\nmaximum likelihood estimate\\n(MLE) of \\nθ\\n is θ^=1.5. If a prior probability distribution \\ng\\n over \\nθ\\n exists, it\\nis possible to take it into account by maximizing \\nℒ\\n(\\nθ\\n|\\nx\\n)g(\\nθ\\n) rather than\\njust maximizing \\nℒ\\n(\\nθ\\n|\\nx\\n). \\nThis is called \\nmaximum a-posteriori\\n (MAP)\\nestimation. Since MAP constrains the parameter values, you can think of\\nit as a regularized version of MLE.\\nNotice that maximizing the likelihood function is equivalent to\\nmaximizing its logarithm (represented in the lower-right plot in \\nFigure 9-\\n19\\n). Indeed, the logarithm is a strictly increasing function, so if \\nθ\\nmaximizes the log likelihood, it also maximizes the likelihood. It turns\\nout that it is generally easier to maximize the log likelihood. For example,\\nif you observed several independent instances \\nx\\n to \\nx\\n, you would need\\nto find the value of \\nθ\\n that maximizes the product of the individual\\nlikelihood functions. But it is equivalent, and much simpler, to maximize\\nthe sum (not the product) of the log likelihood functions, thanks to the\\nmagic of the logarithm which converts products into sums: log(\\nab\\n) =\\nlog(\\na\\n) + log(\\nb\\n).\\nOnce you have estimated θ^, the value of \\nθ\\n that maximizes the likelihood\\nfunction, then you are ready to compute L^=L(θ^,X), which is the value\\nused to compute the AIC and BIC; you can think of it as a measure of\\nhow well the model fits the data.\\nTo compute the BIC and AIC, call the \\nbic()\\n and \\naic()\\n methods:\\n>>> \\ngm\\n.\\nbic\\n(\\nX\\n)\\n8189.747000497186\\n>>> \\ngm\\n.\\naic\\n(\\nX\\n)\\n8102.521720382148\\n(1)\\n(\\nm\\n)'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 453, 'page_label': '454', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-20\\n shows the BIC for different numbers of clusters \\nk\\n. As you can\\nsee, both the BIC and the AIC are lowest when \\nk\\n=3, so it is most likely the\\nbest choice.\\nFigure 9-20. \\nAIC and BIC for different numbers of clusters \\nk'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 454, 'page_label': '455', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Bayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, you can\\nuse the \\nBayesianGaussianMixture\\n class, which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. \\nSet the number of clusters\\nn_components\\n to a value that you have good reason to believe is greater than\\nthe optimal number of clusters (this assumes some minimal knowledge about\\nthe problem at hand), and the algorithm will eliminate the unnecessary\\nclusters automatically. For example, let’s set the number of clusters to 10 and\\nsee what happens:\\n>>> \\nfrom\\n \\nsklearn.mixture\\n \\nimport\\n \\nBayesianGaussianMixture\\n>>> \\nbgm\\n \\n=\\n \\nBayesianGaussianMixture\\n(\\nn_components\\n=\\n10\\n,\\n \\nn_init\\n=\\n10\\n,\\n \\nrandom_state\\n=\\n42\\n)\\n>>> \\nbgm\\n.\\nfit\\n(\\nX\\n)\\n>>> \\nbgm\\n.\\nweights_\\n.\\nround\\n(\\n2\\n)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only three clusters are\\nneeded, and the resulting clusters are almost identical to the ones in \\nFigure 9-\\n16\\n.\\nA final note about Gaussian mixture models: although they work great on\\nclusters with ellipsoidal shapes, they don’t do so well with clusters of very\\ndifferent shapes. For example, let’s see what happens if we use a Bayesian\\nGaussian mixture model to cluster the moons dataset (see \\nFigure 9-21\\n).\\nOops! The algorithm desperately searched for ellipsoids, so it found eight\\ndifferent clusters instead of two. The density estimation is not too bad, so this\\nmodel could perhaps be used for anomaly detection, but it failed to identify\\nthe two moons. To conclude this chapter, let’s take a quick look at a few\\nalgorithms capable of dealing with arbitrarily shaped clusters.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 455, 'page_label': '456', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 9-21. \\nFitting a Gaussian mixture to nonellipsoidal clusters'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 456, 'page_label': '457', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Other Algorithms for Anomaly and Novelty Detection\\nScikit-Learn implements other algorithms dedicated to anomaly detection or\\nnovelty detection:\\nFast-MCD (minimum covariance determinant)\\nImplemented by the \\nEllipticEnvelope\\n class, this algorithm is useful for\\noutlier detection, in particular to clean up a dataset. \\nIt assumes that the\\nnormal instances (inliers) are generated from a single Gaussian\\ndistribution (not a mixture). It also assumes that the dataset is\\ncontaminated with outliers that were not generated from this Gaussian\\ndistribution. When the algorithm estimates the parameters of the Gaussian\\ndistribution (i.e., the shape of the elliptic envelope around the inliers), it is\\ncareful to ignore the instances that are most likely outliers. This technique\\ngives a better estimation of the elliptic envelope and thus makes the\\nalgorithm better at identifying the outliers.\\nIsolation forest\\nThis is an efficient algorithm for outlier detection, especially in high-\\ndimensional datasets. \\nThe algorithm builds a random forest in which each\\ndecision tree is grown randomly: at each node, it picks a feature\\nrandomly, then it picks a random threshold value (between the min and\\nmax values) to split the dataset in two. The dataset gradually gets\\nchopped into pieces this way, until all instances end up isolated from the\\nother instances. Anomalies are usually far from other instances, so on\\naverage (across all the decision trees) they tend to get isolated in fewer\\nsteps than normal instances.\\nLocal outlier factor (LOF)\\nThis algorithm is also good for outlier detection. \\nIt compares the density\\nof instances around a given instance to the density around its neighbors.\\nAn anomaly is often more isolated than its \\nk\\n-nearest neighbors.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 457, 'page_label': '458', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='One-class SVM\\nThis algorithm is better suited for novelty detection. \\nRecall that a\\nkernelized SVM classifier separates two classes by first (implicitly)\\nmapping all the instances to a high-dimensional space, then separating the\\ntwo classes using a linear SVM classifier within this high-dimensional\\nspace (see \\nChapter 5\\n). Since we just have one class of instances, the one-\\nclass SVM algorithm instead tries to separate the instances in high-\\ndimensional space from the origin. In the original space, this will\\ncorrespond to finding a small region that encompasses all the instances. If\\na new instance does not fall within this region, it is an anomaly. There are\\na few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel when it is in fact normal. It\\nworks great, especially with high-dimensional datasets, but like all SVMs\\nit does not scale to large \\ndatasets\\n.\\nPCA and other dimensionality reduction techniques with an\\ninverse_transform()\\n method\\nIf you\\n compare the reconstruction error of a normal instance with the\\nreconstruction error of an anomaly, the latter will usually be much larger.\\nThis is a simple and often quite efficient anomaly detection approach (see\\nthis chapter’s exercises for an example).'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 458, 'page_label': '459', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n1\\n. \\nHow would you define clustering? Can you name a few clustering\\nalgorithms?\\n2\\n. \\nWhat are some of the main applications of clustering algorithms?\\n3\\n. \\nDescribe two techniques to select the right number of clusters when\\nusing \\nk\\n-means\\n.\\n4\\n. \\nWhat is label propagation? Why would you implement it, and how?\\n5\\n. \\nCan you name two clustering algorithms that can scale to large datasets?\\nAnd two that look for regions of high density?\\n6\\n. \\nCan you think of a use case where active learning would be useful? How\\nwould you implement it?\\n7\\n. \\nWhat is the difference between anomaly detection and novelty\\ndetection?\\n8\\n. \\nWhat is a Gaussian mixture? What tasks can you use it for?\\n9\\n. \\nCan you name two techniques to find the right number of clusters when\\nusing a Gaussian mixture model?\\n10\\n. \\nThe classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel\\nimages of faces. Each image is flattened to a 1D vector of size 4,096.\\nForty different people were photographed (10 times each), and the usual\\ntask is to train a model that can predict which person is represented in\\neach picture. Load the dataset using the\\nsklearn.datasets.fetch_olivetti_faces()\\n function, then split it into a\\ntraining set, a validation set, and a test set (note that the dataset is\\nalready scaled between 0 and 1). Since the dataset is quite small, you\\nwill probably want to use stratified sampling to ensure that there are the\\nsame number of images per person in each set. Next, cluster the images\\nusing \\nk\\n-means, and ensure that you have a good number of clusters'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 459, 'page_label': '460', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='(using one of the techniques discussed in this chapter). Visualize the\\nclusters: do you see similar faces in each cluster?\\n11\\n. \\nContinuing with the Olivetti faces dataset, train a classifier to predict\\nwhich person is represented in each picture, and evaluate it on the\\nvalidation set. Next, use \\nk\\n-means as a dimensionality reduction tool, and\\ntrain a classifier on the reduced set. Search for the number of clusters\\nthat allows the classifier to get the best performance: what performance\\ncan you reach? What if you append the features from the reduced set to\\nthe original features (again, searching for the best number of clusters)?\\n12\\n. \\nTrain a Gaussian mixture model on the Olivetti faces dataset. To speed\\nup the algorithm, you should probably reduce the dataset’s\\ndimensionality (e.g., use PCA, preserving 99% of the variance). Use the\\nmodel to generate some new faces (using the \\nsample()\\n method), and\\nvisualize them (if you used PCA, you will need to use its\\ninverse_transform()\\n method). Try to modify some images (e.g., rotate,\\nflip, darken) and see if the model can detect the anomalies (i.e., compare\\nthe output of the \\nscore_samples()\\n method for normal images and for\\nanomalies).\\n13\\n. \\nSome dimensionality reduction techniques can also be used for anomaly\\ndetection. For example, take the Olivetti faces dataset and reduce it with\\nPCA, preserving 99% of the variance. Then compute the reconstruction\\nerror for each image. Next, take some of the modified images you built\\nin the previous exercise and look at their reconstruction error: notice\\nhow much larger it is. If you plot a reconstructed image, you will see\\nwhy: it tries to reconstruct a normal face.\\nSolutions to these exercises are available at the end of this chapter’s\\nnotebook, at \\nhttps://homl.info/colab3\\n.\\n1\\n Stuart P. Lloyd, “Least Squares Quantization in PCM”, \\nIEEE Transactions on Information\\nTheory\\n 28, no. 2 (1982): 129–137.\\n2\\n David Arthur and Sergei Vassilvitskii, “\\nk-Means++\\n: The Advantages of Careful Seeding”,\\nProceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms\\n (2007): 1027–'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 460, 'page_label': '461', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='1035.\\n3\\n Charles Elkan, “Using the Triangle Inequality to Accelerate k-Means”, \\nProceedings of the 20th\\nInternational Conference on Machine Learning\\n (2003): 147–153.\\n4\\n The triangle inequality is AC ≤ AB + BC, where A, B and C are three points and AB, AC, and\\nBC are the distances between these points.\\n5\\n David Sculley, “Web-Scale K-Means Clustering”, \\nProceedings of the 19th International\\nConference on World Wide Web\\n (2010): 1177–1178.\\n6\\n Phi (\\nϕ\\n or \\nφ\\n) is the 21st letter of the Greek alphabet.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 461, 'page_label': '462', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Part II. \\nNeural Networks and Deep\\nLearning'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 462, 'page_label': '463', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Chapter 10. \\nIntroduction to\\nArtificial Neural \\nNetworks with\\nKeras\\nBirds inspired us to fly, burdock plants inspired Velcro, and nature has\\ninspired countless more inventions. \\nIt seems only logical, then, to look at the\\nbrain’s architecture for inspiration on how to build an intelligent machine.\\nThis is the logic that sparked \\nartificial neural networks\\n (ANNs), machine\\nlearning models inspired by the networks of biological neurons found in our\\nbrains. However, although planes were inspired by birds, they don’t have to\\nflap their wings to fly. Similarly, ANNs have gradually become quite\\ndifferent from their biological cousins. Some researchers even argue that we\\nshould drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible\\nsystems.\\n\\u2060\\nANNs are at the very core of deep learning. They are versatile, powerful, and\\nscalable, making them ideal to tackle large and highly complex machine\\nlearning tasks such as classifying billions of images (e.g., Google Images),\\npowering speech recognition services (e.g., Apple’s Siri), recommending the\\nbest videos to watch to hundreds of millions of users every day (e.g.,\\nYouTube), or learning to beat the world champion at the game of Go\\n(DeepMind’s AlphaGo).\\nThe first part of this chapter introduces artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures and leading up to\\nmultilayer perceptrons, which are heavily used today (other architectures will\\nbe explored in the next chapters). \\nIn the second part, we will look at how to\\nimplement neural networks using TensorFlow’s Keras API. This is a\\nbeautifully designed and simple high-level API for building, training,\\nevaluating, and running neural networks. But don’t be fooled by its\\n1'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 463, 'page_label': '464', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='simplicity: it is expressive and flexible enough to let you build a wide variety\\nof neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. And should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, or even use\\nTensorFlow directly, as you will see in \\nChapter 12\\n.\\nBut first, let’s go back in time to see how artificial neural networks came to\\nbe!'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 464, 'page_label': '465', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='From Biological to Artificial Neurons\\nSurprisingly, ANNs have been around for quite a while: \\nthey were first\\nintroduced back in 1943 by the neurophysiologist Warren McCulloch and the\\nmathematician Walter Pitts. In their \\nlandmark paper\\n\\u2060\\n “A Logical Calculus of\\nIdeas Immanent in Nervous Activity”, McCulloch and Pitts presented a\\nsimplified computational model of how biological neurons might work\\ntogether in animal brains to perform complex computations using\\npropositional logic\\n. \\nThis was the first artificial neural network architecture.\\nSince then many other architectures have been invented, as you will see.\\nThe early successes of ANNs led to the widespread belief that we would soon\\nbe conversing with truly intelligent machines. \\nWhen it became clear in the\\n1960s that this promise would go unfulfilled (at least for quite a while),\\nfunding flew elsewhere, and ANNs entered a long winter. In the early 1980s,\\nnew architectures were invented and better training techniques were\\ndeveloped, sparking a revival of interest in \\nconnectionism\\n, the study of neural\\nnetworks. But progress was slow, and by the 1990s other powerful machine\\nlearning techniques had been invented, such as support vector machines (see\\nChapter 5\\n). These techniques seemed to offer better results and stronger\\ntheoretical foundations than ANNs, so once again the study of neural\\nnetworks was put on hold.\\nWe are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, here are a few good reasons to\\nbelieve that this time is different and that the renewed interest in ANNs will\\nhave a much more profound impact on our lives:\\nThere is now a huge quantity of data available to train neural networks,\\nand ANNs frequently outperform other ML techniques on very large and\\ncomplex problems.\\nThe tremendous increase in computing power since the 1990s now\\nmakes it possible to train large neural networks in a reasonable amount\\nof time. This is in part due to Moore’s law (the number of components\\n2'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 465, 'page_label': '466', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='in integrated circuits has doubled about every 2 years over the last 50\\nyears), but also thanks to the gaming industry, which has stimulated the\\nproduction of powerful GPU cards by the millions. Moreover, cloud\\nplatforms have made this power accessible to everyone.\\nThe training algorithms have been improved. To be fair they are only\\nslightly different from the ones used in the 1990s, but these relatively\\nsmall tweaks have had a huge positive impact.\\nSome theoretical limitations of ANNs have turned out to be benign in\\npractice. For example, many people thought that ANN training\\nalgorithms were doomed because they were likely to get stuck in local\\noptima, but it turns out that this is not a big problem in practice,\\nespecially for larger neural networks: the local optima often perform\\nalmost as well as the global optimum.\\nANNs seem to have entered a virtuous circle of funding and progress.\\nAmazing products based on ANNs regularly make the headline news,\\nwhich pulls more and more attention and funding toward them, resulting\\nin more and more progress and even more amazing products.'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 466, 'page_label': '467', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Biological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological\\nneuron (represented in \\nFigure 10-1\\n). \\nIt is an unusual-looking cell mostly\\nfound in animal brains. It’s composed of a \\ncell body\\n containing the nucleus\\nand most of the cell’s complex components, many branching extensions\\ncalled \\ndendrites\\n, plus one very long extension called the \\naxon\\n. The axon’s\\nlength may be just a few times longer than the cell body, or up to tens of\\nthousands of times longer. Near its extremity the axon splits off into many\\nbranches called \\ntelodendria\\n, and at the tip of these branches are minuscule\\nstructures called \\nsynaptic terminals\\n (or simply \\nsynapses\\n), which are\\nconnected to the dendrites or cell bodies of other neurons.\\n\\u2060\\n \\nBiological\\nneurons produce short electrical impulses called \\naction potentials\\n (APs, or\\njust \\nsignals\\n), which travel along the axons and make the synapses release\\nchemical signals called \\nneurotransmitters\\n. \\nWhen a neuron receives a\\nsufficient amount of these neurotransmitters within a few milliseconds, it\\nfires its own electrical impulses (actually, it depends on the neurotransmitters,\\nas some of them inhibit the neuron from firing).\\n3'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 467, 'page_label': '468', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 10-1. \\nA biological neuron\\n\\u2060\\nThus, individual biological neurons seem to behave in a simple way, but\\nthey’re organized in a vast network of billions, with each neuron typically\\nconnected to thousands of other neurons. Highly complex computations can\\nbe performed by a network of fairly simple neurons, much like a complex\\nanthill can emerge from the combined efforts of simple ants. The architecture\\nof biological neural networks (BNNs)\\n\\u2060\\n is the subject of active research, but\\nsome parts of the brain have been mapped. These efforts show that neurons\\nare often organized in consecutive\\n layers, especially in the cerebral cortex\\n(the outer layer of the brain), as shown in \\nFigure 10-2\\n.\\n4\\n5\\n6'),\n",
       " Document(metadata={'producer': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creator': 'calibre 3.48.0 [https://calibre-ebook.com]', 'creationdate': '2022-12-12T16:42:16+00:00', 'author': 'Aurélien Géron', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'total_pages': 1351, 'page': 468, 'page_label': '469', 'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'file_type': 'pdf'}, page_content='Figure 10-2. \\nMultiple layers in a biological neural network (human cortex)\\n\\u2060\\n6'),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed3766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    split_docs = []\n",
    "    for doc in documents:\n",
    "        splits = text_splitter.split_text(doc.page_content)\n",
    "        for i, split in enumerate(splits):\n",
    "            new_doc = doc.copy()\n",
    "            new_doc.page_content = split\n",
    "            new_doc.metadata['chunk_index'] = i\n",
    "            split_docs.append(new_doc)\n",
    "    \n",
    "    print(f\"Total chunks created: {len(split_docs)}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b6abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPAK\\AppData\\Local\\Temp\\ipykernel_11460\\505711966.py:14: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  new_doc = doc.copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3299\n"
     ]
    }
   ],
   "source": [
    "chunks=text_split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36d09890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'macOS Version 14.3 (Build 23D56) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240712201555Z00'00'\", 'title': 'Daily Dose of Data Science Full Archive', 'moddate': \"D:20240712201555Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'total_pages': 531, 'page': 0, 'page_label': '1', 'source_file': 'Daily_Dose_Of_Data_Science_Full_Archive.pdf', 'file_type': 'pdf', 'chunk_index': 0}, page_content='DATASCIENCE\\nFREE2024 EDITION\\nFULL ARCHIVE\\n Avi ChawlaDailyDoseofDS.comDaily Dose ofData Science')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae2b9c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x16d7e2a30b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df3d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x16d50734e00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "259303f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 3299 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9efcfb2f84c4cf3b5413633a63e7c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (3299, 384)\n",
      "Adding 3299 documents to vector store...\n",
      "Successfully added 3299 documents to vector store\n",
      "Total documents in collection: 3299\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110d431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever Pipeline From VectorStore\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c28eb350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x16d5a360fe0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d1c09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'loss found in neural networks'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd8d713c0d54b58bf14923cb4315dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_a9d1daf3_1608',\n",
       "  'content': 'e\\n. \\nWhat happens when neural nets are large enough. Set the number\\nof neurons to eight, and train the network several times. Notice that\\nit is now consistently fast and never gets stuck. This highlights an\\nimportant finding in neural network theory: large neural networks\\nrarely get stuck in local minima, and even when they do these local\\noptima are often almost as good as the global optimum. However,\\nthey can still get stuck on long plateaus for a long time.\\nf\\n. \\nThe risk of vanishing gradients in deep networks. Select the spiral\\ndataset (the bottom-right dataset under “DATA”), and change the\\nnetwork architecture to have four hidden layers with eight neurons\\neach. Notice that training takes much longer and often gets stuck on\\nplateaus for long periods of time. Also notice that the neurons in\\nthe highest layers (on the right) tend to evolve faster than the\\nneurons in the lowest layers (on the left). This problem, called the\\nvanishing gradients',\n",
       "  'metadata': {'author': 'Aurélien Géron',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'page': 543,\n",
       "   'chunk_index': 2,\n",
       "   'total_pages': 1351,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'page_label': '544',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 957,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'doc_index': 1608,\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00'},\n",
       "  'similarity_score': 0.14810431003570557,\n",
       "  'distance': 0.8518956899642944,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_10b71473_1623',\n",
       "  'content': 'The Vanishing/Exploding Gradients Problems\\nAs discussed in \\nChapter 10\\n, the backpropagation algorithm’s second phase\\nworks by going from the output layer to the input layer, propagating the error\\ngradient along the way. \\nOnce the algorithm has computed the gradient of the\\ncost function with regard to each parameter in the network, it uses these\\ngradients to update each parameter with a gradient descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm\\nprogresses down to the lower layers. As a result, the gradient descent update\\nleaves the lower layers’ connection weights virtually unchanged, and training\\nnever converges to a good solution. This is called the \\nvanishing gradients\\nproblem.\\n In some cases, the opposite can happen: the gradients can grow\\nbigger and bigger until layers get insanely large weight updates and the\\nalgorithm diverges. \\nThis is the \\nexploding gradients\\n problem, which surfaces\\nmost often in recurrent neural networks (see \\nChapter 15',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'author': 'Aurélien Géron',\n",
       "   'content_length': 998,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'page_label': '550',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'chunk_index': 2,\n",
       "   'total_pages': 1351,\n",
       "   'doc_index': 1623,\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'page': 549,\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow'},\n",
       "  'similarity_score': 0.10673892498016357,\n",
       "  'distance': 0.8932610750198364,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_9eaa6d8b_1853',\n",
       "  'content': 'output layer and returns its output.\\nBoth the total loss and the reconstruction loss will go down during training:\\nEpoch 1/5\\n363/363 [========] - 1s 820us/step - loss: 0.7640 - reconstruction_error: 1.2728\\nEpoch 2/5\\n363/363 [========] - 0s 809us/step - loss: 0.4584 - reconstruction_error: 0.6340\\n[...]\\nIn most cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex\\narchitectures, losses, and metrics. However, for some architectures, such as\\nGANs (see \\nChapter 17\\n), you will have to customize the training loop itself.\\nBefore we get there, we must look at how to compute gradients automatically\\nin TensorFlow.',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'content_length': 683,\n",
       "   'total_pages': 1351,\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'page': 656,\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'chunk_index': 0,\n",
       "   'author': 'Aurélien Géron',\n",
       "   'doc_index': 1853,\n",
       "   'page_label': '657',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow'},\n",
       "  'similarity_score': 0.08968830108642578,\n",
       "  'distance': 0.9103116989135742,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_06a0c437_1850',\n",
       "  'content': 'Z\\n \\n=\\n \\ninputs\\n        \\nfor\\n \\nlayer\\n \\nin\\n \\nself\\n.\\nhidden\\n:\\n            \\nZ\\n \\n=\\n \\nlayer\\n(\\nZ\\n)\\n        \\nreconstruction\\n \\n=\\n \\nself\\n.\\nreconstruct\\n(\\nZ\\n)\\n        \\nrecon_loss\\n \\n=\\n \\ntf\\n.\\nreduce_mean\\n(\\ntf\\n.\\nsquare\\n(\\nreconstruction\\n \\n-\\n \\ninputs\\n))\\n        \\nself\\n.\\nadd_loss\\n(\\n0.05\\n \\n*\\n \\nrecon_loss\\n)\\n        \\nif\\n \\ntraining\\n:\\n            \\nresult\\n \\n=\\n \\nself\\n.\\nreconstruction_mean\\n(\\nrecon_loss\\n)\\n            \\nself\\n.\\nadd_metric\\n(\\nresult\\n)\\n        \\nreturn\\n \\nself\\n.\\nout\\n(\\nZ\\n)\\nLet’s go through this code:\\nThe constructor creates the DNN with five dense hidden layers and one\\ndense output layer. We also create a \\nMean\\n streaming metric to keep\\ntrack of the reconstruction error during training.\\nThe \\nbuild()\\n method creates an extra dense layer that will be used to\\nreconstruct the inputs of the model. It must be created here because its\\nnumber of units must be equal to the number of inputs, and this number\\nis unknown before the \\nbuild()\\n method is called.\\n\\u2060\\nThe \\ncall()',\n",
       "  'metadata': {'total_pages': 1351,\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'chunk_index': 2,\n",
       "   'content_length': 954,\n",
       "   'page_label': '656',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'doc_index': 1850,\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'page': 655,\n",
       "   'author': 'Aurélien Géron'},\n",
       "  'similarity_score': 0.08388948440551758,\n",
       "  'distance': 0.9161105155944824,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_1e8f99df_1432',\n",
       "  'content': 'Part II. \\nNeural Networks and Deep\\nLearning',\n",
       "  'metadata': {'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'doc_index': 1432,\n",
       "   'page_label': '462',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'chunk_index': 0,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'total_pages': 1351,\n",
       "   'content_length': 43,\n",
       "   'page': 461,\n",
       "   'author': 'Aurélien Géron',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf'},\n",
       "  'similarity_score': 0.07885622978210449,\n",
       "  'distance': 0.9211437702178955,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"loss found in neural networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c330d7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG Pipeline- VectorDB To LLM Output Generation\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#rint(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1894319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "635a637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3044b19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'End-to-End Machine Learning Project'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc3d65894a84c218cb9da8bb78a412f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_f5a600d8_707',\n",
       "  'content': 'Chapter 2. \\nEnd-to-End Machine\\nLearning Project\\nIn this chapter you will work through an example project end to end,\\npretending to be a recently hired data scientist at a real estate company. This\\nexample is fictitious; the goal is to illustrate the main steps of a machine\\nlearning project, not to learn anything about the real estate business. Here are\\nthe main steps we will walk through:\\n1\\n. \\nLook at the big picture.\\n2\\n. \\nGet the data.\\n3\\n. \\nExplore and visualize the data to gain insights.\\n4\\n. \\nPrepare the data for machine learning algorithms.\\n5\\n. \\nSelect a model and train it.\\n6\\n. \\nFine-tune your model.\\n7\\n. \\nPresent your solution.\\n8\\n. \\nLaunch, monitor, and maintain your system.',\n",
       "  'metadata': {'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'chunk_index': 0,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'total_pages': 1351,\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'page_label': '80',\n",
       "   'doc_index': 707,\n",
       "   'author': 'Aurélien Géron',\n",
       "   'page': 79,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'content_length': 686},\n",
       "  'similarity_score': 0.2998082637786865,\n",
       "  'distance': 0.7001917362213135,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_3078ee18_617',\n",
       "  'content': 'will look at the workflow of a typical ML project, discuss the main\\nchallenges you may face, and cover how to evaluate and fine-tune a machine\\nlearning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every\\ndata scientist should know by heart. It will be a high-level overview (it’s the',\n",
       "  'metadata': {'total_pages': 1351,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'page_label': '28',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'doc_index': 617,\n",
       "   'page': 27,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'Aurélien Géron',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'content_length': 317,\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'chunk_index': 2,\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf'},\n",
       "  'similarity_score': 0.25436925888061523,\n",
       "  'distance': 0.7456307411193848,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_cafe4fc0_591',\n",
       "  'content': 'Roadmap\\nThis book is organized in two parts. \\nPart I, “The Fundamentals of \\nMachine\\nLearning\\n”\\n, covers the following topics:\\nWhat machine learning is, what problems it tries to solve, and the main\\ncategories and fundamental concepts of its systems\\nThe steps in a typical machine learning project\\nLearning by fitting a model to data\\nOptimizing a cost function\\nHandling, cleaning, and preparing data\\nSelecting and engineering features\\nSelecting a model and tuning hyperparameters using cross-validation\\nThe challenges of machine learning, in particular underfitting and\\noverfitting (the bias/variance trade-off)\\nThe most common learning algorithms: linear and polynomial\\nregression, logistic regression, \\nk\\n-nearest neighbors, support vector\\nmachines, decision trees, random forests, and ensemble methods\\nReducing the dimensionality of the training data to fight the “curse of\\ndimensionality”\\nOther unsupervised learning techniques, including clustering, density\\nestimation, and anomaly detection',\n",
       "  'metadata': {'chunk_index': 1,\n",
       "   'doc_index': 591,\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'page': 12,\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'author': 'Aurélien Géron',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 1351,\n",
       "   'content_length': 995,\n",
       "   'page_label': '13'},\n",
       "  'similarity_score': 0.2343648076057434,\n",
       "  'distance': 0.7656351923942566,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_845c8664_575',\n",
       "  'content': 'Hands-On Machine Learning with\\nScikit-Learn, Keras, \\nand\\nTensorFlow\\nTHIRD EDITION\\nConcepts, Tools, and Techniques to \\nBuild Intelligent\\nSystems\\nAurélien Géron',\n",
       "  'metadata': {'doc_index': 575,\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'chunk_index': 0,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'page_label': '2',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'total_pages': 1351,\n",
       "   'content_length': 158,\n",
       "   'page': 1,\n",
       "   'author': 'Aurélien Géron',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]'},\n",
       "  'similarity_score': 0.20927900075912476,\n",
       "  'distance': 0.7907209992408752,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_a15443d5_2806',\n",
       "  'content': 'Proceedings of the 34th International Conference on Machine Learning\\n (2017): 2778–2787.\\n28\\n Rui Wang et al., “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly\\nComplex and Diverse Learning Environments and Their Solutions”, arXiv preprint\\narXiv:1901.01753 (2019).',\n",
       "  'metadata': {'author': 'Aurélien Géron',\n",
       "   'content_length': 286,\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'total_pages': 1351,\n",
       "   'page': 1097,\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'chunk_index': 3,\n",
       "   'doc_index': 2806,\n",
       "   'page_label': '1098',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.19556307792663574,\n",
       "  'distance': 0.8044369220733643,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"End-to-End Machine Learning Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c444e9",
   "metadata": {},
   "source": [
    "Integration Vectordb Context pipeline With LLM output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b29afd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "731b86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'End-to-End Machine Learning Project'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6ab978df1347bcba6bc44993972416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_f5a600d8_707',\n",
       "  'content': 'Chapter 2. \\nEnd-to-End Machine\\nLearning Project\\nIn this chapter you will work through an example project end to end,\\npretending to be a recently hired data scientist at a real estate company. This\\nexample is fictitious; the goal is to illustrate the main steps of a machine\\nlearning project, not to learn anything about the real estate business. Here are\\nthe main steps we will walk through:\\n1\\n. \\nLook at the big picture.\\n2\\n. \\nGet the data.\\n3\\n. \\nExplore and visualize the data to gain insights.\\n4\\n. \\nPrepare the data for machine learning algorithms.\\n5\\n. \\nSelect a model and train it.\\n6\\n. \\nFine-tune your model.\\n7\\n. \\nPresent your solution.\\n8\\n. \\nLaunch, monitor, and maintain your system.',\n",
       "  'metadata': {'content_length': 686,\n",
       "   'chunk_index': 0,\n",
       "   'page': 79,\n",
       "   'total_pages': 1351,\n",
       "   'page_label': '80',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'doc_index': 707,\n",
       "   'author': 'Aurélien Géron',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.2998082637786865,\n",
       "  'distance': 0.7001917362213135,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_3078ee18_617',\n",
       "  'content': 'will look at the workflow of a typical ML project, discuss the main\\nchallenges you may face, and cover how to evaluate and fine-tune a machine\\nlearning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every\\ndata scientist should know by heart. It will be a high-level overview (it’s the',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'total_pages': 1351,\n",
       "   'page': 27,\n",
       "   'author': 'Aurélien Géron',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'content_length': 317,\n",
       "   'doc_index': 617,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'page_label': '28',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'chunk_index': 2,\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf'},\n",
       "  'similarity_score': 0.25436925888061523,\n",
       "  'distance': 0.7456307411193848,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_cafe4fc0_591',\n",
       "  'content': 'Roadmap\\nThis book is organized in two parts. \\nPart I, “The Fundamentals of \\nMachine\\nLearning\\n”\\n, covers the following topics:\\nWhat machine learning is, what problems it tries to solve, and the main\\ncategories and fundamental concepts of its systems\\nThe steps in a typical machine learning project\\nLearning by fitting a model to data\\nOptimizing a cost function\\nHandling, cleaning, and preparing data\\nSelecting and engineering features\\nSelecting a model and tuning hyperparameters using cross-validation\\nThe challenges of machine learning, in particular underfitting and\\noverfitting (the bias/variance trade-off)\\nThe most common learning algorithms: linear and polynomial\\nregression, logistic regression, \\nk\\n-nearest neighbors, support vector\\nmachines, decision trees, random forests, and ensemble methods\\nReducing the dimensionality of the training data to fight the “curse of\\ndimensionality”\\nOther unsupervised learning techniques, including clustering, density\\nestimation, and anomaly detection',\n",
       "  'metadata': {'doc_index': 591,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'content_length': 995,\n",
       "   'total_pages': 1351,\n",
       "   'page_label': '13',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'author': 'Aurélien Géron',\n",
       "   'page': 12,\n",
       "   'chunk_index': 1},\n",
       "  'similarity_score': 0.2343648076057434,\n",
       "  'distance': 0.7656351923942566,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_845c8664_575',\n",
       "  'content': 'Hands-On Machine Learning with\\nScikit-Learn, Keras, \\nand\\nTensorFlow\\nTHIRD EDITION\\nConcepts, Tools, and Techniques to \\nBuild Intelligent\\nSystems\\nAurélien Géron',\n",
       "  'metadata': {'page': 1,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'content_length': 158,\n",
       "   'chunk_index': 0,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'doc_index': 575,\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'total_pages': 1351,\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'page_label': '2',\n",
       "   'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'Aurélien Géron',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00'},\n",
       "  'similarity_score': 0.20927900075912476,\n",
       "  'distance': 0.7907209992408752,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_a15443d5_2806',\n",
       "  'content': 'Proceedings of the 34th International Conference on Machine Learning\\n (2017): 2778–2787.\\n28\\n Rui Wang et al., “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly\\nComplex and Diverse Learning Environments and Their Solutions”, arXiv preprint\\narXiv:1901.01753 (2019).',\n",
       "  'metadata': {'source_file': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'author': 'Aurélien Géron',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 1097,\n",
       "   'producer': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'creationdate': '2022-12-12T16:42:16+00:00',\n",
       "   'content_length': 286,\n",
       "   'total_pages': 1351,\n",
       "   'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow',\n",
       "   'creator': 'calibre 3.48.0 [https://calibre-ebook.com]',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf',\n",
       "   'chunk_index': 3,\n",
       "   'page_label': '1098',\n",
       "   'doc_index': 2806},\n",
       "  'similarity_score': 0.19556307792663574,\n",
       "  'distance': 0.8044369220733643,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"End-to-End Machine Learning Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5d18e",
   "metadata": {},
   "source": [
    "Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ca7bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'End-to-End Machine Learning Project'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda73aa684ba4ac1bc6afbad731e4d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Answer: This chapter outlines the steps involved in a typical machine learning project, using a fictional real estate example.  \n",
      "\n",
      "The steps include:\n",
      "\n",
      "1. **Understanding the problem**\n",
      "2. **Gathering data**\n",
      "3. **Exploring and visualizing data**\n",
      "4. **Preparing data for algorithms**\n",
      "5. **Selecting and training a model**\n",
      "6. **Fine-tuning the model**\n",
      "7. **Presenting the solution**\n",
      "8. **Launching, monitoring, and maintaining the system** \n",
      "\n",
      "\n",
      "The chapter aims to provide a high-level overview of these steps and introduce fundamental machine learning concepts. \n",
      "\n",
      "Sources: [{'source': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'page': 79, 'score': 0.2998082637786865, 'preview': 'Chapter 2. \\nEnd-to-End Machine\\nLearning Project\\nIn this chapter you will work through an example project end to end,\\npretending to be a recently hired data scientist at a real estate company. This\\nexample is fictitious; the goal is to illustrate the main steps of a machine\\nlearning project, not to l...'}, {'source': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'page': 27, 'score': 0.25436925888061523, 'preview': 'will look at the workflow of a typical ML project, discuss the main\\nchallenges you may face, and cover how to evaluate and fine-tune a machine\\nlearning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every\\ndata scientist should know by heart. It will be a high-level o...'}, {'source': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'page': 12, 'score': 0.2343648076057434, 'preview': 'Roadmap\\nThis book is organized in two parts. \\nPart I, “The Fundamentals of \\nMachine\\nLearning\\n”\\n, covers the following topics:\\nWhat machine learning is, what problems it tries to solve, and the main\\ncategories and fundamental concepts of its systems\\nThe steps in a typical machine learning project\\nLea...'}]\n",
      "Confidence: 0.2998082637786865\n",
      "Context Preview: Chapter 2. \n",
      "End-to-End Machine\n",
      "Learning Project\n",
      "In this chapter you will work through an example project end to end,\n",
      "pretending to be a recently hired data scientist at a real estate company. This\n",
      "example is fictitious; the goal is to illustrate the main steps of a machine\n",
      "learning project, not to l\n"
     ]
    }
   ],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"End-to-End Machine Learning Project\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d43e929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Look at the big picture of machine learning projects'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e360f5cd0b41f1a1bd8827743aa161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "will look at the workflow of a typical ML project, discuss the main\n",
      "challenges you may face, and cover how to evaluate and fine-tune a machine\n",
      "learning system.\n",
      "This chapter introduces a lot of fundamental concepts (and jargon) that every\n",
      "data scientist should know by heart. It will be a high-level overview (it’s the\n",
      "\n",
      "Part I. \n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "\n",
      "or better-quality training data, or perhaps select a more powerful model (e.g.,\n",
      "a polynomial regression model).\n",
      "In summary:\n",
      "You studied the data.\n",
      "You selected a model.\n",
      "You trained it on the training data (i.e., the learning algorithm searched\n",
      "for the model parameter values that minimize a cost function).\n",
      "Finally, you applied the model to make predictions on new cases (this is\n",
      "called \n",
      "inference\n",
      "), \n",
      "hoping that this model will generalize well.\n",
      "This is what a typical machine learning project looks like. In \n",
      "Chapter 2\n",
      " you\n",
      "will experience this firsthand by going through a project end to end.\n",
      "We have covered a lot of ground so far: you now know what machine\n",
      "learning is really about, why it is useful, what some of the most common\n",
      "\n",
      "Question: Look at the big picture of machine learning projects\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: A typical machine learning project involves studying the data, selecting a suitable model, training the model on training data to minimize errors, and finally applying the trained model to make predictions on new data (inference).  The goal is for the model to generalize well to unseen data. \n",
      "\n",
      "\n",
      "Citations:\n",
      "[1] Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf (page 27)\n",
      "[2] Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf (page 26)\n",
      "[3] Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf (page 58)\n",
      "Summary: Machine learning projects involve analyzing data, choosing an appropriate model, and training it to make accurate predictions.  The trained model is then used to make inferences on new data, aiming for broad applicability and generalization. \n",
      "\n",
      "\n",
      "\n",
      "History: {'question': 'Look at the big picture of machine learning projects', 'answer': 'A typical machine learning project involves studying the data, selecting a suitable model, training the model on training data to minimize errors, and finally applying the trained model to make predictions on new data (inference).  The goal is for the model to generalize well to unseen data. \\n', 'sources': [{'source': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'page': 27, 'score': 0.20049476623535156, 'preview': 'will look at the workflow of a typical ML project, discuss the main\\nchallenges you may face, and cover how to evaluate a...'}, {'source': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'page': 26, 'score': 0.17093080282211304, 'preview': 'Part I. \\nThe Fundamentals of\\nMachine Learning...'}, {'source': 'Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow.pdf', 'page': 58, 'score': 0.1558316946029663, 'preview': 'or better-quality training data, or perhaps select a more powerful model (e.g.,\\na polynomial regression model).\\nIn summa...'}], 'summary': 'Machine learning projects involve analyzing data, choosing an appropriate model, and training it to make accurate predictions.  The trained model is then used to make inferences on new data, aiming for broad applicability and generalization. \\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"Look at the big picture of machine learning projects\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e348a8",
   "metadata": {},
   "source": [
    "Chat with streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f06bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 20:52:43.176 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.176 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.549 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\DEEPAK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-15 20:52:43.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.561 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.572 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.572 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.572 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-15 20:52:43.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 20:52:43.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# --- Your Advanced RAG Pipeline Class ---\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 5,\n",
    "        min_score: float = 0.2,\n",
    "        stream: bool = False,\n",
    "        summarize: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "\n",
    "            # Build prompt\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            # Stream output simulation\n",
    "            if stream:\n",
    "                answer = \"\"\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    chunk = prompt[i:i+80]\n",
    "                    answer += chunk\n",
    "                    yield chunk  # stream chunk to UI\n",
    "                    time.sleep(0.02)\n",
    "            else:\n",
    "                response = self.llm.invoke([prompt])\n",
    "                answer = response.content\n",
    "\n",
    "        # Add citations\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Summarize\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Save history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "\n",
    "# --- Streamlit UI ---\n",
    "st.set_page_config(page_title=\"📄PDF Loader Chatbot using RAG\", layout=\"wide\")\n",
    "st.title(\"PDF Loader Chatbot using RAG\")\n",
    "\n",
    "# Sidebar settings\n",
    "st.sidebar.header(\"⚙️ Settings\")\n",
    "top_k = st.sidebar.slider(\"Top K documents\", 1, 10, 5)\n",
    "min_score = st.sidebar.slider(\"Minimum similarity score\", 0.0, 1.0, 0.2)\n",
    "stream = st.sidebar.checkbox(\"Stream Answer\", value=False)\n",
    "summarize = st.sidebar.checkbox(\"Summarize Answer\", value=False)\n",
    "\n",
    "# Inputs\n",
    "question = st.text_input(\"Enter your question:\")\n",
    "\n",
    "if \"rag_pipeline\" not in st.session_state:\n",
    "    # TODO: Replace with your retriever & llm\n",
    "    st.session_state.rag_pipeline = AdvancedRAGPipeline(retriever=None, llm=None)\n",
    "\n",
    "rag_pipeline = st.session_state.rag_pipeline\n",
    "\n",
    "if st.button(\"Ask\"):\n",
    "    if not question.strip():\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        with st.spinner(\"Retrieving and generating answer...\"):\n",
    "            result = rag_pipeline.query(\n",
    "                question,\n",
    "                top_k=top_k,\n",
    "                min_score=min_score,\n",
    "                stream=stream,\n",
    "                summarize=summarize\n",
    "            )\n",
    "\n",
    "        st.subheader(\"💡 Answer\")\n",
    "        st.write(result['answer'])\n",
    "\n",
    "        if result['summary']:\n",
    "            st.subheader(\"📝 Summary\")\n",
    "            st.info(result['summary'])\n",
    "\n",
    "        if result['sources']:\n",
    "            st.subheader(\"📚 Sources\")\n",
    "            for i, src in enumerate(result['sources'], 1):\n",
    "                st.write(f\"**[{i}]** {src['source']} (page {src['page']}, score {src['score']:.2f})\")\n",
    "                st.caption(src['preview'])\n",
    "\n",
    "        st.subheader(\"📜 Conversation History\")\n",
    "        for hist in result['history']:\n",
    "            st.markdown(f\"**Q:** {hist['question']}\")\n",
    "            st.markdown(f\"**A:** {hist['answer']}\")\n",
    "            if hist['summary']:\n",
    "                st.markdown(f\"**Summary:** {hist['summary']}\")\n",
    "            st.write(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5866564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
